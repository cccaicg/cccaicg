{"docstring": "'''find_type(name):\n    name = type_name\n   returns a Type from c_types or None if the type was not found'''\n", "func_signal": "def find_type(self, name):\n", "code": "if self.table.has_key(name): return self.table[name]\nelse: return None", "path": "tables.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "#print token.lexer.lineno, len(token.value.split('\\n')), token.value.split('\\n')\n", "func_signal": "def t_COMMENT(self, token):\n", "code": "lines = len(token.value.split('\\n')) - 1\nif lines < 0: lines = 0\ntoken.lexer.lineno += lines", "path": "scan.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''init_declarator : declarator\n                   | declarator '=' initializer'''\n", "func_signal": "def p_init_declarator(self, p):\n", "code": "p[0] = Node(p, 'init_declarator')\ni = -1\nwhile p[i].__class__ != Node: i -= 1\ntype = p[i].attrs.type\n#print p[i]\nif len(p) == 4: value = p[3].attrs.value\nelse: value = None\nidentifier = p[1].attrs.identifier\nidentifier.type = type\nidentifier.value = value\np[0].attrs.identifier = identifier", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''declaration_specifiers : storage_class_specifier\n                        | storage_class_specifier declaration_specifiers\n                        | type_specifier\n                        | type_specifier declaration_specifiers'''\n", "func_signal": "def p_declaration_specifiers(self, p):\n", "code": "if p[1].symbol.symbol == 'type_specifier':\n    if len(p) == 3: \n        type_name = p[1].attrs.type.type_name + ' ' + p[2].attrs.type.type_name\n    else:\n        type_name = p[1].attrs.type.type_name\nelse: type_name = None\np[0] = Node(p, 'declaration_specifiers')\np[0].attrs.type = self.typedef_table.find_type(type_name)", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''type_specifier : CHAR\n                  | SHORT\n                  | INT\n                  | LONG\n                  | SIGNED\n                  | UNSIGNED\n                  | FLOAT\n                  | DOUBLE\n                  | CONST\n                  | VOLATILE\n                  | VOID\n                  | struct_or_union_specifier\n                  | enum_specifier\n                  | TYPE_NAME'''\n", "func_signal": "def p_type_specifier(self, p):\n", "code": "type = self.typedef_table.find_type(p[1])\nif type: p[1] = type\np[0] = Node(p, 'type_specifier')\np[0].attrs.type = type\nfor child in p[0].children: child.attrs.type = type", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "#if not reserved.has_key(token.value):\n    #if not self.symbol_table.find_symbol(token.value):\n        #self.symbol_table.create_symbol(c_types.Identifier(token.value))\n", "func_signal": "def t_IDENTIFIER(self, token):\n", "code": "token.type = reserved.get(token.value,'IDENTIFIER')\nreturn token", "path": "scan.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''Identifier(name, value=None, type=None, address=None)\n    value = this needs to be a constant\n    type = the type of the constant should be a ScalarType'''\n", "func_signal": "def __init__(self, value, type=None):\n", "code": "self.value = value\nself.type = type\nassert type and type.__class__ ==  ScalarType", "path": "c_types.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''expression_statement : ';'\n                        | expr ';' '''\n", "func_signal": "def p_expr_statement(self, p):\n", "code": "p[0] = Node(p, 'expression_statement')\np[0].attrs.code = []\nif p[1].__class__ == Node and p[1].symbol.symbol == 'expr':\n    p[0].attrs.code += p[1].attrs.code", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''assignment_operator : '='\n                    | MUL_ASSIGN\n                    | DIV_ASSIGN\n                    | MOD_ASSIGN\n                    | ADD_ASSIGN\n                    | SUB_ASSIGN\n                    | LEFT_ASSIGN\n                    | RIGHT_ASSIGN\n                    | AND_ASSIGN\n                    | XOR_ASSIGN\n                    | OR_ASSIGN'''\n", "func_signal": "def p_assignment_operator(self, p):\n", "code": "p[0] = Node(p, 'assignment_operator')\np[0].attrs.operator = p[1]", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''declarator2 : identifier\n            | '(' declarator ')'\n            | declarator2 '[' ']'\n            | declarator2 '[' constant_expr ']'\n            | declarator2 '(' ')'\n            | declarator2 '(' parameter_type_list ')'\n            | declarator2 '(' parameter_identifier_list ')'  '''\n", "func_signal": "def p_declarator2(self, p):\n", "code": "if p[1].__class__ == Node:\n    if p[1].symbol.symbol == 'identifier':\n        identifier = c_types.Identifier(p[1].attrs.identifier)\n        self.symbol_table.create_symbol(identifier)\n    else:\n        identifier = p[1].attrs.identifier\nelse:\n    identifier = p[2].attrs.identifier\np[0] = Node(p, 'declarator2')\np[0].attrs.identifier = identifier", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''unary_operator : '&'\n                | '*'\n                | '+'\n                | '-'\n                | '~'\n                | '!' '''\n", "func_signal": "def p_unary_operator(self, p):\n", "code": "p[0] = Node(p, 'unary_operator')\np[0].attrs.operator = p[1]", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''right_bracket : '}' '''\n", "func_signal": "def p_right_bracket(self, p):\n", "code": "p[0] = Node(p, 'right_bracket')\nself.symbol_table.pop_level()", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''left_bracket : '{' '''\n", "func_signal": "def p_left_bracket(self, p):\n", "code": "p[0] = Node(p, 'left_bracket')\n#print self.symbol_table\nself.symbol_table.push_level()", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''FunctionType(name, return_type, parameters=None, code=None)'''\n", "func_signal": "def __init__(self, name, return_type, parameters=None, code=None):\n", "code": "self.name = name\nself.return_type = return_type\nself.parameters = parameters\nself.code = code", "path": "c_types.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''UnionType(type_name, members):\n    type_name = name of the union\n    members = a list of Identifiers'''\n", "func_signal": "def __init__(self, type_name, members):\n", "code": "self.members = members\nlongest = 0\nfor member in members:\n    if longest < member.type.size: longest = member.type.size\nsuper(UnionType, self).__init__(type_name, longest)", "path": "c_types.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''Build the lexer'''\n", "func_signal": "def build(self,**kwargs):\n", "code": "self.lexer = lex.lex(object=self, **kwargs)\ndef h(self, f, *args, **kwargs):\n    def token(*args, **kwargs):\n        '''A decorator on the original token function'''\n        t = f()\n        self.token_stack.append(self.next_token)\n        self.next_token = t\n        return t\n    return token\nself.lexer.token = h(self, self.lexer.token)", "path": "scan.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''primary_expr : identifier\n                | CONSTANT\n                | STRING_LITERAL\n                | '(' expr ')' '''\n", "func_signal": "def p_primary_expr(self, p):\n", "code": "p[0] = Node(p, 'primary_expr')\np[0].attrs.code = []\nif p[1].__class__ == Node and p[1].symbol.symbol == 'identifier':\n    p[0].attrs.identifier = self.symbol_table.find_symbol(p[1].attrs.identifier)\nif p[1].__class__ == c_types.Constant:\n    temp, statements = self.irgen.load_constant(p[1])\n    p[0].attrs.code += statements\n    p[0].attrs.identifier = temp", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''StructType(type_name, members):\n    type_name = name of the union\n    members = a list of Identifiers'''\n", "func_signal": "def __init__(self, type_name, members):\n", "code": "super(StructType, self).__init__(type_name, members)\nself.size = len(self.members) * self.size", "path": "c_types.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''function_definition : declarator function_body\n                       | declaration_specifiers declarator function_body'''\n", "func_signal": "def p_function_definition(self, p):\n", "code": "if p[1].__class__ == Node and p[1].symbol.symbol == 'declaration_specifiers':\n    return_type = p[1].attrs.type\n    identifier = p[2].attrs.identifier\nelse:\n    return_type = self.typedef_table.find_type('int')\n    identifier = p[1].attrs.identifier\nidentifier.type = c_types.FunctionType(identifier.name, return_type)\np[0] = Node(p, 'function_definition')\np[0].attrs.identifier = identifier", "path": "gram.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "'''VectorType(base_type, length):\n    base_type = any other kind of type including ScalarType, VectorType, UnionType, StructType\n    length = how long the vector should be'''\n", "func_signal": "def __init__(self, base_type, length):\n", "code": "super(VectorType, self).__init__(base_type.type_name, base_type.size)\nself.length = length\nself.size = self.length * self.size", "path": "c_types.py", "repo_name": "timtadh/pyflow", "stars": 6, "license": "None", "language": "python", "size": 1752}
{"docstring": "\"\"\"stops playback\"\"\"\n", "func_signal": "def stop(self):\n", "code": "self.set_state(State.STOPPED)\nself.mpd.stop()", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"moves plentry up\"\"\"\n", "func_signal": "def up(self, plid):\n", "code": "idx = self.get_plid_index(plid)\nif idx != None:\n    if idx > 0:\n        swapid = self.playlist[idx - 1].id\n        self.mpd.swapid(plid,swapid)\n        self.check_state(0.0)", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"\nreturns an array option starting at 0\nexample: filters0, filters1...\n\"\"\"\n", "func_signal": "def get_array(self,section,option):\n", "code": "ret = []\ni = 0\nwhile 1:\n    tmp = self.get(section,option+str(i),None)\n    if tmp == None:\n        break;\n    ret.append(tmp)\n    i += 1\nreturn ret;", "path": "pythm\\config.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"sets new random\"\"\"\n", "func_signal": "def set_random(self, rand):\n", "code": "if(rand):\n    self.mpd.random(1)\nelse:\n    self.mpd.random(0)", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "#id, artist, title, album, ('>' when playing)\n", "func_signal": "def __init__(self):\n", "code": "self.model = gtk.ListStore(object,str,str,str,str)\nPage.__init__(self)\nself.cfg.get_backend().connect(Signals.PL_CHANGED,self.load_list)\nself.cfg.get_backend().connect(Signals.SONG_CHANGED,self.song_changed)\n\nself.btn_up = ImageButton(gtk.STOCK_GO_UP)\nself.btnbox.add(self.btn_up)\nself.btn_down = ImageButton(gtk.STOCK_GO_DOWN)\nself.btnbox.add(self.btn_down)\nself.btn_play = ImageButton(gtk.STOCK_MEDIA_PLAY)\nself.btnbox.add(self.btn_play)\n#self.btn_del = ImageButton(gtk.STOCK_REMOVE)\n#self.btnbox.add(self.btn_del)\nself.btn_clear = ImageButton(gtk.STOCK_CLEAR)\nself.btnbox.add(self.btn_clear)\nself.btn_up.connect(\"clicked\",self.clicked_up)\nself.btn_down.connect(\"clicked\",self.clicked_down)\nself.btn_play.connect(\"clicked\",self.clicked_play)\n#self.btn_del.connect(\"clicked\",self.clicked_del)\nself.btn_clear.connect(\"clicked\",self.clicked_clear)", "path": "pythm\\gtkgui\\pagelist.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "# join/split is faster than replace\n", "func_signal": "def escape(text):\n", "code": "text = '\\\\\\\\'.join(text.split('\\\\')) # \\ -> \\\\\ntext = '\\\\\"'.join(text.split('\"')) # \" -> \\\"\nreturn text", "path": "pythm\\mpd\\mpdlib2.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "# if type isn't empty, then the object's type is set to it.  otherwise\n# the type is set to the key of the first key/val pair.\n\n# keywords lists the keys that indicate a new object -- like for the\n# 'outputs' command, keywords would be ['outputid'].\n\n", "func_signal": "def one_object(self, keywords, type):\n", "code": "entity = dictobj()\nif type:\n    entity['type'] = type\n\n# make these functions local\ngetline = self.talker.get_line\ngetpair = self.talker.get_pair\ndone = self.talker.done\n\nwhile not done:\n    #self.talker.get_line()\n    getline()", "path": "pythm\\mpd\\mpdlib2.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"browses trough the library/fs - returns a list of BrowseEntries.\"\"\"\n#lsinfo command\n", "func_signal": "def browse(self, parentDir=None):\n", "code": "if parentDir is None:\n    lsinfo = self.mpd.lsinfo()\nelse:\n    lsinfo = self.mpd.lsinfo(parentDir)\nret = []\nif parentDir != None and parentDir !=\"\":\n    ret.append(BrowserEntry(os.path.split(parentDir)[0],\"..\",True))\n\nfor elem in lsinfo:\n    #print elem\n    #print \"===\"\n    dir = elem[\"type\"] == \"directory\"\n    fn = elem[elem[\"type\"]]\n    name = os.path.basename(fn)\n    art = \"\"\n    if elem.has_key(\"artist\"):\n        art = elem[\"artist\"]\n    tit =\"\"\n    if elem.has_key(\"title\"):\n        tit = elem[\"title\"]\n    if not(tit == \"\" or art == \"\"):\n        if art == \"\":\n            name = tit\n        elif tit == \"\":\n            name = art\n        else:\n            name = art + \" - \" + tit\n    ret.append(BrowserEntry(fn,name,dir))\n\nself.emit(Signals.BROWSER_CHANGED,parentDir,ret)", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"removes entry from pl\"\"\"\n", "func_signal": "def remove(self, plid):\n", "code": "self.mpd.deleteid(plid)\nself.check_state(0.0)", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"Issue a warning to the logger, make sure that it is done only once\n\"\"\"\n", "func_signal": "def _warn(self, wstr):\n", "code": "if not wstr in self._wfilter:\n    self._wfilter.append(wstr)\n    logger.warn(wstr)", "path": "pythm\\config.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"moves plentry down\"\"\"\n", "func_signal": "def down(self, plid):\n", "code": "idx = self.get_plid_index(plid)\nif idx != None:\n    if idx+1 < len(self.playlist):\n        swapid = self.playlist[idx + 1].id\n        self.mpd.swapid(plid,swapid)\n        self.check_state(0.0)", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "# <mpdclient2.dictobj at 0x12345678 ..\n#   {\n#     key: val,\n#     key2: val2\n#   }>\n", "func_signal": "def __repr__(self):\n", "code": "return (object.__repr__(self).rstrip('>') + ' ..\\n' +\n        '  {\\n    ' +\n        ',\\n    '.join([ '%s: %s' % (k, v) for k, v in self.items() ]) +\n        '\\n  }>')", "path": "pythm\\mpd\\mpdlib2.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"\ninitializes a new backend.\neventhandler is a gui callback function that ensures that\nthe backend functions emit signals in the gui's thread.\n\"\"\"\n\n", "func_signal": "def __init__(self):\n", "code": "PythmBackend.__init__(self,\"mpd\")\nself.mpd = None", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"pauses playback\"\"\"\n", "func_signal": "def pause(self):\n", "code": "if self.state == State.PAUSED:\n    self.set_state(State.PLAYING)\n    arg = 0\nelse:\n    self.set_state(State.PAUSED)\n    arg = 1\nself.mpd.pause(arg)", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"raises callbacks\"\"\"\n", "func_signal": "def emit(self, signal, *args):\n", "code": "if not self.quiet:\n    if self.cfg.callbacks.has_key(signal):\n        for call in self.cfg.callbacks[signal]:\n            self.eventhandler(call, *args)", "path": "pythm\\backend\\backend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"\ngets index of plid in pl\n\"\"\"\n", "func_signal": "def get_plid_index(self,plid):\n", "code": "i = 0\nfor e in self.playlist:\n    if e.id == plid:\n        return i\n    i += 1\nreturn None", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"Plays a song from the playlist or starts playing of playlist\"\"\"\n", "func_signal": "def play(self, plid=None):\n", "code": "if plid is None:\n    self.mpd.play()\nelse:\n    self.mpd.playid(plid)\n\nself.set_state(State.PLAYING)", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"\nCompares two BrowserEntries\n\"\"\"\n", "func_signal": "def browserEntryCompare(e1, e2):\n", "code": "if e1.isDir and not e2.isDir:\n    return -1\nelif not e1.isDir and e2.isDir:\n    return 1\n\nreturn cmp(e1.caption, e2.caption)", "path": "pythm\\backend\\backend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"sets new repeat\"\"\"\n", "func_signal": "def set_repeat(self, rep):\n", "code": "if(rep):\n    self.mpd.repeat(1)\nelse:\n    self.mpd.repeat(0)", "path": "pythm\\mpd\\mpdbackend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "# DMR old way of doing it.\n#time.sleep(0.1)\n\n# Must be connected to a back-end to bother performing\n# these actions.\n", "func_signal": "def do_work(self):\n", "code": "if (self.backend.is_connected()):\n    self.backend.check_state(self.elapsedTime)\n\n    # Watch for being asked to asynchronously restart playback\n    # after pause from incoming phone call.\n    if (self.backend.resumePhoneTimer > 0):\n        self.backend.resumePhoneTimer -= self.elapsedTime\n        if (self.backend.resumePhoneTimer <= 0):\n            self.backend.resume_from_phone()", "path": "pythm\\backend\\backend.py", "repo_name": "yarikoptic/pythm", "stars": 7, "license": "other", "language": "python", "size": 475}
{"docstring": "\"\"\"\ndemjson, python-cjson API compatibility hook. Use loads(s) instead.\n\"\"\"\n", "func_signal": "def decode(s):\n", "code": "import warnings\nwarnings.warn(\"simplejson.loads(s) should be used instead of decode(s)\",\n    DeprecationWarning)\nreturn loads(s)", "path": "simplejson\\__init__.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"\nReturn a JSON representation of a Python string\n\"\"\"\n", "func_signal": "def encode_basestring(s):\n", "code": "def replace(match):\n    return ESCAPE_DCT[match.group(0)]\nreturn '\"' + ESCAPE.sub(replace, s) + '\"'", "path": "simplejson\\encoder.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"Return a 2-tuple giving the minimum and maximum y-axis\ndata range.\n\"\"\"\n", "func_signal": "def data_y_range(self):\n", "code": "try:\n    lower = min([min(self._filter_none(s))\n                 for type, s in self.annotated_data()\n                 if type == 'y'])\n    upper = max([max(self._filter_none(s)) + 1\n                 for type, s in self.annotated_data()\n                 if type == 'y'])\n    return (lower, upper)\nexcept ValueError:\n    return None     # no y-axis datasets", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = S.loads(JSON)\nout = S.dumps(res)\nself.assertEquals(res, S.loads(out))", "path": "simplejson\\tests\\test_pass3.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "# colours needs to be a list, tuple or None\n", "func_signal": "def set_colours(self, colours):\n", "code": "assert(isinstance(colours, list) or isinstance(colours, tuple) or\n    colours is None)\n# make sure the colours are in the right format\nif colours:\n    for col in colours:\n        _check_colour(col)\nself.colours = colours", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = S.loads(JSON)\nout = S.dumps(res)\nself.assertEquals(res, S.loads(out))", "path": "simplejson\\tests\\test_pass2.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"\nEncode the given object and yield each string\nrepresentation as available.\n\nFor example::\n    \n    for chunk in JSONEncoder().iterencode(bigobject):\n        mysocket.write(chunk)\n\"\"\"\n", "func_signal": "def iterencode(self, o, _one_shot=False):\n", "code": "if self.check_circular:\n    markers = {}\nelse:\n    markers = None\nif self.ensure_ascii:\n    _encoder = encode_basestring_ascii\nelse:\n    _encoder = encode_basestring\nif self.encoding != 'utf-8':\n    def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):\n        if isinstance(o, str):\n            o = o.decode(_encoding)\n        return _orig_encoder(o)\n\ndef floatstr(o, allow_nan=self.allow_nan, _repr=FLOAT_REPR, _inf=INFINITY, _neginf=-INFINITY):\n    # Check for specials.  Note that this type of test is processor- and/or\n    # platform-specific, so do tests which don't depend on the internals.\n\n    if o != o:\n        text = 'NaN'\n    elif o == _inf:\n        text = 'Infinity'\n    elif o == _neginf:\n        text = '-Infinity'\n    else:\n        return _repr(o)\n\n    if not allow_nan:\n        raise ValueError(\"Out of range float values are not JSON compliant: %r\"\n            % (o,))\n\n    return text\n\n\nif _one_shot and c_make_encoder is not None and not self.indent and not self.sort_keys:\n    _iterencode = c_make_encoder(\n        markers, self.default, _encoder, self.indent,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, self.allow_nan)\nelse:\n    _iterencode = _make_iterencode(\n        markers, self.default, _encoder, self.indent, floatstr,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, _one_shot)\nreturn _iterencode(o, 0)", "path": "simplejson\\encoder.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"Scale `self.data` as appropriate for the given data encoding\n(data_class) and return it.\n\nAn optional `y_range` -- a 2-tuple (lower, upper) -- can be\ngiven to specify the y-axis bounds. If not given, the range is\ninferred from the data: (0, <max-value>) presuming no negative\nvalues, or (<min-value>, <max-value>) if there are negative\nvalues.  `self.scaled_y_range` is set to the actual lower and\nupper scaling range.\n\nDitto for `x_range`. Note that some chart types don't have x-axis\ndata.\n\"\"\"\n", "func_signal": "def scaled_data(self, data_class, x_range=None, y_range=None):\n", "code": "self.scaled_data_class = data_class\n\n# Determine the x-axis range for scaling.\nif x_range is None:\n    x_range = self.data_x_range()\n    if x_range and x_range[0] > 0:\n        x_range = (x_range[0], x_range[1])\nself.scaled_x_range = x_range\n\n# Determine the y-axis range for scaling.\nif y_range is None:\n    y_range = self.data_y_range()\n    if y_range and y_range[0] > 0:\n        y_range = (y_range[0], y_range[1])\nself.scaled_y_range = y_range\n\nscaled_data = []\nfor type, dataset in self.annotated_data():\n    if type == 'x':\n        scale_range = x_range\n    elif type == 'y':\n        scale_range = y_range\n    elif type == 'marker-size':\n        scale_range = (0, max(dataset))\n    scaled_dataset = []\n    for v in dataset:\n        if v is None:\n            scaled_dataset.append(None)\n        else:\n            scaled_dataset.append(\n                data_class.scale_value(v, scale_range))\n    scaled_data.append(scaled_dataset)\nreturn scaled_data", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "# use float values instead of integers because we don't need an encode\n# map index\n", "func_signal": "def scale_value(cls, value, range):\n", "code": "scaled = cls.float_scale_value(value, range)\nclipped = cls.clip_value(scaled)\nData.check_clip(scaled, clipped)\nreturn clipped", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"\njsonlib, JsonUtils, python-json, json-py API compatibility hook.\nUse dumps(s) instead.\n\"\"\"\n", "func_signal": "def write(obj):\n", "code": "import warnings\nwarnings.warn(\"simplejson.dumps(s) should be used instead of write(s)\",\n    DeprecationWarning)\nreturn dumps(obj)", "path": "simplejson\\__init__.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "# All datasets are y-axis data.\n", "func_signal": "def annotated_data(self):\n", "code": "for dataset in self.data:\n    yield ('y', dataset)", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "# colours needs to be a list, tuple or None\n", "func_signal": "def set_colours_within_series(self, colours):\n", "code": "assert(isinstance(colours, list) or isinstance(colours, tuple) or\n    colours is None)\n# make sure the colours are in the right format\nif colours:\n    for col in colours:\n        _check_colour(col)\nself.colours_within_series = colours", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"\nReturn a JSON string representation of a Python data structure.\n\n>>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n'{\"foo\": [\"bar\", \"baz\"]}'\n\"\"\"\n# This is for extremely simple cases and benchmarks.\n", "func_signal": "def encode(self, o):\n", "code": "if isinstance(o, basestring):\n    if isinstance(o, str):\n        _encoding = self.encoding\n        if (_encoding is not None \n                and not (_encoding == 'utf-8')):\n            o = o.decode(_encoding)\n    if self.ensure_ascii:\n        return encode_basestring_ascii(o)\n    else:\n        return encode_basestring(o)\n# This doesn't pass the iterator directly to ''.join() because the\n# exceptions aren't as detailed.  The list call should be roughly\n# equivalent to the PySequence_Fast that ''.join() would do.\nchunks = self.iterencode(o, _one_shot=True)\nif not isinstance(chunks, (list, tuple)):\n    chunks = list(chunks)\nreturn ''.join(chunks)", "path": "simplejson\\encoder.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"Return a 2-tuple giving the minimum and maximum x-axis\ndata range.\n\"\"\"\n", "func_signal": "def data_x_range(self):\n", "code": "try:\n    lower = min([min(self._filter_none(s))\n                 for type, s in self.annotated_data()\n                 if type == 'x'])\n    upper = max([max(self._filter_none(s))\n                 for type, s in self.annotated_data()\n                 if type == 'x'])\n    return (lower, upper)\nexcept ValueError:\n    return None     # no x-axis datasets", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"Determines the appropriate data encoding type to give satisfactory\nresolution (http://code.google.com/apis/chart/#chart_data).\n\"\"\"\n", "func_signal": "def data_class_detection(self, data):\n", "code": "assert(isinstance(data, list) or isinstance(data, tuple))\nif not isinstance(self, (LineChart, BarChart, ScatterChart)):\n    # From the link above:\n    #   Simple encoding is suitable for all other types of chart\n    #   regardless of size.\n    return SimpleData\nelif self.height < 100:\n    # The link above indicates that line and bar charts less\n    # than 300px in size can be suitably represented with the\n    # simple encoding. I've found that this isn't sufficient,\n    # e.g. examples/line-xy-circle.png. Let's try 100px.\n    return SimpleData\nelse:\n    return ExtendedData", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "# Datasets are all y-axis data. However, there should only be\n# one dataset for pie charts.\n", "func_signal": "def annotated_data(self):\n", "code": "for dataset in self.data:\n    yield ('x', dataset)", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "# Datasets alternate between x-axis, y-axis.\n", "func_signal": "def annotated_data(self):\n", "code": "for i, dataset in enumerate(self.data):\n    if i % 2 == 0:\n        yield ('x', dataset)\n    else:\n        yield ('y', dataset)", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "# Skip 'BarChart.get_url_bits' and call Chart directly so the parent\n# doesn't add \"chbh\" before we do.\n", "func_signal": "def get_url_bits(self, data_class=None):\n", "code": "url_bits = BarChart.get_url_bits(self, data_class=data_class,\n    skip_chbh=True)\nif self.group_spacing is not None:\n    if self.bar_spacing is None:\n        raise InvalidParametersException('Bar spacing is required ' \\\n            'to be set when setting group spacing')\n    if self.bar_width is None:\n        raise InvalidParametersException('Bar width is required to ' \\\n            'be set when setting bar spacing')\n    url_bits.append('chbh=%i,%i,%i'\n        % (self.bar_width, self.bar_spacing, self.group_spacing))\nelif self.bar_spacing is not None:\n    if self.bar_width is None:\n        raise InvalidParametersException('Bar width is required to ' \\\n            'be set when setting bar spacing')\n    url_bits.append('chbh=%i,%i' % (self.bar_width, self.bar_spacing))\nelif self.bar_width:\n    url_bits.append('chbh=%i' % self.bar_width)\nreturn url_bits", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"\ndemjson, python-cjson compatibility hook. Use dumps(s) instead.\n\"\"\"\n", "func_signal": "def encode(obj):\n", "code": "import warnings\nwarnings.warn(\"simplejson.dumps(s) should be used instead of encode(s)\",\n    DeprecationWarning)\nreturn dumps(obj)", "path": "simplejson\\__init__.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"legend needs to be a list, tuple or None\"\"\"\n", "func_signal": "def set_legend(self, legend):\n", "code": "assert(isinstance(legend, list) or isinstance(legend, tuple) or\n    legend is None)\nif legend:\n    self.legend = [urllib.quote(a) for a in legend]\nelse:\n    self.legend = None", "path": "pygooglechart.py", "repo_name": "elindner/foosalizer", "stars": 4, "license": "None", "language": "python", "size": 132}
{"docstring": "\"\"\"Remove script from buildset. Pack is eigther a db.script or an index\"\"\"\n", "func_signal": "def removeScript(self,  pack):\n", "code": "idx = self.getScriptIndex(pack)\nif not idx:\n    # pack is not a script instance in this buildset, then it must be an index\n    idx = int(pack)\n    \nstore = Store.of(self)\npks = store.find(buildset_script, buildset_script.buildset_id == self.id)\nopks = pks.order_by(Asc(buildset_script.idx))\n# Correct idx for scripts larger that the one removed\nfor i in opks:\n    if i.idx == idx:\n        store.remove(i)\n    elif i.idx > idx:\n        i.idx -= 1\nstore.commit()", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nEnsure that path of dirs exists\n\"\"\"\n", "func_signal": "def mkdirp(path):\n", "code": "ps = path.split('/')\nfpath = \"\"\nfor p in ps:\n    fpath += p + \"/\"\n    if not os.path.exists(fpath):\n        os.mkdir(fpath)\n    elif not os.path.isdir(fpath):\n        raise Exception(\"Cannot chdir '%s' because is it a file\" % (fpath,))", "path": "lib\\utils.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nGet buildset by index\n\"\"\"\n", "func_signal": "def getByIndex(cls, store,  idx):\n", "code": "scripts = cls.getIndexedListing(store)\ncuridx = 0\nfor i in scripts:\n    if curidx == idx:\n        return i\n    curidx += 1\nreturn None", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"Add a new script to this buildset\"\"\"\n", "func_signal": "def addScript(self, pack,  before_script_idx = sys.maxint):\n", "code": "idx = int(before_script_idx)\nstore = Store.of(self)\npks = store.find(buildset_script, buildset_script.buildset_id == self.id)\nm = pks.max(buildset_script.idx) \nif m is None:\n    # First script is added\n    idx = 0\nelif m < idx:\n    # append using m+1\n    idx = m + 1\n\n# move all tasks with higher or equal index that idx one step\nopks = pks.order_by(Desc(buildset_script.idx))\nfor i in opks:\n    if i.idx >= idx:\n        i.idx += 1\n\n# Create new pack\nspack = buildset_script()\nspack.buildset_id = self.id\nspack.script_id = pack.id\nspack.idx = idx\n    \nstore.add(spack)\nstore.commit()", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"Get an exclusive lock when building\"\"\"\n# If a running zbuild is present then abort\n", "func_signal": "def lockPid(pid_file):\n", "code": "if os.path.exists(pid_file):\n    return False\n\n# Run next scheduled build\nglobal pid\npid = os.getpid()\n\n# Create pid_file to signal that a build is in progress\nrunfile = open(pid_file, 'w')\nrunfile.write(str(pid))\nrunfile.flush()\nrunfile.close()\nreturn True", "path": "lib\\utils.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nWrite all scripts to outstream os\n\"\"\"\n", "func_signal": "def dumpNames(cls, store,  os):\n", "code": "buildsets = store.find(cls).order_by(cls.id)\ncounter_len = counter_length(buildsets.count())\narr = [ ['idx ', 'name', ' duration'] ]\nidx = 0\nif not buildsets.count():\n    os(\"    No buildsets - use 'addscript' command to add script to a new buildset\\n\")\n    return\nfor buildset in buildsets:\n    dur = None # buildsets[i].metadata.get('duration',None)\n    if dur:\n        dur = ':'.join(str(dur).split(':')[1:])\n    else:\n        dur = \"  -\"\n    arr2 = []\n    arr2.append(str(idx).rjust(counter_len,'0'))\n    arr2.append(buildset.name)\n    arr2.append(\" %s\" % (str(dur).split('.')[0],))\n    arr.append(arr2)\n    idx += 1\nos(table_layout(arr, True, \"    \"))", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nreturn a string with a table layout of the [[]..]\n\"\"\"\n", "func_signal": "def table_layout(arr, header_line = False, prefix = \"\", do_sort = True):\n", "code": "sizes = []\nfor i in arr:\n    idx = 0\n    for j in i:\n        while True:\n            try:\n                sizes[idx] = max(sizes[idx], len(str(j)))\n                break\n            except:\n                sizes.append(0)\n        idx += 1\n\nout = \"\"\n\nhl = None\nif header_line:\n    hl = arr.pop(0)\nif do_sort:\n    arr.sort(cmp=lambda a,b: cmp(a[0],b[0]))\nif hl:\n    arr.insert(0,hl)\nfor i in arr:\n    idx = 0\n\n    out += prefix\n    for j in sizes:\n        out += i[idx].ljust(j) \n        idx += 1\n    out += \"\\n\"\n\n    if header_line:\n        out += prefix\n        for j in sizes:\n            out += \"-\" * j\n        out += \"\\n\"\n        header_line = False\nreturn out", "path": "lib\\layout.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nFormat seconds to HH:MM:SS or MM:SS if not enough seconds for an\nhour\n\"\"\"\n", "func_signal": "def sec2str(secs):\n", "code": "secs = int(secs)\nh = secs / (60*60)\nm = (secs - (h*60*60)) / 60\ns = secs - (h*60*60 + m * 60)\nres = str(m).rjust(2,'0') + \":\" + str(s).rjust(2,'0')\nif h:\n    return str(h).rjust(2,'0') + \":\" + res\nreturn res", "path": "lib\\utils.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nOpen database STORE - creating it if necessary\n\"\"\"\n", "func_signal": "def initdb(allow_create = False):\n", "code": "sqlitepath = \"%s/zbuild.sqlite\" % OPTIONS.work_dir\nif not os.path.exists(sqlitepath):\n    if not allow_create:\n        return None\n\n    # setup the sqlite database\n    # print \"No existing sqlite database -> creating\"\n    schema = \"lib/dbschema.sql\"\n    if not os.path.exists(schema):\n        schema = OPTIONS.zbuild_install_dir + \"/lib/dbschema.sql\"\n        if not os.path.exists(schema):\n            schema = \"/usr/share/zbuild/lib/dbschema.sql\"\n\n    # print \"Using %s\" % sqlitepath \n    conn = sqlite3.connect(sqlitepath)\n    conn.executescript(open(schema, 'r').read())\n    conn.commit()\n    conn.close()\n    #os.system(\"sqlite3 < '%s'\" % schema)\n\nsdb = create_database(\"sqlite:%s\" % sqlitepath)\nreturn Store(sdb)", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nReturn the part the path (see getPath) that self and\nother_script have in common\n\"\"\"\n", "func_signal": "def getCommonPath(self, other_script):\n", "code": "if not other_script:\n    return []\n\nme = self.getPath()\n\nif self == other_script:\n    return me\n\nother = other_script.getPath()\nres = []\n\nwhile me and other and me[0] == other[0]:\n    me.pop(0)\n    other.pop(0)\n    res.append(i1)\nreturn res", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nReturn a list of all scripts sorted by script grouping\n\"\"\"\n", "func_signal": "def getIndexedListing(cls, store):\n", "code": "scripts = store.find(cls, cls.is_parent == 0).order_by(Asc(cls.parent_id))\ncounter_len = counter_length(scripts.count())\n\nres = []\nlast = None\nfor s in scripts:\n    common, diff = s.getPathChange(last)\n    last = s\n    res.extend(diff)\nreturn res", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nGet buildset by index\n\"\"\"\n", "func_signal": "def getByIndex(cls, store,  idx):\n", "code": "buildsets = store.find(cls).order_by(cls.id)\ncuridx = 0\nfor i in buildsets:\n    if curidx == idx:\n        return i\n    curidx += 1\nreturn None", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nReturn the part of the path (see getPath) that self and\nother_script does not have in common\n\"\"\"\n", "func_signal": "def getPathChange(self, other_script):\n", "code": "me = self.getPath()\n\nif self == other_script:\n    return (me, [])\n\nif not other_script:\n    return ([],me)\n\nother = other_script.getPath()\n\ncommon = []\nwhile me and other and me[0] == other[0]:\n    common.append(me[0])\n    me.pop(0)\n    other.pop(0)\n\nreturn (common, me)", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nReturn number of digits to represent length of arr\n\"\"\"\n", "func_signal": "def counter_length(arrlen):\n", "code": "if not arrlen:\n        return 1\nreturn int(math.log10(arrlen)) + 1", "path": "lib\\layout.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"Create a new build using the buildset provided\"\"\"\n", "func_signal": "def createFromBuildset(cls, dbbuildset,  scheduled_for = None):\n", "code": "store = Store.of(dbbuildset)\nb = cls()\nb.buildset_id = dbbuildset.id\n\nif scheduled_for is None:\n        scheduled_for = datetime.utcnow()\nb.scheduled_for = scheduled_for\nstore.add(b)\nstore.flush()\n\n# assign scripts to be build\n#print str(dbbuildset.buildset_scripts)\nfor sp in dbbuildset.buildset_scripts:\n    bp = build_script_status()\n    bp.buildset_script_id = sp.id\n    bp.build_id = b.id\n    #print \"%s %s\" % (str(sp.script.name),  str(sp.idx))\n    #print \"%s\" % str(sp.idx)\n    #print str(sp.idx) \n    bp.idx = sp.idx\n    store.add(bp)\nstore.commit()\nreturn b", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nHelper log function to return a function that can be used to log\nto a build db entry\n\"\"\"\n", "func_signal": "def log_stdout():\n", "code": "f = open('./build.log', 'w')\ndef do_log(msg):\n    sys.stdout.write(msg)\n    sys.stdout.flush()\n    f.write(msg)\n    f.flush()\nreturn do_log", "path": "lib\\utils.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nWrite all scripts to outstream os\n\"\"\"\n", "func_signal": "def dumpNames(cls, store,  os):\n", "code": "idx = 0\nscripts = cls.getIndexedListing(store)\ncounter_len = counter_length(len(scripts))\narr = [ ['idx ','name',' duration'] ]\n\nif not scripts:\n    os(\"    No scripts - use the import-scripts command\\n\")\n    return\n\nfor pack in scripts:\n    dur = None # pack.metadata.get('duration',None)\n    if dur:\n        dur = ':'.join(str(dur).split(':')[1:])\n    else:\n        dur = \"  -\"\n    arr2 = []\n    arr2.append(str(idx).rjust(counter_len,'0'))\n    arr2.append(\" \" * (pack.depth() * 3) + pack.name)\n    arr2.append(\" %s\" % (str(dur).split('.')[0],))\n    #arr2.append(\"%i\" % pack.depth())\n    arr.append(arr2)\n    idx += 1\nos(table_layout(arr, True, \"    \"))", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"\nwrite list of scripts to logger\n\"\"\"\n", "func_signal": "def dumpScriptNames(self, os):\n", "code": "store = Store.of(self)\npks = store.find(buildset_script, buildset_script.buildset_id == self.id)\nscripts = pks.order_by(Asc(buildset_script.idx))\ncounter_len = counter_length(scripts.count())\narr = [ ['idx ', 'name', ' duration'] ]\nidx = 0\n\ndef scriptPath(_pack):\n    \"\"\"\n    \"\"\"\n    res = []\n    cur = _pack.script\n    while cur.parent:\n        res.insert(0, cur.parent)\n        cur = cur.parent\n    return res\n\ndef new(depth, pack_name, pack_idx, dur = None):\n    if dur is None:\n        dur = \"  -\"\n    arr2 = []\n    arr2.append(pack_idx)\n    arr2.append(\" \" * (depth * 3) + pack_name)\n    arr2.append(\" %s\" % (str(dur).split('.')[0],))\n    return arr2\n\nlast_path = []\nfor pack in scripts:\n\n    path = scriptPath(pack)\n    depth = len(path)\n    if last_path != path and depth:\n        arr.append(new(depth - 1,\n                       path[-1].name,\n                       ' '.rjust(counter_len,' ')))\n    last_path = path\n\n    dur = None\n    # Lookup last duration of script run\n    prevrun = store.find(build_script_status, \n                         build_script_status.buildset_script_id == pack.id,\n                         build_script_status.exit_code == 0\n                         ).order_by(Desc(build_script_status.id)).first()\n    \n    dur = \"  -\"\n    if prevrun and prevrun.end_time and prevrun.start_time:\n        dur = prevrun.end_time - prevrun.start_time\n        dur = utils.sec2str(dur.seconds)\n    arr.append(new(depth,\n                   pack.script.name, \n                   str(pack.idx).rjust(counter_len,'0'),\n                   dur))\n    idx += 1\nos(table_layout(arr, True, \"    \", False))", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\" Return a tuple of (duration, eta duration) \"\"\"\n", "func_signal": "def getDurations(self):\n", "code": "duration = 0\neta_duration = 0\nstore = Store.of(self)\n\nfor status in self.build_script_statuses:\n        start_time = status.start_time\n        end_time = status.end_time or datetime.utcnow()\n        if start_time:\n            duration += (end_time - start_time).seconds\n\n        last = status.buildset_script.last_duration\n        if last:\n            eta_duration += last\n\nreturn (duration, eta_duration)", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"Return the index the given script has in this buildset or None\"\"\"\n", "func_signal": "def getScriptIndex(self,  pack):\n", "code": "if type(pack) != script:\n    return None\nfor i in self.buildset_scripts:\n    if i.script_id == pack.id:\n        return i.idx\nreturn None", "path": "lib\\db.py", "repo_name": "jcd/zbuild", "stars": 4, "license": "None", "language": "python", "size": 144}
{"docstring": "\"\"\"Sets a cookie.\"\"\"\n", "func_signal": "def setcookie(name, value, expires=\"\", domain=None, secure=False):\n", "code": "if expires < 0: \n    expires = -1000000000 \nkargs = {'expires': expires, 'path':'/'}\nif domain: \n    kargs['domain'] = domain\nif secure:\n    kargs['secure'] = secure\n# @@ should we limit cookies to a different path?\ncookie = Cookie.SimpleCookie()\ncookie[name] = urllib.quote(utf8(value))\nfor key, val in kargs.iteritems(): \n    cookie[name][key] = val\nheader('Set-Cookie', cookie.items()[0][1].OutputString())", "path": "vendor\\web\\webapi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"Returns HTTPError with '500 internal error' error from the active application.\n\"\"\"\n", "func_signal": "def InternalError(message=None):\n", "code": "if message:\n    return _InternalError(message)\nelif ctx.get('app_stack'):\n    return ctx.app_stack[-1].internalerror()\nelse:\n    return _InternalError()", "path": "vendor\\web\\webapi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nPrints a prettyprinted version of `args` to stderr.\n\"\"\"\n", "func_signal": "def debug(*args):\n", "code": "try: \n    out = ctx.environ['wsgi.errors']\nexcept: \n    out = sys.stderr\nfor arg in args:\n    print >> out, pprint.pformat(arg)\nreturn ''", "path": "vendor\\web\\webapi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nDecodes `text` that's HTML quoted.\n\n    >>> htmlunquote('&lt;&#39;&amp;&quot;&gt;')\n    '<\\\\'&\">'\n\"\"\"\n", "func_signal": "def htmlunquote(text):\n", "code": "text = text.replace(\"&quot;\", '\"')\ntext = text.replace(\"&#39;\", \"'\")\ntext = text.replace(\"&gt;\", \">\")\ntext = text.replace(\"&lt;\", \"<\")\ntext = text.replace(\"&amp;\", \"&\") # Must be done last!\nreturn text", "path": "vendor\\web\\net.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nRuns a WSGI-compatible `func` using FCGI, SCGI, or a simple web server,\nas appropriate based on context and `sys.argv`.\n\"\"\"\n\n", "func_signal": "def runwsgi(func):\n", "code": "if os.environ.has_key('SERVER_SOFTWARE'): # cgi\n    os.environ['FCGI_FORCE_CGI'] = 'Y'\n\nif (os.environ.has_key('PHP_FCGI_CHILDREN') #lighttpd fastcgi\n  or os.environ.has_key('SERVER_SOFTWARE')):\n    return runfcgi(func, None)\n\nif 'fcgi' in sys.argv or 'fastcgi' in sys.argv:\n    args = sys.argv[1:]\n    if 'fastcgi' in args: args.remove('fastcgi')\n    elif 'fcgi' in args: args.remove('fcgi')\n    if args:\n        return runfcgi(func, validaddr(args[0]))\n    else:\n        return runfcgi(func, None)\n\nif 'scgi' in sys.argv:\n    args = sys.argv[1:]\n    args.remove('scgi')\n    if args:\n        return runscgi(func, validaddr(args[0]))\n    else:\n        return runscgi(func)\n\nreturn httpserver.runsimple(func, validip(listget(sys.argv, 1, '')))", "path": "vendor\\web\\wsgi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nReturns either (ip_address, port) or \"/path/to/socket\" from string_\n\n    >>> validaddr('/path/to/socket')\n    '/path/to/socket'\n    >>> validaddr('8000')\n    ('0.0.0.0', 8000)\n    >>> validaddr('127.0.0.1')\n    ('127.0.0.1', 8080)\n    >>> validaddr('127.0.0.1:8000')\n    ('127.0.0.1', 8000)\n    >>> validaddr('fff')\n    Traceback (most recent call last):\n        ...\n    ValueError: fff is not a valid IP address/port\n\"\"\"\n", "func_signal": "def validaddr(string_):\n", "code": "if '/' in string_:\n    return string_\nelse:\n    return validip(string_)", "path": "vendor\\web\\net.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "# Assuming all templates end with .html\n", "func_signal": "def __getattr__(self, name):\n", "code": "path = name + '.html'\nt = self._lookup.get_template(path)\nreturn t.render", "path": "vendor\\web\\contrib\\template.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nReturns a `storage` object with the GET and POST arguments. \nSee `storify` for how `requireds` and `defaults` work.\n\"\"\"\n", "func_signal": "def input(*requireds, **defaults):\n", "code": "from cStringIO import StringIO\ndef dictify(fs): \n    # hack to make web.input work with enctype='text/plain.\n    if fs.list is None:\n        fs.list = [] \n\n    return dict([(k, fs[k]) for k in fs.keys()])\n\n_method = defaults.pop('_method', 'both')\n\ne = ctx.env.copy()\na = b = {}\n\nif _method.lower() in ['both', 'post', 'put']:\n    if e['REQUEST_METHOD'] in ['POST', 'PUT']:\n        if e.get('CONTENT_TYPE', '').lower().startswith('multipart/'):\n            # since wsgi.input is directly passed to cgi.FieldStorage, \n            # it can not be called multiple times. Saving the FieldStorage\n            # object in ctx to allow calling web.input multiple times.\n            a = ctx.get('_fieldstorage')\n            if not a:\n                fp = e['wsgi.input']\n                a = cgi.FieldStorage(fp=fp, environ=e, keep_blank_values=1)\n                ctx._fieldstorage = a\n        else:\n            fp = StringIO(data())\n            a = cgi.FieldStorage(fp=fp, environ=e, keep_blank_values=1)\n        a = dictify(a)\n\nif _method.lower() in ['both', 'get']:\n    e['REQUEST_METHOD'] = 'GET'\n    b = dictify(cgi.FieldStorage(environ=e, keep_blank_values=1))\n\nout = dictadd(b, a)\ntry:\n    defaults.setdefault('_unicode', True) # force unicode conversion by default.\n    return storify(out, *requireds, **defaults)\nexcept KeyError:\n    raise badrequest()", "path": "vendor\\web\\webapi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nQuotes a string for use in a URL.\n\n    >>> urlquote('://?f=1&j=1')\n    '%3A//%3Ff%3D1%26j%3D1'\n    >>> urlquote(None)\n    ''\n    >>> urlquote(u'\\u203d')\n    '%E2%80%BD'\n\"\"\"\n", "func_signal": "def urlquote(val):\n", "code": "if val is None: return ''\nif not isinstance(val, unicode): val = str(val)\nelse: val = val.encode('utf-8')\nreturn urllib.quote(val)", "path": "vendor\\web\\net.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nReturns a `status` redirect to the new URL. \n`url` is joined with the base URL so that things like \n`redirect(\"about\") will work properly.\n\"\"\"\n", "func_signal": "def __init__(self, url, status='301 Moved Permanently', absolute=False):\n", "code": "newloc = urlparse.urljoin(ctx.path, url)\n\nif newloc.startswith('/'):\n    if absolute:\n        home = ctx.realhome\n    else:\n        home = ctx.home\n    newloc = home + newloc\n\nheaders = {\n    'Content-Type': 'text/html',\n    'Location': newloc\n}\nHTTPError.__init__(self, status, headers, \"\")", "path": "vendor\\web\\webapi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "# give error if Chetah is not installed\n", "func_signal": "def __init__(self, path):\n", "code": "from Cheetah.Template import Template\nself.path = path", "path": "vendor\\web\\contrib\\template.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"Runs a WSGI function as an SCGI server.\"\"\"\n", "func_signal": "def runscgi(func, addr=('localhost', 4000)):\n", "code": "import flup.server.scgi as flups\nreturn flups.WSGIServer(func, bindAddress=addr).run()", "path": "vendor\\web\\wsgi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nConverts `val` so that it's safe for use in UTF-8 HTML.\n\n    >>> websafe(\"<'&\\\\\">\")\n    '&lt;&#39;&amp;&quot;&gt;'\n    >>> websafe(None)\n    ''\n    >>> websafe(u'\\u203d')\n    '\\\\xe2\\\\x80\\\\xbd'\n\"\"\"\n", "func_signal": "def websafe(val):\n", "code": "if val is None:\n    return ''\nif isinstance(val, unicode):\n    val = val.encode('utf-8')\nval = str(val)\nreturn htmlquote(val)", "path": "vendor\\web\\net.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "# quick hack to check if the program is running in dev mode.\n", "func_signal": "def _is_dev_mode():\n", "code": "if os.environ.has_key('SERVER_SOFTWARE') \\\n    or os.environ.has_key('PHP_FCGI_CHILDREN') \\\n    or 'fcgi' in sys.argv or 'fastcgi' in sys.argv \\\n    or 'mod_wsgi' in sys.argv:\n        return False\nreturn True", "path": "vendor\\web\\wsgi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nParses an HTTP date into a datetime object.\n\n    >>> parsehttpdate('Thu, 01 Jan 1970 01:01:01 GMT')\n    datetime.datetime(1970, 1, 1, 1, 1, 1)\n\"\"\"\n", "func_signal": "def parsehttpdate(string_):\n", "code": "try:\n    t = time.strptime(string_, \"%a, %d %b %Y %H:%M:%S %Z\")\nexcept ValueError:\n    return None\nreturn datetime.datetime(*t[:6])", "path": "vendor\\web\\net.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nReturns True if `address` is a valid IPv4 address.\n\n    >>> validipaddr('192.168.1.1')\n    True\n    >>> validipaddr('192.168.1.800')\n    False\n    >>> validipaddr('192.168.1')\n    False\n\"\"\"\n", "func_signal": "def validipaddr(address):\n", "code": "try:\n    octets = address.split('.')\n    if len(octets) != 4:\n        return False\n    for x in octets:\n        if not (0 <= int(x) <= 255):\n            return False\nexcept ValueError:\n    return False\nreturn True", "path": "vendor\\web\\net.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\nReturns a `storage` object with all the cookies in it.\nSee `storify` for how `requireds` and `defaults` work.\n\"\"\"\n", "func_signal": "def cookies(*requireds, **defaults):\n", "code": "cookie = Cookie.SimpleCookie()\ncookie.load(ctx.env.get('HTTP_COOKIE', ''))\ntry:\n    d = storify(cookie, *requireds, **defaults)\n    for k, v in d.items():\n        d[k] = v and urllib.unquote(v)\n    return d\nexcept KeyError:\n    badrequest()\n    raise StopIteration", "path": "vendor\\web\\webapi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"Returns HTTPError with '404 Not Found' error from the active application.\n\"\"\"\n", "func_signal": "def NotFound(message=None):\n", "code": "if message:\n    return _NotFound(message)\nelif ctx.get('app_stack'):\n    return ctx.app_stack[-1].notfound()\nelse:\n    return _NotFound()", "path": "vendor\\web\\webapi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "# unlike the usual scheme of things, the POST is actually called\n# first here\n", "func_signal": "def POST(self):\n", "code": "i = web.input(return_to='/')\nif i.get('action') == 'logout':\n    logout()\n    return web.redirect(i.return_to)\n\ni = web.input('openid', return_to='/')\n\nn = _random_session()\nsessions[n] = {'webpy_return_to': i.return_to}\n\nc = openid.consumer.consumer.Consumer(sessions[n], store)\na = c.begin(i.openid)\nf = a.redirectURL(web.ctx.home, web.ctx.home + web.ctx.fullpath)\n\nweb.setcookie('openid_session_id', n)\nreturn web.redirect(f)", "path": "vendor\\web\\webopenid.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"Returns the data sent with the request.\"\"\"\n", "func_signal": "def data():\n", "code": "if 'data' not in ctx:\n    cl = intget(ctx.env.get('CONTENT_LENGTH'), 0)\n    ctx.data = ctx.env['wsgi.input'].read(cl)\nreturn ctx.data", "path": "vendor\\web\\webapi.py", "repo_name": "yaanno/keepmysecret", "stars": 4, "license": "None", "language": "python", "size": 199}
{"docstring": "\"\"\"\ndemjson, python-cjson API compatibility hook. Use loads(s) instead.\n\"\"\"\n", "func_signal": "def decode(s):\n", "code": "import warnings\nwarnings.warn(\"simplejson.loads(s) should be used instead of decode(s)\",\n    DeprecationWarning)\nreturn loads(s)", "path": "sneeu\\apps\\tumble\\simplejson\\__init__.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"\nReturn the Python representation of ``s`` (a ``str`` or ``unicode``\ninstance containing a JSON document)\n\"\"\"\n", "func_signal": "def decode(self, s, _w=WHITESPACE.match):\n", "code": "obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nend = _w(s, end).end()\nif end != len(s):\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nreturn obj", "path": "sneeu\\apps\\tumble\\simplejson\\decoder.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"\nParameter:  String.\nReturns:    The string, with ``backticks'' -style double quotes\n            translated into HTML curly quote entities.\nExample input:  ``Isn't this fun?''\nExample output: &#8220;Isn't this fun?&#8221;\n\"\"\"\n\n", "func_signal": "def educateBackticks(str):\n", "code": "str = re.sub(r\"\"\"``\"\"\", r\"\"\"&#8220;\"\"\", str)\nstr = re.sub(r\"\"\"''\"\"\", r\"\"\"&#8221;\"\"\", str)\nreturn str", "path": "lib\\smartypants.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"\njsonlib, JsonUtils, python-json, json-py API compatibility hook.\nUse loads(s) instead.\n\"\"\"\n", "func_signal": "def read(s):\n", "code": "import warnings\nwarnings.warn(\"simplejson.loads(s) should be used instead of read(s)\",\n    DeprecationWarning)\nreturn loads(s)", "path": "sneeu\\apps\\tumble\\simplejson\\__init__.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"\nParameter:  String.\n\nReturns:    The string, with each instance of \"--\" translated to\n            an en-dash HTML entity, and each \"---\" translated to\n            an em-dash HTML entity.\n\"\"\"\n\n", "func_signal": "def educateDashesOldSchool(str):\n", "code": "str = re.sub(r\"\"\"---\"\"\", r\"\"\"&#8212;\"\"\", str)    # em (yes, backwards)\nstr = re.sub(r\"\"\"--\"\"\", r\"\"\"&#8211;\"\"\", str)    # en (yes, backwards)\nreturn str", "path": "lib\\smartypants.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"\nParameter:  String.\nReturns:    The string, with each instance of \"...\" translated to\n            an ellipsis HTML entity.\n\nExample input:  Huh...?\nExample output: Huh&#8230;?\n\"\"\"\n\n", "func_signal": "def educateEllipses(str):\n", "code": "str = re.sub(r\"\"\"\\.\\.\\.\"\"\", r\"\"\"&#8230;\"\"\", str)\nstr = re.sub(r\"\"\"\\. \\. \\.\"\"\", r\"\"\"&#8230;\"\"\", str)\nreturn str", "path": "lib\\smartypants.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Deletes the comment with the given ID.\"\"\"\n", "func_signal": "def delete_comment(self, entry_id, comment_id):\n", "code": "self._fetch(\"/api/comment/delete\", {\n    \"entry\": entry_id,\n    \"comment\": comment_id,\n})", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Creates a new FriendFeed session for the given user.\n\nThe credentials are optional for some operations, but required for\nprivate feeds and all operations that write data, like publish_link.\n\"\"\"\n", "func_signal": "def __init__(self, auth_nickname=None, auth_key=None):\n", "code": "self.auth_nickname = auth_nickname\nself.auth_key = auth_key", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Returns the entries the given user has \"liked\".\"\"\"\n", "func_signal": "def fetch_user_likes_feed(self, nickname, **kwargs):\n", "code": "return self._fetch_feed(\n    \"/api/feed/user/\" + urllib.quote_plus(nickname) + \"/likes\",\n    **kwargs)", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Makes all strings in the given JSON-like structure unicode.\"\"\"\n", "func_signal": "def _unicodify(json):\n", "code": "if isinstance(json, str):\n    return json.decode(\"utf-8\")\nelif isinstance(json, dict):\n    for name in json:\n        json[name] = _unicodify(json[name])\nelif isinstance(json, list):\n    for part in json:\n        _unicodify(part)\nreturn json", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"\nYield match, end_idx for each match\n\"\"\"\n", "func_signal": "def iterscan(self, string, idx=0, context=None):\n", "code": "match = self.scanner.scanner(string, idx).match\nactions = self.actions\nlastend = idx\nend = len(string)\nwhile True:\n    m = match()\n    if m is None:\n        break\n    matchbegin, matchend = m.span()\n    if lastend == matchend:\n        break\n    action = actions[m.lastindex]\n    if action is not None:\n        rval, next_pos = action(m, context)\n        if next_pos is not None and next_pos != matchend:\n            # \"fast forward\" the scanner\n            matchend = next_pos\n            match = self.scanner.scanner(string, matchend).match\n        yield rval, matchend\n    lastend = matchend", "path": "sneeu\\apps\\tumble\\simplejson\\scanner.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Returns the entries shared by the user with the given nickname.\n\nAuthentication is required if the user's feed is not public.\n\"\"\"\n", "func_signal": "def fetch_user_feed(self, nickname, **kwargs):\n", "code": "return self._fetch_feed(\n    \"/api/feed/user/\" + urllib.quote_plus(nickname), **kwargs)", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Returns the entries the given user has commented on or \"liked\".\"\"\"\n", "func_signal": "def fetch_user_discussion_feed(self, nickname, **kwargs):\n", "code": "return self._fetch_feed(\n    \"/api/feed/user/\" + urllib.quote_plus(nickname) + \"/discussion\",\n    **kwargs)", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Returns a merged feed with all of the given users' entries.\n\nAuthentication is required if any one of the users' feeds is not\npublic.\n\"\"\"\n", "func_signal": "def fetch_multi_user_feed(self, nicknames, **kwargs):\n", "code": "return self._fetch_feed(\"/api/feed/user\", nickname=\",\".join(nicknames),\n                        **kwargs)", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Un-deletes the comment with the given ID.\"\"\"\n", "func_signal": "def undelete_comment(self, entry_id, comment_id):\n", "code": "self._fetch(\"/api/comment/delete\", {\n    \"entry\": entry_id,\n    \"comment\": comment_id,\n    \"undelete\": 1,\n})", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"\nDecode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning\nwith a JSON document) and return a 2-tuple of the Python\nrepresentation and the index in ``s`` where the document ended.\n\nThis can be used to decode a JSON document from a string that may\nhave extraneous data at the end.\n\"\"\"\n", "func_signal": "def raw_decode(self, s, **kw):\n", "code": "kw.setdefault('context', self)\ntry:\n    obj, end = self._scanner.iterscan(s, **kw).next()\nexcept StopIteration:\n    raise ValueError(\"No JSON object could be decoded\")\nreturn obj, end", "path": "sneeu\\apps\\tumble\\simplejson\\decoder.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"\nParameter:  String.\nReturns:    The string, with each SmartyPants HTML entity translated to\n            its ASCII counterpart.\n\nExample input:  &#8220;Hello &#8212; world.&#8221;\nExample output: \"Hello -- world.\"\n\"\"\"\n\n", "func_signal": "def stupefyEntities(str):\n", "code": "str = re.sub(r\"\"\"&#8211;\"\"\", r\"\"\"-\"\"\", str)  # en-dash\nstr = re.sub(r\"\"\"&#8212;\"\"\", r\"\"\"--\"\"\", str) # em-dash\n\nstr = re.sub(r\"\"\"&#8216;\"\"\", r\"\"\"'\"\"\", str)  # open single quote\nstr = re.sub(r\"\"\"&#8217;\"\"\", r\"\"\"'\"\"\", str)  # close single quote\n\nstr = re.sub(r\"\"\"&#8220;\"\"\", r'''\"''', str)  # open double quote\nstr = re.sub(r\"\"\"&#8221;\"\"\", r'''\"''', str)  # close double quote\n\nstr = re.sub(r\"\"\"&#8230;\"\"\", r\"\"\"...\"\"\", str)# ellipsis\n\nreturn str", "path": "lib\\smartypants.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"\nParameter:  String.\n\nReturns:    The string, with each instance of \"--\" translated to\n            an em-dash HTML entity, and each \"---\" translated to\n            an en-dash HTML entity. Two reasons why: First, unlike the\n            en- and em-dash syntax supported by\n            EducateDashesOldSchool(), it's compatible with existing\n            entries written before SmartyPants 1.1, back when \"--\" was\n            only used for em-dashes.  Second, em-dashes are more\n            common than en-dashes, and so it sort of makes sense that\n            the shortcut should be shorter to type. (Thanks to Aaron\n            Swartz for the idea.)\n\"\"\"\n", "func_signal": "def educateDashesOldSchoolInverted(str):\n", "code": "str = re.sub(r\"\"\"---\"\"\", r\"\"\"&#8211;\"\"\", str)    # em\nstr = re.sub(r\"\"\"--\"\"\", r\"\"\"&#8212;\"\"\", str)    # en\nreturn str", "path": "lib\\smartypants.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"'Likes' the entry with the given ID.\"\"\"\n", "func_signal": "def add_like(self, entry_id):\n", "code": "self._fetch(\"/api/like\", {\n    \"entry\": entry_id,\n})", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Searches over entries in FriendFeed.\n\nIf the request is authenticated, the default scope is over all of the\nentries in the authenticated user's Friends Feed. If the request is\nnot authenticated, the default scope is over all public entries.\n\nThe query syntax is the same syntax as\nhttp://friendfeed.com/advancedsearch\n\"\"\"\n", "func_signal": "def search(self, q, **kwargs):\n", "code": "kwargs[\"q\"] = q\nreturn self._fetch_feed(\"/api/feed/search\", **kwargs)", "path": "sneeu\\apps\\tumble\\friendfeed.py", "repo_name": "sneeu/sneeu_com", "stars": 4, "license": "None", "language": "python", "size": 364}
{"docstring": "\"\"\"Find packages that satisfy the given relation\n\nThis function finds all packages that satisfy the given single\nrelation (possibly with | alternatives).  If the strict flag is set,\nthe function matches only actual package names and not virtual ones;\nwe need this feature to work with Replaces.\n\"\"\"\n\n", "func_signal": "def _SelectPackagesByRelation(relation, strict=False):\n", "code": "if relation.find(' | ') != -1:\n  results = {}\n  for part in relation.split(' | '):\n    results.update(_SelectPackagesByRelation(part, strict))\n  return results\n\ntry:\n  results = {}\n  name = relation.split(' ')[0]\n  for entry in _depi[name]:\n    nva = entry[0]\n\n    # Match both actual and virtual package names (not versioned)\n\n    if relation.find(' ') == -1 and not strict:\n      results[nva] = None\n\n    # Match only actual package names (versioned dependency or\n    # strict matching)\n\n    elif ru.CheckPackageRelation(nva, relation):\n      results[nva] = None\n  return results\nexcept KeyError:\n  return {}", "path": "repository\\src\\verifier_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Start and monitor an unattended Ubuntu install.\n\nThis function will start an unattended Ubuntu install, running in\na VM, as part of a debmarshal test. It will also monitor that\ninstall to completion.\n\nThis is intended to be run as a separate thread of execution, so\nthat many installs can run in parallel.\n\nIts success or failure is reported back to the spawning process\nthrough the results_queue.\n\nArgs:\n  test: A path to a debmarshal test\n  vm: The hostname of the vm within the test to install\n  net_name: The name of a debmarshal network configured for this VM.\n  net_gateway: The gateway of the debmarshal network.\n  mac: The MAC address of this VM.\n  web_port: The port the test is being served over. The spawning\n    process should be serving the directory containing the\n    debmarshal test over HTTP.\n  results_queue: A Queue.Queue object that doInstall will store its\n    success or failure into that\n\"\"\"\n", "func_signal": "def doInstall(test, vm, net_name, net_gateway, mac, web_port, results_queue):\n", "code": "try:\n  config = yaml.safe_load(open(os.path.join(test, 'config.yml')))\n\n  domain = config['domain']\n\n  vm_config = config['vms'][vm]\n\n  memory, suite, arch, disk_size, preseed_path = parseConfig(\n    test, vm, vm_config)\n\n  deb_arch = arch if arch != 'x86_64' else 'amd64'\n\n  disk_path = Ubuntu.diskPath(vm, domain, test, vm_config)\n  disk_dir = os.path.dirname(disk_path)\n  if not os.path.exists(disk_dir):\n    os.makedirs(disk_dir)\n\n  disk_lock = open(disk_path + '.lock', 'w')\n  fcntl.lockf(disk_lock, fcntl.LOCK_EX)\n\n  if os.path.exists(disk_path):\n    results_queue.put((test, vm, True, 'cached'))\n    return\n\n  base.createSparseFile(disk_path, disk_size)\n\n  try:\n    kernel, initrd = loadKernel(suite, deb_arch)\n\n    cmdline = genCommandLine(preseed_path)\n    cmdline += ' preseed/url=http://%s:%s/%s.preseed' % (\n      net_gateway, web_port, vm)\n    cmdline += ' mirror/http/hostname=%s:9999' % net_gateway\n    cmdline += ' mirror/http/directory=/ubuntu'\n    cmdline += ' mirror/http/proxy='\n\n    dom_name = privops.call('createDomain',\n                            memory,\n                            [disk_path],\n                            net_name,\n                            mac,\n                            'qemu',\n                            arch,\n                            {'kernel': kernel,\n                             'initrd': initrd,\n                             'cmdline': cmdline,\n                             'on_reboot': 'destroy'})\n\n    # Now wait for the install to finish...\n    #\n    # libvirt has an API for integration with somebody else's main\n    # loop. Unfortunately, they forgot to make it usable by\n    # humans. libvirt-glib in Debian experimental might be a good\n    # jumping off point.\n    #\n    # TODO(ebroder): Figure out how to use some sort of select()\n    #   loop instead of a while-sleep loop.\n    while True:\n      time.sleep(10)\n\n      if dom_name not in qemu.QEMU.listDomains():\n        break\n\n    results_queue.put((test, vm, True, None))\n  except:\n    os.remove(disk_path)\n    raise\nexcept:\n  results_queue.put((test, vm, False, traceback.format_exc()))", "path": "testing\\debmarshal\\distros\\ubuntu.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Parse the elements of the VM configuration.\n\nThis takes the generic VM configuration information and parses it\ninto the fields necessary both for installations and generating\nconfig hashes.\n\nArgs:\n  test: Path to the debmarshal test for this VM.\n  vm_name: The (unqualified) hostname of the VM.\n  vm_config: The configuration dict for the VM\n\nReturns:\n  A tuple with the parsed config information.\n\"\"\"\n", "func_signal": "def parseConfig(test, vm_name, vm_config):\n", "code": "disk_size = utils.parseBytes(vm_config.get('disk', '10G'))\n\nmemory = vm_config.get('memory', '128M')\narch = vm_config.get('arch', 'x86_64')\n\ndist_opts = vm_config.get('dist_opts', {})\nsuite = dist_opts.get('suite', 'jaunty')\n\npreseed_path = os.path.join(test, '%s.preseed' % vm_name)\n\nreturn (memory, suite, arch, disk_size, preseed_path)", "path": "testing\\debmarshal\\distros\\ubuntu.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Generate a path for a given configuration's disk image.\n\nThis uses a hash of the parameters to a VM to generate the path to\nuse for caching that disk's image.\n\nThe generated path is unique to a given user.\n\nArgs:\n  hostname: Hostname of the VM.\n  domain: Domain name of the VM.\n  test: The path to the debmarshal test.\n  vm_config: The configuration dict for the VM.\n  user: The user for whom the path is being generated.\n\"\"\"\n", "func_signal": "def diskPath(hostname, domain, test, vm_config, user=None):\n", "code": "if user is None:\n  user = os.environ['USER']\n\nhash = hashConfig(hostname, domain, test, vm_config)\n\ndisk = '/var/tmp/debmarshal-%s/disks/ubuntu/%s' % (user, hash)\nreturn disk", "path": "testing\\debmarshal\\distros\\ubuntu.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Make sure the kernel and initrd are available.\n\nThis function downloads the kernel and initrd for the requested\nsuite and architecture into\n/var/tmp/debmarshal-$USER/dists/ubuntu/<suite>.\n\nArgs:\n  suite: Which suite's kernel and initrd to download\n  arch: What architecture to download.\n\"\"\"\n# TODO(ebroder): Don't hardcode mirrors!\n", "func_signal": "def loadKernel(suite, arch):\n", "code": "base_url = (\n  'http://us.archive.ubuntu.com/ubuntu/dists/' +\n  suite +\n  '/main/installer-%s/current/images/netboot/ubuntu-installer/%s/' % (\n    arch, arch))\nbase_cache = os.path.join(\n    '/var/tmp/debmarshal-%s/dists/ubuntu' % os.environ['USER'], suite)\n\nif not os.path.exists(base_cache):\n  os.makedirs(base_cache)\n\nkernel_cache = os.path.join(base_cache, 'linux')\n\nkernel_lock = open(kernel_cache + '.lock', 'w')\nfcntl.lockf(kernel_lock, fcntl.LOCK_EX)\nif not os.path.exists(kernel_cache):\n  try:\n    urllib.urlretrieve(base_url + 'linux', kernel_cache)\n  except:\n    os.unlink(kernel_cache)\n    raise\ndel kernel_lock\n\ninitrd_cache = os.path.join(base_cache, 'initrd.gz')\n\ninitrd_lock = open(initrd_cache + '.lock', 'w')\nfcntl.lockf(initrd_lock, fcntl.LOCK_EX)\nif not os.path.exists(initrd_cache):\n  try:\n    urllib.urlretrieve(base_url + 'initrd.gz', initrd_cache)\n  except:\n    os.unlink(initrd_cache)\n    raise\ndel initrd_lock\n\nreturn (kernel_cache, initrd_cache)", "path": "testing\\debmarshal\\distros\\ubuntu.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Wrapper function for _DoComputeDependency()\n\nThis function adds a 5-second timeout to the dependency solving\nprocedure to avoid blocking on particularly difficult cases.  On a\n2.6GHz Xeon system, the dependency for nearly all packages are\nsolved in fractions of a second.  This function also inserts the set\nof Essential packages into the queue to catch any package that\nconflicts with them.\n\"\"\"\n\n", "func_signal": "def _ComputeDependency(queue):\n", "code": "global _notified\n\ndef Handler(signum, frame):\n  _Error('Cannot solve dependency within time bound')\n  raise MemoryError\n\ntry:\n  try:\n    _notified = {}\n    signal.signal(signal.SIGALRM, Handler)\n    signal.alarm(5)\n    return _DoComputeDependency(_essential + queue, {}, {})\n  except MemoryError:\n    return None\nfinally:\n  signal.alarm(0)", "path": "repository\\src\\verifier_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Check that the given track is valid (exit on error)\n\"\"\"\n\n", "func_signal": "def ConfirmTrack(track):\n", "code": "_LoadSettings()\nif track == 'snapshot':  return\nif track not in _settings:\n  lg.error('There is no such a track called ' + track)\n  sys.exit()", "path": "repository\\src\\setting_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Given a preseed file, generate a list of command line args.\n\nIf the preseed file has some number of blocks starting with \"###\nBEGIN COMMAND LINE ARGS\" and ending with \"### END COMMAND LINE\nARGS\", those Debconf options will be extracted and reformatted as\npart of the kernel command line.\n\nThis function is not your mother; if your preseed file forgets the\nclosing block, we won't tell you, and the behavior of the function\nis not well-defined.\n\nIt's also your problem to be sure that none of the options to be\npassed in on the command line contain spaces, because the kernel\nisn't clever enough to deal with them.\n\nArgs:\n  preseed: Path to a Debconf preseed file.\n\nReturns:\n  A string containing the arguments to pass on the kernel command\n    line.\n\"\"\"\n", "func_signal": "def genCommandLine(preseed):\n", "code": "args = {}\ncapturing_args = False\nfor line in open(preseed):\n  if line == '### BEGIN COMMAND LINE ARGS\\n':\n    capturing_args = True\n  elif line == '### END COMMAND LINE ARGS\\n':\n    capturing_args = False\n\n  if capturing_args and not line.lstrip().startswith('#'):\n    opt, _, value = line.split()[1:5]\n    args[opt] = value\n\nreturn ' '.join('%s=%s' % (k, v) for k, v in args.iteritems())", "path": "testing\\debmarshal\\distros\\ubuntu.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"List all tracks mentioned in the configuration file\n\"\"\"\n\n", "func_signal": "def ListTracks():\n", "code": "_LoadSettings()\ntracks = ['snapshot']\nfor track in _settings:\n  if track is None:  continue\n  tracks.append(track)\nreturn tracks", "path": "repository\\src\\setting_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Parse the contents of a configuration file\n\nThis function parses a configuration file and returns a dictionary\ncontaining the results.  It splits the input into sections by\nbracket-quoted headings, and store the attributes in each section in\na separate dictionary indexed by the section name.  The top-level\nsection is indexed by the key None.\n\"\"\"\n\n", "func_signal": "def _ParseConfig(lines):\n", "code": "repo_dict = {}\ntrack_id = None\ntrack_lines = []\n\nfor line in lines:\n  line = line.split('#')[0]\n  if line == '' or line.isspace():  continue\n  if line.startswith('[') and line.endswith(']\\n'):\n    repo_dict[track_id] = pu.ParseAttributes(track_lines)\n    track_id = line[1:-2]\n    track_lines = []\n  else:\n    track_lines.append(line.rstrip())\n\nrepo_dict[track_id] = pu.ParseAttributes(track_lines)\nrepo_dict[None].setdefault('Component', ['local'])\nrepo_dict[None].setdefault('Architectures', ['i386'])\nreturn repo_dict", "path": "repository\\src\\setting_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Find a dependency solution for a set of packages\n\nThis function determines a minimum set of packages that satisfies\nthe transitive dependency relation of the given set of packages.\nThe queue argument is the list of alternative packages that we wish\nto install (eg., [['foo_3_all'], ['bar_1_all', 'buz_1_all']] means\nthat we wish to install either foo and bar, or foo and buz), and the\nbase dictionary has the set of currently installed packages as its\nkeys.  The exclude dictionary has the set of packages that must not\nbe installed (eg., due to Conflicts).  The function returns None if\nit cannot find any such solution.\n\"\"\"\n\n", "func_signal": "def _DoComputeDependency(queue, base, exclude):\n", "code": "global _notified\n\n# If there are no packages in the queue, we are done tracing the\n# dependency graph and the base is the answer.\n\nif queue == []:\n  return base.keys()\n\n# The head are the packages we want to try fit into base, and the\n# packages in queue we will deal with later.\n\nhead = queue[0]\ntail = queue[1:]\n\n# If there is a package in head that is already in base, then a\n# package we want has already been included, and we can continue\n# working on the tail of the queue.\n\nfor pkg in head:\n  if pkg in base:\n    return _DoComputeDependency(tail, base, exclude)\n\n# Try to add each package in head into base in turn and call\n# _DoComputeDependency() recursively in a depth-first search.\n\nfor pkg in head:\n\n  # Skip with a warning message if some package in base conflicts\n  # with this one.\n\n  if pkg in exclude:\n    _ReportConflict(exclude[pkg], pkg)\n    continue\n\n  entry = _GetPackage(pkg)\n\n  # Skip if the package is not in the release (this situation should\n  # never happen).\n\n  if entry is None:\n    _Warning('Bug: package ' + pkg + ' does not exist')\n    continue\n\n  # Skip if this package conflicts with any package in base;\n  # otherwise add the Conflicts packages to new_exclude.  Note that\n  # following Policy 7.3, we do not check if the package Conflicts\n  # with itself.\n\n  proceed = True\n  new_exclude = dict(exclude)\n  for relation in entry[2]:\n    for conflicted in _SelectPackagesWithMemo(relation):\n      if conflicted in base:\n        _ReportConflict(pkg, conflicted)\n        proceed = False\n        break\n      new_exclude[conflicted] = pkg\n    if not proceed:\n      break\n  if not proceed:\n    continue\n\n  # Add dependencies of this package into new_queue.\n\n  new_queue = list(tail)\n  for relation in entry[1]:\n    depends = sorted(_SelectPackagesWithMemo(relation).keys())\n\n    # There are no packages in the release which satisfies the\n    # dependency, we know that there cannot be any solutions in this\n    # branch of the search tree and skip with a warning message.\n\n    if depends == []:\n      _ReportUnsatisfiableRelation(relation)\n      return None\n\n    # To speed up the search process, we insert dependencies with\n    # only one choice to the beginning of the queue, and all others\n    # at the tail.\n\n    if len(depends) == 1:\n      new_queue.insert(0, depends)\n    else:\n      new_queue.append(depends)\n\n  # Add this package to the new_base dictionary.\n\n  new_base = dict(base)\n  new_base[pkg] = None\n\n  # Continue working on the queue.\n\n  result = _DoComputeDependency(new_queue, new_base, new_exclude)\n  if result is not None:\n    return result\n\nreturn None", "path": "repository\\src\\verifier_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Get a setting attribute in a maintenance track\n\"\"\"\n\n", "func_signal": "def GetSetting(track, key):\n", "code": "_LoadSettings()\nif track in _settings:\n  if key in _settings[track]:\n    return ', '.join(_settings[track][key])\nif key in _settings[None]:\n  return ', '.join(_settings[None][key])\nreturn None", "path": "repository\\src\\setting_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"See if the package matches any of the relations\n\"\"\"\n\n", "func_signal": "def _MatchRelations(relations, nva, strict=False):\n", "code": "for relation in relations:\n  if nva in _SelectPackagesByRelation(relation, strict):\n    return True\nreturn False", "path": "repository\\src\\verifier_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Retrieve a _depi package entry by name_version_arch\n\"\"\"\n\n", "func_signal": "def _GetPackage(nva):\n", "code": "name = nva.split('_')[0]\ntry:\n  for entry in _depi[name]:\n    if entry[0] == nva:\n      return entry\nexcept KeyError:\n  pass\n_Warning('Package ' + nva + ' is not in the release')\nreturn None", "path": "repository\\src\\verifier_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Organize dependency information for a release\n\nThis function organizes dependency metadata for packages listed in\nthe pkg_list argument in a form readily usable for dependency and\nimplicit conflict checking.  The entry for each package is a tuple\nwith the following attributes: name_ver_arch, Depends (including\nPre-Depends), Conflicts, and Replaces.  The key to _depi is a\nvirtual or actual package name, which maps to a list of\ncorresponding package entries.\n\"\"\"\n\n", "func_signal": "def _BuildDependencyGraph(pkg_list, pkg_deps):\n", "code": "global _depi, _essential\n\npkgs_to_check = {}\nfor nva in sorted(pkg_list):\n  [n, v, a] = nva.split('_')\n  di = pkg_deps[nva]\n\n  # Ignore debian-installer packages.  Since they are not\n  # installable on a normal system, they can have all kinds of\n  # dependency breakage and we do not really care.\n\n  if di['Component'][0].endswith('/debian-installer'):\n    continue\n\n  if di['Essential'][0] != 'no':\n    _essential.append([nva])\n\n  pkgs_to_check[nva] = None\n  dep = di['Depends']\n  dep.extend(di['Pre-Depends'])\n  cfl = di['Conflicts']\n  repl = di['Replaces']\n\n  # Allow looking up the _depi entry for a package through both\n  # actual and virtual (defined with Provides) package names.\n\n  for name in [n] + di['Provides']:\n    if name not in _depi:\n      _depi[name] = []\n    _depi[name].append((nva, dep, cfl, repl))\nreturn pkgs_to_check", "path": "repository\\src\\verifier_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Memoized version of _SelectPackagesByRelation()\n\"\"\"\n\n", "func_signal": "def _SelectPackagesWithMemo(relation):\n", "code": "global _relation_pkg\n\nif relation not in _relation_pkg:\n  _relation_pkg[relation] = _SelectPackagesByRelation(relation)\nreturn _relation_pkg[relation]", "path": "repository\\src\\verifier_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Generate a hash representing a domain's configuration.\n\nThis hash should be usable for things like disk image reuse.\n\nArgs:\n  hostname: Hostname of the VM.\n  domain: Domain name of the VM.\n  test: The path to the debmarshal test.\n  vm_config: The configuration dict for the VM.\n\nReturns:\n  Something that should sort of be a cryptographic hash of all the\n    input.\n\"\"\"\n", "func_signal": "def hashConfig(hostname, domain, test, vm_config):\n", "code": "to_hash = []\nto_hash.append(str(hostname))\nto_hash.append(str(domain))\n\nmemory, suite, arch, disk_size, preseed_path = parseConfig(\n  test, hostname, vm_config)\nto_hash.append(str(suite))\nto_hash.append(str(arch))\nto_hash.append(str(disk_size))\nto_hash.append(open(preseed_path).read())\n\nreturn md5.md5('\\n'.join(to_hash)).hexdigest()", "path": "testing\\debmarshal\\distros\\ubuntu.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Build a list of implicitly conflicting packages\n\nTwo packages conflict implicitly if they both install a file to the\nsame path but neither declares Conflicts or Replaces on the other.\nThis function compiles a list of implicitly conflicting package\npairs in the release.\n\"\"\"\n\n", "func_signal": "def _BuildConflictList(file_pkg, pkg_dict):\n", "code": "cfl = {}\nfor f in file_pkg:\n  mutual_ex = {}\n\n  # Add packages in the release that contain the pathname f into the\n  # mutual_ex dictionary.\n\n  for pkg in file_pkg[f].split(', '):\n    entry = _GetPackage(pkg)\n    if entry is not None:\n      mutual_ex[pkg] = entry\n\n  # Add all implicitly conflicting package pairs in mutual_ex into\n  # the cfl dictionary, with the conflicted pathname f as the value.\n\n  for p1 in mutual_ex:\n    for p2 in mutual_ex:\n      if p1 >= p2:  continue\n      if not (p1 in pkg_dict or p2 in pkg_dict):  continue\n      if _MatchRelations(mutual_ex[p1][3], p2, True):  continue\n      if _MatchRelations(mutual_ex[p2][3], p1, True):  continue\n      if _MatchRelations(mutual_ex[p1][2], p2):  continue\n      if _MatchRelations(mutual_ex[p2][2], p1):  continue\n      cfl[(p1, p2)] = f\nreturn cfl", "path": "repository\\src\\verifier_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Verify dependency integrity of a release\n\"\"\"\n\n", "func_signal": "def CheckDependency(pkg_list, underlying=[]):\n", "code": "global _package, _silent, _relation_pkg\n\ndef Initialize(_arg, dbs):\n  global _depi, _essential\n\n  _depi = {}\n  _essential = []\n  pkg_deps = du.ParseDependencyTable(dbs['pkg_deps'])\n  _BuildDependencyGraph(underlying, pkg_deps)\n  pkgs_to_check = _BuildDependencyGraph(pkg_list, pkg_deps)\n  cfl = _BuildConflictList(dbs['file_pkg'], pkgs_to_check)\n  return cfl, pkgs_to_check\n\nsys.setrecursionlimit(10000)\ndb_list = ['pkg_deps', 'file_pkg']\ncfl, pkgs_to_check = bu.RunWithDB(db_list, Initialize)\n\n_silent = False\n_relation_pkg = {}\n\n# Check that every individual package is installable.\n\nfor pkg in sorted(pkgs_to_check.keys()):\n  _package = pkg\n  if _GetPackage(pkg) is None:\n    _Error('Package ' + pkg + ' does not exist')\n  elif _ComputeDependency([[pkg]]) is None:\n    _Error(pkg + ' is uninstallable')\n\n# Check that packages providing the same pathnames contain metadata\n# that prevents them from being installed at the same time.\n\n_silent = True\nfor pkg_1, pkg_2 in sorted(cfl.keys()):\n  if _ComputeDependency([[pkg_1], [pkg_2]]) is not None:\n    lg.error('Implicit conflict between ' + pkg_1 + ' and '\n             + pkg_2 + ' on /' + cfl[(pkg_1, pkg_2)])", "path": "repository\\src\\verifier_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"Load the configuration file into the _settings dictionary\n\"\"\"\n\n", "func_signal": "def _LoadSettings():\n", "code": "global _settings\n\nif _settings is None:\n  _settings = ou.RunWithFileInput(_ParseConfig, 'config/repository')\n  if _settings is None:\n    lg.error('Cannot load repository settings')\n    sys.exit()", "path": "repository\\src\\setting_utils.py", "repo_name": "ebroder/debmarshal", "stars": 5, "license": "None", "language": "python", "size": 680}
{"docstring": "\"\"\"\nFactorises a positive integer\n\"\"\"\n# Halfway needs to be a float as rounding may\n# introduce errors for small numbers.\n# Therefore can't use range() for iterating as that takes ints.\n", "func_signal": "def __factor_positive_integer(self, number):\n", "code": "halfway = number ** 0.5\nbig_factors = []\nfor factor in itertools.count(1):\n\tif factor > halfway:\n\t\tbreak\n\tif number % factor == 0:\n\t\tyield factor\n\t\tbig_factor = number // factor\n\t\tif big_factor != factor:\n\t\t\tbig_factors.insert(0, big_factor)\nfor factor in big_factors:\n\tyield factor", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"\\\ntransform_fn(dimensions, w, h): returns a QTransform\n\n * dimensions: plotter dimension object\n * w: text width in pixels\n * h: text height in pixels\n\"\"\"\n", "func_signal": "def __init__(self, text, scene, dimensions, transform_fn):\n", "code": "QObject.__init__(self)\n\nself.text = text\nself.scene = scene\nself.dimensions = dimensions\nself.transform = transform_fn\n\nself.text_item = None\nself.rect = None\nfor signal in text.changed, dimensions.changed:\n\tsignal.connect(self.update)\n\nself.update()", "path": "Graph\\plotter.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "# Place in middle of left margin, at centre of the grid\n# and rotated 90 degrees anticlockwise\n", "func_signal": "def y_title_transform(dimensions, w, h):\n", "code": "return QTransform().translate(\n\t\tdimensions.left_margin/2,\n\t\tdimensions.top_margin + dimensions.grid_height/2\n\t\t).rotate(270).translate(-w/2, -h/2)", "path": "Graph\\plotter.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"\nUpdate the axis about line removal.\n\nSee add_line for notes.\n\"\"\"\n", "func_signal": "def remove_line(self, id):\n", "code": "del self.lines[id]\nself._update_data(id)", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "# Place in middle of top margin, at centre of whole graph\n# or should I centre it on the grid?\n", "func_signal": "def main_title_transform(dimensions, w, h):\n", "code": "return QTransform().translate(\n\t\tdimensions.left_margin + dimensions.grid_width/2,\n\t\tdimensions.top_margin/2\n\t\t).translate(-w/2, -h/2)", "path": "Graph\\plotter.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"Returns a list, not an iterator.\"\"\"\n# If scale is invalid, return an empty list\n", "func_signal": "def _floats_to_coordinates(self, floats):\n", "code": "if self.scale is None:\n\treturn []\nreturn [self.__float_to_coordinate(x) for x in floats]", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "# Place in middle of bottom margin, at centre of the grid.\n", "func_signal": "def x_title_transform(dimensions, w, h):\n", "code": "return QTransform().translate(\n\t\tdimensions.left_margin + dimensions.grid_width/2,\n\t\tdimensions.height - dimensions.bottom_margin/2\n\t\t).translate(-w/2, -h/2)", "path": "Graph\\plotter.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"\nUpdate the axis about line addition.\n\nid should be a unique, immutable identifier of the line.\nI suggest id(line).\n\"\"\"\n", "func_signal": "def add_line(self, id, data):\n", "code": "self.lines[id] = self.__variants_to_floats(data)\nself._update_data(id)", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"\nLinks two pyqtProperty/pyqtSignal pairs together.\n\nNote that each object's setter must not emit the changed signal\nwhen set to its current value, or link() will cause infinite recursion.\n\"\"\"\n# Connect each other's change signals\n", "func_signal": "def link(obj1, name1, obj2, name2):\n", "code": "obj1.__getattribute__(name1 + \"_changed\").connect(lambda x: set_property_value(obj2, name2, x))\nobj2.__getattribute__(name2 + \"_changed\").connect(lambda x: set_property_value(obj1, name1, x))\n# Give both the value of the first\nset_property_value(obj2, name2, get_property_value(obj1, name1))", "path": "Base\\property.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "# Round self.min down to the nearest multiple of major_scale.\n", "func_signal": "def __get_scale_start(self, unit_scale, base):\n", "code": "major_scale = base * unit_scale\nreturn math.floor(self.min/major_scale) * major_scale", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"\nReturns an iterator or list of tuples describing each grid unit's\nticks and text.\n\nThe first item of the tuple describes the type of tick: either\ngrid.major, grid.minor or grid.unit.\n\nThe second item of the tuple is a string giving a representation\nof the value at that point. It should always be filled in;\nhowever, I will probably only print the string for major grid\nunits.\n\nIf the scales are invalid, the tick type will always be filled in,\nbut the value string may be an empty string.\n\"\"\"\n", "func_signal": "def tick_info(self):\n", "code": "base = 10\nminor_base = 5\nfor i in range(self.length + 1):\n\tif i % base == 0:\n\t\ttick = grid.major\n\telif i % minor_base == 0:\n\t\ttick = grid.minor\n\telse:\n\t\ttick = grid.unit\n\t# floats = units * floats/unit\n\t#        = units * scale\n\t# ...and adjust for start\n\tstring = str(self.__coordinate_to_float(i))\n\tyield tick, string", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"\nReturn an editor widget suitable for editing the data type.\n\nFor example, for QDateTime return a QDateTimeEdit(parent)\n\"\"\"\n", "func_signal": "def createWidget(self, qt_parent=None):\n", "code": "sb = QDoubleSpinBox(qt_parent)\nsb.setFrame(False)\nsb.setMinimum(-float_info.max)\nsb.setMaximum(float_info.max)\nreturn sb", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"\nGets the factors of a number and adds extra numbers so that\neach number is no more than double the previous.\n\nIf this doesn't work out, remove base argument and go back\nto using 10 as base and [1,2,5] as multipliers.\n\"\"\"\n", "func_signal": "def __get_scale_multipliers(self, base):\n", "code": "multipliers = []\nfor fact in self.__factor_positive_integer(base):\n\tif len(multipliers) == 0:\n\t\t# Initialise list\n\t\tmultipliers.append(fact)\n\t\tcontinue\n\twhile multipliers[-1]*2 < fact:\n\t\tmultipliers.append(multipliers[-1]*2)\n\tmultipliers.append(fact)\nreturn multipliers", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "# Sanity check: if no lines, or there are lines, but none with\n# data points, then the __update_* functions will store max/min\n# as None.\n\n", "func_signal": "def _update_data(self, id):\n", "code": "oldmax = self.max\noldmin = self.min\n\nself.__update_max()\nself.__update_min()\n\n# See if max/min has changed and update if so\nif oldmax != self.max or oldmin != self.min:\n\tself._update_scale()\n\tself.changed.emit()", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"Private!!\"\"\"\n", "func_signal": "def get_fset(mem, type, sig):\n", "code": "def fset(self, value):\n\ttry:\n\t\tif self.__getattribute__(mem) == value:\n\t\t\treturn\n\texcept AttributeError:\n\t\t# First time, set the value\n\t\tpass\n\t# delete() equivalent here if needed\n\t# Filter value through new() here if needed\n\tself.__setattr__(mem, value)\n\ttry_connect_signals(self, mem, sig)\n\t# Emit the filtered value, not the given value\n\tself.__getattribute__(sig).emit(self.__getattribute__(mem))\nreturn fset", "path": "Base\\property.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"\nUpdate the axis about line data change.\n\nSee add_line for notes.\n\"\"\"\n", "func_signal": "def change_line(self, id, data):\n", "code": "self.lines[id] = self.__variants_to_floats(data)\nself._update_data(id)", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "# Delete previous text item\n", "func_signal": "def update(self):\n", "code": "if self.text_item != None:\n\tself.scene.removeItem(self.text_item)\n\t# Line below not for QGraphicsSimpleTextItem\n\tself.text_item.deleteLater()\n\tself.scene.removeItem(self.rect)\n\n# Exit if any parameters unset\nif self.text.text == None or self.text.style == None:\n\treturn\n\nself.text_item = self.get_text_item()\nsize = self.text_item.boundingRect()\nw = size.width()\nh = size.height()\ntransform = self.transform(self.dimensions, w, h)\nself.text_item.setTransform(transform)\nself.scene.addItem(self.text_item)\n\nself.rect = QGraphicsRectItem(0,0, w,h)\nself.rect.setTransform(transform)\nif config.debug.plotter.show_text_boxes:\n\tself.scene.addItem(self.rect)", "path": "Graph\\plotter.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"\nScaling requirements:\n- The graph should use round units. By this I mean that a power\n\tof 10 multiplied by a factor of 10 should be used as the unit.\n- For each axis:\n\t- The lowest value should be as close to the origin as possible.\n\t- The highest value should be as high as possible; preferably\n\t\tin the top half of the axis.\n\nSolution:\nstart with power of 10 below range/axis_length\nfor increasing powers of 10:\n\tfor multiplier = 1, 2, 2.5, 4, 5:\n\t\taxis start is min rounded down to\n\t\t\tnearest 10^(power+1) * multiplier\n\t\tscale is 10^power * multiplier\n\t\t\t(for small squares => *10 for big squares)\n\t\tfind half of axis and top of axis\n\t\tif max is in top half of axis, stop\n\t\telse continue.\n\nNote that since our aim is the top _half_ of the axis, the\nmultipliers need to be at most double the previous one.\nAlso experiment with more multipliers than less for a better fit.\n\nIf the multipliers are not all at most double the previous,\nnothing catastrophic will happen: the scale might be a bit\ntoo accommodating, though.\n\nGiven scale/start:\n\tUse that as the temporary value for scale/start.\n\tIt will be reassigned to itself each iteration.\n\tNo guarantee that my algorithm will get the max value in\n\t\tthe top half of the axis, so\n\t\tif we overshoot (max is above top of axis) then use\n\t\tthe results from the previous iteration.\n\nC++ version of the algorithm worked backwards:\nfor decreasing powers of 10:\n\tfor multiplier = 5, 2, 1:\n\"\"\"\n# Sanity checks\n\n# Check we have any lines.\n# Set scale to None to imply an invalid scale.\n", "func_signal": "def _autocalc_scale(self, base):\n", "code": "if len(self.lines) == 0:\n\tself.scale = None\n\treturn\n\n# If there are lines but no data points,\n# self.max and self.min will be None.\nif self.max is None:\n\tself.scale = None\n\treturn\n\n# If axis length is unset, we can't calculate scales.\nif self.length < 1:\n\tself.scale = None\n\treturn\n\n# We're gonna have problems if we only have one point!\n# We could return an invalid scale, but we can\n# work out start and fudge scale instead.\nif self.max == self.min:\n\t# Set one of them to 0 or, failing that, 1.\n\tif self.min == 0:\n\t\tself.max = 1\n\telif self.min > 0:\n\t\tself.min = 0\n\telse:\n\t\tself.max = 0\n\n# Start with power of 10 below range/length\nrange = self.max - self.min\nunit_range = range/self.length\nmagnitude = base ** math.floor(math.log(unit_range, base))\n\nmultipliers = self.__get_scale_multipliers(base)\n\n# Heh heh pun\never = itertools.count()\nfor ever in ever:\n\tfor multiplier in multipliers:\n\t\tunit_scale = magnitude * multiplier\n\t\tstart = self.__get_scale_start(unit_scale, base)\n\t\thalfway = start + (unit_scale * self.length / 2)\n\t\tend = start + (unit_scale * self.length)\n\n\t\t# See if scale contains the largest value\n\t\tif end > self.max:\n\t\t\t# Write scale_start, scale_end and scale\n\t\t\tself.scale_start = QVariant(start)\n\t\t\tself.scale_end = QVariant(end)\n\t\t\tself.scale = unit_scale\n\t\t\treturn\n\t# Increase magnitude\n\tmagnitude = magnitude * base", "path": "types\\float.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"Restore current module state (multi-use - for non-stacked use)\"\"\"\n", "func_signal": "def reset_env():\n", "code": "restore_env()\nsave_env()", "path": "debug.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"Private!!\"\"\"\n", "func_signal": "def get_fget(mem, type, sig, default):\n", "code": "def fget(self):\n\ttry:\n\t\tself.__getattribute__(mem)\n\texcept AttributeError:\n\t\t# Need to have the behaviour that an unset default gives\n\t\t# a default value => special-case default==None.\n\t\tif default is None:\n\t\t\tself.__setattr__(mem, type())\n\t\telse:\n\t\t\tself.__setattr__(mem, default)\n\t\ttry_connect_signals(self, mem, sig)\n\treturn self.__getattribute__(mem)\nreturn fget", "path": "Base\\property.py", "repo_name": "khedron/Plot", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 173}
{"docstring": "\"\"\"Setting tag[key] sets the value of the 'key' attribute for the\ntag.\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self._getAttrMap()\nself.attrMap[key] = value\nfound = False\nfor i in range(0, len(self.attrs)):\n    if self.attrs[i][0] == key:\n        self.attrs[i] = (key, value)\n        found = True\nif not found:\n    self.attrs.append((key, value))\nself._getAttrMap()[key] = value", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is listlike.\"\"\"\n", "func_signal": "def isList(l):\n", "code": "return ((hasattr(l, '__iter__') and not isString(l))\n        or (type(l) in (types.ListType, types.TupleType)))", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML and/or XML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.soup.convertHTMLEntities:\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\n\nif not data and self.soup.convertXMLEntities:\n        data = self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)\n\nif not data and self.soup.convertHTMLEntities and \\\n    not self.soup.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):\n        # TODO: We've got a problem here. We're told this is\n        # an entity reference, but it's not an XML entity\n        # reference or an HTML entity reference. Nonetheless,\n        # the logical thing to do is to pass it through as an\n        # unrecognized entity reference.\n        #\n        # Except: when the input is \"&carol;\" this function\n        # will be called with input \"carol\". When the input is\n        # \"AT&T\", this function will be called with input\n        # \"T\". We have no way of knowing whether a semicolon\n        # was present originally, so we don't know whether\n        # this is an unknown entity or just a misplaced\n        # ampersand.\n        #\n        # The more common case is a misplaced ampersand, so I\n        # escape the ampersand and omit the trailing semicolon.\n        data = \"&amp;%s\" % ref\nif not data:\n    # This case is different from the one above, because we\n    # haven't already gone through a supposedly comprehensive\n    # mapping of entities to Unicode characters. We might not\n    # have gone through any mapping at all. So the chances are\n    # very high that this is a real entity, and not a\n    # misplaced ampersand.\n    data = \"&%s;\" % ref\nself.handle_data(data)", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Create a new NavigableString.\n\nWhen unpickling a NavigableString, this method is called with\nthe string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be\npassed in to the superclass's __new__ or the superclass won't know\nhow to handle non-ASCII characters.\n\"\"\"\n", "func_signal": "def __new__(cls, value):\n", "code": "if isinstance(value, unicode):\n    return unicode.__new__(cls, value)\nreturn unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = HTMLParser.parse_declaration(self, i)\n    except HTMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value\nreturn self.attrMap", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\nentities with the appropriate Unicode characters. If HTML\nentities are being converted, any unrecognized entities are\nescaped.\"\"\"\n", "func_signal": "def _convertEntities(self, match):\n", "code": "x = match.group(1)\nif self.convertHTMLEntities and x in name2codepoint:\n    return unichr(name2codepoint[x])\nelif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\n    if self.convertXMLEntities:\n        return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\n    else:\n        return u'&%s;' % x\nelif len(x) > 0 and x[0] == '#':\n    # Handle numeric entities\n    if len(x) > 1 and x[1] == 'x':\n        return unichr(int(x[2:], 16))\n    else:\n        return unichr(int(x[1:]))\n\nelif self.escapeUnrecognizedEntities:\n    return u'&amp;%s;' % x\nelse:\n    return u'&%s;' % x", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Destructively rips this element out of the tree.\"\"\"\n", "func_signal": "def extract(self):\n", "code": "if self.parent:\n    try:\n        self.parent.contents.remove(self)\n    except ValueError:\n        pass\n\n#Find the two elements that would be next to each other if\n#this element (and any children) hadn't been parsed. Connect\n#the two.\nlastChild = self._lastRecursiveChild()\nnextElement = lastChild.next\n\nif self.previous:\n    self.previous.next = nextElement\nif nextElement:\n    nextElement.previous = self.previous\nself.previous = None\nlastChild.next = None\n\nself.parent = None\nif self.previousSibling:\n    self.previousSibling.nextSibling = self.nextSibling\nif self.nextSibling:\n    self.nextSibling.previousSibling = self.previousSibling\nself.previousSibling = self.nextSibling = None\nreturn self", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Changes a MS smart quote character to an XML or HTML\nentity.\"\"\"\n", "func_signal": "def _subMSChar(self, match):\n", "code": "orig = match.group(1)\nsub = self.MS_CHARS.get(orig)\nif type(sub) == types.TupleType:\n    if self.smartQuotesTo == 'xml':\n        sub = '&#x'.encode() + sub[1].encode() + ';'.encode()\n    else:\n        sub = '&'.encode() + sub[0].encode() + ';'.encode()\nelse:\n    sub = sub.encode()\nreturn sub", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is stringlike.\"\"\"\n", "func_signal": "def isString(s):\n", "code": "try:\n    return isinstance(s, unicode) or isinstance(s, basestring)\nexcept NameError:\n    return isinstance(s, str)", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and\nNESTING_RESET_TAGS maps out of lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif isList(portion) and not isString(portion):\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Encodes an object to a string in some encoding, or to Unicode.\n.\"\"\"\n", "func_signal": "def toEncoding(self, s, encoding=None):\n", "code": "if isinstance(s, unicode):\n    if encoding:\n        s = s.encode(encoding)\nelif isinstance(s, str):\n    if encoding:\n        s = s.encode(encoding)\n    else:\n        s = unicode(s)\nelse:\n    if encoding:\n        s  = self.toEncoding(str(s), encoding)\n    else:\n        s = unicode(s)\nreturn s", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=None, previous=None):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = None\nself.previousSibling = None\nself.nextSibling = None\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None, isHTML=False):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\n    self.declaredHTMLEncoding = dammit.declaredHTMLEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE\n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\n        # TODO: We get rid of markupMassage so that the\n        # soup object can be deepcopied later on. Some\n        # Python installations can't copy regexes. If anyone\n        # was relying on the existence of markupMassage, this\n        # might cause problems.\n        del(self.markupMassage)\nself.builder.reset()\n\nself.builder.feed(markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Returns either the given Unicode string or its encoding.\"\"\"\n", "func_signal": "def sob(unicode, encoding):\n", "code": "if encoding is None:\n    return unicode\nelse:\n    return unicode.encode(encoding)", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Recursively destroys the contents of this tree.\"\"\"\n", "func_signal": "def decompose(self):\n", "code": "contents = [i for i in self.contents]\nfor i in contents:\n    if isinstance(i, Tag):\n        i.decompose()\n    else:\n        i.extract()\nself.extract()", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data, isHTML=False):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\nexcept:\n    xml_encoding_match = None\nxml_encoding_re = '^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>'.encode()\nxml_encoding_match = re.compile(xml_encoding_re).match(xml_data)\nif not xml_encoding_match and isHTML:\n    meta_re = '<\\s*meta[^>]+charset=([^>]*?)[;\\'\">]'.encode()\n    regexp = re.compile(meta_re, re.I)\n    xml_encoding_match = regexp.search(xml_data)\nif xml_encoding_match is not None:\n    xml_encoding = xml_encoding_match.groups()[0].decode(\n        'ascii').lower()\n    if isHTML:\n        self.declaredHTMLEncoding = xml_encoding\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "BeautifulSoup.py", "repo_name": "cheeaun/toransureto", "stars": 4, "license": "None", "language": "python", "size": 104}
{"docstring": "\"\"\" Trait initializer. \"\"\"\n\n", "func_signal": "def _about_dialog_default(self):\n", "code": "about_dialog = AboutDialog(\n    parent = self.workbench.active_window.control,\n    image  = ImageResource('about')\n)\n\nreturn about_dialog", "path": "chaco_brain_app.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "'''params: a list of numpy arrays'''\n", "func_signal": "def __init__(self, fnames=None, params=None):\n", "code": "super(MovementParamPlot, self).__init__()\n\nif fnames is None:\n    # XXX Hardcode hack to get envisage working\n    fnames = ['data/movement_params.txt']\n    params = np.recfromtxt(fnames[0], dtype=PARAM_DTYPE)\n\nself.file_list = fnames\nself.file_name = fnames[0]\nself.params_num = 0\nself.max_params = len(params) - 1\n\nself.params = params\n\n# XXX - the following info should go (soon) into \n# nipype.interfaces.fsl.McFLIRT\nself.trans_plot = self.create_line_plot('trans', 'mm')\nself.rot_plot = self.create_line_plot('rot', 'radians (clockwise)',\n                                      self.trans_plot.index_range)\n\n# Note - it doesn't matter which plot you use to init the tools\nself.zoom = tools.ZoomTool(self.trans_plot, tool_mode='range', \n                           axis='index')\nself.pan = tools.PanTool(self.rot_plot, constrain=True,\n                         constrain_direction='x')\nself.trans_plot.tools.extend((self.zoom, self.pan))\nself.rot_plot.tools.extend((self.pan, self.zoom))", "path": "plot_params.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\"Set the colormap of the plot.\n\nParameters\n----------\ncmap : str or chaco callable colormap\n    A colormap name or a colormap from\n    enthought.chaco.default_colormaps\n\n\"\"\"\n\n", "func_signal": "def set_colormap(self, cmap):\n", "code": "clr_range = self.renderer.color_mapper.range\ntry:\n    # Try chaco colormap\n    color_mapper = cmap(clr_range)\nexcept TypeError:\n    # If cmap is not callable (like if it's a string)\n    try:\n        # Try cmap as a string\n        cmap = chaco_colormaps.color_map_name_dict[cmap]\n        color_mapper = cmap(clr_range)\n    except KeyError:\n        # Not a string assume a chaco colormap function\n        msg = \"Unable to find colormap '%s'\" % cmap\n        raise KeyError(msg)\nself.renderer.color_mapper = color_mapper\nself.redraw()", "path": "slice_view.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Update image slices based on selecte voxel coords.\n", "func_signal": "def update_slices(self):\n", "code": "axial = self.img.get_axial_slice(self.voxel.z)\ncoronal = self.img.get_coronal_slice(self.voxel.y)\nsagittal = self.img.get_sagittal_slice(self.voxel.x)\n\nif self.plotdata is None:\n    # Create array data container\n    self.plotdata = ArrayPlotData(axial=axial, \n                                  sagittal=sagittal, \n                                  coronal=coronal)\nelse:\n    self.plotdata.set_data('axial', axial)\n    self.plotdata.set_data('coronal', coronal)\n    self.plotdata.set_data('sagittal', sagittal)", "path": "slice_plot.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# XXX Need to think about better way to handle preferences.  Just\n# handling colormap for now.  Need to abstract preferences into an\n# object that can be serialized and easily passed between objects\n# (a dict?).  Also when we open the preferences, the value in the\n# drop-down list is just the first in the chaco_colormaps dict,\n# not the current colormap of the plots.  Again, a more robust\n# solution would fix this.\n", "func_signal": "def launch_prefs(caller):\n", "code": "prefs = Preferences()\nprefs.edit_traits(kind='modal')\ntry:\n    caller.update_preferences(prefs.cmap)\nexcept AttributeError:\n    pass", "path": "slice_plot.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# XXX implement real slicing.  Assuming xyz ordering.\n", "func_signal": "def get_coronal_slice(self, yindex):\n", "code": "data = self.data[:, yindex, :]\n# transpose so it's C ordered\nreturn data.T", "path": "image.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Adding a cursor to the axial plot to test cursor\n# functionality in Chaco\n", "func_signal": "def init_cursor(self):\n", "code": "self.cursor = CursorTool(self.renderer, drag_button='left', \n                         color='blue')\nx, y = self.data.get_data(self.slicename).shape\nself.cursor.current_position = x/2, y/2\n#self.cursor.current_position = self.voxel.x, self.voxel.y\nself.renderer.overlays.append(self.cursor)\nself.renderer.tools.append(Crosshairs(self.renderer))", "path": "slice_plot.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "'''main(fnames): simply reads each file in fnames'''\n", "func_signal": "def main(fnames, verbose=False, plot=True):\n", "code": "if verbose:\n    simplefilter('always', UserWarning)\nelse:\n    simplefilter('ignore', UserWarning)\n\n# Allow passing in a single string\nif isinstance(fnames, str):\n    fnames = [fnames]\n\nparams = []\nfor fname in fnames:\n    params.append(np.recfromtxt(fname, dtype=PARAM_DTYPE))\n\nplot_obj = MovementParamPlot(fnames, params)\nplot_obj.configure_traits()\nreturn plot_obj", "path": "plot_params.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Voxel(x, y, z)\n", "func_signal": "def __repr__(self):\n", "code": "outstr = 'Voxel(%d, %d, %d)' % (self.x, self.y, self.z)\nreturn outstr", "path": "slice_view.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Adding a cursor to the axial plot to test cursor\n# functionality in Chaco\n", "func_signal": "def init_cursor(self):\n", "code": "self.cursor = CursorTool(self.renderer, drag_button='left', \n                         color='blue')\nx, y = self.data.get_data(self.slicename).shape\nself.cursor.current_position = x/2, y/2\n#self.cursor.current_position = self.voxel.x, self.voxel.y\nself.renderer.overlays.append(self.cursor)\nself.renderer.tools.append(Crosshairs(self.renderer))", "path": "slice_view.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Load image\n", "func_signal": "def load_image(self, info=None):\n", "code": "img, filename = load_image()\nif img is not None:\n    self.img = img\n    self.filename = filename\n    xdim, ydim, zdim = self.img.shape\n    if self.voxel is None:\n        self.voxel = Voxel(x=xdim/2, y=ydim/2, z=zdim/2)\n    else:\n        self.voxel.x = xdim/2\n        self.voxel.y = ydim/2\n        self.voxel.z = zdim/2\n    self.update_slices()", "path": "slice_plot.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Map event coords to data coords.\n", "func_signal": "def map_data(self, event):\n", "code": "x = self.component.x_mapper.map_data(event.x)\ny = self.component.y_mapper.map_data(event.y)\nreturn x, y", "path": "slice_plot.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# XXX implement real slicing.  Assuming xyz ordering.\n", "func_signal": "def get_sagittal_slice(self, xindex):\n", "code": "data = self.data[xindex, :, :]\n# transpose so it's C ordered\nreturn data.T", "path": "image.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# XXX implement real slicing.  Assuming xyz ordering.\n", "func_signal": "def get_axial_slice(self, zindex):\n", "code": "data = self.data[:, :, zindex]\n# transpose so it's C ordered\nreturn data.T", "path": "image.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Map event coords to data coords.\n", "func_signal": "def map_data(self, event):\n", "code": "x = self.component.x_mapper.map_data(event.x)\ny = self.component.y_mapper.map_data(event.y)\nreturn x, y", "path": "slice_view.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Load image\n# XXX HACK\n", "func_signal": "def load_image(self, info=None):\n", "code": "import os\nfilename = os.path.expanduser('~/data/MNI152_T1_2mm.nii.gz')\nimg = Image(filename)\n#img, filename = load_image()\nif img is not None:\n    self.img = img\n    self.filename = filename\n    xdim, ydim, zdim = self.img.shape\n    if self.voxel is None:\n        self.voxel = Voxel(x=xdim/2, y=ydim/2, z=zdim/2)\n    else:\n        self.voxel.x = xdim/2\n        self.voxel.y = ydim/2\n        self.voxel.z = zdim/2\n    self.update_slices()", "path": "slice_view.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Update image slices based on selecte voxel coords.\n", "func_signal": "def update_slices(self):\n", "code": "axial = self.img.get_axial_slice(self.voxel.z)\ncoronal = self.img.get_coronal_slice(self.voxel.y)\nsagittal = self.img.get_sagittal_slice(self.voxel.x)\n\nif self.plotdata is None:\n    # Create array data container\n    self.plotdata = ArrayPlotData(axial=axial, \n                                  sagittal=sagittal, \n                                  coronal=coronal)\nelse:\n    self.plotdata.set_data('axial', axial)\n    self.plotdata.set_data('coronal', coronal)\n    self.plotdata.set_data('sagittal', sagittal)", "path": "slice_view.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "# Voxel(x, y, z)\n", "func_signal": "def __repr__(self):\n", "code": "outstr = 'Voxel(%d, %d, %d)' % (self.x, self.y, self.z)\nreturn outstr", "path": "slice_plot.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\"Handler for File > Open menu\"\"\"\n", "func_signal": "def load_image():\n", "code": "file_name = File\nfile_name = open_file()\nif file_name != '':\n    img = Image(file_name)\n    return img, file_name\nelse:\n    return None, None", "path": "slice_view.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\"Handler for File > Open menu\"\"\"\n", "func_signal": "def load_image():\n", "code": "file_name = File\nfile_name = open_file()\nif file_name != '':\n    img = Image(file_name)\n    return img, file_name\nelse:\n    return None, None", "path": "slice_plot.py", "repo_name": "cburns/chaco_brain", "stars": 4, "license": "None", "language": "python", "size": 99}
{"docstring": "\"\"\"this is the callback activation method if the bound object is a method.\n\nif the bound object is a function, function_call will be used instead. It differs because methods will be called on each instance.\n\"\"\"\n", "func_signal": "def method_call(self, event):\n", "code": "if self.filter_check(event):\n    self.instances = filter(lambda x: x() is not None, self.instances)\n    for instance in self.instances:\n        self.func(instance(), event)", "path": "event.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"unbinds all event handlers for this instance. The worst thing that can be done to an object short of actually destroying it\"\"\"\n", "func_signal": "def kill(self):\n", "code": "for name, method in inspect.getmembers(self, lambda mem: inspect.ismethod(mem) and hasattr(mem, 'binders')):\n    for binder in method.binders:\n        binder.instances = filter(lambda i: i() is not self, binder.instances)", "path": "event.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"initialize a sprite object. The img argument must be a pygame.Surface object\"\"\"\n", "func_signal": "def __init__(self, img=None, pos=(0,0), speed=(0,0)):\n", "code": "gunge.event.Handler.__init__(self)\n\nself.img = img\nself.rect = self.img.get_rect() if type(self.img) is pygame.Surface else pygame.Rect(0, 0, 0, 0)\nself.rect.topleft = pos\nself.prev_rect = self.rect\n\nself.speed = list(speed)\nself.hidden = False", "path": "sprite.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"locate(filename, alpha=False) -> pygame.Surface\nuses the base class locate to load the file, then performs a little post-processing.\nIf you need per-pixel alpha, you can't use indexing notation and you'll have to preload\nwith that option explicitly\n\"\"\"\n", "func_signal": "def locate(self, filename, alpha=False):\n", "code": "image = ResourceLoader.locate(self, filename)\nif alpha:\n    image.convert_alpha()\nelse:\n    image.convert()\nreturn image", "path": "media.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"preload(self, *filenames) -> None\nthis can be used to preload some files before you actually need them.\nThis can be usefull to prevent latencies during a game. Since you don't\nneed the resources just yet, nothing is returned.\n\"\"\"\n", "func_signal": "def preload(self, *filenames):\n", "code": "for filename in filenames:\n    dict.__setitem__(self, filename, self.locate(filename))", "path": "media.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"intialize event handlers\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "for name, method in inspect.getmembers(self, lambda mem: inspect.ismethod(mem) and hasattr(mem, 'binders')):\n    for binder in method.binders:\n        binder.instances.append(weakref.ref(self))", "path": "event.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"set the amount of ticks per second of the clock. Can only be done before the clock starts\"\"\"\n", "func_signal": "def set_tps(self, tps):\n", "code": "if self.started:\n    raise Exception(\"can not set tps if the clock already started\")\n\nself.tps = tps\nself.tick = 1. / tps", "path": "clock.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"indexing operation -> [resource]\nKey is usually the filename of something. What is returned depends on the filetype being loaded\n\"\"\"\n", "func_signal": "def __missing__(self, key):\n", "code": "dict.__setitem__(self, key, self.locate(key))\nreturn dict.__getitem__(self, key)", "path": "media.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"called on an UPDATE event. should be the first handler to be called.\n\nchecks if doing an update now would bring game time closer to real time than it currently is. If so, the update is\nallowed. Otherwise, StopHandling is raised to stop the update from happening.\n\"\"\"\n", "func_signal": "def update(self, event):\n", "code": "if not self.started:\n    return\n\ntime_now = self.get_time()\ntime_passed = time_now - self.clock_time\n\nself.clock_time = time_now\nself.time += time_passed\nself.game_latency += time_passed\n\nif self.game_latency - self.tick > 0:\n#if abs(self.game_latency - self.tick) < abs(self.game_latency):\n    #this means that updating now would bring the game time closer to real time\n    self.game_latency -= self.tick\n    self.game_time += self.tick\n    return gunge.event.HANDLE_AGAIN\nelse:\n    #if that is not the case, we should not update\n    self.interpolate = (self.time - self.game_time) / self.tick\n    return gunge.event.HANDLE_STOP", "path": "clock.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"locate(filename) -> pygame.mixer.Sound\nif the pygame.mixer module is not loaded, a dummy soundclass is returned\nwhich does nothing.\n\"\"\"\n", "func_signal": "def locate(self, filename):\n", "code": "if not pygame.mixer:\n    return NoSound()\nreturn ResourceLoader.locate(self, filename)", "path": "media.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"create a new event binder for the specified type\n\nThe callback will be invoked if the event occurs, provided the binder is actually bound in the manager. The attr_filter can be used to specify\nconstraints that certain attributes of the event must satisfy for the callback to be invoked. It is a dictionary of the form {'attr_name': filter_value}\n\nThe filter value can have several different types, with different behaviours:\n- set:      passes if event.attr_name in filter_value\n- function: passes if filter_value(event.attr_name) is True\n- other:    passes if filter_value == event.attr_name\n\"\"\"\n", "func_signal": "def __init__(self, eventtype, attr_filter, callback):\n", "code": "self.type = eventtype\nself.filter = attr_filter\nself.func = callback\n\nif len(inspect.getargspec(callback)[0]) == 2:\n    self.instances = []\n    self.call = self.method_call\nelse:\n    self.call = self.function_call", "path": "event.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"unbind previously bound handler\n\nThe event_type argument is only needed if the handler argument is not a Binder instance\n\"\"\"\n", "func_signal": "def unbind(self, handler, event_type=None):\n", "code": "event_type = event_type or handler.type\nself.handlers[event_type].remove(handler)", "path": "event.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"check whether the filter matches the event.\"\"\"\n", "func_signal": "def filter_check(self, event):\n", "code": "for key, value in self.filter.items():\n    event_key = getattr(event, key)\n    #set: pass if any of the sets' values matches the event attribute\n    if type(value) is set:\n        if event_key not in value: return False\n    #function: pass if the function returns True when called with the event attribute\n    elif inspect.isroutine(value):\n        if not value(event_key): return False\n    #anything else: pass if the value matches the event attribute\n    elif value != event_key:\n        return False\n\nreturn True", "path": "event.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"initialize the clock.\n\ntps = ticks per second, the amount of game updates allowed per second\n\"\"\"\n", "func_signal": "def __init__(self, tps=25):\n", "code": "gunge.event.Handler.__init__(self)\nself.tps = tps\nself.tick = 1. / tps\nself.started = False\nself.interpolate = 1.\n\n# time.clock is a low resolution timer on some systems, use time.time instead if it is\nself.get_time = time.clock if time.clock() - time.clock() != 0 else time.time", "path": "clock.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"decorator that can be used to statically bind methods. the first argument is a dict that must be declared as a class variable\n\nNote that this decorator must be used with an object derived from the Handler class, as the actual binding is done in the __init__ of that class.\nIt is perfectly legal for a function to have multiple event bindings, and the bind decorator handles this.\n\"\"\"\n", "func_signal": "def bind(eventtype, attr_filter=None):\n", "code": "if attr_filter is None:\n    attr_filter = {}\n\ndef decorator(func):\n    if len(inspect.getargspec(func)[0]) > 2:\n        raise ValueError(\"Function does not have correct number of arguments (expected (self, event) or (event))\")\n    binder = Binder(eventtype, attr_filter, func)\n    manager.bind(binder)\n\n    try:\n        func.binders.append(binder)\n    except AttributeError:\n        func.binders = [binder]\n    return func\nreturn decorator", "path": "event.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"bind handler object. The handler's callback will be called if the event occurs\n\nThe event_type argument is only needed if the handler argument is not a Binder instance\n\"\"\"\n", "func_signal": "def bind(self, handler, event_type=None):\n", "code": "event_type = event_type or handler.type\nif event_type not in self.handlers:\n    self.handlers[event_type] = []\n\nself.handlers[event_type].append(handler)", "path": "event.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"cosine interpolation between points prev and new. Slower than linear, but much smoother\"\"\"\n", "func_signal": "def cosine_interpolate(prev, new, interpolation):\n", "code": "interpolation = (1 - math.cos(interpolation * math.pi)) / 2.\nreturn lerp(prev, new, interpolation)", "path": "sprite.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"preload(self, tuples) -> None\npreloads a list of files. The arguments are all tuples with the filename\nfirst, and an alpha argument second which specifies wether you want per-pixel alpha.\n\"\"\"\n", "func_signal": "def preload(self, *tuples):\n", "code": "for args in tuples:\n    self.resources[filename] = self.locate(*args)", "path": "media.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"main loop of the program, dispatches events to handlers\"\"\"\n", "func_signal": "def mainloop(self):\n", "code": "exit_events = [pygame.event.Event(UPDATE, {}),\n               pygame.event.Event(RENDER, {'interpolate': lambda: gunge.locals.clock.interpolate, 'display': gunge.locals.display}),\n               pygame.event.Event(BUFSWAP, {})]\nself.keeprunning = True\nwhile self.keeprunning:\n    events = pygame.event.get() + exit_events\n    while not len(events) == 0:\n        event = events.pop()\n        for handler in self.handlers.get(event.type, []):\n            a = handler(event)\n            if a == HANDLE_STOP:\n                break\n            elif a == HANDLE_AGAIN:\n                events.insert(0, event)", "path": "event.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"start the clock\n\nYou'll want to start handling events soon after you call this. It will expect update to be called soon after\nIf you wait with the mainloop too long, the game time will lag behind the real time too much, resulting in a flood of\nupdates without any renders.\n\"\"\"\n", "func_signal": "def start(self):\n", "code": "self.clock_time = self.get_time()\nself.since_last_render = self.get_time()\n\nself.time = 0\nself.game_time = 0\nself.game_latency = 0\nself.last_update = 0\n\nself.frame_count = 0\nself.frame_rate = 0\nself.average_framerate = 0\n\nself.started = True", "path": "clock.py", "repo_name": "HugoArts/gunge", "stars": 7, "license": "None", "language": "python", "size": 502}
{"docstring": "\"\"\"URL, filename, or string --> stream\n\nThis function lets you define parsers that take any input source\n(URL, pathname to local or network file, or actual data as a string)\nand deal with it in a uniform manner.  Returned object is guaranteed\nto have all the basic stdio read methods (read, readline, readlines).\nJust .close() the object when you're done with it.\n\nIf the etag argument is supplied, it will be used as the value of an\nIf-None-Match request header.\n\nIf the modified argument is supplied, it can be a tuple of 9 integers\n(as returned by gmtime() in the standard Python time module) or a date\nstring in any format supported by feedparser. Regardless, it MUST\nbe in GMT (Greenwich Mean Time). It will be reformatted into an\nRFC 1123-compliant date and used as the value of an If-Modified-Since\nrequest header.\n\nIf the agent argument is supplied, it will be used as the value of a\nUser-Agent request header.\n\nIf the referrer argument is supplied, it will be used as the value of a\nReferer[sic] request header.\n\nIf handlers is supplied, it is a list of handlers used to build a\nurllib2 opener.\n\"\"\"\n\n", "func_signal": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers):\n", "code": "if hasattr(url_file_stream_or_string, 'read'):\n    return url_file_stream_or_string\n\nif url_file_stream_or_string == '-':\n    return sys.stdin\n\nif urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):\n    if not agent:\n        agent = USER_AGENT\n    # test for inline user:password for basic auth\n    auth = None\n    if base64:\n        urltype, rest = urllib.splittype(url_file_stream_or_string)\n        realhost, rest = urllib.splithost(rest)\n        if realhost:\n            user_passwd, realhost = urllib.splituser(realhost)\n            if user_passwd:\n                url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)\n                auth = base64.encodestring(user_passwd).strip()\n\n    # iri support\n    try:\n        if isinstance(url_file_stream_or_string,unicode):\n            url_file_stream_or_string = url_file_stream_or_string.encode('idna')\n        else:\n            url_file_stream_or_string = url_file_stream_or_string.decode('utf-8').encode('idna')\n    except:\n        pass\n\n    # try to open with urllib2 (to use optional headers)\n    request = urllib2.Request(url_file_stream_or_string)\n    request.add_header('User-Agent', agent)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if type(modified) == type(''):\n        modified = _parse_date(modified)\n    if modified:\n        # format into an RFC 1123-compliant timestamp. We can't use\n        # time.strftime() since the %a and %b directives can be affected\n        # by the current locale, but RFC 2616 states that dates must be\n        # in English.\n        short_weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n        request.add_header('If-Modified-Since', '%s, %02d %s %04d %02d:%02d:%02d GMT' % (short_weekdays[modified[6]], modified[2], months[modified[1] - 1], modified[0], modified[3], modified[4], modified[5]))\n    if referrer:\n        request.add_header('Referer', referrer)\n    if gzip and zlib:\n        request.add_header('Accept-encoding', 'gzip, deflate')\n    elif gzip:\n        request.add_header('Accept-encoding', 'gzip')\n    elif zlib:\n        request.add_header('Accept-encoding', 'deflate')\n    else:\n        request.add_header('Accept-encoding', '')\n    if auth:\n        request.add_header('Authorization', 'Basic %s' % auth)\n    if ACCEPT_HEADER:\n        request.add_header('Accept', ACCEPT_HEADER)\n    request.add_header('A-IM', 'feed') # RFC 3229 support\n    opener = apply(urllib2.build_opener, tuple([_FeedURLHandler()] + handlers))\n    opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent\n    try:\n        return opener.open(request)\n    finally:\n        opener.close() # JohnD\n\n# try to open with native open function (if url_file_stream_or_string is a filename)\ntry:\n    return open(url_file_stream_or_string)\nexcept:\n    pass\n\n# treat url_file_stream_or_string as string\nreturn _StringIO(str(url_file_stream_or_string))", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Changes an XML data stream on the fly to specify a new encoding\n\ndata is a raw sequence of bytes (not Unicode) that is presumed to be in %encoding already\nencoding is a string recognized by encodings.aliases\n'''\n", "func_signal": "def _toUTF8(data, encoding):\n", "code": "if _debug: sys.stderr.write('entering _toUTF8, trying encoding %s\\n' % encoding)\n# strip Byte Order Mark (if present)\nif (len(data) >= 4) and (data[:2] == '\\xfe\\xff') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16be':\n            sys.stderr.write('trying utf-16be instead\\n')\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16le':\n            sys.stderr.write('trying utf-16le instead\\n')\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-8':\n            sys.stderr.write('trying utf-8 instead\\n')\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32be':\n            sys.stderr.write('trying utf-32be instead\\n')\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32le':\n            sys.stderr.write('trying utf-32le instead\\n')\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nif _debug: sys.stderr.write('successfully converted %s data to unicode\\n' % encoding)\ndeclmatch = re.compile('^<\\?xml[^>]*?>')\nnewdecl = '''<?xml version='1.0' encoding='utf-8'?>'''\nif declmatch.search(newdata):\n    newdata = declmatch.sub(newdecl, newdata)\nelse:\n    newdata = newdecl + u'\\n' + newdata\nreturn newdata.encode('utf-8')", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Parse a variety of ISO-8601-compatible formats like 20040105'''\n", "func_signal": "def _parse_date_iso8601(dateString):\n", "code": "m = None\nfor _iso8601_match in _iso8601_matches:\n    m = _iso8601_match(dateString)\n    if m: break\nif not m: return\nif m.span() == (0, 0): return\nparams = m.groupdict()\nordinal = params.get('ordinal', 0)\nif ordinal:\n    ordinal = int(ordinal)\nelse:\n    ordinal = 0\nyear = params.get('year', '--')\nif not year or year == '--':\n    year = time.gmtime()[0]\nelif len(year) == 2:\n    # ISO 8601 assumes current century, i.e. 93 -> 2093, NOT 1993\n    year = 100 * int(time.gmtime()[0] / 100) + int(year)\nelse:\n    year = int(year)\nmonth = params.get('month', '-')\nif not month or month == '-':\n    # ordinals are NOT normalized by mktime, we simulate them\n    # by setting month=1, day=ordinal\n    if ordinal:\n        month = 1\n    else:\n        month = time.gmtime()[1]\nmonth = int(month)\nday = params.get('day', 0)\nif not day:\n    # see above\n    if ordinal:\n        day = ordinal\n    elif params.get('century', 0) or \\\n             params.get('year', 0) or params.get('month', 0):\n        day = 1\n    else:\n        day = time.gmtime()[2]\nelse:\n    day = int(day)\n# special case of the century - is the first year of the 21st century\n# 2000 or 2001 ? The debate goes on...\nif 'century' in params.keys():\n    year = (int(params['century']) - 1) * 100 + 1\n# in ISO 8601 most fields are optional\nfor field in ['hour', 'minute', 'second', 'tzhour', 'tzmin']:\n    if not params.get(field, None):\n        params[field] = 0\nhour = int(params.get('hour', 0))\nminute = int(params.get('minute', 0))\nsecond = int(float(params.get('second', 0)))\n# weekday is normalized by mktime(), we can ignore it\nweekday = 0\ndaylight_savings_flag = -1\ntm = [year, month, day, hour, minute, second, weekday,\n      ordinal, daylight_savings_flag]\n# ISO 8601 time zone adjustments\ntz = params.get('tz')\nif tz and tz != 'Z':\n    if tz[0] == '-':\n        tm[3] += int(params.get('tzhour', 0))\n        tm[4] += int(params.get('tzmin', 0))\n    elif tz[0] == '+':\n        tm[3] -= int(params.get('tzhour', 0))\n        tm[4] -= int(params.get('tzmin', 0))\n    else:\n        return None\n# Python's time.mktime() is a wrapper around the ANSI C mktime(3c)\n# which is guaranteed to normalize d/m/y/h/m/s.\n# Many implementations have bugs, but we'll pretend they don't.\nreturn time.localtime(time.mktime(tm))", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n# Reconstruct the original character reference.\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if ref.startswith('x'):\n    value = unichr(int(ref[1:],16))\nelse:\n    value = unichr(int(ref))\n\nif value in _cp1252.keys():\n    self.pieces.append('&#%s;' % hex(ord(_cp1252[value]))[1:])\nelse:\n    self.pieces.append('&#%(ref)s;' % locals())", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n# Reconstruct the original entity reference.\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if name2codepoint.has_key(ref):\n    self.pieces.append('&%(ref)s;' % locals())\nelse:\n    self.pieces.append('&amp;%(ref)s' % locals())", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Strips DOCTYPE from XML document, returns (rss_version, stripped_data)\n\nrss_version may be 'rss091n' or None\nstripped_data is the same XML document, minus the DOCTYPE\n'''\n", "func_signal": "def _stripDoctype(data):\n", "code": "start = re.search('<\\w',data)\nstart = start and start.start() or -1\nhead,data = data[:start+1], data[start+1:]\n\nentity_pattern = re.compile(r'^\\s*<!ENTITY([^>]*?)>', re.MULTILINE)\nentity_results=entity_pattern.findall(head)\nhead = entity_pattern.sub('', head)\ndoctype_pattern = re.compile(r'^\\s*<!DOCTYPE([^>]*?)>', re.MULTILINE)\ndoctype_results = doctype_pattern.findall(head)\ndoctype = doctype_results and doctype_results[0] or ''\nif doctype.lower().count('netscape'):\n    version = 'rss091n'\nelse:\n    version = None\n\n# only allow in 'safe' inline entity definitions\nreplacement=''\nif len(doctype_results)==1 and entity_results:\n   safe_pattern=re.compile('\\s+(\\w+)\\s+\"(&#\\w+;|[^&\"]*)\"')\n   safe_entities=filter(lambda e: safe_pattern.match(e),entity_results)\n   if safe_entities:\n       replacement='<!DOCTYPE feed [\\n  <!ENTITY %s>\\n]>' % '>\\n  <!ENTITY '.join(safe_entities)\ndata = doctype_pattern.sub(replacement, head) + data\n\nreturn version, data, dict(replacement and safe_pattern.findall(replacement))", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Get the character encoding of the XML document\n\nhttp_headers is a dictionary\nxml_data is a raw string (not Unicode)\n\nThis is so much trickier than it sounds, it's not even funny.\nAccording to RFC 3023 ('XML Media Types'), if the HTTP Content-Type\nis application/xml, application/*+xml,\napplication/xml-external-parsed-entity, or application/xml-dtd,\nthe encoding given in the charset parameter of the HTTP Content-Type\ntakes precedence over the encoding given in the XML prefix within the\ndocument, and defaults to 'utf-8' if neither are specified.  But, if\nthe HTTP Content-Type is text/xml, text/*+xml, or\ntext/xml-external-parsed-entity, the encoding given in the XML prefix\nwithin the document is ALWAYS IGNORED and only the encoding given in\nthe charset parameter of the HTTP Content-Type header should be\nrespected, and it defaults to 'us-ascii' if not specified.\n\nFurthermore, discussion on the atom-syntax mailing list with the\nauthor of RFC 3023 leads me to the conclusion that any document\nserved with a Content-Type of text/* and no charset parameter\nmust be treated as us-ascii.  (We now do this.)  And also that it\nmust always be flagged as non-well-formed.  (We now do this too.)\n\nIf Content-Type is unspecified (input was local file or non-HTTP source)\nor unrecognized (server just got it totally wrong), then go by the\nencoding given in the XML prefix of the document and default to\n'iso-8859-1' as per the HTTP specification (RFC 2616).\n\nThen, assuming we didn't find a character encoding in the HTTP headers\n(and the HTTP Content-type allowed us to look in the body), we need\nto sniff the first few bytes of the XML data and try to determine\nwhether the encoding is ASCII-compatible.  Section F of the XML\nspecification shows the way here:\nhttp://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\n\nIf the sniffed encoding is not ASCII-compatible, we need to make it\nASCII compatible so that we can sniff further into the XML declaration\nto find the encoding attribute, which will tell us the true encoding.\n\nOf course, none of this guarantees that we will be able to parse the\nfeed in the declared character encoding (assuming it was declared\ncorrectly, which many are not).  CJKCodecs and iconv_codec help a lot;\nyou should definitely install them if you can.\nhttp://cjkpython.i18n.org/\n'''\n\n", "func_signal": "def _getCharacterEncoding(http_headers, xml_data):\n", "code": "def _parseHTTPContentType(content_type):\n    '''takes HTTP Content-Type header and returns (content type, charset)\n\n    If no charset is specified, returns (content type, '')\n    If no content type is specified, returns ('', '')\n    Both return parameters are guaranteed to be lowercase strings\n    '''\n    content_type = content_type or ''\n    content_type, params = cgi.parse_header(content_type)\n    return content_type, params.get('charset', '').replace(\"'\", '')\n\nsniffed_xml_encoding = ''\nxml_encoding = ''\ntrue_encoding = ''\nhttp_content_type, http_encoding = _parseHTTPContentType(http_headers.get('content-type'))\n# Must sniff for non-ASCII-compatible character encodings before\n# searching for XML declaration.  This heuristic is defined in\n# section F of the XML specification:\n# http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = _ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        # ASCII-compatible\n        pass\n    xml_encoding_match = re.compile('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode', 'iso-10646-ucs-4', 'ucs-4', 'csucs4', 'utf-16', 'utf-32', 'utf_16', 'utf_32', 'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nacceptable_content_type = 0\napplication_content_types = ('application/xml', 'application/xml-dtd', 'application/xml-external-parsed-entity')\ntext_content_types = ('text/xml', 'text/xml-external-parsed-entity')\nif (http_content_type in application_content_types) or \\\n   (http_content_type.startswith('application/') and http_content_type.endswith('+xml')):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or xml_encoding or 'utf-8'\nelif (http_content_type in text_content_types) or \\\n     (http_content_type.startswith('text/')) and http_content_type.endswith('+xml'):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or 'us-ascii'\nelif http_content_type.startswith('text/'):\n    true_encoding = http_encoding or 'us-ascii'\nelif http_headers and (not http_headers.has_key('content-type')):\n    true_encoding = xml_encoding or 'iso-8859-1'\nelse:\n    true_encoding = xml_encoding or 'utf-8'\n# some feeds claim to be gb2312 but are actually gb18030.\n# apparently MSIE and Firefox both do the following switch:\nif true_encoding.lower() == 'gb2312':\n    true_encoding = 'gb18030'\nreturn true_encoding, http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "# disallow urls\n", "func_signal": "def sanitize_style(self, style):\n", "code": "style=re.compile('url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*').sub(' ',style)\n\n# gauntlet\nif not re.match(\"\"\"^([:,;#%.\\sa-zA-Z0-9!]|\\w-\\w|'[\\s\\w]+'|\"[\\s\\w]+\"|\\([\\d,\\s]+\\))*$\"\"\", style): return ''\nif not re.match(\"^(\\s*[-\\w]+\\s*:\\s*[^:;]*(;|$))*$\", style): return ''\n\nclean = []\nfor prop,value in re.findall(\"([-\\w]+)\\s*:\\s*([^:;]*)\",style):\n  if not value: continue\n  if prop.lower() in self.acceptable_css_properties:\n      clean.append(prop + ': ' + value + ';')\n  elif prop.split('-')[0].lower() in ['background','border','margin','padding']:\n      for keyword in value.split():\n          if not keyword in self.acceptable_css_keywords and \\\n              not self.valid_css_values.match(keyword):\n              break\n      else:\n          clean.append(prop + ': ' + value + ';')\n  elif self.svgOK and prop.lower() in self.acceptable_svg_properties:\n      clean.append(prop + ': ' + value + ';')\n\nreturn ' '.join(clean)", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Return the Time Zone Designator as an offset in seconds from UTC.'''\n", "func_signal": "def __extract_tzd(m):\n", "code": "if not m:\n    return 0\ntzd = m.group('tzd')\nif not tzd:\n    return 0\nif tzd == 'Z':\n    return 0\nhours = int(m.group('tzdhours'))\nminutes = m.group('tzdminutes')\nif minutes:\n    minutes = int(minutes)\nelse:\n    minutes = 0\noffset = (hours*60 + minutes) * 60\nif tzd[0] == '+':\n    return -offset\nreturn offset", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Parse a string according to a Greek 8-bit date format.'''\n", "func_signal": "def _parse_date_greek(dateString):\n", "code": "m = _greek_date_format_re.match(dateString)\nif not m: return\ntry:\n    wday = _greek_wdays[m.group(1)]\n    month = _greek_months[m.group(3)]\nexcept:\n    return\nrfc822date = '%(wday)s, %(day)s %(month)s %(year)s %(hour)s:%(minute)s:%(second)s %(zonediff)s' % \\\n             {'wday': wday, 'day': m.group(2), 'month': month, 'year': m.group(4),\\\n              'hour': m.group(5), 'minute': m.group(6), 'second': m.group(7),\\\n              'zonediff': m.group(8)}\nif _debug: sys.stderr.write('Greek date parsed as: %s\\n' % rfc822date)\nreturn _parse_date_rfc822(rfc822date)", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "# Check if\n# - server requires digest auth, AND\n# - we tried (unsuccessfully) with basic auth, AND\n# - we're using Python 2.3.3 or later (digest auth is irreparably broken in earlier versions)\n# If all conditions hold, parse authentication information\n# out of the Authorization header we sent the first time\n# (for the username and password) and the WWW-Authenticate\n# header the server sent back (for the realm) and retry\n# the request with the appropriate digest auth headers instead.\n# This evil genius hack has been brought to you by Aaron Swartz.\n", "func_signal": "def http_error_401(self, req, fp, code, msg, headers):\n", "code": "host = urlparse.urlparse(req.get_full_url())[1]\ntry:\n    assert sys.version.split()[0] >= '2.3.3'\n    assert base64 != None\n    user, passw = base64.decodestring(req.headers['Authorization'].split(' ')[1]).split(':')\n    realm = re.findall('realm=\"([^\"]*)\"', headers['WWW-Authenticate'])[0]\n    self.add_password(realm, host, user, passw)\n    retry = self.http_error_auth_reqed('www-authenticate', host, req, headers)\n    self.reset_retry_count()\n    return retry\nexcept:\n    return self.http_error_default(req, fp, code, msg, headers)", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Parse a string according to the MS SQL date format'''\n", "func_signal": "def _parse_date_mssql(dateString):\n", "code": "m = _mssql_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('MS SQL date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if not self.elementstack: return\nif _debug: sys.stderr.write('entering handle_entityref with %s\\n' % ref)\nif ref in ('lt', 'gt', 'quot', 'amp', 'apos'):\n    text = '&%s;' % ref\nelif ref in self.entities.keys():\n    text = self.entities[ref]\n    if text.startswith('&#') and text.endswith(';'):\n        return self.handle_entityref(text)\nelse:\n    try: name2codepoint[ref]\n    except KeyError: text = '&%s;' % ref\n    else: text = unichr(name2codepoint[ref]).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Parse an RFC822, RFC1123, RFC2822, or asctime-style date'''\n", "func_signal": "def _parse_date_rfc822(dateString):\n", "code": "data = dateString.split()\nif data[0][-1] in (',', '.') or data[0].lower() in rfc822._daynames:\n    del data[0]\nif len(data) == 4:\n    s = data[3]\n    i = s.find('+')\n    if i > 0:\n        data[3:] = [s[:i], s[i+1:]]\n    else:\n        data.append('')\n    dateString = \" \".join(data)\nif len(data) < 5:\n    dateString += ' 00:00:00 GMT'\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n    return time.gmtime(rfc822.mktime_tz(tm))", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "# called for each end tag, e.g. for </pre>, tag will be 'pre'\n# Reconstruct the original end tag.\n", "func_signal": "def unknown_endtag(self, tag):\n", "code": "if tag not in self.elements_no_end_tag:\n    self.pieces.append(\"</%(tag)s>\" % locals())", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Parse a string according to the OnBlog 8-bit date format'''\n", "func_signal": "def _parse_date_onblog(dateString):\n", "code": "m = _korean_onblog_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('OnBlog date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"parse a date in yyyy/mm/dd hh:mm:ss TTT format\"\"\"\n# Fri, 2006/09/15 08:19:53 EDT\n", "func_signal": "def _parse_date_perforce(aDateString):\n", "code": "_my_date_pattern = re.compile( \\\n\tr'(\\w{,3}), (\\d{,4})/(\\d{,2})/(\\d{2}) (\\d{,2}):(\\d{2}):(\\d{2}) (\\w{,3})')\n\ndow, year, month, day, hour, minute, second, tz = \\\n\t_my_date_pattern.search(aDateString).groups()\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\ndateString = \"%s, %s %s %s %s:%s:%s %s\" % (dow, day, months[int(month) - 1], year, hour, minute, second, tz)\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n\treturn time.gmtime(rfc822.mktime_tz(tm))", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Parse a string according to a Hungarian 8-bit date format.'''\n", "func_signal": "def _parse_date_hungarian(dateString):\n", "code": "m = _hungarian_date_format_re.match(dateString)\nif not m: return\ntry:\n    month = _hungarian_months[m.group(2)]\n    day = m.group(3)\n    if len(day) == 1:\n        day = '0' + day\n    hour = m.group(4)\n    if len(hour) == 1:\n        hour = '0' + hour\nexcept:\n    return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': month, 'day': day,\\\n             'hour': hour, 'minute': m.group(5),\\\n             'zonediff': m.group(6)}\nif _debug: sys.stderr.write('Hungarian date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n", "func_signal": "def handle_data(self, text, escape=1):\n", "code": "if not self.elementstack: return\nif escape and self.contentparams.get('type') == 'application/xhtml+xml':\n    text = _xmlescape(text)\nself.elementstack[-1][2].append(text)", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "'''Parse a string according to the Nate 8-bit date format'''\n", "func_signal": "def _parse_date_nate(dateString):\n", "code": "m = _korean_nate_date_re.match(dateString)\nif not m: return\nhour = int(m.group(5))\nampm = m.group(4)\nif (ampm == _korean_pm):\n    hour += 12\nhour = str(hour)\nif len(hour) == 1:\n    hour = '0' + hour\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': hour, 'minute': m.group(6), 'second': m.group(7),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('Nate date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "feedparser.py", "repo_name": "webiest/twollow", "stars": 7, "license": "None", "language": "python", "size": 277}
{"docstring": "\"\"\"Tests that a combined model class has (faked) Django fields.\"\"\"\n", "func_signal": "def testDjangoModelFields(self):\n", "code": "if VERSION >= (0, 97, None):\n  fields = TestModelWithProperties._meta.local_fields\nelse:\n  fields = TestModelWithProperties._meta.fields\nself.assertEqual(3, len(fields))\n# Check each fake field has the minimal properties that Django needs.\nfor field in fields:\n  # The Django serialization code looks for rel to determine if the field\n  # is a relationship/reference to another model.\n  self.assert_(hasattr(field, \"rel\"))\n  # serialize is required to tell Django to serialize the field.\n  self.assertEqual(True, field.serialize)\n  if field.name == \"property3\":\n    # Extra checks for the Reference field.\n    # rel.field_name is used during serialization to find the field in the\n    # other model that this field is related to. This should always be\n    # 'key_name' for appengine models.\n    self.assertEqual(\"key_name\", field.rel.field_name)", "path": "appengine\\appengine_django\\tests\\model_test.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Template tag registered as 'auth_login_url' and 'auth_logout_url'\nwhen the module is imported.\n\nBoth tags take an optional argument that specifies the redirect URL and\ndefaults to '/'.\n\"\"\"\n", "func_signal": "def auth_login_urls(parser, token):\n", "code": "bits = list(token.split_contents())\nif len(bits) == 2:\n  redirect = bits[1]\nelse:\n  redirect = \"/\"\nlogin = bits[0] == \"auth_login_url\"\nreturn AuthLoginUrlsNode(login, redirect)", "path": "appengine\\appengine_django\\auth\\__init__.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Serialize the object to XML and add to the list of objects to output.\n\nThe output of ToXml is manipulated to replace the datastore model name in\nthe \"kind\" tag with the Django model name (which includes the Django\napplication name) to make importing easier.\n\"\"\"\n", "func_signal": "def end_object(self, obj):\n", "code": "xml = obj._entity.ToXml()\nxml = xml.replace(u\"\"\"kind=\"%s\" \"\"\" % obj._entity.kind(),\n                  u\"\"\"kind=\"%s\" \"\"\" % unicode(obj._meta))\nself._objects.append(xml)", "path": "appengine\\appengine_django\\serializer\\xml.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Tests that each model instance has a 'primary key' generated.\"\"\"\n", "func_signal": "def testDjangoModelPK(self):\n", "code": "obj = RegistrationTestModel(key_name=\"test\")\nobj.put()\npk = obj._get_pk_val()\nself.assert_(pk)\nnew_obj = RegistrationTestModel.get(pk)\nself.assertEqual(obj.key(), new_obj.key())", "path": "appengine\\appengine_django\\tests\\model_test.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Replacement next method to look for 'entity'.\n\nThe default next implementation exepects 'object' nodes which is not\nwhat the entity's ToXml output provides.\n\"\"\"\n", "func_signal": "def next(self):\n", "code": "for event, node in self.event_stream:\n  if event == \"START_ELEMENT\" and node.nodeName == \"entity\":\n    self.event_stream.expandNode(node)\n    return self._handle_object(node)\nraise StopIteration", "path": "appengine\\appengine_django\\serializer\\xml.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Tests that delete removes values from the cache.\"\"\"\n", "func_signal": "def testDelete(self):\n", "code": "self._cache.set(\"test_key\", \"test_value\")\nself.assertEqual(self._cache.has_key(\"test_key\"), True)\nself._cache.delete(\"test_key\")\nself.assertEqual(self._cache.has_key(\"test_key\"), False)", "path": "appengine\\appengine_django\\tests\\memcache_test.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"\nSimilar to smart_unicode, except that lazy instances are resolved to\nstrings, rather than kept as lazy objects.\n\nIf strings_only is True, don't convert (some) non-string-like objects.\n\"\"\"\n", "func_signal": "def force_unicode(s, encoding='utf-8', strings_only=False, errors='strict'):\n", "code": "non_strings = (types.NoneType, int, long, datetime.datetime, datetime.date,\n               datetime.time, float)\nif strings_only and isinstance(s, non_strings):\n  return s\nif not isinstance(s, basestring,):\n  if hasattr(s, '__unicode__'):\n    s = unicode(s)\n  else:\n    s = unicode(str(s), encoding, errors)\nelif not isinstance(s, unicode):\n  # Note: We use .decode() here, instead of unicode(s, encoding,\n  # errors), so that if s is a SafeString, it ends up being a\n  # SafeUnicode at the end.\n  s = s.decode(encoding, errors)\nreturn s", "path": "appengine\\appengine_django\\serializer\\python.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Wrap the serialized objects with XML headers and return.\"\"\"\n", "func_signal": "def getvalue(self):\n", "code": "str = u\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\"\"\"\nstr += u\"\"\"<django-objects version=\"1.0\">\\n\"\"\"\nstr += u\"\".join(self._objects)\nstr += u\"\"\"</django-objects>\"\"\"\nreturn str", "path": "appengine\\appengine_django\\serializer\\xml.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Tests that a simple set/get operation through the cache works.\"\"\"\n", "func_signal": "def testSimpleSetGet(self):\n", "code": "self._cache.set(\"test_key\", \"test_value\")\nself.assertEqual(self._cache.get(\"test_key\"), \"test_value\")", "path": "appengine\\appengine_django\\tests\\memcache_test.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Clears the current datastore. \"\"\"\n# This wrapper function is used instead of passing the real function directly\n# to Django because Django 0.96 wants to read its docstring and args\n# attribute for commandline help.\n", "func_signal": "def v096_command(*args):\n", "code": "from django.db import connection\nconnection.flush()", "path": "appengine\\appengine_django\\management\\commands\\reset.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Clears the current datastore and loads the initial fixture data. \"\"\"\n# This wrapper function is used instead of passing the real function directly\n# to Django because Django 0.96 wants to read its docstring and args\n# attribute for commandline help.\n", "func_signal": "def v096_command(*args):\n", "code": "from django.db import connection\nconnection.flush()\nfrom django.core.management import load_data\nload_data(['initial_data'])", "path": "appengine\\appengine_django\\management\\commands\\flush.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "# Build kwargs\n", "func_signal": "def _build_thumbnail(self, args):\n", "code": "kwargs = {}\nfor k, v in args.items():\n    kwargs[ALL_ARGS[k]] = v\n# Build relative source path\nfilename = getattr(self.instance, self.field.name).path\nmedia_root_len = len(os.path.normpath(settings.MEDIA_ROOT))\nfilename = os.path.normpath(filename)\nfilename = filename[media_root_len:].lstrip(os.path.sep)\n# Return thumbnail\nreturn DjangoThumbnail(filename, **kwargs)", "path": "coopdirectory\\sorl\\thumbnail\\fields.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Convert an <entity> node to a DeserializedObject\"\"\"\n", "func_signal": "def _handle_object(self, node):\n", "code": "Model = self._get_model_from_node(node, \"kind\")\ndata = {}\nkey = db.Key(node.getAttribute(\"key\"))\nif key.name():\n  data[\"key_name\"] = key.name()\nparent = None\nif key.parent():\n  parent = FakeParent(key.parent())\nm2m_data = {}\n\n# Deseralize each field.\nfor field_node in node.getElementsByTagName(\"property\"):\n  # If the field is missing the name attribute, bail (are you\n  # sensing a pattern here?)\n  field_name = field_node.getAttribute(\"name\")\n  if not field_name:\n      raise base.DeserializationError(\"<field> node is missing the 'name' \"\n                                      \"attribute\")\n  field = Model.properties()[field_name]\n  field_value = getInnerText(field_node).strip()\n\n  if isinstance(field, db.Reference):\n    m = re.match(\"tag:.*\\[(.*)\\]\", field_value)\n    if not m:\n      raise base.DeserializationError(u\"Invalid reference value: '%s'\" %\n                                      field_value)\n    key = m.group(1)\n    key_obj = db.Key(key)\n    if not key_obj.name():\n      raise base.DeserializationError(u\"Cannot load Reference with \"\n                                      \"unnamed key: '%s'\" % field_value)\n    data[field.name] = key_obj\n  else:\n    data[field.name] = field.validate(field_value)\n\n# Create the new model instance with all it's data, but no parent.\nobject = Model(**data)\n# Now add the parent into the hidden attribute, bypassing the type checks\n# in the Model's __init__ routine.\nobject._parent = parent\n# When the deserialized object is saved our replacement DeserializedObject\n# class will set object._parent to force the real parent model to be loaded\n# the first time it is referenced.\nreturn base.DeserializedObject(object, m2m_data)", "path": "appengine\\appengine_django\\serializer\\xml.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Tests the parts of a model required by Django are correctly stubbed.\"\"\"\n# Django requires model options to be found at ._meta.\n", "func_signal": "def testDjangoModelClass(self):\n", "code": "self.assert_(isinstance(RegistrationTestModel._meta, ModelOptions))\n# Django requires a manager at .objects\nself.assert_(isinstance(RegistrationTestModel.objects, ModelManager))\n# Django 0.97 also requires ._default_manager.\nself.assert_(hasattr(RegistrationTestModel, \"_default_manager\"))", "path": "appengine\\appengine_django\\tests\\model_test.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "# TODO: parse GET search parameters\n", "func_signal": "def coop_list(request):\n", "code": "coops = Coop.objects.all()\npaginator = Paginator(coops, 10)\npage = paginator.page(request.GET.get('p', 1))\nreturn render_to_response('coops/list.html', {\n        'page': page, \n        'querystring': '', \n        'first_item_number': paginator.per_page * (page.number - 1) + 1\n})", "path": "coopdirectory\\coops\\views.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"\nReturns a unicode object representing 's'. Treats bytestrings using the\n'encoding' codec.\n\nIf strings_only is True, don't convert (some) non-string-like objects.\n\"\"\"\n", "func_signal": "def smart_unicode(s, encoding='utf-8', strings_only=False, errors='strict'):\n", "code": "if isinstance(s, Promise):\n  # The input is the result of a gettext_lazy() call.\n  return s\nreturn force_unicode(s, encoding, strings_only, errors)", "path": "appengine\\appengine_django\\serializer\\python.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "# import this so that we run through the checks at the beginning\n# and report the appropriate errors\n", "func_signal": "def run_appcfg():\n", "code": "import appcfg\n\n# We don't really want to use that one though, it just executes this one\nfrom google.appengine.tools import appcfg\n\n# Reset the logging level to WARN as appcfg will spew tons of logs on INFO\nlogging.getLogger().setLevel(logging.WARN)\n\n# Note: if we decide to change the name of this command to something other\n#       than 'update' we will have to munge the args to replace whatever\n#       we called it with 'update'\nnew_args = sys.argv[:]\nnew_args.append('.')\nappcfg.main(new_args)", "path": "appengine\\appengine_django\\management\\commands\\update.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Deserialize simple Python objects back into Model instances.\n\nIt's expected that you pass the Python objects themselves (instead of a\nstream or a string) to the constructor\n\"\"\"\n", "func_signal": "def Deserializer(object_list, **options):\n", "code": "models.get_apps()\nfor d in object_list:\n  # Look up the model and starting build a dict of data for it.\n  Model = python._get_model(d[\"model\"])\n  data = {}\n  key = resolve_key(Model._meta.module_name, d[\"pk\"])\n  if key.name():\n    data[\"key_name\"] = key.name()\n  parent = None\n  if key.parent():\n    parent = FakeParent(key.parent())\n  m2m_data = {}\n\n  # Handle each field\n  for (field_name, field_value) in d[\"fields\"].iteritems():\n    if isinstance(field_value, str):\n      field_value = smart_unicode(\n          field_value, options.get(\"encoding\",\n                                   settings.DEFAULT_CHARSET),\n          strings_only=True)\n    field = Model.properties()[field_name]\n\n    if isinstance(field, db.Reference):\n      # Resolve foreign key references.\n      data[field.name] = resolve_key(Model._meta.module_name, field_value)\n      if not data[field.name].name():\n        raise base.DeserializationError(u\"Cannot load Reference with \"\n                                        \"unnamed key: '%s'\" % field_value)\n    else:\n      data[field.name] = field.validate(field_value)\n  # Create the new model instance with all it's data, but no parent.\n  object = Model(**data)\n  # Now add the parent into the hidden attribute, bypassing the type checks\n  # in the Model's __init__ routine.\n  object._parent = parent\n  # When the deserialized object is saved our replacement DeserializedObject\n  # class will set object._parent to force the real parent model to be loaded\n  # the first time it is referenced.\n  yield base.DeserializedObject(object, m2m_data)", "path": "appengine\\appengine_django\\serializer\\python.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Tests that the options stub has the required properties by Django.\"\"\"\n# Django requires object_name and app_label for serialization output.\n", "func_signal": "def testDjangoModelOptionsStub(self):\n", "code": "self.assertEqual(\"RegistrationTestModel\",\n                 RegistrationTestModel._meta.object_name)\nself.assertEqual(\"appengine_django\", RegistrationTestModel._meta.app_label)\n# The pk.name member is required during serialization for dealing with\n# related fields.\nself.assertEqual(\"key_name\", RegistrationTestModel._meta.pk.name)\n# The many_to_many method is called by Django in the serialization code to\n# find m2m relationships. m2m is not supported by the datastore.\nself.assertEqual([], RegistrationTestModel._meta.many_to_many)\n# The Django 0.96 serialization code relies on the str() representation of\n# the model options being \"app_label.model_name\" in lower case.\nself.assertEqual(\"appengine_django.registrationtestmodel\",\n                 str(RegistrationTestModel._meta))", "path": "appengine\\appengine_django\\tests\\model_test.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"\nReturns a list containing unique valid options from a list of processors\nin correct order.\n\"\"\"\n", "func_signal": "def get_valid_options(processors):\n", "code": "valid_options = []\nfor processor in processors:\n    if hasattr(processor, 'valid_options'):\n        valid_options.extend([opt for opt in processor.valid_options\n                              if opt not in valid_options])\nreturn valid_options", "path": "appengine\\sorl\\thumbnail\\processors.py", "repo_name": "jingoro/coop-directory", "stars": 4, "license": "None", "language": "python", "size": 388}
{"docstring": "\"\"\"Add the services available to the array of services.\"\"\"\n", "func_signal": "def _register_services(self):\n", "code": "availables = dict(Facebook=FacebookService,\n                  Feed=FeedService,\n                  Gmail=GmailService,\n                  Twitter=TwitterService)\nfor name in iter(availables):\n    self.services.append(availables[name]())", "path": "libnotifyall\\NotifyAll.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Shows the messages unseen.\"\"\"\n", "func_signal": "def _show_unseen_messages(self):\n", "code": "for msg in self.messages:\n    if not msg.viewed:\n        if not self.disable_libnotify and os.environ.has_key('DISPLAY'):\n            if not msg.show():\n                break\n        self.logger.info(msg.title + \": \" + msg.summary)\n        msg.viewed = True", "path": "libnotifyall\\Service.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Send messages throug pynotify.\"\"\"\n", "func_signal": "def show(self):\n", "code": "pynotify.init('Notify All')\ntry:\n    m = pynotify.Notification(self.title, self.summary, self.icon)\n    m.show()\n    return True\nexcept:\n    return False", "path": "libnotifyall\\Message.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Start the loop to update the service and display their own messages.\"\"\"\n", "func_signal": "def run(self):\n", "code": "self._load_messages()\nwhile True:\n    try:\n        entries = self._get_updates()\n    except ServiceError as error:\n        self.logger.error(error.description)\n    else:\n        new_messages = self._normalize_entries(entries)\n        self._update_messages(new_messages)\n        self._save_messages()\n        self._show_unseen_messages()\n\n    self.logger.debug(\"Unseen message(s): \" + str(self._unseen_messages()) + \" of \" + str(len(self.messages)))\n    time.sleep(self.interval)", "path": "libnotifyall\\Service.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Retrieves updates from Facebook notification feed and return an array of entries.\"\"\"\n", "func_signal": "def _get_updates(self):\n", "code": "entries = []\nopener = urllib2.build_opener()\ntry:\n    feed = opener.open(self.feed_url)\nexcept:\n    raise ServiceError('Update error')\nelse:\n    a = feedparser.parse(feed)\n    entries.extend(a['entries'])\n    self.logger.debug(\"Updated\")\n\nreturn entries", "path": "libnotifyall\\services\\FacebookService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Returns the same array in reverse order.\"\"\"\n", "func_signal": "def _reverse(self, data):\n", "code": "for index in range(len(data)-1, -1, -1):\n    yield data[index]", "path": "libnotifyall\\Service.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Normalizes and sorts an array of entries and returns an array of messages.\"\"\"\n", "func_signal": "def _normalize_entries(self, entries):\n", "code": "messages = []\n\nfor entry in entries:\n    if entry.has_key('link') and entry.has_key('title'):\n\n        icon = os.path.join(os.path.realpath(os.path.dirname(sys.argv[0])), 'icons', 'gmail.png')\n        m = Message(entry.link, self.SRV_NAME,\n                    entry.author_detail.name, entry.title, icon)\n        messages.append(m)\n\nreturn messages", "path": "libnotifyall\\services\\GmailService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Load configuration settings from the gmail section in CONFIG_FILE.\"\"\"\n", "func_signal": "def load_config(self):\n", "code": "Service._load_config(self)\n\nconfig = ConfigParser.ConfigParser()\n\nconfig.read(CONFIG_FILE)\nself._disabled = config.getboolean(self.SRV_NAME, \"disabled\")\nself.username = config.get(self.SRV_NAME, \"username\")\nself.password = config.get(self.SRV_NAME, \"password\")\nself.interval = int(config.get(self.SRV_NAME, \"interval\"))\nself.labels = config.items(\"labels\")", "path": "libnotifyall\\services\\GmailService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Normalizes and sorts an array of entries and returns an array of messages.\"\"\"\n", "func_signal": "def _normalize_entries(self, entries):\n", "code": "messages = []\n\nfor entry in self._reverse(entries):\n\n    icon = os.path.join(CONFIG_DIR, self.SRV_NAME, str(entry.user.id))\n    if not os.path.exists(icon):\n        try:\n            avatar = urllib2.urlopen(entry.user.profile_image_url)\n            self.logger.debug(\"Fetching image profile for \" + entry.user.screen_name)\n        except:\n            self.logger.error(\"Error fetching image profile for \" + entry.user.screen_name)\n            icon = os.path.join(os.path.realpath(os.path.dirname(sys.argv[0])), 'icons', 'twitter.png')\n        else:\n            avatar_file = open(os.path.join(CONFIG_DIR, self.SRV_NAME, str(entry.user.id)), 'wb')\n            avatar_file.write(avatar.read())\n            avatar_file.close()\n            icon = os.path.join(self.configdir, self.SRV_NAME,\n                   str(entry.user.id))\n\n    m = Message(entry.id, self.SRV_NAME,\n                entry.user.name + \" (\" + \\\n                entry.user.screen_name + \")\",\n                entry.text, icon)\n \n    messages.append(m)\n\nreturn messages", "path": "libnotifyall\\services\\TwitterService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Normalizes and sorts an array of entries and returns an array of messages.\"\"\"\n", "func_signal": "def _normalize_entries(self, entries):\n", "code": "messages = []\n\nfor entry in self._reverse(entries):\n    if entry.has_key('link') and entry.has_key('title'):\n\n        icon = os.path.join(os.path.realpath(os.path.dirname(sys.argv[0])), 'icons', 'rss.png')\n\n        # We use the link as an id because the id \n        # provided by some feeds is not reliable\n        m = Message(entry.link, self.SRV_NAME,\n                    entry.title, entry.link, icon)\n        messages.append(m)\n\nreturn messages", "path": "libnotifyall\\services\\FeedService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Load configuration settings for NotifyAll.\"\"\"\n", "func_signal": "def _load_config(self):\n", "code": "LOG_LEVELS = {'debug': logging.DEBUG,\n              'info': logging.INFO,\n              'warning': logging.WARNING,\n              'error': logging.ERROR,\n              'critical': logging.CRITICAL}\n\nconfig = ConfigParser.ConfigParser()\n\nconfig.read(CONFIG_FILE)\nself.disable_libnotify = config.getboolean(\"notifyall\",\n                                           \"disable_libnotify\")\nself.loglevel = config.get(\"notifyall\", \"loglevel\")\nself.logger.setLevel(LOG_LEVELS.get(self.loglevel, logging.INFO))", "path": "libnotifyall\\Service.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Returns the number of unseen messages.\"\"\"\n", "func_signal": "def _unseen_messages(self):\n", "code": "i = 0\nfor message in self.messages:\n    if not message.viewed:\n        i += 1\nreturn i", "path": "libnotifyall\\Service.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Load configuration settings from the feed section in CONFIG_FILE.\"\"\"\n", "func_signal": "def load_config(self):\n", "code": "Service._load_config(self)\n\nconfig = ConfigParser.ConfigParser()\n\nconfig.read(CONFIG_FILE)\nself._disabled = config.getboolean(self.SRV_NAME, \"disabled\")\nself.interval = int(config.get(self.SRV_NAME, \"interval\"))\nself.feeds = config.items(\"feeds\")", "path": "libnotifyall\\services\\FeedService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Start a thread for each registered service.\"\"\"\n", "func_signal": "def start(self):\n", "code": "for service in self.services:\n    if not service._disabled:\n        service.start()", "path": "libnotifyall\\NotifyAll.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Normalizes and sorts an array of entries and returns an array of messages.\"\"\"\n", "func_signal": "def _normalize_entries(self, entries):\n", "code": "messages = []\n\nfor entry in self._reverse(entries):\n    if entry.has_key('link') and entry.has_key('title') and entry.has_key('date'):\n\n        icon = os.path.join(os.path.realpath(os.path.dirname(sys.argv[0])), 'icons', 'facebook.png')\n\n        # We use the entry.guid as an id because the id \n        # provided is not reliable\n        m = Message(entry.guid, self.SRV_NAME,\n                    entry.title, entry.date, icon)\n        messages.append(m)\n\nreturn messages", "path": "libnotifyall\\services\\FacebookService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Load configuration settings from the facebook section in CONFIG_FILE.\"\"\"\n", "func_signal": "def load_config(self):\n", "code": "Service._load_config(self)\n\nconfig = ConfigParser.ConfigParser()\n\nconfig.read(CONFIG_FILE)\nself._disabled = config.getboolean(self.SRV_NAME, \"disabled\")\nself.fbid = config.get(self.SRV_NAME, \"id\")\nself.viewer = config.get(self.SRV_NAME, \"viewer\")\nself.key = config.get(self.SRV_NAME, \"key\")\nself.interval = int(config.get(self.SRV_NAME, \"interval\"))\nself.feed_url = self.FB_URL + \\\n                \"id=\" + self.fbid + \\\n                \"&viewer=\" + self.viewer + \\\n                \"&key=\" + self.key + \\\n                \"&format=rss20\"", "path": "libnotifyall\\services\\FacebookService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Retrieves updates from feed and return an array of entries.\"\"\"\n", "func_signal": "def _get_updates(self):\n", "code": "all_entries = []\nopener = urllib2.build_opener()\nfor feed in self.feeds:\n    try:\n        f = opener.open(feed[1])\n    except:\n        raise ServiceError(\"Update error\")\n    else:\n        a = feedparser.parse(f)\n        all_entries.extend(a['entries'])\n        self.logger.debug(\"Updated \" + feed[1])\n\nreturn all_entries", "path": "libnotifyall\\services\\FeedService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Update the array of messages.\"\"\"\n# Fixed: maybe this could be improved\n", "func_signal": "def _update_messages(self, new_messages):\n", "code": "for new_message in new_messages:\n\n    for message in self.messages:\n        if new_message.id == message.id:\n            if message.viewed == True:\n                new_message.viewed = True\n            break\n\nself.messages = new_messages\nreturn", "path": "libnotifyall\\Service.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Load the required settings for each service.\"\"\"\n", "func_signal": "def _load_config(self):\n", "code": "for service in self.services:\n    service.load_config()", "path": "libnotifyall\\NotifyAll.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"Retrieves updates from Twitter API and return an array of entries.\"\"\"\n", "func_signal": "def _get_updates(self):\n", "code": "statuses = []\napi = twitter.Api(self.username, self.password)\n\ntry:\n    statuses = api.GetFriendsTimeline()\nexcept:\n    raise ServiceError('Update error')\nelse:\n    self.logger.debug(\"Updated\")\n\nreturn statuses", "path": "libnotifyall\\services\\TwitterService.py", "repo_name": "pabluk/NotifyAll", "stars": 5, "license": "None", "language": "python", "size": 252}
{"docstring": "\"\"\"\nRun the examples in `test`.  Write the outcome of each example\nwith one of the `DocTestRunner.report_*` methods, using the\nwriter function `out`.  `compileflags` is the set of compiler\nflags that should be used to execute examples.  Return a tuple\n`(f, t)`, where `t` is the number of examples tried, and `f`\nis the number of examples that failed.  The examples are run\nin the namespace `test.globs`.\n\"\"\"\n# Keep track of the number of failures and tries.\n", "func_signal": "def run_one_test(self, test, compileflags, out):\n", "code": "failures = tries = 0\n\n# Save the option flags (since option directives can be used\n# to modify them).\noriginal_optionflags = self.optionflags\n\nSUCCESS, FAILURE, BOOM = range(3) # `outcome` state\n\ncheck = self._checker.check_output\n\n# Process each example.\nfor examplenum, example in enumerate(test.examples):\n\n    # If REPORT_ONLY_FIRST_FAILURE is set, then supress\n    # reporting after the first failure.\n    quiet = (self.optionflags & REPORT_ONLY_FIRST_FAILURE and\n             failures > 0)\n\n    # Merge in the example's options.\n    self.optionflags = original_optionflags\n    if example.options:\n        for (optionflag, val) in example.options.items():\n            if val:\n                self.optionflags |= optionflag\n            else:\n                self.optionflags &= ~optionflag\n\n    # If 'SKIP' is set, then skip this example.\n    if self.optionflags & SKIP:\n        continue\n\n    # Record that we started this example.\n    tries += 1\n    if not quiet:\n        self.report_start(out, test, example)\n\n    # Use a special filename for compile(), so we can retrieve\n    # the source code during interactive debugging (see\n    # __patched_linecache_getlines).\n    filename = '<doctest %s[%d]>' % (test.name, examplenum)\n\n    # Run the example in the given context (globs), and record\n    # any exception that gets raised.  (But don't intercept\n    # keyboard interrupts.)\n    try:\n        # Don't blink!  This is where the user's code gets run.\n        self.run_one_example(test, example, filename, compileflags)\n        self.debugger.set_continue() # ==== Example Finished ====\n        exception = None\n    except KeyboardInterrupt:\n        raise\n    except:\n        exception = sys.exc_info()\n        self.debugger.set_continue() # ==== Example Finished ====\n\n    got = self._fakeout.getvalue()  # the actual output\n    self._fakeout.truncate(0)\n    outcome = FAILURE   # guilty until proved innocent or insane\n\n    # If the example executed without raising any exceptions,\n    # verify its output.\n    if exception is None:\n        if check(example.want, got, self.optionflags):\n            outcome = SUCCESS\n\n    # The example raised an exception:  check if it was expected.\n    else:\n        exc_info = sys.exc_info()\n        exc_msg = traceback.format_exception_only(*exc_info[:2])[-1]\n        if not quiet:\n            got += _exception_traceback(exc_info)\n\n        # If `example.exc_msg` is None, then we weren't expecting\n        # an exception.\n        if example.exc_msg is None:\n            outcome = BOOM\n\n        # We expected an exception:  see whether it matches.\n        elif check(example.exc_msg, exc_msg, self.optionflags):\n            outcome = SUCCESS\n\n        # Another chance if they didn't care about the detail.\n        elif self.optionflags & IGNORE_EXCEPTION_DETAIL:\n            m1 = re.match(r'[^:]*:', example.exc_msg)\n            m2 = re.match(r'[^:]*:', exc_msg)\n            if m1 and m2 and check(m1.group(0), m2.group(0),\n                                   self.optionflags):\n                outcome = SUCCESS\n\n    # Report the outcome.\n    if outcome is SUCCESS:\n        if not quiet:\n            self.report_success(out, test, example, got)\n    elif outcome is FAILURE:\n        if not quiet:\n            self.report_failure(out, test, example, got)\n        failures += 1\n    elif outcome is BOOM:\n        if not quiet:\n            self.report_unexpected_exception(out, test, example,\n                                             exc_info)\n        failures += 1\n    else:\n        assert False, (\"unknown outcome\", outcome)\n\n# Restore the option flags (in case they were modified)\nself.optionflags = original_optionflags\n\n# Record and return the number of failures and tries.\nself.__record_outcome(test, failures, tries)\nreturn failures, tries", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"square() -> square TestClass's associated value\n\n>>> _TestClass(13).square().get()\n169\n\"\"\"\n\n", "func_signal": "def square(self):\n", "code": "self.val = self.val ** 2\nreturn self", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nGiven the lines of a source string (including prompts and\nleading indentation), check to make sure that every prompt is\nfollowed by a space character.  If any line is not followed by\na space character, then raise ValueError.\n\"\"\"\n", "func_signal": "def _check_prompt_blank(self, lines, indent, name, lineno):\n", "code": "for i, line in enumerate(lines):\n    if len(line) >= indent+4 and line[indent+3] != ' ':\n        raise ValueError('line %r of the docstring for %s '\n                         'lacks blank after %s: %r' %\n                         (lineno+i+1, name,\n                          line[indent:indent+3], line))", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReturn a line number of the given object's docstring.  Note:\nthis method assumes that the object has a docstring.\n\"\"\"\n", "func_signal": "def _find_lineno(self, obj, source_lines):\n", "code": "lineno = None\n\n# Find the line number for modules.\nif inspect.ismodule(obj):\n    lineno = 0\n\n# Find the line number for classes.\n# Note: this could be fooled if a class is defined multiple\n# times in a single file.\nif inspect.isclass(obj):\n    if source_lines is None:\n        return None\n    pat = re.compile(r'^\\s*class\\s*%s\\b' %\n                     getattr(obj, '__name__', '-'))\n    for i, line in enumerate(source_lines):\n        if pat.match(line):\n            lineno = i\n            break\n\n# Find the line number for functions & methods.\nif inspect.ismethod(obj): obj = obj.im_func\nif inspect.isfunction(obj): obj = obj.func_code\nif inspect.istraceback(obj): obj = obj.tb_frame\nif inspect.isframe(obj): obj = obj.f_code\nif inspect.iscode(obj):\n    lineno = getattr(obj, 'co_firstlineno', None)-1\n\n# Find the line number where the docstring starts.  Assume\n# that it's the first line that begins with a quote mark.\n# Note: this could be fooled by a multiline function\n# signature, where a continuation line begins with a quote\n# mark.\nif lineno is not None:\n    if source_lines is None:\n        return lineno+1\n    pat = re.compile('(^|.*:)\\s*\\w*(\"|\\')')\n    for lineno in range(lineno, len(source_lines)):\n        if pat.match(source_lines[lineno]):\n            return lineno\n\n# We couldn't find the line number.\nreturn None", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nDivide the given string into examples and intervening text,\nand return them as a list of alternating Examples and strings.\nLine numbers for the Examples are 0-based.  The optional\nargument `name` is a name identifying this string, and is only\nused for error messages.\n\"\"\"\n", "func_signal": "def parse(self, string, name='<string>'):\n", "code": "string = string.expandtabs()\n# If all lines begin with the same indentation, then strip it.\nmin_indent = self._min_indent(string)\nif min_indent > 0:\n    string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n\noutput = []\ncharno, lineno = 0, 0\n# Find all doctest examples in the string:\nfor m in self._EXAMPLE_RE.finditer(string):\n    # Add the pre-example text to `output`.\n    output.append(string[charno:m.start()])\n    # Update lineno (lines before this example)\n    lineno += string.count('\\n', charno, m.start())\n    # Extract info from the regexp match.\n    (source, options, want, exc_msg) = \\\n             self._parse_example(m, name, lineno)\n    # Create an Example, and add it to the list.\n    if not self._IS_BLANK_OR_COMMENT(source):\n        output.append( Example(source, want, exc_msg,\n                            lineno=lineno,\n                            indent=min_indent+len(m.group('indent')),\n                            options=options) )\n    # Update lineno (lines inside this example)\n    lineno += string.count('\\n', m.start(), m.end())\n    # Update charno.\n    charno = m.end()\n# Add any remaining post-example text to `output`.\noutput.append(string[charno:])\nreturn output", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReturn true if the given object is defined in the given\nmodule.\n\"\"\"\n", "func_signal": "def _from_module(self, module, object):\n", "code": "if module is None:\n    return True\nelif inspect.isfunction(object):\n    return module.__dict__ is object.func_globals\nelif inspect.isclass(object):\n    return module.__name__ == object.__module__\nelif inspect.getmodule(object) is not None:\n    return module is inspect.getmodule(object)\nelif hasattr(object, '__module__'):\n    return module.__name__ == object.__module__\nelif isinstance(object, property):\n    return True # [XX] no way not be sure.\nelse:\n    raise ValueError(\"object must be a class or function\")", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nRun the examples in `test`, and display the results using the\nwriter function `out`.\n\nThe examples are run in the namespace `test.globs`.  If\n`clear_globs` is true (the default), then this namespace will\nbe cleared after the test runs, to help with garbage\ncollection.  If you would like to examine the namespace after\nthe test completes, then use `clear_globs=False`.\n\n`compileflags` gives the set of flags that should be used by\nthe Python compiler when running the examples.  If not\nspecified, then it will default to the set of future-import\nflags that apply to `globs`.\n\nThe output of each example is checked using\n`DocTestRunner.check_output`, and the results are formatted by\nthe `DocTestRunner.report_*` methods.\n\"\"\"\n", "func_signal": "def run(self, test, compileflags=None, out=None, clear_globs=True):\n", "code": "self.test = test\n\nif compileflags is None:\n    compileflags = _extract_future_flags(test.globs)\n\nsave_stdout = sys.stdout\nif out is None:\n    out = save_stdout.write\nsys.stdout = self._fakeout\n\n# Patch pdb.set_trace to restore sys.stdout during interactive\n# debugging (so it's not still redirected to self._fakeout).\n# Note that the interactive output will go to *our*\n# save_stdout, even if that's not the real sys.stdout; this\n# allows us to write test cases for the set_trace behavior.\nsave_set_trace = pdb.set_trace\nself.debugger = _OutputRedirectingPdb(save_stdout)\nself.debugger.reset()\npdb.set_trace = self.debugger.set_trace\n\n# Patch linecache.getlines, so we can see the example's source\n# when we're inside the debugger.\nself.save_linecache_getlines = linecache.getlines\nlinecache.getlines = self.__patched_linecache_getlines\n\ntry:\n    return self.run_one_test(test, compileflags, out)\nfinally:\n    sys.stdout = save_stdout\n    pdb.set_trace = save_set_trace\n    linecache.getlines = self.save_linecache_getlines\n    if clear_globs:\n        test.globs.clear()", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nCreate a new DocTest containing the given examples.  The\nDocTest's globals are initialized with a copy of `globs`.\n\"\"\"\n", "func_signal": "def __init__(self, examples, globs, name, filename, lineno, docstring):\n", "code": "assert not isinstance(examples, basestring), \\\n       \"DocTest no longer accepts str; use DocTestParser instead\"\nself.examples = examples\nself.docstring = docstring\nself.globs = globs.copy()\nself.name = name\nself.filename = filename\nself.lineno = lineno", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nEssentially the only subtle case:\n>>> _ellipsis_match('aa...aa', 'aaa')\nFalse\n\"\"\"\n", "func_signal": "def _ellipsis_match(want, got):\n", "code": "if ELLIPSIS_MARKER not in want:\n    return want == got\n\n# Find \"the real\" strings.\nws = want.split(ELLIPSIS_MARKER)\nassert len(ws) >= 2\n\n# Deal with exact matches possibly needed at one or both ends.\nstartpos, endpos = 0, len(got)\nw = ws[0]\nif w:   # starts with exact match\n    if got.startswith(w):\n        startpos = len(w)\n        del ws[0]\n    else:\n        return False\nw = ws[-1]\nif w:   # ends with exact match\n    if got.endswith(w):\n        endpos -= len(w)\n        del ws[-1]\n    else:\n        return False\n\nif startpos > endpos:\n    # Exact end matches required more characters than we have, as in\n    # _ellipsis_match('aa...aa', 'aaa')\n    return False\n\n# For the rest, we only need to find the leftmost non-overlapping\n# match for each piece.  If there's no overall match that way alone,\n# there's no overall match period.\nfor w in ws:\n    # w may be '' at times, if there are consecutive ellipses, or\n    # due to an ellipsis at the start or end of `want`.  That's OK.\n    # Search for an empty string succeeds, and doesn't change startpos.\n    startpos = got.find(w, startpos, endpos)\n    if startpos < 0:\n        return False\n    startpos += len(w)\n\nreturn True", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReturn a string describing the differences between the\nexpected output for a given example (`example`) and the actual\noutput (`got`).  `optionflags` is the set of option flags used\nto compare `want` and `got`.\n\"\"\"\n", "func_signal": "def output_difference(self, example, got, optionflags):\n", "code": "want = example.want\n# If <BLANKLINE>s are being used, then replace blank lines\n# with <BLANKLINE> in the actual output string.\nif not (optionflags & DONT_ACCEPT_BLANKLINE):\n    got = re.sub('(?m)^[ ]*(?=\\n)', BLANKLINE_MARKER, got)\n\n# Check if we should use diff.\nif self._do_a_fancy_diff(want, got, optionflags):\n    # Split want & got into lines.\n    want_lines = want.splitlines(True)  # True == keep line ends\n    got_lines = got.splitlines(True)\n    # Use difflib to find their differences.\n    if optionflags & REPORT_UDIFF:\n        diff = difflib.unified_diff(want_lines, got_lines, n=2)\n        diff = list(diff)[2:] # strip the diff header\n        kind = 'unified diff with -expected +actual'\n    elif optionflags & REPORT_CDIFF:\n        diff = difflib.context_diff(want_lines, got_lines, n=2)\n        diff = list(diff)[2:] # strip the diff header\n        kind = 'context diff with expected followed by actual'\n    elif optionflags & REPORT_NDIFF:\n        engine = difflib.Differ(charjunk=difflib.IS_CHARACTER_JUNK)\n        diff = list(engine.compare(want_lines, got_lines))\n        kind = 'ndiff with -expected +actual'\n    else:\n        assert 0, 'Bad diff option'\n    # Remove trailing whitespace on diff output.\n    diff = [line.rstrip() + '\\n' for line in diff]\n    return 'Differences (%s):\\n' % kind + _indent(''.join(diff))\n\n# If we're not using diff, then simply list the expected\n# output followed by the actual output.\nif want and got:\n    return 'Expected:\\n%sGot:\\n%s' % (_indent(want), _indent(got))\nelif want:\n    return 'Expected:\\n%sGot nothing\\n' % _indent(want)\nelif got:\n    return 'Expected nothing\\nGot:\\n%s' % _indent(got)\nelse:\n    return 'Expected nothing\\nGot nothing\\n'", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReport that the given example ran successfully.  (Only\ndisplays a message if verbose=True)\n\"\"\"\n", "func_signal": "def report_success(self, out, test, example, got):\n", "code": "if self._verbose:\n    out(\"ok\\n\")", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nCheck that every line in the given list starts with the given\nprefix; if any line does not, then raise a ValueError.\n\"\"\"\n", "func_signal": "def _check_prefix(self, lines, prefix, name, lineno):\n", "code": "for i, line in enumerate(lines):\n    if line and not line.startswith(prefix):\n        raise ValueError('line %r of the docstring for %s has '\n                         'inconsistent leading whitespace: %r' %\n                         (lineno+i+1, name, line))", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReturn the module specified by `module`.  In particular:\n  - If `module` is a module, then return module.\n  - If `module` is a string, then import and return the\n    module with that name.\n  - If `module` is None, then return the calling module.\n    The calling module is assumed to be the module of\n    the stack frame at the given depth in the call stack.\n\"\"\"\n", "func_signal": "def _normalize_module(module, depth=2):\n", "code": "if inspect.ismodule(module):\n    return module\nelif isinstance(module, (str, unicode)):\n    return __import__(module, globals(), locals(), [\"*\"])\nelif module is None:\n    return sys.modules[sys._getframe(depth).f_globals['__name__']]\nelse:\n    raise TypeError(\"Expected a module, string, or None\")", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nExtract all doctest examples from the given string, and return\nthem as a list of `Example` objects.  Line numbers are\n0-based, because it's most common in doctests that nothing\ninteresting appears on the same line as opening triple-quote,\nand so the first interesting line is called \\\"line 1\\\" then.\n\nThe optional argument `name` is a name identifying this\nstring, and is only used for error messages.\n\"\"\"\n", "func_signal": "def get_examples(self, string, name='<string>'):\n", "code": "return [x for x in self.parse(string, name)\n        if isinstance(x, Example)]", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"A unittest suite for one or more doctest files.\n\nThe path to each doctest file is given as a string; the\ninterpretation of that string depends on the keyword argument\n\"module_relative\".\n\nA number of options may be provided as keyword arguments:\n\nmodule_relative\n  If \"module_relative\" is True, then the given file paths are\n  interpreted as os-independent module-relative paths.  By\n  default, these paths are relative to the calling module's\n  directory; but if the \"package\" argument is specified, then\n  they are relative to that package.  To ensure os-independence,\n  \"filename\" should use \"/\" characters to separate path\n  segments, and may not be an absolute path (i.e., it may not\n  begin with \"/\").\n\n  If \"module_relative\" is False, then the given file paths are\n  interpreted as os-specific paths.  These paths may be absolute\n  or relative (to the current working directory).\n\npackage\n  A Python package or the name of a Python package whose directory\n  should be used as the base directory for module relative paths.\n  If \"package\" is not specified, then the calling module's\n  directory is used as the base directory for module relative\n  filenames.  It is an error to specify \"package\" if\n  \"module_relative\" is False.\n\nsetUp\n  A set-up function.  This is called before running the\n  tests in each file. The setUp function will be passed a DocTest\n  object.  The setUp function can access the test globals as the\n  globs attribute of the test passed.\n\ntearDown\n  A tear-down function.  This is called after running the\n  tests in each file.  The tearDown function will be passed a DocTest\n  object.  The tearDown function can access the test globals as the\n  globs attribute of the test passed.\n\nglobs\n  A dictionary containing initial global variables for the tests.\n\noptionflags\n  A set of doctest option flags expressed as an integer.\n\nparser\n  A DocTestParser (or subclass) that should be used to extract\n  tests from the files.\n\nencoding\n  An encoding that will be used to convert the files to unicode.\n\"\"\"\n", "func_signal": "def DocFileSuite(*paths, **kw):\n", "code": "suite = unittest.TestSuite()\n\n# We do this here so that _normalize_module is called at the right\n# level.  If it were called in DocFileTest, then this function\n# would be the caller and we might guess the package incorrectly.\nif kw.get('module_relative', True):\n    kw['package'] = _normalize_module(kw.get('package'))\n\nfor path in paths:\n    suite.addTest(DocFileTest(path, **kw))\n\nreturn suite", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReport that the test runner is about to process the given\nexample.  (Only displays a message if verbose=True)\n\"\"\"\n", "func_signal": "def report_start(self, out, test, example):\n", "code": "if self._verbose:\n    if example.want:\n        out('Trying:\\n' + _indent(example.source) +\n            'Expecting:\\n' + _indent(example.want))\n    else:\n        out('Trying:\\n' + _indent(example.source) +\n            'Expecting nothing\\n')", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReport that the given example raised an unexpected exception.\n\"\"\"\n", "func_signal": "def report_unexpected_exception(self, out, test, example, exc_info):\n", "code": "out(self._failure_header(test, example) +\n    'Exception raised:\\n' + _indent(_exception_traceback(exc_info)))", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReturn a list of the DocTests that are defined by the given\nobject's docstring, or by any of its contained objects'\ndocstrings.\n\nThe optional parameter `module` is the module that contains\nthe given object.  If the module is not specified or is None, then\nthe test finder will attempt to automatically determine the\ncorrect module.  The object's module is used:\n\n    - As a default namespace, if `globs` is not specified.\n    - To prevent the DocTestFinder from extracting DocTests\n      from objects that are imported from other modules.\n    - To find the name of the file containing the object.\n    - To help find the line number of the object within its\n      file.\n\nContained objects whose module does not match `module` are ignored.\n\nIf `module` is False, no attempt to find the module will be made.\nThis is obscure, of use mostly in tests:  if `module` is False, or\nis None but cannot be found automatically, then all objects are\nconsidered to belong to the (non-existent) module, so all contained\nobjects will (recursively) be searched for doctests.\n\nThe globals for each DocTest is formed by combining `globs`\nand `extraglobs` (bindings in `extraglobs` override bindings\nin `globs`).  A new copy of the globals dictionary is created\nfor each DocTest.  If `globs` is not specified, then it\ndefaults to the module's `__dict__`, if specified, or {}\notherwise.  If `extraglobs` is not specified, then it defaults\nto {}.\n\n\"\"\"\n# If name was not specified, then extract it from the object.\n", "func_signal": "def find(self, obj, name=None, module=None, globs=None, extraglobs=None):\n", "code": "if name is None:\n    name = getattr(obj, '__name__', None)\n    if name is None:\n        raise ValueError(\"DocTestFinder.find: name must be given \"\n                \"when obj.__name__ doesn't exist: %r\" %\n                         (type(obj),))\n\n# Find the module that contains the given object (if obj is\n# a module, then module=obj.).  Note: this may fail, in which\n# case module will be None.\nif module is False:\n    module = None\nelif module is None:\n    module = inspect.getmodule(obj)\n\n# Read the module's source code.  This is used by\n# DocTestFinder._find_lineno to find the line number for a\n# given object's docstring.\ntry:\n    file = inspect.getsourcefile(obj) or inspect.getfile(obj)\n    source_lines = linecache.getlines(file)\n    if not source_lines:\n        source_lines = None\nexcept TypeError:\n    source_lines = None\n\n# Initialize globals, and merge in extraglobs.\nif globs is None:\n    if module is None:\n        globs = {}\n    else:\n        globs = module.__dict__.copy()\nelse:\n    globs = globs.copy()\nif extraglobs is not None:\n    globs.update(extraglobs)\n\n# Recursively expore `obj`, extracting DocTests.\ntests = []\nself._find(tests, obj, name, module, source_lines, globs, {})\n# Sort the tests by alpha order of names, for consistency in\n# verbose-mode output.  This was a feature of doctest in Pythons\n# <= 2.3 that got lost by accident in 2.4.  It was repaired in\n# 2.4.4 and 2.5.\ntests.sort()\nreturn tests", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReturn a string containing a traceback message for the given\nexc_info tuple (as returned by sys.exc_info()).\n\"\"\"\n# Get a traceback message.\n", "func_signal": "def _exception_traceback(exc_info):\n", "code": "excout = StringIO()\nexc_type, exc_val, exc_tb = exc_info\ntraceback.print_exception(exc_type, exc_val, exc_tb, file=excout)\nreturn excout.getvalue()", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"\nReturn a dictionary containing option overrides extracted from\noption directives in the given source string.\n\n`name` is the string's name, and `lineno` is the line number\nwhere the example starts; both are used for error messages.\n\"\"\"\n", "func_signal": "def _find_options(self, source, name, lineno):\n", "code": "options = {}\n# (note: with the current regexp, this will match at most once:)\nfor m in self._OPTION_DIRECTIVE_RE.finditer(source):\n    option_strings = m.group(1).replace(',', ' ').split()\n    for option in option_strings:\n        if (option[0] not in '+-' or\n            option[1:] not in OPTIONFLAGS_BY_NAME):\n            raise ValueError('line %r of the doctest for %s '\n                             'has an invalid option: %r' %\n                             (lineno+1, name, option))\n        flag = OPTIONFLAGS_BY_NAME[option[1:]]\n        options[flag] = (option[0] == '+')\nif options and self._IS_BLANK_OR_COMMENT(source):\n    raise ValueError('line %r of the doctest for %s has an option '\n                     'directive on a line with no example: %r' %\n                     (lineno, name, source))\nreturn options", "path": "spkg\\standard\\sage_scripts\\ncadoctest.py", "repo_name": "certik/spd", "stars": 4, "license": "other", "language": "python", "size": 1377}
{"docstring": "\"\"\"Initialize the data table from a table schema and (optionally) data.\n\nSee the class documentation for more information on table schema and data\nvalues.\n\nArgs:\n  table_description: A table schema, following one of the formats described\n                     in TableDescriptionParser(). Schemas describe the\n                     column names, data types, and labels. See\n                     TableDescriptionParser() for acceptable formats.\n  data: Optional. If given, fills the table with the given data. The data\n        structure must be consistent with schema in table_description. See\n        the class documentation for more information on acceptable data. You\n        can add data later by calling AppendData().\n\nRaises:\n  DataTableException: Raised if the data and the description did not match,\n                      or did not use the supported formats.\n\"\"\"\n", "func_signal": "def __init__(self, table_description, data=None):\n", "code": "self.__columns = self.TableDescriptionParser(table_description)\nself.__data = []\nif data:\n  self.LoadData(data)", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Writes the data table as a JS code string.\n\nThis method writes a string of JS code that can be run to\ngenerate a DataTable with the specified data. Typically used for debugging\nonly.\n\nArgs:\n  name: The name of the table. The name would be used as the DataTable's\n        variable name in the created JS code.\n  columns_order: Optional. Specifies the order of columns in the\n                 output table. Specify a list of all column IDs in the order\n                 in which you want the table created.\n                 Note that you must list all column IDs in this parameter,\n                 if you use it.\n  order_by: Optional. Specifies the name of the column(s) to sort by.\n            Passed as is to _PreparedData.\n\nReturns:\n  A string of JS code that, when run, generates a DataTable with the given\n  name and the data stored in the DataTable object.\n  Example result:\n    \"var tab1 = new google.visualization.DataTable();\n     tab1.addColumn('string', 'a', 'a');\n     tab1.addColumn('number', 'b', 'b');\n     tab1.addColumn('boolean', 'c', 'c');\n     tab1.addRows(10);\n     tab1.setCell(0, 0, 'a');\n     tab1.setCell(0, 1, 1);\n     tab1.setCell(0, 2, true);\n     ...\n     tab1.setCell(9, 0, 'c');\n     tab1.setCell(9, 1, 3, '3$');\n     tab1.setCell(9, 2, false);\"\n\nRaises:\n  DataTableException: The data does not match the type.\n\"\"\"\n", "func_signal": "def ToJSCode(self, name, columns_order=None, order_by=()):\n", "code": "if columns_order is None:\n  columns_order = [col[\"id\"] for col in self.__columns]\ncol_dict = dict([(col[\"id\"], col) for col in self.__columns])\n\n# We first create the table with the given name\njscode = \"var %s = new google.visualization.DataTable();\\n\" % name\n\n# We add the columns to the table\nfor col in columns_order:\n  jscode += \"%s.addColumn('%s', '%s', '%s');\\n\" % (name,\n                                                   col_dict[col][\"type\"],\n                                                   col_dict[col][\"label\"],\n                                                   col_dict[col][\"id\"])\njscode += \"%s.addRows(%d);\\n\" % (name, len(self.__data))\n\n# We now go over the data and add each row\nfor (i, row) in enumerate(self._PreparedData(order_by)):\n  # We add all the elements of this row by their order\n  for (j, col) in enumerate(columns_order):\n    if col not in row or row[col] is None:\n      continue\n    value = self.SingleValueToJS(row[col], col_dict[col][\"type\"])\n    if isinstance(value, tuple):\n      # We have a formatted value as well\n      jscode += (\"%s.setCell(%d, %d, %s, %s);\\n\" %\n                 (name, i, j, value[0], value[1]))\n    else:\n      jscode += \"%s.setCell(%d, %d, %s);\\n\" % (name, i, j, value)\nreturn jscode", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Parses the table_description object for internal use.\n\nParses the user-submitted table description into an internal format used\nby the Python DataTable class. Returns the flat list of parsed columns.\n\nArgs:\n  table_description: A description of the table which should comply\n                     with one of the formats described below.\n  depth: Optional. The depth of the first level in the current description.\n         Used by recursive calls to this function.\n\nReturns:\n  List of columns, where each column represented by a dictionary with the\n  keys: id, label, type, depth, container which means the following:\n  - id: the id of he column\n  - name: The name of the column\n  - type: The datatype of the elements in this column. Allowed types are\n          described in ColumnTypeParser().\n  - depth: The depth of this column in the table description\n  - container: 'dict', 'iter' or 'scalar' for parsing the format easily.\n  The returned description is flattened regardless of how it was given.\n\nRaises:\n  DataTableException: Error in a column description or in the description\n                      structure.\n\nExamples:\n  A column description can be of the following forms:\n   'id'\n   ('id',)\n   ('id', 'type')\n   ('id', 'type', 'label')\n   or as a dictionary:\n   'id': 'type'\n   'id': ('type',)\n   'id': ('type', 'label')\n  If the type is not specified, we treat it as string.\n  If no specific label is given, the label is simply the id.\n\n  input: [('a', 'date'), ('b', 'timeofday')]\n  output: [{'id': 'a', 'label': 'a', 'type': 'date',\n            'depth': 0, 'container': 'iter'},\n           {'id': 'b', 'label': 'b', 'type': 'timeofday',\n           'depth': 0, 'container': 'iter'}]\n\n  input: {'a': [('b', 'number'), ('c', 'string', 'column c')]}\n  output: [{'id': 'a', 'label': 'a', 'type': 'string',\n            'depth': 0, 'container': 'dict'},\n           {'id': 'b', 'label': 'b', 'type': 'number',\n            'depth': 1, 'container': 'iter'},\n           {'id': 'c', 'label': 'column c', 'type': 'string',\n            'depth': 1, 'container': 'iter'}]\n\n  input:  {('a', 'number', 'column a'): { 'b': 'number', 'c': 'string'}}\n  output: [{'id': 'a', 'label': 'column a', 'type': 'number',\n            'depth': 0, 'container': 'dict'},\n           {'id': 'b', 'label': 'b', 'type': 'number',\n            'depth': 1, 'container': 'dict'},\n           {'id': 'c', 'label': 'c', 'type': 'string',\n            'depth': 1, 'container': 'dict'}]\n\n  input: { ('w', 'string', 'word'): ('c', 'number', 'count') }\n  output: [{'id': 'w', 'label': 'word', 'type': 'string',\n            'depth': 0, 'container': 'dict'},\n           {'id': 'c', 'label': 'count', 'type': 'number',\n            'depth': 1, 'container': 'scalar'}]\n\"\"\"\n# For the recursion step, we check for a scalar object (string or tuple)\n", "func_signal": "def TableDescriptionParser(table_description, depth=0):\n", "code": "if isinstance(table_description, (types.StringTypes, tuple)):\n  parsed_col = DataTable.ColumnTypeParser(table_description)\n  parsed_col[\"depth\"] = depth\n  parsed_col[\"container\"] = \"scalar\"\n  return [parsed_col]\n\n# Since it is not scalar, table_description must be iterable.\nif not hasattr(table_description, \"__iter__\"):\n  raise DataTableException(\"Expected an iterable object, got %s\" %\n                           type(table_description))\nif not isinstance(table_description, dict):\n  # We expects a non-dictionary iterable item.\n  columns = []\n  for desc in table_description:\n    parsed_col = DataTable.ColumnTypeParser(desc)\n    parsed_col[\"depth\"] = depth\n    parsed_col[\"container\"] = \"iter\"\n    columns.append(parsed_col)\n  if not columns:\n    raise DataTableException(\"Description iterable objects should not\"\n                             \" be empty.\")\n  return columns\n# The other case is a dictionary\nif not table_description:\n  raise DataTableException(\"Empty dictionaries are not allowed inside\"\n                           \" description\")\n\n# The number of keys in the dictionary separates between the two cases of\n# more levels below or this is the most inner dictionary.\nif len(table_description) != 1:\n  # This is the most inner dictionary. Parsing types.\n  columns = []\n  # We sort the items, equivalent to sort the keys since they are unique\n  for key, value in sorted(table_description.items()):\n    # We parse the column type as (key, type) or (key, type, label) using\n    # ColumnTypeParser.\n    if isinstance(value, tuple):\n      parsed_col = DataTable.ColumnTypeParser((key,) + value)\n    else:\n      parsed_col = DataTable.ColumnTypeParser((key, value))\n    parsed_col[\"depth\"] = depth\n    parsed_col[\"container\"] = \"dict\"\n    columns.append(parsed_col)\n  return columns\n# This is an outer dictionary, must have at most one key.\nparsed_col = DataTable.ColumnTypeParser(table_description.keys()[0])\nparsed_col[\"depth\"] = depth\nparsed_col[\"container\"] = \"dict\"\nreturn ([parsed_col] +\n        DataTable.TableDescriptionParser(table_description.values()[0],\n                                         depth=depth + 1))", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Inner function to assist LoadData.\"\"\"\n# We first check that col_index has not exceeded the columns size\n", "func_signal": "def _InnerAppendData(self, prev_col_values, data, col_index):\n", "code": "if col_index >= len(self.__columns):\n  raise DataTableException(\"The data does not match description, too deep\")\n\n# Dealing with the scalar case, the data is the last value.\nif self.__columns[col_index][\"container\"] == \"scalar\":\n  prev_col_values[self.__columns[col_index][\"id\"]] = data\n  self.__data.append(prev_col_values)\n  return\n\nif self.__columns[col_index][\"container\"] == \"iter\":\n  if not hasattr(data, \"__iter__\") or isinstance(data, dict):\n    raise DataTableException(\"Expected iterable object, got %s\" %\n                             type(data))\n  # We only need to insert the rest of the columns\n  # If there are less items than expected, we only add what there is.\n  for value in data:\n    if col_index >= len(self.__columns):\n      raise DataTableException(\"Too many elements given in data\")\n    prev_col_values[self.__columns[col_index][\"id\"]] = value\n    col_index += 1\n  self.__data.append(prev_col_values)\n  return\n\n# We know the current level is a dictionary, we verify the type.\nif not isinstance(data, dict):\n  raise DataTableException(\"Expected dictionary at current level, got %s\" %\n                           type(data))\n# We check if this is the last level\nif self.__columns[col_index][\"depth\"] == self.__columns[-1][\"depth\"]:\n  # We need to add the keys in the dictionary as they are\n  for col in self.__columns[col_index:]:\n    if col[\"id\"] in data:\n      prev_col_values[col[\"id\"]] = data[col[\"id\"]]\n  self.__data.append(prev_col_values)\n  return\n\n# We have a dictionary in an inner depth level.\nif not data.keys():\n  # In case this is an empty dictionary, we add a record with the columns\n  # filled only until this point.\n  self.__data.append(prev_col_values)\nelse:\n  for key in sorted(data):\n    col_values = dict(prev_col_values)\n    col_values[self.__columns[col_index][\"id\"]] = key\n    self._InnerAppendData(col_values, data[key], col_index + 1)", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Return a list of all time zones known to the system.\"\"\"\n", "func_signal": "def list():\n", "code": "handle = _winreg.ConnectRegistry(None, _winreg.HKEY_LOCAL_MACHINE)\ntzkey = _winreg.OpenKey(handle, TZKEYNAME)\nresult = [_winreg.EnumKey(tzkey, i)\n          for i in range(_winreg.QueryInfoKey(tzkey)[0])]\ntzkey.Close()\nhandle.Close()\nreturn result", "path": "dateutil\\tzwin.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Parses a single column description. Internal helper method.\n\nArgs:\n  description: a column description in the possible formats:\n   'id'\n   ('id',)\n   ('id', 'type')\n   ('id', 'type', 'label')\nReturns:\n  Dictionary with the following keys: id, label and type where:\n    - If label not given, it equals the id.\n    - If type not given, string is used by default.\n\nRaises:\n  DataTableException: The column description did not match the RE.\n\"\"\"\n", "func_signal": "def ColumnTypeParser(description):\n", "code": "if not description:\n  raise DataTableException(\"Description error: empty description given\")\n\nif not isinstance(description, (types.StringTypes, tuple)):\n  raise DataTableException(\"Description error: expected either string or \"\n                           \"tuple, got %s.\" % type(description))\n\nif isinstance(description, types.StringTypes):\n  description = (description,)\n\n# According to the tuple's length, we fill the keys\n# We verify everything is of type string\nfor elem in description:\n  if not isinstance(elem, types.StringTypes):\n    raise DataTableException((\"Description error: expected tuple of \"\n                              \"strings, current element of type %s.\" %\n                              type(elem)))\ndesc_dict = {\"id\": description[0],\n             \"label\": description[0],\n             \"type\": \"string\"}\nif len(description) > 1:\n  desc_dict[\"type\"] = description[1].lower()\n  if len(description) > 2:\n    desc_dict[\"label\"] = description[2]\n    if len(description) > 3:\n      raise DataTableException(\"Description error: tuple of length > 3\")\nreturn desc_dict", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Writes the data table as an HTML table code string.\n\nArgs:\n  columns_order: Optional. Specifies the order of columns in the\n                 output table. Specify a list of all column IDs in the order\n                 in which you want the table created.\n                 Note that you must list all column IDs in this parameter,\n                 if you use it.\n  order_by: Optional. Specifies the name of the column(s) to sort by.\n            Passed as is to _PreparedData.\n\nReturns:\n  An HTML table code string.\n  Example result (the result is without the newlines):\n   <html><body><table border='1'>\n    <thead><tr><th>a</th><th>b</th><th>c</th></tr></thead>\n    <tbody>\n     <tr><td>1</td><td>\"z\"</td><td>2</td></tr>\n     <tr><td>\"3$\"</td><td>\"w\"</td><td></td></tr>\n    </tbody>\n   </table></body></html>\n\nRaises:\n  DataTableException: The data does not match the type.\n\"\"\"\n", "func_signal": "def ToHtml(self, columns_order=None, order_by=()):\n", "code": "table_template = \"<html><body><table border='1'>%s</table></body></html>\"\ncolumns_template = \"<thead><tr>%s</tr></thead>\"\nrows_template = \"<tbody>%s</tbody>\"\nrow_template = \"<tr>%s</tr>\"\nheader_cell_template = \"<th>%s</th>\"\ncell_template = \"<td>%s</td>\"\n\nif columns_order is None:\n  columns_order = [col[\"id\"] for col in self.__columns]\ncol_dict = dict([(col[\"id\"], col) for col in self.__columns])\n\ncolumns_list = []\nfor col in columns_order:\n  columns_list.append(header_cell_template % col_dict[col][\"label\"])\ncolumns_html = columns_template % \"\".join(columns_list)\n\nrows_list = []\n# We now go over the data and add each row\nfor row in self._PreparedData(order_by):\n  cells_list = []\n  # We add all the elements of this row by their order\n  for col in columns_order:\n    # For empty string we want empty quotes (\"\").\n    value = \"\"\n    if col in row and row[col] is not None:\n      value = self.SingleValueToJS(row[col], col_dict[col][\"type\"])\n    if isinstance(value, tuple):\n      # We have a formatted value and we're going to use it\n      cells_list.append(cell_template % cgi.escape(value[1]))\n    else:\n      cells_list.append(cell_template % cgi.escape(value))\n  rows_list.append(row_template % \"\".join(cells_list))\nrows_html = rows_template % \"\".join(rows_list)\n\nreturn table_template % (columns_html + rows_html)", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Writes the data table as a CSV string.\n\nArgs:\n  columns_order: Optional. Specifies the order of columns in the\n                 output table. Specify a list of all column IDs in the order\n                 in which you want the table created.\n                 Note that you must list all column IDs in this parameter,\n                 if you use it.\n  order_by: Optional. Specifies the name of the column(s) to sort by.\n            Passed as is to _PreparedData.\n\nReturns:\n  A CSV string representing the table.\n  Example result:\n   'a', 'b', 'c'\n   1, 'z', 2\n   3, 'w', ''\n\nRaises:\n  DataTableException: The data does not match the type.\n\"\"\"\n", "func_signal": "def ToCsv(self, columns_order=None, order_by=()):\n", "code": "if columns_order is None:\n  columns_order = [col[\"id\"] for col in self.__columns]\ncol_dict = dict([(col[\"id\"], col) for col in self.__columns])\n\ncolumns_list = []\nfor col in columns_order:\n  columns_list.append(DataTable._EscapeValue(col_dict[col][\"label\"]))\ncolumns_line = \", \".join(columns_list)\n\nrows_list = []\n# We now go over the data and add each row\nfor row in self._PreparedData(order_by):\n  cells_list = []\n  # We add all the elements of this row by their order\n  for col in columns_order:\n    value = \"''\"\n    if col in row and row[col] is not None:\n      value = self.SingleValueToJS(row[col], col_dict[col][\"type\"])\n    if isinstance(value, tuple):\n      # We have a formatted value. Using it only for date/time types.\n      if col_dict[col][\"type\"] in [\"date\", \"datetime\", \"timeofday\"]:\n        cells_list.append(value[1])\n      else:\n        cells_list.append(value[0])\n    else:\n      # We need to quote date types, because they contain commas.\n      if (col_dict[col][\"type\"] in [\"date\", \"datetime\", \"timeofday\"] and\n          value != \"''\"):\n        value = \"'%s'\" % value\n      cells_list.append(value)\n  rows_list.append(\", \".join(cells_list))\nrows = \"\\n\".join(rows_list)\n\nreturn \"%s\\n%s\" % (columns_line, rows)", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Puts the string in quotes, and escapes any inner quotes and slashes.\"\"\"\n", "func_signal": "def _EscapeValue(v):\n", "code": "if isinstance(v, unicode):\n  # Here we use repr as in the usual case, but on unicode strings, it\n  # also escapes the unicode characters (which we want to leave as is).\n  # So, after repr() we decode using raw-unicode-escape, which decodes\n  # only the unicode characters, and leaves all the rest (\", ', \\n and\n  # more) escaped.\n  # We don't take the first character, because repr adds a u in the\n  # beginning of the string (usual repr output for unicode is u'...').\n  return repr(v).decode(\"raw-unicode-escape\")[1:]\n# Here we use python built-in escaping mechanism for string using repr.\nreturn repr(str(v))", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "# move to info\n", "func_signal": "def validate(self, res):\n", "code": "if res.year is not None:\n    res.year = self.convertyear(res.year)\nif res.tzoffset == 0 and not res.tzname or res.tzname == 'Z':\n    res.tzname = \"UTC\"\n    res.tzoffset = 0\nelif res.tzoffset != 0 and res.tzname and self.utczone(res.tzname):\n    res.tzoffset = 0\nreturn True", "path": "dateutil\\parser.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Convert a registry key's values to a dictionary.\"\"\"\n", "func_signal": "def valuestodict(key):\n", "code": "dict = {}\nsize = _winreg.QueryInfoKey(key)[1]\nfor i in range(size):\n    data = _winreg.EnumValue(key, i)\n    dict[data[0]] = data[1]\nreturn dict", "path": "dateutil\\tzwin.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Writes a JSON string that can be used in a JS DataTable constructor.\n\nThis method writes a JSON string that can be passed directly into a Google\nVisualization API DataTable constructor. Use this output if you are\nhosting the visualization HTML on your site, and want to code the data\ntable in Python. Pass this string into the\ngoogle.visualization.DataTable constructor, e.g,:\n  ... on my page that hosts my visualization ...\n  google.setOnLoadCallback(drawTable);\n  function drawTable() {\n    var data = new google.visualization.DataTable(_my_JSon_string, 0.5);\n    myTable.draw(data);\n  }\n\nArgs:\n  columns_order: Optional. Specifies the order of columns in the\n                 output table. Specify a list of all column IDs in the order\n                 in which you want the table created.\n                 Note that you must list all column IDs in this parameter,\n                 if you use it.\n  order_by: Optional. Specifies the name of the column(s) to sort by.\n            Passed as is to _PreparedData().\n\nReturns:\n  A JSon constructor string to generate a JS DataTable with the data\n  stored in the DataTable object.\n  Example result (the result is without the newlines):\n   {cols: [{id:'a',label:'a',type:'number'},\n           {id:'b',label:'b',type:'string'},\n          {id:'c',label:'c',type:'number'}],\n    rows: [{c:[{v:1},{v:'z'},{v:2}]}, c:{[{v:3,f:'3$'},{v:'w'},{v:null}]}]}\n\nRaises:\n  DataTableException: The data does not match the type.\n\"\"\"\n", "func_signal": "def ToJSon(self, columns_order=None, order_by=()):\n", "code": "if columns_order is None:\n  columns_order = [col[\"id\"] for col in self.__columns]\ncol_dict = dict([(col[\"id\"], col) for col in self.__columns])\n\n# Creating the columns jsons\ncols_jsons = [\"{id:'%(id)s',label:'%(label)s',type:'%(type)s'}\" %\n              col_dict[col_id] for col_id in columns_order]\n\n# Creating the rows jsons\nrows_jsons = []\nfor row in self._PreparedData(order_by):\n  cells_jsons = []\n  for col in columns_order:\n    # We omit the {v:null} for a None value of the not last column\n    value = row.get(col, None)\n    if value is None and col != columns_order[-1]:\n      cells_jsons.append(\"\")\n    else:\n      value = self.SingleValueToJS(value, col_dict[col][\"type\"])\n      if isinstance(value, tuple):\n        # We have a formatted value as well\n        cells_jsons.append(\"{v:%s,f:%s}\" % value)\n      else:\n        cells_jsons.append(\"{v:%s}\" % value)\n  rows_jsons.append(\"{c:[%s]}\" % \",\".join(cells_jsons))\n\n# We now join the columns jsons and the rows jsons\njson = \"{cols: [%s],rows: [%s]}\" % (\",\".join(cols_jsons),\n                                    \",\".join(rows_jsons))\nreturn json", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Writes the right response according to the request string passed in tqx.\n\nThis method parses the tqx request string (format of which is defined in\nthe documentation for implementing a data source of Google Visualization),\nand returns the right response according to the request.\nIt parses out the \"out\" parameter of tqx, calls the relevant response\n(ToJSonResponse() for \"json\", ToCsv() for \"csv\" and ToHtml() for \"html\")\nand passes the response function the rest of the relevant request keys.\n\nArgs:\n  columns_order: Optional. Passed as is to the relevant response function.\n  order_by: Optional. Passed as is to the relevant response function.\n  tqx: Optional. The request string as received by HTTP GET. Should be in\n       the format \"key1:value1;key2:value2...\". All keys have a default\n       value, so an empty string will just do the default (which is calling\n       ToJSonResponse() with no extra parameters).\n\nReturns:\n  A response string, as returned by the relevant response function.\n\nRaises:\n  DataTableException: One of the parameters passed in tqx is not supported.\n\"\"\"\n", "func_signal": "def ToResponse(self, columns_order=None, order_by=(), tqx=\"\"):\n", "code": "tqx_dict = {}\nif tqx:\n  tqx_dict = dict(opt.split(\":\") for opt in tqx.split(\";\"))\nif tqx_dict.get(\"version\", 0.5) != 0.5:\n  raise DataTableException(\n      \"Version (%s) passed by request is not supported.\"\n      % tqx_dict[\"version\"])\n\nif tqx_dict.get(\"out\", \"json\") == \"json\":\n  response_handler = tqx_dict.get(\"responseHandler\",\n                                  \"google.visualization.Query.setResponse\")\n  return self.ToJSonResponse(columns_order, order_by,\n                             req_id=tqx_dict.get(\"reqId\", 0),\n                             response_handler=response_handler)\nelif tqx_dict[\"out\"] == \"html\":\n  return self.ToHtml(columns_order, order_by)\nelif tqx_dict[\"out\"] == \"csv\":\n  return self.ToCsv(columns_order, order_by)\nelse:\n  raise DataTableException(\n      \"'out' parameter: '%s' is not supported\" % tqx_dict[\"out\"])", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"dayofweek == 0 means Sunday, whichweek 5 means last instance\"\"\"\n", "func_signal": "def picknthweekday(year, month, dayofweek, hour, minute, whichweek):\n", "code": "first = datetime.datetime(year, month, 1, hour, minute)\nweekdayone = first.replace(day=((dayofweek-first.isoweekday())%7+1))\nfor n in xrange(whichweek):\n    dt = weekdayone+(whichweek-n)*ONEWEEK\n    if dt.month == month:\n        return dt", "path": "dateutil\\tzwin.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "# We can't use mktime here. It is unstable when deciding if\n# the hour near to a change is DST or not.\n# \n# timestamp = time.mktime((dt.year, dt.month, dt.day, dt.hour,\n#                         dt.minute, dt.second, dt.weekday(), 0, -1))\n# return time.localtime(timestamp).tm_isdst\n#\n# The code above yields the following result:\n#\n#>>> import tz, datetime\n#>>> t = tz.tzlocal()\n#>>> datetime.datetime(2003,2,15,23,tzinfo=t).tzname()\n#'BRDT'\n#>>> datetime.datetime(2003,2,16,0,tzinfo=t).tzname()\n#'BRST'\n#>>> datetime.datetime(2003,2,15,23,tzinfo=t).tzname()\n#'BRST'\n#>>> datetime.datetime(2003,2,15,22,tzinfo=t).tzname()\n#'BRDT'\n#>>> datetime.datetime(2003,2,15,23,tzinfo=t).tzname()\n#'BRDT'\n#\n# Here is a more stable implementation:\n#\n", "func_signal": "def _isdst(self, dt):\n", "code": "timestamp = ((dt.toordinal() - EPOCHORDINAL) * 86400\n             + dt.hour * 3600\n             + dt.minute * 60\n             + dt.second)\nreturn time.localtime(timestamp+time.timezone).tm_isdst", "path": "dateutil\\tz.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Appends new data to the table.\n\nData is appended in rows. Data must comply with\nthe table schema passed in to __init__(). See SingleValueToJS() for a list\nof acceptable data types. See the class documentation for more information\nand examples of schema and data values.\n\nArgs:\n  data: The row to add to the table. The data must conform to the table\n        description format.\n\nRaises:\n  DataTableException: The data structure does not match the description.\n\"\"\"\n# If the maximal depth is 0, we simply iterate over the data table\n# lines and insert them using _InnerAppendData. Otherwise, we simply\n# let the _InnerAppendData handle all the levels.\n", "func_signal": "def AppendData(self, data):\n", "code": "if not self.__columns[-1][\"depth\"]:\n  for line in data:\n    self._InnerAppendData({}, line, 0)\nelse:\n  self._InnerAppendData({}, data, 0)", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Prepares the data for enumeration - sorting it by order_by.\n\nArgs:\n  order_by: Optional. Specifies the name of the column(s) to sort by, and\n            (optionally) which direction to sort in. Default sort direction\n            is asc. Following formats are accepted:\n            \"string_col_name\"  -- For a single key in default (asc) order.\n            (\"string_col_name\", \"asc|desc\") -- For a single key.\n            [(\"col_1\",\"asc|desc\"), (\"col_2\",\"asc|desc\")] -- For more than\n                one column, an array of tuples of (col_name, \"asc|desc\").\n\nReturns:\n  The data sorted by the keys given.\n\nRaises:\n  DataTableException: Sort direction not in 'asc' or 'desc'\n\"\"\"\n", "func_signal": "def _PreparedData(self, order_by=()):\n", "code": "if not order_by:\n  return self.__data\n\nproper_sort_keys = []\nif isinstance(order_by, types.StringTypes) or (\n    isinstance(order_by, tuple) and len(order_by) == 2 and\n    order_by[1].lower() in [\"asc\", \"desc\"]):\n  order_by = (order_by,)\nfor key in order_by:\n  if isinstance(key, types.StringTypes):\n    proper_sort_keys.append((key, 1))\n  elif (isinstance(key, (list, tuple)) and len(key) == 2 and\n        key[1].lower() in (\"asc\", \"desc\")):\n    proper_sort_keys.append((key[0], key[1].lower() == \"asc\" and 1 or -1))\n  else:\n    raise DataTableException(\"Expected tuple with second value: \"\n                             \"'asc' or 'desc'\")\n\ndef SortCmpFunc(row1, row2):\n  \"\"\"cmp function for sorted. Compares by keys and 'asc'/'desc' keywords.\"\"\"\n  for key, asc_mult in proper_sort_keys:\n    cmp_result = asc_mult * cmp(row1.get(key), row2.get(key))\n    if cmp_result:\n      return cmp_result\n  return 0\n\nreturn sorted(self.__data, cmp=SortCmpFunc)", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Translates a single value and type into a JS value.\n\nInternal helper method.\n\nArgs:\n  value: The value which should be converted\n  value_type: One of \"string\", \"number\", \"boolean\", \"date\", \"datetime\" or\n              \"timeofday\".\n\nReturns:\n  The proper JS format (as string) of the given value according to the\n  given value_type. For None, we simply return \"null\".\n  If a tuple is given, it should be of the form (value, formatted value)\n  where the formatted value is a string. In such a case, we return\n  the tuple of (JS value as string, formatted value).\n  The real type of the given value is not strictly checked. For example,\n  any type can be used for string - as we simply take its str( ) and for\n  boolean value we just check \"if value\".\n  Examples:\n    SingleValueToJS(False, \"boolean\") returns \"false\"\n    SingleValueToJS((5, \"5$\"), \"number\") returns (\"5\", \"'5$'\")\n\nRaises:\n  DataTableException: The value and type did not match in a not-recoverable\n                      way, for example given value 'abc' for type 'number'.\n\"\"\"\n", "func_signal": "def SingleValueToJS(value, value_type):\n", "code": "if isinstance(value, tuple):\n  # In case of a tuple, we run the same function on the value itself and\n  # add the formatted value.\n  if len(value) != 2:\n    raise DataTableException(\"Wrong format for value and formatting - %s.\" %\n                             str(value))\n  if not isinstance(value[1], types.StringTypes):\n    raise DataTableException(\"Formatted value is not string, given %s.\" %\n                             type(value[1]))\n  js_value = DataTable.SingleValueToJS(value[0], value_type)\n  if js_value == \"null\":\n    raise DataTableException(\"An empty cell can not have formatting.\")\n  return (js_value, DataTable._EscapeValue(value[1]))\n\n# The standard case - no formatting.\nt_value = type(value)\nif value is None:\n  return \"null\"\nif value_type == \"boolean\":\n  if value:\n    return \"true\"\n  return \"false\"\n\nelif value_type == \"number\":\n  if isinstance(value, (int, long, float)):\n    return str(value)\n  raise DataTableException(\"Wrong type %s when expected number\" % t_value)\n\nelif value_type == \"string\":\n  if isinstance(value, tuple):\n    raise DataTableException(\"Tuple is not allowed as string value.\")\n  return DataTable._EscapeValue(value)\n\nelif value_type == \"date\":\n  if not isinstance(value, (datetime.date, datetime.datetime)):\n    raise DataTableException(\"Wrong type %s when expected date\" % t_value)\n    # We need to shift the month by 1 to match JS Date format\n  return \"new Date(%d,%d,%d)\" % (value.year, value.month - 1, value.day)\n\nelif value_type == \"timeofday\":\n  if not isinstance(value, (datetime.time, datetime.datetime)):\n    raise DataTableException(\"Wrong type %s when expected time\" % t_value)\n  return \"[%d,%d,%d]\" % (value.hour, value.minute, value.second)\n\nelif value_type == \"datetime\":\n  if not isinstance(value, datetime.datetime):\n    raise DataTableException(\"Wrong type %s when expected datetime\" %\n                             t_value)\n  return \"new Date(%d,%d,%d,%d,%d,%d)\" % (value.year,\n                                          value.month - 1,  # To match JS\n                                          value.day,\n                                          value.hour,\n                                          value.minute,\n                                          value.second)\n# If we got here, it means the given value_type was not one of the\n# supported types.\nraise DataTableException(\"Unsupported type %s\" % value_type)", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Loads new data to the data table, clearing existing data.\"\"\"\n", "func_signal": "def LoadData(self, data):\n", "code": "self.__data = []\nself.AppendData(data)", "path": "gviz_api.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"Parse a I[.F] seconds value into (seconds, microseconds).\"\"\"\n", "func_signal": "def _parsems(value):\n", "code": "if \".\" not in value:\n    return int(value), 0\nelse:\n    i, f = value.split(\".\")\n    return int(i), int(f.ljust(6, \"0\")[:6])", "path": "dateutil\\parser.py", "repo_name": "jazzido/arbolito", "stars": 4, "license": "None", "language": "python", "size": 312}
{"docstring": "\"\"\"\n\u0423\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441\u0447\u0435\u0442\u0447\u0438\u043a\u0430 \u043d\u0430 \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.\n\n@param value: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0438\u043d\u043a\u0440\u0435\u043c\u0435\u043d\u0442\u0430\n\"\"\"\n", "func_signal": "def increment(self, value=1):\n", "code": "self.counter += value\nself.increments += 1", "path": "spamfighter\\core\\counters.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0442\u043e\u0440.\n\n@param dir: \u043f\u043e\u0434\u043a\u0430\u0442\u0430\u043b\u043e\u0433, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0434\u043e\u043b\u0436\u043d\u044b \u0445\u0440\u0430\u043d\u0438\u0442\u044c\u0441\u044f \u0444\u0430\u0439\u043b\u044b \u0411\u0414\n@type dir: C{str}\n@param name: \u0438\u043c\u044f \u0411\u0414\n@type name: C{str}\n\"\"\"\n", "func_signal": "def __init__(self, dir, name):\n", "code": "dir = os.path.join(config.storage.dbm.path, dir)\nif not os.path.exists(dir):\n    os.mkdir(dir)\n\nself.db = anydbm.open(os.path.join(dir, name + '.db'), 'c')", "path": "spamfighter\\core\\storage\\dbm.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n@param marker: \u0442\u0438\u043f \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438, \"good\" \u0438\u043b\u0438 \"bad\"\n@type marker: C{str}\n\"\"\"\n", "func_signal": "def analyze(self, domain, message, marker=\"good\", **kwargs):\n", "code": "super(modelTrain, self).analyze(domain, message, **kwargs)\n\nreturn self.model.train(self.text, marker == \"good\").addCallback(lambda _: True)", "path": "spamfighter\\rules\\model.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0442\u043e\u0440.\n\n@param name: \u0438\u043c\u044f \u043f\u043b\u0430\u0433\u0438\u043d\u0430-\u0434\u043e\u043c\u0435\u043d\u0430\n@type name: C{str}\n@param dict: \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0435 \u0434\u043e\u043c\u0435\u043d\u0430\n@type dict: C{dict}\n\"\"\"\n", "func_signal": "def __init__(self, name, dict, parent=None):\n", "code": "self._name = name\nself.domain = BaseDomain(dict=dict, key=name, parent=parent)", "path": "spamfighter\\plugins\\static_domain_provider.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u0417\u0430\u043f\u0438\u0441\u0430\u0442\u044c (\u043f\u0435\u0440\u0435\u0437\u0430\u043f\u0438\u0441\u0430\u0442\u044c) \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u043b\u044e\u0447\u0430.\n\n@param key: \u043a\u043b\u044e\u0447\n@type key: C{str}\n@param value: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n@type value: C{str} \u0438\u043b\u0438 C{int}\n@param expire: \u0432\u0440\u0435\u043c\u044f \u0436\u0438\u0437\u043d\u0438 \u043a\u043b\u044e\u0447\u0430 \u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0430\u0445, 0 - \u0445\u0440\u0430\u043d\u0438\u0442\u044c \"\u0432\u0435\u0447\u043d\u043e\"\n@type expire: C{int}\n@return: Deferred \u043e \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0438\u0438 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438\n@rtype: C{twisted.internet.Deferred}\n\"\"\"\n", "func_signal": "def set(self, key, value, expire):\n", "code": "if expire != 0:\n    expire += time()\n\nself.db[key] = pickle.dumps((expire, value), pickle.HIGHEST_PROTOCOL)\nreturn defer.succeed(None)", "path": "spamfighter\\core\\storage\\dbm.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u043b\u044e\u0447\u0430.\n\n\u0415\u0441\u043b\u0438 \u043a\u043b\u044e\u0447 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d (\u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442, \u043f\u043e\u0442\u0435\u0440\u044f\u043d, \u0438\u0441\u0442\u0435\u043a\u043b\u043e \u0432\u0440\u0435\u043c\u044f \u0436\u0438\u0437\u043d\u0438), \n\u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 C{KeyError}.\n\n@param key: \u043a\u043b\u044e\u0447\n@type key: C{str}\n@return: Deferred \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u043b\u044e\u0447\u0430, C{str} \u0438\u043b\u0438 C{int}\n@rtype: C{twisted.internet.Deferred}\n\"\"\"\n", "func_signal": "def get(self, key):\n", "code": "(expire, value) = self._fetch(key)\n\nif value is None:\n    return defer.fail(KeyError(key))\n\nreturn defer.succeed(value)", "path": "spamfighter\\core\\storage\\dbm.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0442\u043e\u0440.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.counter = 0\nself.increments = 0\nself.start = time()", "path": "spamfighter\\core\\counters.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u0423\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441\u0447\u0435\u0442\u0447\u0438\u043a\u0430 \u043d\u0430 \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.\n\n@param value: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0438\u043d\u043a\u0440\u0435\u043c\u0435\u043d\u0442\u0430\n\"\"\"\n", "func_signal": "def increment(self, value=1):\n", "code": "self.counter[self.active] += value\nself.increments[self.active] += 1", "path": "spamfighter\\core\\counters.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0442\u043e\u0440.\n\n@param period: \u043f\u0435\u0440\u0438\u043e\u0434 \u0441\u0447\u0435\u0442\u0447\u0438\u043a\u0430, \u0441\u0435\u043a\u0443\u043d\u0434\n@type period: C{int}\n\"\"\"\n", "func_signal": "def __init__(self, period):\n", "code": "self.counter = [0, 0]\nself.increments = [0, 0]\nself.active = 0\nself._period = period\nreactor.callLater(self._period, self.exchangeCounters)", "path": "spamfighter\\core\\counters.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u0418\u0437\u0432\u0435\u0449\u0435\u043d\u0438\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u0443 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043e\u043d \u0431\u044b\u043b \u043f\u043e\u043c\u0435\u0449\u0435\u043d \u0432 \u0434\u043e\u043c\u0435\u043d.\n\n@param domain: \u0434\u043e\u043c\u0435\u043d\n@type domain: L{IDomain}\n@param name: \u0438\u043c\u044f \u0432 \u0434\u043e\u043c\u0435\u043d\u0435\n@type name: C{str}\n\"\"\"\n", "func_signal": "def bind(self, domain, name):\n", "code": "assert self.db is None\nself.db = DBMStorage(domain.key(), name)", "path": "spamfighter\\core\\storage\\dbm.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u043b\u044e\u0447\u0430.\n\n@param key: \u043a\u043b\u044e\u0447\n@type key: C{str}\n@return: \u043f\u0430\u0440\u0430 (\u0441\u0440\u043e\u043a_\u0433\u043e\u0434\u043d\u043e\u0441\u0442\u0438, \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435), \u0435\u0441\u043b\u0438 \u043a\u043b\u044e\u0447 \u0435\u0441\u0442\u044c \u0432 \u0411\u0414 \u0438\u043b\u0438 (None, None), \u0435\u0441\u043b\u0438 \u043e\u043d \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442\n\"\"\"\n", "func_signal": "def _fetch(self, key):\n", "code": "if not self.db.has_key(key):\n    return (None, None)\n\n(expire, value) = pickle.loads(self.db[key])\n\nif expire != 0 and expire <= time():\n    del self.db[key]\n    return (None, None)\n\nreturn (expire, value)", "path": "spamfighter\\core\\storage\\dbm.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041f\u043e\u0434\u043f\u0438\u0441\u044c \u0441\u0447\u0435\u0442\u0447\u0438\u043a\u0430, \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u044e\u0449\u0430\u044f \u043f\u0435\u0440\u0438\u043e\u0434 \u0435\u0433\u043e \u0440\u0430\u0431\u043e\u0442\u044b.\n\n@rtype: C{str}\n\"\"\"\n", "func_signal": "def label(self):\n", "code": "if self._period <= 60:\n    return \"\u0437\u0430 %d \u0441\u0435\u043a\u0443\u043d\u0434\" % self._period\nreturn \"\u0437\u0430 %d \u043c\u0438\u043d\u0443\u0442\" % (self._period // 60)", "path": "spamfighter\\core\\counters.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u043e\u0432\u0430\u0439\u0434\u0435\u0440 \u0430\u0432\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0430\u0440\u0442\u043d\u0435\u0440\u043e\u0432.\n\n\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043a\u0430\u043a \u043f\u043b\u0430\u0433\u0438\u043d, \u043f\u0440\u0435\u0434\u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0438\u0439 \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441 L{IPartnerAuthorizerProvider} \u0441 \u0438\u043c\u0435\u043d\u0435\u043c \n\u0438\u0437 \u043a\u043e\u043d\u0444\u0438\u0433\u0430: config.plugins.partner.default_provider\n\n@return: \u043f\u0440\u043e\u0432\u0430\u0439\u0434\u0435\u0440 \u0430\u0432\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0430\u0440\u0442\u043d\u0435\u0440\u043e\u0432\n@rtype: L{IPartnerAuthorizerProvider}\n\"\"\"\n", "func_signal": "def getPartnerAuthorizerProvider():\n", "code": "global defaultPartnerAuthorizerProvider\n\nif defaultPartnerAuthorizerProvider is None:\n    defaultPartnerAuthorizerProvider = loadPlugin(IPartnerAuthorizerProvider, config.plugins.partner.default_provider)\n\nreturn defaultPartnerAuthorizerProvider", "path": "spamfighter\\core\\partner.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041f\u0435\u0440\u0438\u043e\u0434 \u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u0437\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u043e\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441\u0447\u0435\u0442\u0447\u0438\u043a\u0430.\n\n@return: \u043f\u0435\u0440\u0438\u043e\u0434 \u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u0441\u0435\u043a\u0443\u043d\u0434\u044b\n@rtype: C{int}\n\"\"\"\n", "func_signal": "def period(self):\n", "code": "period = time()-self.start\nif period == 0:\n    period = 1\n\nreturn period", "path": "spamfighter\\core\\counters.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u0423\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441\u0447\u0435\u0442\u0447\u0438\u043a\u0430 \u043d\u0430 \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.\n\n@param value: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0438\u043d\u043a\u0440\u0435\u043c\u0435\u043d\u0442\u0430\n\"\"\"\n", "func_signal": "def increment(self, value=1):\n", "code": "for counter in self.counters:\n    counter.increment(value)", "path": "spamfighter\\core\\counters.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u0423\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u043b\u044e\u0447\u0430 \u043d\u0430 \u0435\u0434\u0438\u043d\u0438\u0446\u0443 (\u0442\u0438\u043f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f - \u0446\u0435\u043b\u043e\u0435 \u0447\u0438\u0441\u043b\u043e).\n\u0420\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430\u0434  \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043b\u044e\u0447\u0430\u043c\u0438, \u0435\u0441\u043b\u0438 \u043a\u043b\u044e\u0447\n\u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442, \u0431\u0443\u0434\u0435\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0435\u043d\u0430 \u043e\u0448\u0438\u0431\u043a\u0430 C{KeyError}.\n\n@param key: \u043a\u043b\u044e\u0447\n@type key: C{str}\n@param value: \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u0430 \u0438\u043d\u043a\u0440\u0435\u043c\u0435\u043d\u0442\u0430\n@type value: C{int}\n@return: Deferred \u0441 \u043d\u043e\u0432\u044b\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u043a\u043b\u044e\u0447\u0430, C{int}\n@rtype: C{twisted.internet.Deferred}\n\"\"\"\n", "func_signal": "def incr(self, key, value=1):\n", "code": "expire, old = self._fetch(key)\n\nif old is None:\n    return defer.fail(KeyError(key))\n\nif not isinstance(value, int):\n    return defer.fail(TypeError(value))\n\nif not isinstance(old, int):\n    return defer.fail(TypeError(old))\n\nvalue = old + value\nself.db[key] = pickle.dumps((expire, value), pickle.HIGHEST_PROTOCOL)\nreturn defer.succeed(None)", "path": "spamfighter\\core\\storage\\dbm.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041f\u043e\u043c\u0435\u043d\u044f\u0442\u044c \u043c\u0435\u0441\u0442\u0430\u043c\u0438 \u0442\u0435\u043d\u0435\u0432\u043e\u0439 \u0438 \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u0441\u0447\u0435\u0442\u0447\u0438\u043a.\n\"\"\"\n", "func_signal": "def exchangeCounters(self):\n", "code": "self.active = 1-self.active\nself.counter[self.active] = 0\nself.increments[self.active] = 0\nreactor.callLater(self._period, self.exchangeCounters)", "path": "spamfighter\\core\\counters.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c \u0430\u0432\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0430\u0440\u0442\u043d\u0435\u0440\u043e\u0432.\n\n@return: \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c \u0430\u0432\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0430\u0440\u0442\u043d\u0435\u0440\u043e\u0432\n@rtype: L{spamfighter.interfaces.IPartnerAuthorizer}\n\"\"\"\n", "func_signal": "def getPartnerAuthorizer():\n", "code": "global defaultPartnerAuthorizer\n\nif defaultPartnerAuthorizer is None:\n    defaultPartnerAuthorizer = getPartnerAuthorizerProvider().getPartnerAuthorizer()\n\nreturn defaultPartnerAuthorizer", "path": "spamfighter\\core\\partner.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u0423\u0434\u0430\u043b\u0438\u0442\u044c \u043a\u043b\u044e\u0447 \u0438\u0437 \u0445\u0440\u0430\u043d\u0438\u043b\u0438\u0449\u0430. \n\n\u0415\u0441\u043b\u0438 \u043a\u043b\u044e\u0447 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 C{KeyError}.\n\n@param key: \u043a\u043b\u044e\u0447\n@type key: C{str}\n@return: Deferred \u043e \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0438\u0438 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438\n@rtype: C{twisted.internet.Deferred}\n\"\"\"\n", "func_signal": "def delete(self, key):\n", "code": "expire, value = self._fetch(key)\n\nif value is None:\n    return defer.fail(KeyError(key))\n\ndel self.db[key]\nreturn defer.succeed(None)", "path": "spamfighter\\core\\storage\\dbm.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "\"\"\"\n\u0414\u043e\u043f\u0438\u0441\u0430\u0442\u044c \u0432 \u043a\u043e\u043d\u0435\u0446 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u043b\u044e\u0447\u0430 \u0435\u0449\u0435 \u043e\u0434\u0438\u043d \u044d\u043b\u0435\u043c\u0435\u043d\u0442.\n\u0420\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430\u0434  \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u043a\u043b\u044e\u0447\u0430\u043c\u0438, \u0435\u0441\u043b\u0438 \u043a\u043b\u044e\u0447\n\u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442, \u0431\u0443\u0434\u0435\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0435\u043d\u0430 \u043e\u0448\u0438\u0431\u043a\u0430 C{KeyError}.\n\n@param key: \u043a\u043b\u044e\u0447\n@type key: C{str}\n@param value: \u0434\u043e\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n@return: Deferred \u043e \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0438\u0438 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438\n@rtype: C{twisted.internet.Deferred}\n\"\"\"\n", "func_signal": "def append(self, key, value):\n", "code": "expire, old = self._fetch(key)\n\nif old is None:\n    return defer.fail(KeyError(key))\n\nif not isinstance(value, str):\n    return defer.fail(TypeError(value))\n\nif not isinstance(old, str):\n    return defer.fail(TypeError(old))\n\nvalue = old + value\nself.db[key] = pickle.dumps((expire, value), pickle.HIGHEST_PROTOCOL)\nreturn defer.succeed(None)", "path": "spamfighter\\core\\storage\\dbm.py", "repo_name": "smira/spamfighter", "stars": 5, "license": "gpl-3.0", "language": "python", "size": 512}
{"docstring": "'''Retrieves the relatesTo header. This is used for callbacks'''\n", "func_signal": "def get_relates_to_info():\n", "code": "from soaplib.wsgi_soap import request\nheaderElement = request.header\nif headerElement:\n    headers = headerElement.getchildren()\n    for header in headers:\n        if header.tag.lower().find('relatesto') != -1:\n            return header.text", "path": "soaplib\\util.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nThis method accepts an Attachment object, and returns\nthe filename of the archived file\n'''\n", "func_signal": "def archive_document(self, document):\n", "code": "fd, fname = mkstemp()\nos.close(fd)\n\ndocument.fileName = fname\ndocument.save_to_file()\n\nreturn fname", "path": "examples\\binary.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nThis method returns an Attachment object that contains\nthe base64 decoded string of the text of the given element\n'''\n", "func_signal": "def from_xml(cls, element):\n", "code": "data = base64.decodestring(element.text)\na = Attachment(data=data)\nreturn a", "path": "soaplib\\serializers\\binary.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''Returns the datatype base64Binary'''\n", "func_signal": "def get_datatype(cls, nsmap=None):\n", "code": "if nsmap is not None:\n    return nsmap.get(cls.get_namespace_id()) + 'base64Binary'\nreturn 'base64Binary'", "path": "soaplib\\serializers\\binary.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nRebuilds the calling url from values found in the\nenvironment.\n\nThis algorithm was found via PEP 333, the wsgi spec and\ncontributed by Ian Bicking.\n'''\n", "func_signal": "def reconstruct_url(environ):\n", "code": "url = environ['wsgi.url_scheme'] + '://'\nif environ.get('HTTP_HOST'):\n    url += environ['HTTP_HOST']\nelse:\n    url += environ['SERVER_NAME']\n\n    if environ['wsgi.url_scheme'] == 'https':\n        if environ['SERVER_PORT'] != '443':\n            url += ':' + environ['SERVER_PORT']\n    else:\n        if environ['SERVER_PORT'] != '80':\n            url += ':' + environ['SERVER_PORT']\n\nif (quote(environ.get('SCRIPT_NAME', '')) == '/' and\n    quote(environ.get('PATH_INFO', ''))[0:1] == '/'):\n    #skip this if it is only a slash\n    pass\nelif quote(environ.get('SCRIPT_NAME', ''))[0:2] == '//':\n    url += quote(environ.get('SCRIPT_NAME', ''))[1:]\nelse:\n    url += quote(environ.get('SCRIPT_NAME', ''))\n\nurl += quote(environ.get('PATH_INFO', ''))\nif environ.get('QUERY_STRING'):\n    url += '?' + environ['QUERY_STRING']\nreturn url", "path": "soaplib\\util.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "# applied patch from Julius Volz\n#e = _generic_to_xml(str(value).lower(),name,cls.get_datatype(nsmap))\n", "func_signal": "def to_xml(cls, value, name='retval', nsmap=ns):\n", "code": "if value == None:\n    return Null.to_xml('', name, nsmap)\nelse:\n    e = _generic_to_xml(str(bool(value)).lower(), name, cls, nsmap)\nreturn e", "path": "soaplib\\serializers\\primitive.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nThis method loads a document from the specified file path\nand returns it.  If the path isn't found, an exception is\nraised.\n'''\n", "func_signal": "def get_archived_document(self, file_path):\n", "code": "if not os.path.exists(file_path):\n    raise Exception(\"File [%s] not found\"%file_path)\n\ndocument = Attachment(fileName=file_path)\n# the service automatically loads the data from the file.\n# alternatively, The data could be manually loaded into memory\n# and loaded into the Attachment like:\n#   document = Attachment(data=data_from_file)\nreturn document", "path": "examples\\binary.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nThis method executes the http request to the remote web service.  With\nthe exception of 'headers', 'msgid', and 'mtom'; all keyword arguments\nto this method are put in the http header.  The 'headers' keyword is to\ndenote a list of elements to be included in the soap header, 'msgid'\nis a convenience keyword used in async web services which creates a\nWS-Addressing messageid header to be included in the soap headers, and\n'mtom' enables the Message Transmission Optimization Mechanism.\n\n@param the arguments to the remote method\n@param the keyword arguments\n'''\n", "func_signal": "def __call__(self, *args, **kwargs):\n", "code": "if len(args) != len(self.descriptor.inMessage.params):\n    argstring = '\\r\\n'.join(['    ' + str(arg) for arg in args])\n    paramstring = '\\r\\n'.join(['    ' + str(p[0])\n        for p in self.descriptor.inMessage.params])\n    err_msg = _err_format % (argstring, paramstring)\n    raise Exception(err_msg)\n\nmsg = self.descriptor.inMessage.to_xml(*args)\n\n# grab the soap headers passed into this call\nheaders = kwargs.get('headers', [])\nmtom = kwargs.get('mtom', False)\nmsgid = kwargs.get('msgid')\nif msgid:\n    # special case for the msgid field as a convenience\n    # when dealing with async callback methods\n    headers.append(create_relates_to_header(msgid))\n\ntns = self.descriptor.inMessage.ns\nenvelope = make_soap_envelope(msg, tns, header_elements=headers)\n\nbody = ElementTree.tostring(envelope)\nmethodName = '\\\"%s\\\"' % self.descriptor.soapAction\nhttpHeaders = {'Content-Length': len(body),\n              'Content-type': 'text/xml; charset=\"UTF-8\"',\n              'Accept': ('application/soap+xml, application/dime, '\n                        'multipart/related, text/*'),\n              'User-Agent': 'Soaplib/1.0',\n              'SOAPAction': methodName,\n              }\n\nfor k, v in kwargs.items():\n    # add all the other keywords to the http headers\n    if k not in ('headers', 'msgid', 'mtom'):\n        httpHeaders[k] = v\n\nif mtom:\n    httpHeaders, body = apply_mtom(httpHeaders, body,\n        self.descriptor.inMessage.params, args)\n\ndump(self.host, self.path, httpHeaders, body)\n\nif self.scheme == \"http\":\n    conn = httplib.HTTPConnection(self.host)\nelif self.scheme == \"https\":\n    conn = httplib.HTTPSConnection(self.host)\nelse:\n    raise RuntimeError(\"Unsupported URI connection scheme: %s\" %\n        self.scheme)\n\nconn.request(\"POST\", self.path, body=body, headers=httpHeaders)\nresponse = conn.getresponse()\ndata = response.read()\n\ndump(self.host, self.path, dict(response.getheaders()), data)\n\ncontenttype = response.getheader('Content-Type')\ndata = collapse_swa(contenttype, data)\n\nconn.close()\nif str(response.status) not in['200', '202']:\n    # consider everything NOT 200 or 202 as an error response\n\n    if str(response.status) == '500':\n        fault = None\n        try:\n            payload, headers = from_soap(data)\n            fault = Fault.from_xml(payload)\n        except:\n            trace = StringIO()\n            import traceback\n            traceback.print_exc(file=trace)\n\n            fault = Exception(\"Unable to read response \\n\"\n                \"%s %s \\n %s \\n %s\" %\n                (response.status, response.reason, trace.getvalue(),\n                 data))\n        raise fault\n    else:\n        raise Exception(\"%s %s\" % (response.status, response.reason))\n\nif not self.descriptor.outMessage.params:\n    return\n\npayload, headers = from_soap(data)\nresults = self.descriptor.outMessage.from_xml(payload)\nreturn results[0]", "path": "soaplib\\client.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''This method writes the data to the specified file.  This method\nassumes that the filename is the full path to the file to be written.\nThis method also assumes that self.data is the base64 decoded data,\nand will do no additional transformations on it, simply write it to\ndisk.\n'''\n", "func_signal": "def save_to_file(self):\n", "code": "if not self.data:\n    raise Exception(\"No data to write\")\nif not self.fileName:\n    raise Exception(\"No filename specified\")\nf = open(self.fileName, 'wb')\nf.write(self.data)\nf.flush()\nf.close()", "path": "soaplib\\serializers\\binary.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nThis is a utility method for debugging client request and responses\n@param boolean for turning debug on or off\n@param filelike object to write to\n'''\n", "func_signal": "def debug(is_on, out=sys.stdout):\n", "code": "global _out, _debug\n_out = out\n_debug = is_on", "path": "soaplib\\client.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nDebugging method for dumping request information to a file or stdout\n@param host server host\n@param path server path\n@param headers http headers\n@param envelope soap envelope\n'''\n", "func_signal": "def dump(host, path, headers, envelope):\n", "code": "global _out, _debug\nif not _debug:\n    return\n\ndef writeln(text):\n    _out.write(text)\n    _out.write('\\r\\n')\n    _out.flush()\n\nwriteln('-------------------------------------------------')\nwriteln('Host: ' + host)\nwriteln('Path: ' + path)\nwriteln('Headers:')\nfor k, v in headers.items():\n    writeln('    %s -> %s' % (k.ljust(20), v))\nwriteln('Envelope:')\nwriteln(envelope)\nwriteln('-------------------------------------------------')", "path": "soaplib\\client.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nLookup and return a given namespace\n'''\n", "func_signal": "def get(self, key):\n", "code": "if key in self.nsmap:\n    ns = self.nsmap[key]\nelse:\n    ns = ''\nreturn \"{%s}\" % ns", "path": "soaplib\\xml.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "# Look up the requested method.  First, check to see if the\n# method is present on ourself (utility functions), then\n# check to see if it exists on the client service.\n", "func_signal": "def _dynamic_(self, name, lcid, wFlags, args):\n", "code": "is_service_method = False\nitem = getattr(self, name, None)\nif item is None and hasattr(self, 'client'):\n    item = getattr(self.client, name)\n    is_service_method = True\n\nif item is None:\n    raise COMException('No attribute of that name.',\n                        winerror.DISP_E_MEMBERNOTFOUND)\n\n# Figure out what parameters this web service call accepts,\n# and what it returns, so that we can properly wrap the objects\n# on the way in and unwrap them on the way out.\nif is_service_method:\n    all_methods = self.client_type.methods()\n    method_descriptor = [method for method in all_methods\n                         if method.name == name][0]\n    return_type = method_descriptor.outMessage.params[0][1]\n    parameter_types = [parameter[1] for parameter in\n                       method_descriptor.inMessage.params]\n\n    # Now that we have this data, go ahead and unwrap any\n    # wrapped parameters recursively.\n    newargs = []\n    for param_type, param in zip(parameter_types, args):\n        if (hasattr(param_type, '__bases__') and\n            ClassSerializer in param_type.__bases__):\n            param = unwrap_complex_type(param, param_type)\n        elif param_type is DateTime:\n            param = coerce_date_time(param)\n        newargs.append(param)\n    args = newargs\n\n# Call the supplied method\nresult = apply(item, args)\n\n# Now wrap the return value, recursively.\nif isinstance(result, ClassSerializer):\n    result = wrap_complex_type(result, return_type)\n\n# Return our data\nreturn result", "path": "soaplib\\ext\\comproxy.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''Creates a 'relatesTo' header for async callbacks'''\n", "func_signal": "def create_relates_to_header(relatesTo, attrs={}):\n", "code": "relatesToElement = ElementTree.Element(\n    '{http://schemas.xmlsoap.org/ws/2003/03/addressing}RelatesTo')\nfor k, v in attrs.items():\n    relatesToElement.set(k, v)\nrelatesToElement.text = relatesTo\nreturn relatesToElement", "path": "soaplib\\util.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''Creates MessageId and ReplyTo headers for initiating an\nasync function'''\n", "func_signal": "def create_callback_info_headers(messageId, replyTo):\n", "code": "messageIdElement = ElementTree.Element(\n    '{http://schemas.xmlsoap.org/ws/2003/03/addressing}MessageID')\nmessageIdElement.text = messageId\n\nreplyToElement = ElementTree.Element(\n    '{http://schemas.xmlsoap.org/ws/2003/03/addressing}ReplyTo')\naddressElement = ElementTree.SubElement(replyToElement,\n    '{http://schemas.xmlsoap.org/ws/2003/03/addressing}Address')\naddressElement.text = replyTo\nreturn messageIdElement, replyToElement", "path": "soaplib\\util.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''This class method takes the data from the attachment and\nbase64 encodes it as the text of an Element. An attachment can\nspecify a filename and if no data is given, it will read the data\nfrom the file\n'''\n", "func_signal": "def to_xml(cls, value, name='retval', nsmap=ns):\n", "code": "if value.__class__ is not Attachment:\n    raise Exception(\"Do not know how to serialize class %s\" %\n        type(value))\n\nelement = create_xml_element(name, nsmap)\nif value.data:\n    # the data has already been loaded, just encode\n    # and return the element\n    element.text = base64.encodestring(value.data)\nelif value.fileName:\n    # the data hasn't been loaded, but a file has been\n    # specified\n    data_string = cStringIO.StringIO()\n\n    fileName = value.fileName\n    file = open(fileName, 'rb')\n    base64.encode(file, data_string)\n    file.close()\n\n    # go back to the begining of the data\n    data_string.seek(0)\n    element.text = str(data_string.read())\nelse:\n    raise Exception(\"Neither data nor a filename has been specified\")\n\nreturn element", "path": "soaplib\\serializers\\binary.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nFactory method to create a new XML element\n@param default_ns The default namespace to use for the element.\n@param extended_map A mapping of any additional namespaces to add.\n'''\n", "func_signal": "def create_xml_element(name, nslookup, default_ns=None):\n", "code": "if default_ns is not None:\n    namespace_map = {None: default_ns}\nelse:\n    namespace_map = {}\nfor key, value in nslookup.get_all().iteritems():\n    if value != default_ns:\n        namespace_map[key] = value\nreturn ElementTree.Element(name, nsmap=namespace_map)", "path": "soaplib\\xml.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''Retrieves the messageId and replyToAddress from the message header.\nThis is used for async calls.'''\n", "func_signal": "def get_callback_info():\n", "code": "messageId = None\nreplyToAddress = None\nfrom soaplib.wsgi_soap import request\nheaderElement = request.header\nif headerElement:\n    headers = headerElement.getchildren()\n    for header in headers:\n        if header.tag.lower().endswith(\"messageid\"):\n            messageId = header.text\n        if header.tag.lower().find(\"replyto\") != -1:\n            replyToElems = header.getchildren()\n            for replyTo in replyToElems:\n                if replyTo.tag.lower().endswith(\"address\"):\n                    replyToAddress = replyTo.text\nreturn messageId, replyToAddress", "path": "soaplib\\util.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''Splits a url into (uri_scheme, host[:port], path)'''\n", "func_signal": "def split_url(url):\n", "code": "scheme, remainder = urllib.splittype(url)\nhost, path = urllib.splithost(remainder)\nreturn scheme.lower(), host, path", "path": "soaplib\\util.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "'''\nThis method loads the data from the specified file, and does\nno encoding/decoding of the data\n'''\n", "func_signal": "def load_from_file(self):\n", "code": "if not self.fileName:\n    raise Exception(\"No filename specified\")\nf = open(self.fileName, 'rb')\nself.data = f.read()\nf.close()", "path": "soaplib\\serializers\\binary.py", "repo_name": "gsson/soaplib", "stars": 6, "license": "lgpl-2.1", "language": "python", "size": 189}
{"docstring": "\"\"\"\nmove the highlight to the item at given index\n\"\"\"\n# since the keybinding won't know all the time how long the\n# list is, and it makes a copy of the int self.len if we were\n# to give that as the userdata\n", "func_signal": "def move(self, index):\n", "code": "if index == 'end':\n    index = len(self.items)\n    \nself.highlighted = index\nif self.highlighted < 0:\n    self.highlighted = 0\nelif self.highlighted >= len(self.items):\n    self.highlighted = len(self.items) - 1\n    if self.highlighted < 0:\n        self.highlighted = 0\n\nself.touch()\n\nself.fireEvent(HighlightChanged(self, self.items[self.highlighted]))", "path": "dtk\\ListBox.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nmove the selection one to the left, wrapping\naround to the beginning if necessary\n\"\"\"\n", "func_signal": "def selectPrev(self):\n", "code": "if self.selected is None:\n    self.selected = self.items[0]\n\nelse:\n    self.selected = (self.selected - 1) % len(self.items)\n\nself.touch()\n\nself.fireEvent(SelectionChanged(self, self.selected))", "path": "dtk\\Select.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nif the highlighted item is not selected, select it,\nand vice-versa\n\"\"\"\n", "func_signal": "def toggleSelect(self):\n", "code": "if not self.allowSelection:\n    return\n\nif self.highlighted in self.selected:\n    self.selected.remove(self.highlighted)\n\nelif self.multipleSelection:\n    self.selected.append(self.highlighted)\n\nelse:\n    self.selected = [self.highlighted]\n\nself.touch()\n\nself.fireEvent(SelectionChanged(self, self.selected))", "path": "dtk\\ListBox.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nTextEdtior raises an exception if you scroll down too far.\nThis test is considered to pass if it raises no exception\n\"\"\"\n\n", "func_signal": "def testLongTextEditor(self):\n", "code": "args = ['down'] * 25\nargs.append('esc')\nself.scr.set_input(*args)\n\ne = dtk.Engine()\nl = dtk.TextEditor()\nl.setText([\n        'Lorem ipsum dolor sit amet,',\n        'consectetuer adipiscing elit.',\n        'Nullam ac elit. Nullam consectetuer ultrices elit. Morbi a sem.',\n        'Sed urna.',\n        'Nam magna erat', 'pellentesque quis', 'ultricies id',\n        'hendrerit molestie, nibh.',\n        'Cras placerat pellentesque erat.',\n        'Vestibulum fringilla,',\n        'ligula eget tristique convallis',\n        'enim',\n        'Lorem ipsum dolor sit amet,',\n        'consectetuer adipiscing elit.',\n        'Nullam ac elit. Nullam consectetuer ultrices elit. Morbi a sem.',\n        'Sed urna.',\n        'Nam magna erat', 'pellentesque quis', 'ultricies id',\n        'hendrerit molestie, nibh.',\n        'Cras placerat pellentesque erat.',\n        'Vestibulum fringilla,',\n        'ligula eget tristique convallis',\n        'enim',\n        'risus dignissim lacus, at pharetra velit purus vitae lectus.',\n        'Vivamus aliquet',\n        'vehicula nunc. Nulla commodo ligula eu felis.',\n        'Duis eu sapien quis',\n        'leo posuere rutrum. Suspendisse porttitor',\n        'pulvinar elit.',\n        'In scelerisque elit a elit. Praesent accumsan.',\n        'Fusce sed lacus',\n        'in lectus varius aliquam. Proin lobortis.',\n        'Vestibulum ante ipsum primis in faucibus orci luctus et',\n        'ultrices posuere cubilia Curae; Suspendisse aliquam ultrices pede.',\n        'Suspendisse bibendum dolor vel diam. Morbi vel arcu vitae',\n        'magna fermentum interdum. Donec ornare sollicitudin enim.'\n        ])\n\ne.bindKey('esc', e.quit)\ne.setRoot(l)\ne.mainLoop()", "path": "test\\bugs\\bug22.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nset the chosen item to the currently highlighted one,\nand fire the 'clicked' event\n\"\"\"\n", "func_signal": "def clicked(self):\n", "code": "self.chosen = self.selected\nself.touch()\n\nself.fireEvent(Clicked(self))", "path": "dtk\\Select.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nDraw a box of border characters, beginning at (row, col)\nrelative to the upper left of this Drawable, extending\nfor w characters wide (inclusive of both borders) and\nh characters high (inclusive of both borders).\n\"\"\"\n", "func_signal": "def box(self, row, col, w, h, **kwargs):\n", "code": "self.log.debug(\"queing box(%d, %d, %d, %d, %s)\" % (row, col, w, h, kwargs))\nself.draw_queue.append( lambda: self.__box( row, col, w, h, **kwargs ) )\nself.touch()", "path": "dtk\\Canvas.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nDraw a line starting at (row, col) relative to this\nDrawable's upper-left and going right for len characters.\nEnding characters may be specified with the leftEnd and\nrightEnd attributes.\n\"\"\"\n", "func_signal": "def line(self, row, col, len, **kwargs):\n", "code": "self.log.debug(\"queing line(%d, %d, %d, %s)\" % (row, col, len, kwargs))\nself.draw_queue.append( lambda: self.__line( row, col, len, **kwargs ) )\nself.touch()", "path": "dtk\\Canvas.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nsets the items list and currently highlighted item. touches\nthe ListBox (forces redraw next time through drawing loop)\n\"\"\"\n\n", "func_signal": "def setItems(self, items, highlighted = 0, selected = None):\n", "code": "self.items = list(items)\n\nself.highlighted = highlighted\nif selected is not None:\n    self.selected = selected\nelse:\n    self.selected = []\n\nself.firstVisible = 0\nself.touch()", "path": "dtk\\ListBox.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nget the printable string for this list item. \nif the item has defined a __dtk_str__, use that (or its return value)\notherwise just use str(item)\n\"\"\"\n", "func_signal": "def _get_repr(self, item):\n", "code": "str_rep = getattr(item, \"__dtk_str__\", None)\nif str_rep:\n    if callable(str_rep): \n        str_rep = str_rep()\nelse:\n    str_rep = str(item)\nreturn str_rep", "path": "dtk\\ListBox.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nDraw the string starting at (row, col) relative to the\nupper left of this Drawable. Drawing will be bounded\nto stay within this Drawable's size.\n\"\"\"\n", "func_signal": "def draw(self, str, row, col, **kwargs):\n", "code": "self.log.debug(\"queing draw('%s', %d, %d, %s)\" % (str, row, col, kwargs))\nself.draw_queue.append( lambda: self.__draw( str, row, col, **kwargs ) )\nself.touch()", "path": "dtk\\Canvas.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nDraw a line starting at (row, col) relative to the upper\nleft of this Drawable and going down for len characters.\nEnding characters may be specified with the topEnd and\nbottomEnd attributes.\n\"\"\"\n", "func_signal": "def lineDown(self, row, col, len, **kwargs):\n", "code": "self.log.debug(\"queing lineDown(%d, %d, %d, %s)\" % (row, col, len, kwargs))\nself.draw_queue.append( lambda: self.__lineDown( row, col, len, **kwargs ) )\nself.touch()", "path": "dtk\\Canvas.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nmove the selection one to the right, wrapping\naround to the beginning if necessary\n\"\"\"\n", "func_signal": "def selectNext(self):\n", "code": "if self.selected is None:\n    self.selected = self.items[0]\n\nelse:\n    self.selected = (self.selected + 1) % len(self.items)\n\nself.touch()\n\nself.fireEvent(SelectionChanged(self, self.selected))", "path": "dtk\\Select.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\npops the end of the list by default\ntodo this should return the popped item\n\"\"\"\n", "func_signal": "def pop(self, index = None):\n", "code": "if index is None:\n    index = len(self.items) - 1\n\nif index in self.selected:\n    self.selected.remove(index)\n\nout = self.items.pop(index)\nself.touch()\n\nreturn out", "path": "dtk\\ListBox.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nclears the whole drawable\n\"\"\"\n", "func_signal": "def clear(self):\n", "code": "self.log.debug(\"queing clear()\")\nself.draw_queue.append( lambda: self.__clear() )\nself.touch()", "path": "dtk\\Canvas.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nreturn the highlighted item\n\"\"\"\n", "func_signal": "def getHighlightedItem(self):\n", "code": "if len(self.items):\n    return self.items[self.highlighted]", "path": "dtk\\ListBox.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nprocess the draw queue. all elements in the queue are just\ncurried callables, so just call them with no arguments\n\"\"\"\n\n", "func_signal": "def render(self):\n", "code": "for command in self.draw_queue:\n    command()", "path": "dtk\\Canvas.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nSet the selection type for this ListBox. If changing from\nmultiple selection to single selection, the existing\nselection is cleared. When changing to no selection, any\nexisting selection is cleared.\n\nselectionType is one of 'multiple', 'single', or 'none'\n\"\"\"\n", "func_signal": "def setSelectionType(self, selectionType):\n", "code": "selectionType = selectionType.lower()\ntry:\n    assert selectionType in ('multiple', 'single', 'none')\nexcept AssertionError:\n    raise Exception(\"selection type must be one of multiple, single or none\")\n\nif selectionType == 'multiple':\n    self.allowSelection = True\n    self.multipleSelection = True\n\nelif selectionType == 'single':\n    # toggleSelection expects the list to have\n    # at least one element -- this should not\n    # cause anything to be displayed as selected\n    self.selected = [None]\n\n    self.allowSelection = True\n    self.multipleSelection = False\n\nelse:\n    self.selected = []\n\n    self.allowSelection = False\n    self.multipleSelection = False", "path": "dtk\\ListBox.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nmake the given item the chosen one if it is\nin the list of items\n\"\"\"\n", "func_signal": "def setChosenItem(self, item):\n", "code": "if item in self.items:\n    self.chosen = self.items.index(item)\n    self.touch()", "path": "dtk\\Select.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nListBox takes optional parameters 'selection' and 'vimlike':\n\n * selection controls the selection style - 'multiple', 'single'\n   or 'none'\n * vimlike: when True enables bindings for vim-like navigation:\n   j/k for up/down\n\nEvents:\n * SelectionChanged\n * HighlightChanged\n\"\"\"\n", "func_signal": "def __init__(self, selection = 'multiple', vimlike = False, **kwargs):\n", "code": "super(ListBox, self).__init__(**kwargs)\n\n# the higlighted element is shown in reverse mode\nself.highlighted = 0\n\nself.setDrawStyle()\n\n# these get immediately overwritten\nself.allowSelection = False\nself.multipleSelection = False\nself.selected = []\nself.items = []\n\n# remember the selection type\nself.setSelectionType(selection)\n\n# sensible defaults \nself.firstVisible = 0\n\n# set up the usual keybindings\nself.bindKey('down', self.moveDown)\nself.bindKey('up', self.moveUp)\nself.bindKey('page down', self.pageDown)\nself.bindKey('page up', self.pageUp)\nif self.allowSelection:\n    self.bindKey(' ', self.toggleSelect)\nself.bindKey('home', self.moveToTop)\nself.bindKey('end', self.moveToBottom)\n\nif vimlike:\n    self.bindKey('j', self.moveDown)\n    self.bindKey('k', self.moveUp)", "path": "dtk\\ListBox.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "\"\"\"\nDraw the string starting at (row, col) relative to the\nupper left of this Drawable and going down. Drawing \nwill be bounded to stay within the Drawable's size.\n\"\"\"\n", "func_signal": "def drawDown(self, str, row, col, **kwargs):\n", "code": "self.log.debug(\"queing drawDown('%s', %d, %d, %s)\" % (str, row, col, kwargs))\nself.draw_queue.append( lambda: self.__drawDown( str, row, col, **kwargs ) )\nself.touch()", "path": "dtk\\Canvas.py", "repo_name": "dcrosta/dtk", "stars": 4, "license": "lgpl-3.0", "language": "python", "size": 412}
{"docstring": "# Imports lives inside this method so User won't get imported if you \n# over-ride this in your own sub-class and use something else.\n", "func_signal": "def lookup_openid(self, request, identity_url):\n", "code": "from django.contrib.auth.models import User\nreturn list(\n    User.objects.filter(openids__openid = identity_url).distinct()\n)", "path": "django_openid\\auth.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Remember, openid might be None (after registration with none set)\n", "func_signal": "def log_in_user(self, request, user):\n", "code": "from django.contrib.auth import login\n# Nasty but necessary - annotate user and pretend it was the regular \n# auth backend. This is needed so django.contrib.auth.get_user works:\nuser.backend = 'django.contrib.auth.backends.ModelBackend'\nlogin(request, user)", "path": "django_openid\\auth.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Add extension args (for things like simple registration)\n", "func_signal": "def add_extension_args(self, request, auth_request):\n", "code": "extension_args = dict(self.extension_args) # Create a copy\nif self.sreg:\n    extension_args['sreg.optional'] = ','.join(self.sreg)\nif self.sreg_required:\n    extension_args['sreg.required'] = ','.join(self.sreg_required)\nif self.sreg_policy_url:\n    extension_args['sreg.policy_url'] = self.sreg_policy_url\n\nfor name, value in extension_args.items():\n    namespace, key = name.split('.', 1)\n    namespace = self.extension_namespaces.get(namespace, namespace)\n    auth_request.addExtensionArg(namespace, key, value)", "path": "django_openid\\consumer.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"\nAccepts openid as optional keyword argument, for password validation.\nAlso accepts optional reserved_usernames keyword argument which is a\nlist of usernames that should not be registered (e.g. 'security')\n\"\"\"\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "try:\n    self.openid = kwargs.pop('openid')\nexcept KeyError:\n    self.openid = None\ntry:\n    self.reserved_usernames = kwargs.pop('reserved_usernames')\nexcept KeyError:\n    self.reserved_usernames = []\ntry:\n    self.no_duplicate_emails = kwargs.pop('no_duplicate_emails')\nexcept KeyError:\n    self.no_duplicate_emails = False\n\n# Super's __init__ creates self.fields for us\nsuper(RegistrationForm, self).__init__(*args, **kwargs)\n# Now we can modify self.fields with our extra required information\nfor field in self.extra_required:\n    self.fields[field].required = True", "path": "django_openid\\forms.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"\nSimilar to parent class, but returns the request object as soon as it\nhas created it.\n\"\"\"\n", "func_signal": "def request(self, **request):\n", "code": "environ = {\n    'HTTP_COOKIE': self.cookies,\n    'PATH_INFO': '/',\n    'QUERY_STRING': '',\n    'REQUEST_METHOD': 'GET',\n    'SCRIPT_NAME': '',\n    'SERVER_NAME': 'testserver',\n    'SERVER_PORT': 80,\n    'SERVER_PROTOCOL': 'HTTP/1.1',\n}\nenviron.update(self.defaults)\nenviron.update(request)\nreturn WSGIRequest(environ)", "path": "django_openid\\tests\\request_factory.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Stash the incomplete orequest in a signed cookie\n", "func_signal": "def show_landing_page(self, request, orequest):\n", "code": "response = self.render(request, self.landing_page_template, {\n    'identity_url': orequest.identity,\n})\nself.stash_incomplete_orequest(request, response, orequest)\nreturn response", "path": "django_openid\\provider.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# After logging out the OpenID, log out the user auth session too\n", "func_signal": "def on_logged_out(self, request):\n", "code": "from django.contrib.auth import logout\nresponse = super(AuthConsumer, self).on_logged_out(request)\nlogout(request)\nreturn response", "path": "django_openid\\auth.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# This can be over-ridden to show a registration form\n", "func_signal": "def show_unknown_openid(self, request, openid):\n", "code": "return self.show_message(\n    request, 'Unknown OpenID', '%s is an unknown OpenID' % openid\n)", "path": "django_openid\\auth.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# We need to behave differently from the default AuthConsumer\n# success behaviour. For simplicity, we do the following:\n# 1. \"Log them in\" as that OpenID i.e. stash it in the session\n# 2. If it's already associated with an account, log them in as \n#    that account and show a message.\n# 2. If NOT already associated, redirect back to /register/ again\n", "func_signal": "def on_success(request, identity_url, openid_response):\n", "code": "openid_object = OpenID.from_openid_response(openid_response)\nmatches = self.lookup_openid(request, identity_url)\nif matches:\n    # Log them in and show the message\n    self.log_in_user(request, matches[0])\n    response = self.show_i_have_logged_you_in(request)\nelse:\n    response = HttpResponseRedirect(urlparse.urljoin(\n        request.path, '../register/'\n    ))\nself.persist_openid(request, response, openid_object)\nreturn response", "path": "django_openid\\registration.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Do we recognise their OpenID?\n", "func_signal": "def on_logged_in(self, request, openid, openid_response):\n", "code": "matches = self.lookup_openid(request, openid)\n# Are they logged in already?\nif request.user.is_authenticated():\n    # Did we find their account already? If so, ignore login\n    if request.user.id in [u.id for u in matches]:\n        response = self.redirect_if_valid_next(request)\n        if not response:\n            response = Redirect(self.after_login_redirect_url)\n        return response\n    else:\n        # Offer to associate this OpenID with their account\n        return self.show_associate(request, openid)\nif matches:\n    # If there's only one match, log them in as that user\n    if len(matches) == 1:\n        user = matches[0]\n        if self.user_can_login(request, user):\n            self.log_in_user(request, user)\n            return self.on_login_complete(request, user, openid)\n        else:\n            # User is not allowed to log in for some other reason - \n            # for example, they have not yet validated their e-mail \n            # or they have been banned from the site.\n            return self.show_you_cannot_login(request, user, openid)\n    # Otherwise, let them to pick which account they want to log in as\n    else:\n        return self.show_pick_account(request, openid)\nelse:\n    # We don't know anything about this openid\n    return self.show_unknown_openid(request, openid)", "path": "django_openid\\auth.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Show a registration / signup form, provided the user is not \n# already logged in\n", "func_signal": "def do_register(self, request, message=None):\n", "code": "if not request.user.is_anonymous():\n    return self.show_already_signed_in(request)\n\n# Spot incoming openid_url authentication requests\nif request.POST.get('openid_url', None):\n    return self.start_openid_process(request,\n        user_url = request.POST.get('openid_url'),\n        on_complete_url = urlparse.urljoin(\n            request.path, '../register_complete/'\n        ),\n        trust_root = urlparse.urljoin(request.path, '..')\n    )\n\nRegistrationForm = self.get_registration_form_class(request)\n\ntry:\n    openid = request.openid and request.openid.openid or None\nexcept AttributeError:\n    return self.show_error(\n        request, 'Add CookieConsumer or similar to your middleware'\n    )\n\nif request.method == 'POST':\n    # TODO: The user might have entered an OpenID as a starting point,\n    # or they might have decided to sign up normally\n    form = RegistrationForm(\n        request.POST,\n        openid = openid,\n        reserved_usernames = self.reserved_usernames,\n        no_duplicate_emails = self.no_duplicate_emails\n    )\n    if form.is_valid():\n        user = self.create_user(request, form.cleaned_data, openid)\n        # Now log that new user in\n        self.log_in_user(request, user)\n        return self.on_registration_complete(request)\nelse:\n    form = RegistrationForm(\n        initial = request.openid and self.initial_from_sreg(\n            request.openid.sreg\n        ) or {},\n        openid = openid,\n        reserved_usernames = self.reserved_usernames,\n        no_duplicate_emails = self.no_duplicate_emails\n    )\n\nreturn self.render(request, self.register_template, {\n    'form': form,\n    'message': message,\n    'openid': request.openid,\n    'logo': self.logo_path or (urlparse.urljoin(\n        request.path, '../logo/'\n    )),\n    'no_thanks': self.sign_next(request.path),\n    'action': request.path,\n})", "path": "django_openid\\registration.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"\nReturns URL-safe, sha1 signed base64 compressed pickle. If secret is \nNone, settings.SECRET_KEY is used instead.\n\nIf compress is True (not the default) checks if compressing using zlib can\nsave some space. Prepends a '.' to signify compression. This is included \nin the signature, to protect against zip bombs.\n\nextra_salt can be used to further salt the hash, in case you're worried \nthat the NSA might try to brute-force your SHA-1 protected secret.\n\"\"\"\n", "func_signal": "def dumps(obj, secret = None, compress = False, extra_salt = ''):\n", "code": "pickled = pickle.dumps(obj)\nis_compressed = False # Flag for if it's been compressed or not\nif compress:\n    import zlib # Avoid zlib dependency unless compress is being used\n    compressed = zlib.compress(pickled)\n    if len(compressed) < (len(pickled) - 1):\n        pickled = compressed\n        is_compressed = True\nbase64d = encode(pickled).strip('=')\nif is_compressed:\n    base64d = '.' + base64d\nreturn sign(base64d, (secret or settings.SECRET_KEY) + extra_salt)", "path": "django_openid\\signed.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# If user is logged in, ask if they want to trust this trust_root\n# If they are NOT logged in, show the landing page:\n", "func_signal": "def show_decide(self, request, orequest):\n", "code": "if not self.user_is_logged_in(request):\n    return self.show_landing_page(request, orequest)\n\n# Check that the user owns the requested identity\nif not self.user_owns_openid(request, orequest.identity):\n    return self.show_error(request, self.not_your_openid_message)\n\n# They are logged in - ask if they want to trust this root\nreturn self.render(request, self.decide_template, {\n    'trust_root': orequest.trust_root,\n    'identity': orequest.identity,\n    'orequest': signed.dumps(orequest, self.secret_key),\n    'action': request.path,\n    'save_trusted_roots': self.save_trusted_roots\n})", "path": "django_openid\\provider.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Default behaviour is to introspect self for do_* methods\n", "func_signal": "def get_urlpatterns(self):\n", "code": "from django.conf.urls.defaults import url \nurlpatterns = []\nfor method in dir(self):\n    if method.startswith('do_'):\n        callback = getattr(self, method)\n        name = method.replace('do_', '')\n        urlname = self.urlname_pattern % name\n        urlregex = getattr(callback, 'urlregex', '^%s/$' % name)\n        urlpatterns.append(\n            url(urlregex, callback, name=urlname)\n        )\nreturn urlpatterns", "path": "django_openid\\consumer.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Incomplete orequests are stashed in a cookie\n", "func_signal": "def extract_incomplete_orequest(self, request):\n", "code": "try:\n    return signed.loads(request.COOKIES.get(\n        self.incomplete_orequest_cookie_key, ''\n    ), extra_salt = self.orequest_salt)\nexcept signed.BadSignature:\n    return None", "path": "django_openid\\provider.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# If this is a POST from the decide page, behave differently\n", "func_signal": "def __call__(self, request):\n", "code": "if '_decide' in request.POST:\n    return self.process_decide(request)\n\nquerydict = dict(request.REQUEST.items())\norequest = self.get_server(request).decodeRequest(querydict)\nif not orequest:\n    # This case (accessing the /server/ page without any args) serves \n    # two purposes. If the user has a partially complete OpenID \n    # request stashed in a signed cookie (i.e. they weren't logged \n    # in when they hit the anti-phishing landing page, then went \n    # away and logged in again, then were pushed back to here) we \n    # need to offer to complete that. Otherwise, just show a message.\n    orequest = self.extract_incomplete_orequest(request)\n    if orequest:\n        return self.show_decide(request, orequest)\n    return self.show_this_is_an_openid_server(request)\n\nif orequest.mode in (\"checkid_immediate\", \"checkid_setup\"):\n    if self.openid_is_authorized(\n            request, orequest.identity, orequest.trust_root\n        ):\n        oresponse = orequest.answer(True)\n    elif orequest.immediate:\n        oresponse = orequest.answer(\n            False, request.build_absolute_uri()\n        )\n    else:\n        return self.show_decide(request, orequest)\nelse:\n    oresponse = self.get_server(request).handleRequest(orequest)\nreturn self.server_response(request, oresponse)", "path": "django_openid\\provider.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"\nThe user's OpenID is associated with more than one account - ask them\nwhich one they would like to sign in as\n\"\"\"\n", "func_signal": "def show_pick_account(self, request, openid):\n", "code": "return self.render(request, self.pick_account_template, {\n    'action': urljoin(request.path, '../pick/'),\n    'openid': openid,\n    'users': self.lookup_openid(request, openid),\n})", "path": "django_openid\\auth.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "\"\"\"\nThe template is baked the first time you try to access \nresponse.content or iterate over it. This is a bit ugly, but is \nnecessary because Django middleware sometimes expects to be able to \nover-write the content of a response.\n\"\"\"\n", "func_signal": "def bake(self):\n", "code": "if not self.baked:\n    self.force_bake()", "path": "django_openid\\response.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Has nonce expired?\n", "func_signal": "def useNonce(self, server_url, timestamp, salt):\n", "code": "if abs(timestamp - time.time()) > openid.store.nonce.SKEW:\n    return False\ntry:\n    nonce = Nonce.objects.get(\n        server_url__exact = server_url,\n        timestamp__exact = timestamp,\n        salt__exact = salt\n    )\nexcept Nonce.DoesNotExist:\n    nonce = Nonce.objects.create(\n        server_url = server_url,\n        timestamp = timestamp,\n        salt = salt\n    )\n    return True\nnonce.delete()\nreturn False", "path": "django_openid\\models.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Code is {hex-days}.{hex-userid}.{signature}\n", "func_signal": "def generate_recovery_code(self, user):\n", "code": "days = int_to_hex(\n    (datetime.date.today() - self.recovery_origin_date).days\n)\ntoken = '%s.%s' % (days, int_to_hex(user.id))\nreturn signed.sign(token, key = (\n    self.recovery_link_secret or settings.SECRET_KEY\n) + self.recovery_link_salt)", "path": "django_openid\\auth.py", "repo_name": "clones/django-openid", "stars": 4, "license": "None", "language": "python", "size": 173}
{"docstring": "# Init window, setup window close handler\n", "func_signal": "def __init__(self):\n", "code": "Toplevel.__init__(self)\nself.protocol(\"WM_DELETE_WINDOW\", self.newClose)\n\n# Set old behavior flag & init socketOpened\nself.socketOpened = False\nself.octaveOpened = False\nif acqfile != \"\":\n\tself.old = True\nelse:\n\tself.old = False\n\n# Draw interface widgets\nself.makeAcqWidgets()\nself.makeDispWidgets()\nself.makeCtrlWidgets()", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Enable/disable arrows based on current page\"\"\"\n\n", "func_signal": "def updateArrows(self):\n", "code": "self.larr.configure(state=NORMAL)\nself.llarr.configure(state=NORMAL)\nself.rarr.configure(state=NORMAL)\nself.rrarr.configure(state=NORMAL)\n\nif self.acqlpg == 0:\n\tself.larr.configure(state=DISABLED)\n\tself.llarr.configure(state=DISABLED)\n\nif self.acqlpg == self.lastpg:\n\tself.rarr.configure(state=DISABLED)\n\tself.rrarr.configure(state=DISABLED)", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"If search term has been changed, disable arrow buttons until\nsearch term is submitted\"\"\"\n\n", "func_signal": "def searchChanged(self, event=None):\n", "code": "self.larr.configure(state=DISABLED)\nself.llarr.configure(state=DISABLED)\nself.rarr.configure(state=DISABLED)\nself.rrarr.configure(state=DISABLED)", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\" Given the selection number, return the acquisition name as\nused in the file name (i.e. DDMonYYYY_N)\"\"\"\n\n", "func_signal": "def getAcqFromSel(self, sel):\n", "code": "acqval = self.acqlist.get(sel)\nacqvalsplit = acqval.split('  -  ')\n\nif len(acqvalsplit) == 1:\n\treturn acqval[4:6]+acqval[0:3]+acqval[8:12]+'_'+acqval[14:len(acqval)+1]\nelse:\n\treturn acqval[4:6]+acqval[0:3]+acqval[8:12]+'_'+acqval[14:len(acqvalsplit[0])]", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Deletes currently selected acquisitions, including their related\nfiles in pathtodata and any pertinent entries in acqinfo and acqdisp. Clears\nthe acquisition(s) from the acquisition list as well.\"\"\"\n\n# Get currently selected acquisitions\n", "func_signal": "def deleteAcq(self):\n", "code": "sel = self.acqlist.curselection()\nacqsToDel = []\ninfoLines = []\n\n# Build and sort acqsToDel\nfor i in range(len(sel)):\n\tacqsToDel.append(int(sel[i]))\n\nacqsToDel = sorted(acqsToDel)\n\n# Open acqinfo and read first line\ninfofile = open(\"acqinfo.txt\", \"r\")\nnextLine = infofile.readline()\n\n# Filter out all to-be-deleted acqs from acqinfo and store in infoLines\ni = 0;\ndelInd = 0;\nwhile nextLine != \"\":\n\tnextLine = nextLine.rstrip(\"\\r\\n\")\n\n\tif delInd < len(acqsToDel):\n\t\tif i != acqsToDel[delInd]:\n\t\t\tinfoLines.append(nextLine);\n\t\telse:\n\t\t\tdelInd += 1\n\telse:\n\t\tinfoLines.append(nextLine);\n\n\tnextLine = infofile.readline()\n\ti = i + 1\n\ninfofile.close()\n\n\n# Delete acq files\nfor i in range(len(acqsToDel)):\n\tacq = self.getAcqFromSel(acqsToDel[i])\n\tfor acqfile in glob.glob(pathtodata+acq+'*'+fext):\n\t\tos.remove(acqfile)\n\n\n# Update acqinfo\ninfofile = open(\"acqinfo.txt\", \"w\")\n\nfor i in range(len(infoLines)):\n\tinfofile.write(infoLines[i]+\"\\n\")\ninfofile.close()\n\n\n# Clear acqdisp if necessary\ndispfile = open(\"acqdisp.txt\", \"r\")\nacqdisp = dispfile.readline()\ndispfile.close()\n\nfor i in range(len(acqsToDel)):\n\tif acqdisp.split(\":\")[0] == self.acqlist.get(acqsToDel[i]):\n\t\tdipsfile = open(\"acqdisp.txt\", \"w\")\n\t\tdispfile.write(\"\")\n\t\tdipsfile.close()\n\t\tbreak\n\n# Refresh listbox\nself.updatePageParams()\nself.fillListBox()", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Reads acquisition list from acqinfo.txt, clears old contents of acquisition\nlist and re-adds them.\"\"\"\n\n# Re-open acqinfo, read first line\n", "func_signal": "def fillListBox(self):\n", "code": "infofile = open(\"acqinfo.txt\", \"r\")\nrln = infofile.readline()\n\n# Clear acqlist\nself.acqlist.delete(0, END)\n\ni = 0\nlcount = 0\nwhile (rln != \"\") and (lcount < acqpgsize):\n\t# Strip trailing newline and return characters, then split label from line\n\trln = rln.strip(\"\\r\\n\")\n\trlnsplit = rln.split('|')\n\n\tif self.slbl == '' or rlnsplit[1].find(self.slbl) != -1:\n\t\tif i >= self.acqlpg*acqpgsize and i < (self.acqlpg+1)*acqpgsize:\n\t\t\t# Prettify the acqlist entry\n\t\t\tacqlistline = rln[2:5]+' '+rln[0:2]+', '+rln[5:9]+': '+rln[10:len(rlnsplit[0])-1]\n\n\t\t\t# Add label if it exists\n\t\t\tif rlnsplit[1] != '':\n\t\t\t\tacqlistline += '  -  '+rlnsplit[1].replace('(>$%pipe%$<)', '|')\n\n\t\t\t# Add entry to acqlist, increment lcount\n\t\t\tself.acqlist.insert(END, acqlistline)\n\t\t\tlcount = lcount+1\n\n\t\ti = i+1\n\n\t# Read next line\n\trln = infofile.readline()\n\n# Close acqinfo, update arrows\ninfofile.close()\nself.updateArrows()", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Perform data acquisition\"\"\"\n\n# Disable beginAcq, disable labelentry \n", "func_signal": "def performAcq(self):\n", "code": "self.beginAcq.configure(state=DISABLED)\nself.labelentry.configure(state=DISABLED)\n\n# Force a re-draw\nself.beginAcq.update_idletasks()\nself.labelentry.update_idletasks()\n\n# If old behavior, call acqbin. Otherwise launch acqdata in a subprocess\n# and send \"begin\" message over socket\nif self.old == True:\n\t# get acqlabel\n\ttheacqlabel = self.acqlabel.get()\n\n\t# pipes used to delimit label so replace all instances\n\ttheacqlabel = theacqlabel.replace('|', '(>$%pipe%$<)')\n\n\t# Acquire data\n\tacqdata.acqbin(acqfile, theacqlabel)\nelse:\n\t# Open socket, launch subprocess & accept sock connection\n\tself.openSocket()\n\tself.proc = subprocess.Popen(['python', 'acqdata.py'],shell=True)\n\tself.conn, self.addr = self.sckt.accept()\n\n\tdata = self.conn.recv(1024)\n\n\tif data == \"init\":\n\t\ttermtrig = \"\"\n\t\tfor i in range(0, 8):\n\t\t\ttermtrig += str(self.t[i].get())\n\n\t\tself.conn.send('begin:'+termtrig)\n\n\t\t# If no old behavior, send a terminate message, wait for child and close socket\n\t\tif self.old == False:\n\t\t\tself.conn.sendto(\"end\", self.addr)\n\t\t\tself.proc.wait()\n\t\t\tself.closeSocket()\n\t\t\n\n# Pause for half a second, give acquisition time to do its thang\nsleep(0.5)\n\n# Enable labelentry and beginAcq\nself.beginAcq.configure(state=NORMAL)\nself.labelentry.configure(state=NORMAL)\n\n# Update acqtotalnum, update page params\nself.acqtotalnum = self.acqtotalnum + 1\nself.updatePageParams()\n\t\n# If viewnewacq is true and page isn't last, change to last\nif viewnewacq == True and self.acqlpg != self.lastpg:\n\tself.acqlpg = self.lastpg\n\nself.fillListBox()\n\n# Set display to end of page if past visible threshhold\nif self.acqlpg == self.lastpg:\n\tpglen = (self.acqlnum - 1) % acqpgsize\n\tif pglen >= 10:\n\t\tself.acqlist.yview(pglen - 10 + 1)", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Sends acqlist forward one page\"\"\"\n\n# Increment acqlpg, re-fill listbox & update arrows\n", "func_signal": "def forwardPage(self):\n", "code": "self.acqlpg = self.acqlpg + 1\nself.fillListBox()", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Writes currently selected acquisition to acqdisp.txt\"\"\"\n\n# Get current selection\n", "func_signal": "def displayAcq(self):\n", "code": "sel = self.acqlist.curselection()\n\n# Open acqinfo and read first line\ninfofile = open(\"acqinfo.txt\", \"r\")\nnextLine = infofile.readline()\n\n# While last read line is not empty (i.e. no EOF)\nwhile nextLine != \"\":\n\t# Strip off newline\n\tnextLine = nextLine.rstrip(\"\\r\\n\")\n\n\t# acqinfo stores acquisitions one per line in the following format:\n\t#\n\t# DDMonYYYY_P:c\n\t#\n\t# \"DD\"   - two-digit date of acquisition \n\t# \"Mon\"  - 3-letter abbreviation of the month of the acquisition\n\t# \"YYYY\" - four digit year of the acquisition\n\t# \"P\"    - denotes the Pth acquisition on said day\n\t# \"c\"    - character of the last part of the acquisition\n\t#\n\t# The acquisition itself is fully described by everything before the colon,\n\t# so split string at the colon and compare with the name of the selected\n\t# acquisition, which does not include the last part. The only purpose\n\t# of this loop is to determine the last part of the acquisition, which we\n\t# cannot determine from the list selection alone.\n\tif nextLine.split(\":\")[0] == self.getAcqFromSel(int(sel[0])):\n\t\ttoWrite = nextLine\n\t\tbreak\n\n\t# Read next line\n\tnextLine = infofile.readline()\n\n\n# Close acqinfo, write to acqdisp\ninfofile.close()\ndispfile = open(\"acqdisp.txt\", \"w\")\ndispfile.write(toWrite)\ndispfile.close()\n\n\n# if Octave was previously launched, kill it\nif self.octaveOpened == True:\n\tself.dispproc.terminate()\n\tself.dispproc.wait()\n\tself.octaveOpened == False\n\n# Launch octave to display data\nself.dispproc = subprocess.Popen(['octave', '--persist', '--silent', 'dispd.m'], shell=True)\nself.octaveOpened = True", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Sends acqlist to last page\"\"\"\n\n# Set acqlpg to 0, re-fill listbox & update arrows\n", "func_signal": "def firstPage(self):\n", "code": "self.acqlpg = 0\nself.fillListBox()", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Deletes all acquisition files in pathtodata, clears contents of acqinfo.txt and\nacqdisp.txt, clears acquisition list.\"\"\"\n\n# Get acquisition names\n", "func_signal": "def clearAcqs(self):\n", "code": "acqnames = []\ninfofile = open(\"acqinfo.txt\", \"r\")\nrln = infofile.readline()\nwhile rln != \"\":\n\tacqnames.append(rln.split(':')[0])\n\trln = infofile.readline()\n\n\n# Delete acq files\nfor i in range(len(acqnames)):\n\t# reconstruct acq filename\n\tfor acqfile in glob.glob(pathtodata+acqnames[i]+'*'+fext):\n\t\tos.remove(acqfile)\n\n# Clear listbox entries\nself.acqlist.delete(0, END)\n\n\n# Clear acqinfo & acqdisp\ninfofile = open(\"acqinfo.txt\", \"w\")\ninfofile.write(\"\")\ninfofile.close()\n\ndispfile = open(\"acqdisp.txt\", \"w\")\ndispfile.write(\"\")\ndispfile.close()\n\n# Reset paging vars\nself.acqtotalnum = 0\nself.acqsrchnum = 0\nself.acqlnum = 0\nself.acqlpg = 0\nself.lastpg = 0", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Sends acqlist back one page\"\"\"\n\n# Decrement acqlpg, re-fill listbox & update arrows\n", "func_signal": "def backPage(self):\n", "code": "self.acqlpg = self.acqlpg - 1\nself.fillListBox()", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Handles Listbox Select events\"\"\"\n\n# Get number of selections\n", "func_signal": "def checkSel(self, evt):\n", "code": "sellen = len(self.acqlist.curselection())\n\nif sellen > 1:\n\tself.displayB.configure(state=DISABLED)\n\tself.deleteB.configure(state=NORMAL)\t\nelif sellen == 0:\n\tself.displayB.configure(state=DISABLED)\n\tself.deleteB.configure(state=DISABLED)\t\nelse:\n\tself.displayB.configure(state=NORMAL)\n\tself.deleteB.configure(state=NORMAL)", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "# Control Interface LabelFrame\n", "func_signal": "def makeCtrlWidgets(self):\n", "code": "self.ctrlfrm = LabelFrame(self, text=\"Control Interface\")\nself.ctrlfrm.grid(row=1, column=0, in_=self, columnspan=2, sticky=E+W)\n\n# \"CONFIG 1-6\" label\nconfig16Lbl = Label(self.ctrlfrm, text=\"Config. 1-6:\")\nconfig16Lbl.grid(row=1, column=0, columnspan=6, sticky=W)\n\n# Generate and draw checkboxes, do some column resizing while we're at it\nc = []\ncnfgChck = []\n\nfor i in range(0, 6):\n\tc.append(IntVar())\n\tcnfgChck.append(Checkbutton(self.ctrlfrm, text=\"\", variable=c[i]))\n\tcnfgChck[i].grid(row=2, column=i, in_=self.ctrlfrm)\n\tself.ctrlfrm.columnconfigure(i, weight=0, minsize=15)\n\n\n# Active Input radio label\nactiveinLbl = Label(self.ctrlfrm, text=\"Active Input Type:\")\nactiveinLbl.grid(row=1, column=7, in_=self.ctrlfrm, columnspan=2, sticky=W)\n\n# Active Input analog/digital buttons\nr = IntVar()\nself.rad1 = Radiobutton(self.ctrlfrm, text=\"Analog\", variable=r, value=1)\nself.rad2 = Radiobutton(self.ctrlfrm, text=\"Digital\", variable=r, value=2)\nself.rad1.grid(row=2, column=7, in_=self.ctrlfrm, columnspan=1)\nself.rad2.grid(row=2, column=8, in_=self.ctrlfrm, columnspan=1)\n\n\n# Output enable label\nouten = IntVar()\noutenChck = Checkbutton(self.ctrlfrm, text=\"Output Enable\", variable=outen)\noutenChck.grid(row=2, column=10, in_=self.ctrlfrm, sticky=W)\n\n\n# Resize for prettiness\nself.ctrlfrm.rowconfigure(3, weight=0, minsize=10)\nself.ctrlfrm.columnconfigure(6, weight=0, minsize=40)\nself.ctrlfrm.columnconfigure(9, weight=0, minsize=40)", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Close socket connection\"\"\"\n\n", "func_signal": "def closeSocket(self):\n", "code": "self.conn.shutdown(socket.SHUT_RDWR)\nself.sckt.close()\nself.socketOpened = False", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Create socket connection\"\"\"\n\n# Establish socket\n", "func_signal": "def openSocket(self):\n", "code": "self.sckt = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nself.sckt.bind((sckhost, sckport))\nself.sckt.listen(1)\nself.socketOpened = True", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Update acqlnum, lastpg\"\"\"\n\n# Get current label search entry\n", "func_signal": "def updatePageParams(self):\n", "code": "searchterm = self.searchentry.get()\n\n# If search term is new, set page to first\nif searchterm != self.slbl:\n\tself.acqlpg = 0\n\nself.slbl = searchterm\nif self.slbl == \"Search...\":\n\tself.slbl = ''\n\n# First acqinfo open, get number of search matches\nself.acqsrchnum = 0\n\nif self.slbl != '':\n\tinfofile = open(\"acqinfo.txt\", \"r\")\n\tfor line in infofile:\n\t\tif line.split('|')[1].find(self.slbl) != -1:\n\t\t\tself.acqsrchnum += 1\n\tinfofile.close()\n\n# Update acqlnum & lastpg\nif self.acqsrchnum == 0:\n\tself.acqlnum = self.acqtotalnum\nelse:\n\tself.acqlnum = self.acqsrchnum\n\nif self.acqlnum == 0:\n\tself.lastpg = 0\nelse:\n\tself.lastpg = ceil(1.0*self.acqlnum/acqpgsize) - 1", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "# Data Acquisition LabelFrame\n", "func_signal": "def makeAcqWidgets(self):\n", "code": "self.acqfrm = LabelFrame(self, text=\"Data Acquisition\", padx=5, pady=5)\nself.acqfrm.grid(row=0, column=0, in_=self, sticky=N+E+S+W)\n\n\n# \"Label\" label\nLabel(self.acqfrm, text=\"Label (optional): \").grid(row=1, column=0, sticky=E)\n\n# \"Label\" entry box, init searchlbl\nself.acqlabel = StringVar()\nself.labelentry = Entry(self.acqfrm, textvariable=self.acqlabel)\nself.labelentry.grid(row=1, column=1, columnspan=8, sticky=E+W)\nself.searchlbl = ''\n\n\n# \"Stop trigger\" label\nLabel(self.acqfrm, text=\"Stop Trigger: \").grid(row=3, column=0, sticky=E)\n\n# Generate trigger checkbuttons\nself.t = []\ntrigChck = []\n\nfor i in range(0, 8):\n\tself.t.append(IntVar())\n\ttrigChck.append(Checkbutton(self.acqfrm, padx=0, pady=0, variable=self.t[i]))\n\ttrigChck[i].grid(row=3, column=(i+1), rowspan=2, sticky=N+S)\n\tself.acqfrm.columnconfigure((i+1), weight=0, minsize=0)\n\n# \"MSB first\" label\nLabel(self.acqfrm, text=\"(MSB first)\").grid(row=4, column=0, sticky=E)\n\n# Begin/End Acquisition buttons\nself.beginAcq = Button(self.acqfrm, text=\"Begin Acquisition\", width=15, command=self.performAcq)\nself.beginAcq.grid(row=6, column=0, columnspan=9, in_=self.acqfrm, sticky=S)\n\n# Do some resizing\nself.acqfrm.columnconfigure(0, minsize=40)\nself.acqfrm.rowconfigure(0, minsize=20)\nself.acqfrm.rowconfigure(2, minsize=20)\nself.acqfrm.rowconfigure(5, minsize=20)", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "\"\"\"Sends acqlist to last page\"\"\"\n\n# Set acqlpg to last. re-fill listbox and update arrows\n", "func_signal": "def lastPage(self):\n", "code": "self.acqlpg = self.lastpg\nself.fillListBox()", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "# Data Display LabelFrame\n", "func_signal": "def makeDispWidgets(self):\n", "code": "self.dispfrm = LabelFrame(self, text=\"Data Display\", padx=5, pady=5)\nself.dispfrm.grid(row=0, column=1, in_=self)\n\n\n# Search entry box\nself.searchentry = Entry(self.dispfrm, width=7)\nself.searchentry.grid(row=0, column=0, in_=self.dispfrm, sticky=E+W)\nself.searchentry.insert(0, \"Search...\")\nself.searchentry.bind(\"<Button-1>\", self.searchClick)\nself.searchentry.bind(\"<Double-Button-1>\", self.searchDClick)\nself.searchentry.bind(\"<Return>\", self.refreshListBox)\nself.searchentry.bind(\"<Key>\", self.searchChanged)\n\n\n# Make arrow buttons\nself.llarr = Button(self.dispfrm, text=\"<<\", width=1, command=self.firstPage)\nself.llarr.grid(row=0, column=3, in_=self.dispfrm, sticky=E+W)\nself.larr = Button(self.dispfrm, text=\"<\", width=1, command=self.backPage)\nself.larr.grid(row=0, column=4, in_=self.dispfrm, sticky=E+W)\n\nself.rarr = Button(self.dispfrm, text=\">\", width=1, command=self.forwardPage)\nself.rarr.grid(row=0, column=5, in_=self.dispfrm, sticky=E+W)\nself.rrarr = Button(self.dispfrm, text=\">>\", width=1, command=self.lastPage)\nself.rrarr.grid(row=0, column=6, in_=self.dispfrm, sticky=E+W)\n\n\n# Make acq listbox, bind select event\nself.acqlist = Listbox(self.dispfrm, selectmode=EXTENDED)\nself.acqlist.grid(row=1, column=0, in_=self.dispfrm, columnspan=7, rowspan=3, sticky=E+W)\nself.acqlist.bind('<<ListboxSelect>>', self.checkSel)\n\n# Make acq list scrollbar\nlistscroll = Scrollbar(self.dispfrm, orient=VERTICAL, relief=SUNKEN)\nlistscroll.config(command=self.acqlist.yview)\nself.acqlist.config(yscrollcommand=listscroll.set)\nlistscroll.grid(row=1, column=7, in_=self.dispfrm, columnspan=1, rowspan=3, sticky=N+S)\n\n# Init acqlpg, slbl\nself.acqlpg = 0 # Current page\nself.slbl = ''\n\n# Get acqtotalnum & acqlnum\nself.acqtotalnum = 0 # Number of acqs\nf = open(\"acqinfo.txt\", \"r\")\nfor line in f:\n\tself.acqtotalnum = self.acqtotalnum + 1\nf.close()\n\n# Set acqlnum & lastpg\nself.acqlnum = self.acqtotalnum\n\nif self.acqlnum == 0:\n\tself.lastpg = 0\nelse:\n\tself.lastpg = ceil(1.0*self.acqlnum/acqpgsize) - 1\n\n# Put acqs in list and update arrows\nself.fillListBox()\nself.updateArrows()\n\n\n# Refresh/Clear/Delete/Display buttons\nrefreshList = Button(self.dispfrm, text=\"Refresh\", width=8, command=self.refreshListBox)\nclearList = Button(self.dispfrm, text=\"Clear\", width=8, command=self.clearAcqs)\nself.deleteB = Button(self.dispfrm, text=\"Delete\", width=8, command=self.deleteAcq)\nself.displayB = Button(self.dispfrm, text=\"Display\", width=8, command=self.displayAcq)\n\n# Add all four buttons to grid\nrefreshList.grid(row=4, column=0, in_=self.dispfrm, columnspan=1)\nclearList.grid(row=4, column=1, in_=self.dispfrm, columnspan=1)\nself.deleteB.grid(row=4, column=3, in_=self.dispfrm, columnspan=2)\nself.displayB.grid(row=4, column=5, in_=self.dispfrm, columnspan=2)\n\n# Disable display&delete buttons to begin\nself.displayB.configure(state=DISABLED)\nself.deleteB.configure(state=DISABLED)", "path": "src\\pcsoft\\inter.py", "repo_name": "lordofhyphens/ece495a", "stars": 7, "license": "None", "language": "python", "size": 25581}
{"docstring": "# create an interface, pull out all 'public_*' methods\n# into our namespace, striping the prefix\n", "func_signal": "def initialise(self):\n", "code": "psc = PelotonManagementInterface(self.kernel)\npublicMethods =  [i for i in dir(psc) if i.startswith('public_')]\nnamespace={}\nfor m in publicMethods:\n    namespace[m[7:]] = getattr(psc, m)\n\nself.pmh = PasswordManhole(int(self.config.port),\n                           self.config.username,\n                           self.config.password,\n                           namespace)", "path": "src\\peloton\\plugins\\shell.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" A worker registers by sending a KernelInterface\nreferenceable and a token. The token was passed to the worker\ngenerator and is used simply to verify that this is indeed a valid\nand wanted contact.\"\"\"\n", "func_signal": "def remote_registerWorker(self, worker, token):\n", "code": "self.logger.info(\"Starting worker, token=%s NOT VALIDATED\" % token)        \nserviceName, publishedName, runtimeConfig = self.kernel.addWorker(worker, token)\npwa = PelotonWorkerAdapter(self, serviceName, self.kernel)\nworker.checkBeat = pwa.checkBeat\n\nworkerInfo = { 'pwa' : pwa,\n              'serviceName' : serviceName,\n              'publishedName' : publishedName,\n              'runtimeConfig' : runtimeConfig,\n              'loglevel' : self.kernel.settings.loglevel,\n              'logdir' : self.kernel.settings.logdir,\n              'servicePath' : self.kernel.settings.servicepath,\n              }\n\nreturn workerInfo", "path": "src\\peloton\\adapters\\pb.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Use the following process to call the method:\n    - obtain a worker reference\n    - call the method in there\n    - park the deferred; return a new deferred to the caller of this method\n    - if error, reset and try again.\n    - if no error, put result onto return deferred.\n    \nThe coreio call method will receive a deferred OR a NoWorkersError\nwill be raised.\n\"\"\"\n", "func_signal": "def call(self, service, method, *args, **kwargs):\n", "code": "rd = Deferred()\nrd._peloton_loopcount = 0 # used in _call\nself._call(rd, service, method, args, kwargs)\nreturn rd", "path": "src\\peloton\\pscproxies.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "# Let's read the command line\n", "func_signal": "def main():\n", "code": "usage = \"usage: %prog [options]\" # add 'arg1 arg2' etc as required\nparser = FilteredOptionParser(usage=usage, version=\"Peloton version %s\" % peloton.RELEASE_VERSION)", "path": "src\\peloton\\psc.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Takes ciphertext made by encrypt, decrypts and de-pickles. \"\"\"\n", "func_signal": "def decrypt(ciphertext, key):\n", "code": "blocksize = key.size()/8 + 1\npt = []\nwhile ciphertext:\n    if len(ciphertext) <= blocksize:\n        chunk = ciphertext\n        ciphertext=''\n    else:\n        chunk = ciphertext[:blocksize]\n        ciphertext = ciphertext[blocksize:]\n    pt.append(key.decrypt(chunk))\npt = ''.join(pt)\ntry:\n    v = pickle.loads(pt)\n    return v\nexcept:\n    raise PelotonError(\"Invalid ciphertext given to 'decode'\")", "path": "src\\peloton\\utils\\crypto.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Called from the PSC to check whether the worker\nis OK. the heartBeat counter is incremented. If the counter \nexceeds the threshold value (default 5) checkBeat returns False,\notherwise returns True. \"\"\"\n", "func_signal": "def checkBeat(self, threshold=5):\n", "code": "self.heartBeat += 1\nreturn self.heartBeat <= threshold", "path": "src\\peloton\\adapters\\pb.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Loads a key and cookie file returning a tuple of (cookie, key). \"\"\"\n", "func_signal": "def loadKeyAndCookieFile(keyfile):\n", "code": "f = open(keyfile, 'rt')\nasciiForm = f.readlines()\npkle = base64.decodestring(\"\".join(asciiForm))\nreturn pickle.loads(pkle)", "path": "src\\peloton\\utils\\crypto.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Creates a Peloton key and cookie file and writes to keyfile if\nspecified. Returns the contents as a string. \"\"\"\n", "func_signal": "def makeKeyAndCookieFile(keyfile = None, keylength=512, tokenlength=50):\n", "code": "cookie = makeCookie(tokenlength)\nkey = newKey(keylength)\ncontents = (cookie, key)\nasciiForm = base64.encodestring(pickle.dumps(contents))\nif keyfile:\n    f = open(keyfile,'wt')\n    f.writelines(asciiForm)\n    f.close()\nreturn asciiForm", "path": "src\\peloton\\utils\\crypto.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Start a PSC. By default the first step is to daemonise, either by \na double fork (on a POSIX system) or by re-spawning the process (on Windows)\nto detach from the console; this can be over-ridden by specifying --nodetach \non the command line.\n\nThe kernel is then started. This creates worker processes as required via the\nsubprocess module.\n\"\"\"\n", "func_signal": "def start(pc):\n", "code": "if not pc.nodetach:\n    makeDaemon()\n\nlogging.initLogging(rootLoggerName='PSC', \n                    logLevel=getattr(logging, pc.loglevel),\n                    logdir=pc.logdir, \n                    logfile='psc.log', \n                    logToConsole=pc.nodetach)\nlogging.getLogger().info(\"Kernel starting; pid = %d\" % os.getpid())\n\nkernel = PelotonKernel(pc)\nlogging.setAdditionalLoggers(kernel)\nex = kernel.start()\nreturn ex", "path": "src\\peloton\\psc.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Returns the serialised profile for the referenced PSC or self if guid\nis None. \"\"\"\n", "func_signal": "def remote_getPSCProfile(self, guid=None):\n", "code": "if not guid:\n    return repr(self.kernel.profile)\nelse:\n    try:\n        return repr(self.routingTable.pscByGUID[guid].profile)\n    except KeyError:\n        raise PelotonError(\"%s is unknown\" % guid)", "path": "src\\peloton\\adapters\\pb.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Register to receive events with the given handler. Handler\nmust be a Referenceable providing remote_eventReceived.\"\"\"\n", "func_signal": "def remote_register(self, key, handler, exchange='events'):\n", "code": "handler = RemoteEventHandler(handler)\nself.eventHandlers.append(handler)\nself.dispatcher.register(key, handler, exchange)", "path": "src\\peloton\\adapters\\pb.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" A remote PSC will call registerPSC with a token encrypted\nwith the domain key. Provided this decrypts we know the remote PSC is\npermitted to join in this domain. the remotePSC is a remote instance of\nPelotonGridAdapter which provides methods for inter-PSC work.\n\n@todo: it may be that the token can be included in the remotePSC using\ncopyable type stuff.\n\"\"\"\n", "func_signal": "def remote_registerPSC(self, token):\n", "code": "self.logger.info(\"RegisterPSC %s: ref returned with NO VALIDATION\" % token)\nref = PelotonInternodeAdapter(self.kernel, token)\nreturn ref", "path": "src\\peloton\\adapters\\pb.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Create a cookie tokenlength characters long made from \ncharacters randomly chosen from the provided tokenspace (for which\na suitable default is provided.)\n\"\"\"\n", "func_signal": "def makeCookie(tokenlength, tokenspace=tokenspace):\n", "code": "tchars = len(tokenspace)\ncookie = \"\".join([tokenspace[random.randrange(0,tchars)] \n                  for _ in xrange(tokenlength)])\nreturn cookie", "path": "src\\peloton\\utils\\crypto.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" De-register handler as a listener. \"\"\"\n", "func_signal": "def remote_deregister(self, handler):\n", "code": "for h in self.eventHandlers:\n    if h.remoteHandler == handler:\n        handler = h\n        break\nelse:\n    # no handler registered\n    self.logger.error(\"Attempt to de-register handler for event that is not registered.\")\n    return\n\nself.dispatcher.deregister(handler)\nself.eventHandlers.remove(handler)", "path": "src\\peloton\\adapters\\pb.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Takes data, pickles to string and encrypts into ASCII for\nsafe transmission over unknown wire protocols. \n\nBeware: the encryption is strong but the computation is slow!\n\"\"\"\n", "func_signal": "def encrypt(data, key):\n", "code": "pt = pickle.dumps(data)\nblocksize = key.size()/8\nct = []\nwhile pt:\n    if len(pt) <= blocksize:\n        chunk = pt\n        pt=''\n    else:\n        chunk=pt[:blocksize]\n        pt=pt[blocksize:]\n    ct.append(key.encrypt(chunk,'')[0])\n\nreturn \"\".join(ct)", "path": "src\\peloton\\utils\\crypto.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" De-register handler as a listener. \"\"\"\n", "func_signal": "def remote_deregister(self, handler):\n", "code": "for h in self.eventHandlers:\n    if h.remoteHandler == handler:\n        handler = h\n        break\nelse:\n    # no handler registered\n    self.logger.error(\"Attempt to de-register handler for event that is not registered.\")\n    return\n\nself.kernel.dispatcher.deregister(handler)\nself.eventHandlers.remove(handler)", "path": "src\\peloton\\adapters\\pb.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Return a new key pair lenbits long. \"\"\"\n", "func_signal": "def newKey(lenbits=512):\n", "code": "randpool.stir()\nkey = _Pub_Alg_.generate(lenbits, randpool.get_bytes)\nreturn key", "path": "src\\peloton\\utils\\crypto.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" A twisted error occured when making the remote call. This is going\nto be one of:\n\n    - An application error raised in the service - this must pass through. It\n      will be characterised by err.value being a string\n    - A connection was broken whilst the call was being made; flag the worker\n      as dead and start again. This is signified by err.value being a \n      pb.PBConnectionLost error.\n    - The connection was closed cleanly whilst performing the operation; likely\n      the service was re-started. Current protocol is to re-issue the request but\n      in future the old worker may well finish the job so this error will not\n      be raised. This condition is signified by err.value being a ConnectionDone\n      instance.\n\"\"\"\n", "func_signal": "def __callError(self, err, rd, p, service, method, args, kwargs):\n", "code": "if isinstance(err.value, pb.PBConnectionLost) or \\\n     isinstance(err.value, ConnectionDone):\n    if self.RUNNING:\n        self.kernel.workerStore[service].notifyDeadWorker(p)\n        self._call(rd, service, method, args, kwargs)\n    else:\n        rd.errback(NoWorkersError(\"No workers for service %s\" % service))\nelse:\n    rd.errback(err)", "path": "src\\peloton\\pscproxies.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" Register to receive events with the given handler. Handler\nmust be a Referenceable providing remote_eventReceived.\"\"\"\n", "func_signal": "def remote_register(self, key, handler, exchange='events'):\n", "code": "handler = RemoteEventHandler(handler)\nself.eventHandlers.append(handler)\nself.kernel.dispatcher.register(key, handler, exchange)", "path": "src\\peloton\\adapters\\pb.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\" In this startup the adapter seeks to bind to a port. It obtains\nthe host/port to which to bind from the kernel profile, but it\nmay, according to whether the 'anyport' switch is set or not, seek an \nalternative port should its chosen target be bound by another application.\n\"\"\"\n", "func_signal": "def start(self):\n", "code": "interface,port = self.kernel.settings.bind.split(':')\nport = int(port)\n\nsvr = pb.PBServerFactory(self)\nwhile True:\n    try:\n        self.connection = reactor.listenTCP(port, svr, interface=interface)\n        self.kernel.profile['bind']= \"%s:%d\" % (interface, port)\n        self.kernel.profile['bind_interface'] = interface\n        self.kernel.profile['bind_port'] = port\n        break\n    except CannotListenError:\n        if self.kernel.settings.anyport == True:\n            port += 1\n        else:\n            raise RuntimeError(\"Cannot bind to port %d\" % port)\n    except Exception:\n        self.logger.exception(\"Could not connect %s\" % self.adapterName)\n\nself.logger.info(\"Bound to %s:%d\" % (interface, port))", "path": "src\\peloton\\adapters\\pb.py", "repo_name": "aquamatt/Peloton", "stars": 7, "license": "other", "language": "python", "size": 2284}
{"docstring": "\"\"\"Called automatically when an XML element is open\n    Input: name=element name\n             attrs: element attributes\"\"\"\n\n", "func_signal": "def start_elementContact(self,name,attrs):\n", "code": "if name == 'PresenceAuth-Request' :\n    self.bTransactionID = 0\n    self.bPresence = 1\n    self.bUserID = 0\nelif self.bPresence and name == 'UserID' :\n    self.bUserID = 1\nelif name == 'TransactionID' :\n    self.bTransactionID = 1\n    self.bPresence = 0\n    self.bUserID = 0", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is open\n    Input: name=element name\n             attrs: element attributes\"\"\"\n\n", "func_signal": "def start_elementSession(self,name, attrs):\n", "code": "if name == 'SessionID' :\n    self.bSessionID = 1", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is read\n    Input: name=element name\"\"\"\n\n", "func_signal": "def end_elementContact(self,name):\n", "code": "if name == 'NickName' :\n    self.Contacts[self.userId]=self.name\n    self.bNickName = 0\n    self.bUserID = 0\n    self.bName = 0", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is closed\n    Input: name=element name\"\"\"\n\n", "func_signal": "def end_elementMessage(self,name):\n", "code": "if name == 'NewMessage' :\n    self.bNewMessage = 0\n    self.bSender = 0\n    self.bUserID = 0\n    self.bContentData = 0\nelif name == 'Sender' :\n    self.bSender = 0\n    self.bUserID = 0\nelif name == 'UserID' :\n    self.bSender = 0\n    self.bUserID = 0\nelif name == 'ContentData' :\n    self.bContentData = 0", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is open\n    Input: name=element name\n             attrs: element attributes\"\"\"\n\n", "func_signal": "def start_elementContact(self,name,attrs):\n", "code": "if name == 'NickName' :\n    self.bNickName = 1\n    self.bUserID = 0\n    self.bName = 0\nelif self.bNickName and name == 'Name' :\n    self.bUserID = 0\n    self.bName = 1\nelif self.bNickName and name == 'UserID' :\n    self.bUserID = 1\n    self.bName = 0", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Polling to receive async notifications\n    Input: none\n    Returns: Response text\"\"\"\n\n# Send <Polling-Request>\n", "func_signal": "def Polling(self) :\n", "code": "params = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID /></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><Polling-Request /></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\nconn.close()\n\nreturn response", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is closed\n    Input: name=element name\"\"\"\n\n", "func_signal": "def end_elementContact(self,name):\n", "code": "if name == 'PresenceAuth-Request' :\n    self.bPresence = 0\n    self.bUserID = 0\nelif name == 'UserID' :\n    self.bUserID = 0\nelif name == 'TransactionID' :\n    self.bTransactionID = 0", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is read\n    Input: data=element data\"\"\"\n\n", "func_signal": "def char_dataSession(self,data):\n", "code": "if self.bSessionID == 1 :\n    self.sessionID = data", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Authorizes a contact to know your presence status\n    Input: user=contact's user Id (wv:6xxxxxxxx@movistar.es)\n             transaction=transaction id received in the authorization request\n    Returns: void\"\"\"\n\n# Send <GetPresence-Request>\n", "func_signal": "def AuthorizeContact(self, user, transaction) :\n", "code": "params = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><GetPresence-Request><User><UserID>\"\"\"+user+\"\"\"</UserID></User><PresenceSubList xmlns=\"http://www.openmobilealliance.org/DTD/WV-PA1.2\"><OnlineStatus /><ClientInfo /><GeoLocation /><FreeTextLocation /><CommCap /><UserAvailability /><StatusText /><StatusMood /><Alias /><StatusContent /><ContactInfo /></PresenceSubList></GetPresence-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <Status> para hacer el ack de la peticin\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Response</TransactionMode><TransactionID>\"\"\"+transaction+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><Status><Result><Code>200</Code></Result></Status></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <PresenceAuth-User>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId+1)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><PresenceAuth-User><UserID>\"\"\"+user+\"\"\"</UserID><Acceptance>T</Acceptance></PresenceAuth-User></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\nself.nTransId = self.nTransId + 2\n\nconn.close()", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is open\n    Input: name=element name\n             attrs: element attributes\"\"\"\n\n", "func_signal": "def start_elementMessage(self,name,attrs):\n", "code": "if name == 'NewMessage' :\n    self.bNewMessage = 1\n    self.bSender = 0\n    self.bUserID = 0\n    self.bContentData = 0\nelif self.bNewMessage == 1 and name == 'Sender' :\n    self.bSender = 1\n    self.bUserID = 0\n    self.bContentData = 0\nelif self.bNewMessage == 1 and name == 'ContentData' :\n    self.bSender = 0\n    self.bUserID = 0\n    self.bContentData = 1\nelif self.bSender == 1 and name == 'UserID' :\n    self.bUserID = 1", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Gets userId and transaction id\n    Input: response=string with XML data received from server\n    Returns: contact user ID | transaction id\"\"\"\n\n", "func_signal": "def GetUser(self, response) :\n", "code": "p = xml.parsers.expat.ParserCreate()\n\np.StartElementHandler = self.start_elementContact\np.EndElementHandler = self.end_elementContact\np.CharacterDataHandler = self.char_dataContact\n\np.Parse(response)\n\nreturn self.userId+'|'+self.transactionId", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is read\n    Input: data=element data\"\"\"\n\n", "func_signal": "def char_dataMessage(self,data):\n", "code": "if self.bUserID == 1 :\n    self.userId = str(data)\nelif self.bContentData == 1 :\n    self.message = self.message + unicode(data)", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Adds a contact to your list\n    Input: log=string with your telephone number\n             contact=string with contact telephone number\n    Returns: List of present contacts\"\"\"\n\n", "func_signal": "def AddContact(self, log, contact) :\n", "code": "RetList = {}\n\n# Send <Search-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><Search-Request><SearchPairList><SearchElement>USER_MOBILE_NUMBER</SearchElement><SearchString>\"\"\"+contact+\"\"\"</SearchString></SearchPairList><SearchLimit>50</SearchLimit></Search-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <GetPresence-Request> to get contact presence status\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId+1)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><GetPresence-Request><User><UserID>wv:\"\"\"+contact+\"\"\"@movistar.es</UserID></User><PresenceSubList xmlns=\"http://www.openmobilealliance.org/DTD/WV-PA1.2\"><OnlineStatus /><ClientInfo /><GeoLocation /><FreeTextLocation /><CommCap /><UserAvailability /><StatusText /><StatusMood /><Alias /><StatusContent /><ContactInfo /></PresenceSubList></GetPresence-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\ncp=PresenceParser()\nRetList=cp.GetPresentsList(response)\n\nnickname=RetList['wv:'+contact+'@movistar.es'][1]\n\n# Send <ListManage-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId+2)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><ListManage-Request><ContactList>wv:\"\"\"+log+\"\"\"/~PEP1.0_subscriptions@movistar.es</ContactList><AddNickList><NickName><Name>\"\"\"+nickname+\"\"\"</Name><UserID>wv:\"\"\"+contact+\"\"\"@movistar.es</UserID></NickName></AddNickList><ReceiveList>T</ReceiveList></ListManage-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <ListManage-Request> esta vez para la PrivateList\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId+3)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><ListManage-Request><ContactList>wv:\"\"\"+log+\"\"\"/~PEP1.0_privatelist@movistar.es</ContactList><AddNickList><NickName><Name>\"\"\"+nickname+\"\"\"</Name><UserID>wv:\"\"\"+contact+\"\"\"@movistar.es</UserID></NickName></AddNickList><ReceiveList>T</ReceiveList></ListManage-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\nself.nTransId = self.nTransId + 4\n\nconn.close()\n\nreturn RetList", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Deletes a contact from your list\n    Input: log=string with your user telephone number\n             contact=user id of the contact to delete (wv:6xxxxxxxx@movistar.es)\n    Returns: void\"\"\"\n\n# Send <ListManage-Request> to delete contact from subscription list\n", "func_signal": "def DeleteContact(self, log, contact) :\n", "code": "params = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><ListManage-Request><ContactList>wv:\"\"\"+log+\"\"\"/~PEP1.0_subscriptions@movistar.es</ContactList><RemoveNickList><UserID>\"\"\"+contact+\"\"\"</UserID></RemoveNickList><ReceiveList>T</ReceiveList></ListManage-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <ListManage-Request> to delete contact from private list\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId+1)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><ListManage-Request><ContactList>wv:\"\"\"+log+\"\"\"/~PEP1.0_privatelist@movistar.es</ContactList><RemoveNickList><UserID>\"\"\"+contact+\"\"\"</UserID></RemoveNickList><ReceiveList>T</ReceiveList></ListManage-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <UnsubscribePresence-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId+2)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><UnsubscribePresence-Request><User><UserID>\"\"\"+contact+\"\"\"</UserID></User></UnsubscribePresence-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <DeleteAttributeList-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId+3)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><DeleteAttributeList-Request><UserID>\"\"\"+contact+\"\"\"</UserID><DefaultList>F</DefaultList></DeleteAttributeList-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <CancelAuth-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId+4)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><CancelAuth-Request><UserID>\"\"\"+contact+\"\"\"</UserID></CancelAuth-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\nself.nTransId = self.nTransId + 5\n\nconn.close()", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Sends a message to destination number\n    Input: log=string with your telephone number\n             destination=string with destination user id (wv:6xxxxxxxx@movistar.es)\n             message=message text\n    Returns: void\"\"\"\n\n# Send <SendMessage-Request>\n", "func_signal": "def SendMessage(self, log, destination, message) :\n", "code": "paramsMsg = '<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>'+self.sessionID.encode('utf8')+'</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>'+str(self.nTransId)+'</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><SendMessage-Request><DeliveryReport>F</DeliveryReport><MessageInfo><ContentType>text/html</ContentType><ContentSize>148</ContentSize><Recipient><User><UserID>'+destination+'</UserID></User></Recipient><Sender><User><UserID>'+log+'@movistar.es</UserID></User></Sender></MessageInfo><ContentData>&lt;span style=\"color:#000000;font-family:\"Microsoft Sans Serif\";font-style:normal;font-weight:normal;font-size:12px;\"&gt;'+message+'&lt;/span&gt;</ContentData></SendMessage-Request></TransactionContent></Transaction></Session></WV-CSP-Message>'\n\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", paramsMsg, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\nself.nTransId = self.nTransId + 1\n\nconn.close()", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is closed\n    Input: name=element name\"\"\"\n\n", "func_signal": "def end_elementSession(self,name):\n", "code": "if name == 'SessionID' :\n    self.bSessionID = 0", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Gets presence list\n    Input: response=string with XML data received from server\n    Returns: contact list\"\"\"\n\n", "func_signal": "def GetPresentsList(self, response) :\n", "code": "p = xml.parsers.expat.ParserCreate()\n\np.StartElementHandler = self.start_elementPresence\np.EndElementHandler = self.end_elementPresence\np.CharacterDataHandler = self.char_dataPresence\n\np.Parse(response)\n\nreturn self.Presents", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Connects to SMS2.0 service\n    Input: log=string with telephone number\n              nickname=cadena con el nickname que queramos utilizar (slo es necesario la primera vez)\n    Returns: Contact list of the given telephone number\"\"\"\n\n# Send <ClientCapability-Request>\n", "func_signal": "def Connect(self, log, nickname) :\n", "code": "params = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>1</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><ClientCapability-Request><ClientID><URL>WV:InstantMessenger-1.0.2309.16485@COLIBRIA.PC-CLIENT</URL></ClientID><CapabilityList><ClientType>COMPUTER</ClientType><InitialDeliveryMethod>P</InitialDeliveryMethod><AcceptedContentType>text/plain</AcceptedContentType><AcceptedContentType>text/html</AcceptedContentType><AcceptedContentType>image/png</AcceptedContentType><AcceptedContentType>image/jpeg</AcceptedContentType><AcceptedContentType>image/gif</AcceptedContentType><AcceptedContentType>audio/x-wav</AcceptedContentType><AcceptedContentType>image/jpg</AcceptedContentType><AcceptedTransferEncoding>BASE64</AcceptedTransferEncoding><AcceptedContentLength>256000</AcceptedContentLength><MultiTrans>1</MultiTrans><ParserSize>300000</ParserSize><SupportedCIRMethod>STCP</SupportedCIRMethod><ColibriaExtensions>T</ColibriaExtensions></CapabilityList></ClientCapability-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <Service-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>2</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><Service-Request><ClientID><URL>WV:InstantMessenger-1.0.2309.16485@COLIBRIA.PC-CLIENT</URL></ClientID><Functions><WVCSPFeat><FundamentalFeat /><PresenceFeat /><IMFeat /><GroupFeat /></WVCSPFeat></Functions><AllFunctionsRequest>T</AllFunctionsRequest></Service-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <UpdatePresence-Request> to inform that you are online\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>3</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><UpdatePresence-Request><PresenceSubList xmlns=\"http://www.openmobilealliance.org/DTD/WV-PA1.2\"><OnlineStatus><Qualifier>T</Qualifier></OnlineStatus><ClientInfo><Qualifier>T</Qualifier><ClientType>COMPUTER</ClientType><ClientTypeDetail xmlns=\"http://imps.colibria.com/PA-ext-1.2\">PC</ClientTypeDetail><ClientProducer>Colibria As</ClientProducer><Model>TELEFONICA Messenger</Model><ClientVersion>1.0.2309.16485</ClientVersion></ClientInfo><CommCap><Qualifier>T</Qualifier><CommC><Cap>IM</Cap><Status>OPEN</Status></CommC></CommCap><UserAvailability><Qualifier>T</Qualifier><PresenceValue>AVAILABLE</PresenceValue></UserAvailability></PresenceSubList></UpdatePresence-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <GetList-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>4</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><GetList-Request /></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <GetPresence-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>5</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><GetPresence-Request><User><UserID>wv:\"\"\"+log+\"\"\"@movistar.es</UserID></User></GetPresence-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\npp0=PresenceParser()\nlistI=pp0.GetPresentsList(response)\nmyStatus = listI['wv:' + log + '@movistar.es']\nself.myAlias = myStatus[1]\n\n# Send <ListManage-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>6</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><ListManage-Request><ContactList>wv:\"\"\"+log+\"\"\"/~pep1.0_privatelist@movistar.es</ContactList><ReceiveList>T</ReceiveList></ListManage-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\ncp=ContactsParser()\nRetList=cp.GetContactList(response)\n\n# Send <CreateList-Request>\nnickList = ''\nif RetList != {} :\n    nickList = '<NickList>'\n    for k,v in RetList.iteritems() :\n        nickList = nickList + '<NickName><Name>%s</Name><UserID>%s</UserID></NickName>' % (v, k)\n    nickList = nickList + '</NickList>'\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>7</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><CreateList-Request><ContactList>wv:\"\"\"+log+\"\"\"/~PEP1.0_subscriptions@movistar.es</ContactList>\"\"\"+nickList+\"\"\"</CreateList-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <SubscribePresence-Request>\nparams = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>8</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><SubscribePresence-Request><ContactList>wv:\"\"\"+log+\"\"\"/~PEP1.0_subscriptions@movistar.es</ContactList><PresenceSubList xmlns=\"http://www.openmobilealliance.org/DTD/WV-PA1.2\"><OnlineStatus /><ClientInfo /><FreeTextLocation /><CommCap /><UserAvailability /><StatusText /><StatusMood /><Alias /><StatusContent /><ContactInfo /></PresenceSubList><AutoSubscribe>T</AutoSubscribe></SubscribePresence-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\n# Send <UpdatePresence-Request> to give your nick\n# Only first time or when you want to change your nickname\nif nickname != '' :\n    params = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>9</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><UpdatePresence-Request><PresenceSubList xmlns=\"http://www.openmobilealliance.org/DTD/WV-PA1.2\"><Alias><Qualifier>T</Qualifier><PresenceValue>\"\"\"+nickname+\"\"\"</PresenceValue></Alias></PresenceSubList></UpdatePresence-Request></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\n    headers = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\n    conn=httplib.HTTPConnection(\"sms20.movistar.es\")\n    conn.request (\"POST\", \"/\", params, headers)\n    resp=conn.getresponse()\n\n    self.myAlias = nickname\n\nconn.close()\n\nself.nTransId = 10\n\nreturn RetList", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Called automatically when an XML element is read\n    Input: data=element data\"\"\"\n\n", "func_signal": "def char_dataContact(self,data):\n", "code": "if self.bUserID == 1 :\n    self.userId = str(data)\nif self.bTransactionID == 1 :\n    self.transactionId = str(data)", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"Disconnects from SMS2.0 service\n    Input: none\n    Returns: void\"\"\"\n\n# Send <Logout-Request>\n", "func_signal": "def Disconnect(self) :\n", "code": "params = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><WV-CSP-Message xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.openmobilealliance.org/DTD/WV-CSP1.2\"><Session><SessionDescriptor><SessionType>Inband</SessionType><SessionID>\"\"\"+self.sessionID+\"\"\"</SessionID></SessionDescriptor><Transaction><TransactionDescriptor><TransactionMode>Request</TransactionMode><TransactionID>\"\"\"+str(self.nTransId)+\"\"\"</TransactionID></TransactionDescriptor><TransactionContent xmlns=\"http://www.openmobilealliance.org/DTD/WV-TRC1.2\"><Logout-Request /></TransactionContent></Transaction></Session></WV-CSP-Message>\"\"\"\nheaders = {'Content-type':'application/vnd.wv.csp.xml', 'Expect':'100-continue'}\nconn=httplib.HTTPConnection(\"sms20.movistar.es\")\nconn.request (\"POST\", \"/\", params, headers)\nresp=conn.getresponse()\nresponse=resp.read()\n\nself.nTransId =  1\n\nconn.close()", "path": "classes\\SMS20API.py", "repo_name": "ForjaOMF/OMF-PythonSDK", "stars": 4, "license": "None", "language": "python", "size": 176}
{"docstring": "\"\"\"\nCheck if the given regular expression matches the last part of the\nshebang if one exists.\n\n    >>> from pygments.util import shebang_matches\n    >>> shebang_matches('#!/usr/bin/env python', r'python(2\\.\\d)?')\n    True\n    >>> shebang_matches('#!/usr/bin/python2.4', r'python(2\\.\\d)?')\n    True\n    >>> shebang_matches('#!/usr/bin/python-ruby', r'python(2\\.\\d)?')\n    False\n    >>> shebang_matches('#!/usr/bin/python/ruby', r'python(2\\.\\d)?')\n    False\n    >>> shebang_matches('#!/usr/bin/startsomethingwith python',\n    ...                 r'python(2\\.\\d)?')\n    True\n\nIt also checks for common windows executable file extensions::\n\n    >>> shebang_matches('#!C:\\\\Python2.4\\\\Python.exe', r'python(2\\.\\d)?')\n    True\n\nParameters (``'-f'`` or ``'--foo'`` are ignored so ``'perl'`` does\nthe same as ``'perl -e'``)\n\nNote that this method automatically searches the whole string (eg:\nthe regular expression is wrapped in ``'^$'``)\n\"\"\"\n", "func_signal": "def shebang_matches(text, regex):\n", "code": "index = text.find('\\n')\nif index >= 0:\n    first_line = text[:index].lower()\nelse:\n    first_line = text.lower()\nif first_line.startswith('#!'):\n    try:\n        found = [x for x in split_path_re.split(first_line[2:].strip())\n                 if x and not x.startswith('-')][-1]\n    except IndexError:\n        return False\n    regex = re.compile('^%s(\\.(exe|cmd|bat|bin))?$' % regex, re.IGNORECASE)\n    if regex.search(found) is not None:\n        return True\nreturn False", "path": "app\\console\\app\\pygments\\util.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Test a token against a token expression.  This can either be a\ntoken type or ``'token_type:token_value'``.  This can only test\nagainst string values and types.\n\"\"\"\n# here we do a regular string equality check as test_any is usually\n# passed an iterable of not interned strings.\n", "func_signal": "def test(self, expr):\n", "code": "if self.type == expr:\n    return True\nelif ':' in expr:\n    return expr.split(':', 1) == [self.type, self.value]\nreturn False", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"This method tokenizes the text and returns the tokens in a\ngenerator.  Use this method if you just want to tokenize a template.\n\"\"\"\n", "func_signal": "def tokeniter(self, source, name, filename=None, state=None):\n", "code": "source = '\\n'.join(unicode(source).splitlines())\npos = 0\nlineno = 1\nstack = ['root']\nif state is not None and state != 'root':\n    assert state in ('variable', 'block'), 'invalid state'\n    stack.append(state + '_begin')\nelse:\n    state = 'root'\nstatetokens = self.rules[stack[-1]]\nsource_length = len(source)\n\nbalancing_stack = []\n\nwhile 1:\n    # tokenizer loop\n    for regex, tokens, new_state in statetokens:\n        m = regex.match(source, pos)\n        # if no match we try again with the next rule\n        if m is None:\n            continue\n\n        # we only match blocks and variables if brances / parentheses\n        # are balanced. continue parsing with the lower rule which\n        # is the operator rule. do this only if the end tags look\n        # like operators\n        if balancing_stack and \\\n           tokens in ('variable_end', 'block_end',\n                      'linestatement_end'):\n            continue\n\n        # tuples support more options\n        if isinstance(tokens, tuple):\n            for idx, token in enumerate(tokens):\n                # failure group\n                if token.__class__ is Failure:\n                    raise token(lineno, filename)\n                # bygroup is a bit more complex, in that case we\n                # yield for the current token the first named\n                # group that matched\n                elif token == '#bygroup':\n                    for key, value in m.groupdict().iteritems():\n                        if value is not None:\n                            yield lineno, key, value\n                            lineno += value.count('\\n')\n                            break\n                    else:\n                        raise RuntimeError('%r wanted to resolve '\n                                           'the token dynamically'\n                                           ' but no group matched'\n                                           % regex)\n                # normal group\n                else:\n                    data = m.group(idx + 1)\n                    if data or token not in ignore_if_empty:\n                        yield lineno, token, data\n                    lineno += data.count('\\n')\n\n        # strings as token just are yielded as it.\n        else:\n            data = m.group()\n            # update brace/parentheses balance\n            if tokens == 'operator':\n                if data == '{':\n                    balancing_stack.append('}')\n                elif data == '(':\n                    balancing_stack.append(')')\n                elif data == '[':\n                    balancing_stack.append(']')\n                elif data in ('}', ')', ']'):\n                    if not balancing_stack:\n                        raise TemplateSyntaxError('unexpected \"%s\"' %\n                                                  data, lineno, name,\n                                                  filename)\n                    expected_op = balancing_stack.pop()\n                    if expected_op != data:\n                        raise TemplateSyntaxError('unexpected \"%s\", '\n                                                  'expected \"%s\"' %\n                                                  (data, expected_op),\n                                                  lineno, name,\n                                                  filename)\n            # yield items\n            if data or tokens not in ignore_if_empty:\n                yield lineno, tokens, data\n            lineno += data.count('\\n')\n\n        # fetch new position into new variable so that we can check\n        # if there is a internal parsing error which would result\n        # in an infinite loop\n        pos2 = m.end()\n\n        # handle state changes\n        if new_state is not None:\n            # remove the uppermost state\n            if new_state == '#pop':\n                stack.pop()\n            # resolve the new state by group checking\n            elif new_state == '#bygroup':\n                for key, value in m.groupdict().iteritems():\n                    if value is not None:\n                        stack.append(key)\n                        break\n                else:\n                    raise RuntimeError('%r wanted to resolve the '\n                                       'new state dynamically but'\n                                       ' no group matched' %\n                                       regex)\n            # direct state name given\n            else:\n                stack.append(new_state)\n            statetokens = self.rules[stack[-1]]\n        # we are still at the same position and no stack change.\n        # this means a loop without break condition, avoid that and\n        # raise error\n        elif pos2 == pos:\n            raise RuntimeError('%r yielded empty string without '\n                               'stack change' % regex)\n        # publish new function and start again\n        pos = pos2\n        break\n    # if loop terminated without break we havn't found a single match\n    # either we are at the end of the file or we have a problem\n    else:\n        # end of text\n        if pos >= source_length:\n            return\n        # something went wrong\n        raise TemplateSyntaxError('unexpected char %r at %d' %\n                                  (source[pos], pos), lineno,\n                                  name, filename)", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Test against multiple token expressions.\"\"\"\n", "func_signal": "def test_any(self, *iterable):\n", "code": "for expr in iterable:\n    if self.test(expr):\n        return True\nreturn False", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Constructs PickledProperty.\n\nArgs:\n  require_type: requires the property value to be of this type.\n\"\"\"\n", "func_signal": "def __init__(self, require_type=None, **kwargs):\n", "code": "self._require_parameter(kwargs, 'indexed', False)\nkwargs['indexed'] = True\nself.require_type = require_type\nsuper(PickledProperty, self).__init__(**kwargs)", "path": "app\\appengine\\gaefy\\db\\properties.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Constructs CsvProperty.\n\nArgs:\n  csv_params: CSV formatting parameters. See:\n    http://docs.python.org/library/csv.html#csv-fmt-params\n  field_count: If set, enforces all items to have this number of fields.\n\"\"\"\n", "func_signal": "def __init__(self, csv_params={}, field_count=None, default=None, **kwargs):\n", "code": "self._require_parameter(kwargs, 'indexed', False)\nkwargs['indexed'] = True\nif default is None:\n    default = []\nself.field_count = field_count\nself.csv_params = csv_params\nsuper(CsvProperty, self).__init__(default=default, **kwargs)", "path": "app\\appengine\\gaefy\\db\\properties.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"\nCheck if a doctype exists or if we have some tags.\n\"\"\"\n", "func_signal": "def looks_like_xml(text):\n", "code": "key = hash(text)\ntry:\n    return _looks_like_xml_cache[key]\nexcept KeyError:\n    m = doctype_lookup_re.match(text)\n    if m is not None:\n        return True\n    rv = tag_re.search(text[:1000]) is not None\n    _looks_like_xml_cache[key] = rv\n    return rv", "path": "app\\console\\app\\pygments\\util.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Encodes the value to JSON.\"\"\"\n", "func_signal": "def get_value_for_datastore(self, model_instance):\n", "code": "value = super(JsonProperty, self).get_value_for_datastore(\n    model_instance)\nif value is not None:\n    return db.Text(simplejson.dumps(value))", "path": "app\\appengine\\gaefy\\db\\properties.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Compiles all the rules from the environment into a list of rules.\"\"\"\n", "func_signal": "def compile_rules(environment):\n", "code": "e = re.escape\nrules = [\n    (len(environment.comment_start_string), 'comment',\n     e(environment.comment_start_string)),\n    (len(environment.block_start_string), 'block',\n     e(environment.block_start_string)),\n    (len(environment.variable_start_string), 'variable',\n     e(environment.variable_start_string))\n]\n\nif environment.line_statement_prefix is not None:\n    rules.append((len(environment.line_statement_prefix), 'linestatement',\n                  r'^\\s*' + e(environment.line_statement_prefix)))\nif environment.line_comment_prefix is not None:\n    rules.append((len(environment.line_comment_prefix), 'linecomment',\n                  r'(?:^|(?<=\\S))[^\\S\\r\\n]*' +\n                  e(environment.line_comment_prefix)))\n\nreturn [x[1:] for x in sorted(rules, reverse=True)]", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Validates the property on __set__.\"\"\"\n", "func_signal": "def validate(self, value):\n", "code": "value = super(CsvProperty, self).validate(value)\nif value is not None:\n    if not isinstance(value, (list, tuple)):\n        raise db.BadValueError('Property %s must be a list or tuple.' %\n            (self.name))\n\n    value = self.validate_list_contents(value)\n\nreturn value", "path": "app\\appengine\\gaefy\\db\\properties.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Perform the token test and return the token if it matched.\nOtherwise the return value is `None`.\n\"\"\"\n", "func_signal": "def next_if(self, expr):\n", "code": "if self.current.test(expr):\n    return next(self)", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Go one token ahead and return the old one\"\"\"\n", "func_signal": "def next(self):\n", "code": "rv = self.current\nif self._pushed:\n    self.current = self._pushed.popleft()\nelif self.current.type is not TOKEN_EOF:\n    try:\n        self.current = self._next()\n    except StopIteration:\n        self.close()\nreturn rv", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Return true if the variable is a sequence. Sequences are variables\nthat are iterable.\n\"\"\"\n", "func_signal": "def test_sequence(value):\n", "code": "try:\n    len(value)\n    value.__getitem__\nexcept:\n    return False\nreturn True", "path": "app\\jinja2\\jinja2\\tests.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "# shortcuts\n", "func_signal": "def __init__(self, environment):\n", "code": "c = lambda x: re.compile(x, re.M | re.S)\ne = re.escape\n\n# lexing rules for tags\ntag_rules = [\n    (whitespace_re, TOKEN_WHITESPACE, None),\n    (float_re, TOKEN_FLOAT, None),\n    (integer_re, TOKEN_INTEGER, None),\n    (name_re, TOKEN_NAME, None),\n    (string_re, TOKEN_STRING, None),\n    (operator_re, TOKEN_OPERATOR, None)\n]\n\n# assamble the root lexing rule. because \"|\" is ungreedy\n# we have to sort by length so that the lexer continues working\n# as expected when we have parsing rules like <% for block and\n# <%= for variables. (if someone wants asp like syntax)\n# variables are just part of the rules if variable processing\n# is required.\nroot_tag_rules = compile_rules(environment)\n\n# block suffix if trimming is enabled\nblock_suffix_re = environment.trim_blocks and '\\\\n?' or ''\n\nself.newline_sequence = environment.newline_sequence\n\n# global lexing rules\nself.rules = {\n    'root': [\n        # directives\n        (c('(.*?)(?:%s)' % '|'.join(\n            [r'(?P<raw_begin>(?:\\s*%s\\-|%s)\\s*raw\\s*%s)' % (\n                e(environment.block_start_string),\n                e(environment.block_start_string),\n                e(environment.block_end_string)\n            )] + [\n                r'(?P<%s_begin>\\s*%s\\-|%s)' % (n, r, r)\n                for n, r in root_tag_rules\n            ])), (TOKEN_DATA, '#bygroup'), '#bygroup'),\n        # data\n        (c('.+'), TOKEN_DATA, None)\n    ],\n    # comments\n    TOKEN_COMMENT_BEGIN: [\n        (c(r'(.*?)((?:\\-%s\\s*|%s)%s)' % (\n            e(environment.comment_end_string),\n            e(environment.comment_end_string),\n            block_suffix_re\n        )), (TOKEN_COMMENT, TOKEN_COMMENT_END), '#pop'),\n        (c('(.)'), (Failure('Missing end of comment tag'),), None)\n    ],\n    # blocks\n    TOKEN_BLOCK_BEGIN: [\n        (c('(?:\\-%s\\s*|%s)%s' % (\n            e(environment.block_end_string),\n            e(environment.block_end_string),\n            block_suffix_re\n        )), TOKEN_BLOCK_END, '#pop'),\n    ] + tag_rules,\n    # variables\n    TOKEN_VARIABLE_BEGIN: [\n        (c('\\-%s\\s*|%s' % (\n            e(environment.variable_end_string),\n            e(environment.variable_end_string)\n        )), TOKEN_VARIABLE_END, '#pop')\n    ] + tag_rules,\n    # raw block\n    TOKEN_RAW_BEGIN: [\n        (c('(.*?)((?:\\s*%s\\-|%s)\\s*endraw\\s*(?:\\-%s\\s*|%s%s))' % (\n            e(environment.block_start_string),\n            e(environment.block_start_string),\n            e(environment.block_end_string),\n            e(environment.block_end_string),\n            block_suffix_re\n        )), (TOKEN_DATA, TOKEN_RAW_END), '#pop'),\n        (c('(.)'), (Failure('Missing end of raw directive'),), None)\n    ],\n    # line statements\n    TOKEN_LINESTATEMENT_BEGIN: [\n        (c(r'\\s*(\\n|$)'), TOKEN_LINESTATEMENT_END, '#pop')\n    ] + tag_rules,\n    # line comments\n    TOKEN_LINECOMMENT_BEGIN: [\n        (c(r'(.*?)()(?=\\n|$)'), (TOKEN_LINECOMMENT,\n         TOKEN_LINECOMMENT_END), '#pop')\n    ]\n}", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Calls tokeniter + tokenize and wraps it in a token stream.\n\"\"\"\n", "func_signal": "def tokenize(self, source, name=None, filename=None, state=None):\n", "code": "stream = self.tokeniter(source, name, filename, state)\nreturn TokenStream(self.wrap(stream, name, filename), name, filename)", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Converts the CSV data back to a list.\"\"\"\n", "func_signal": "def make_value_from_datastore(self, value):\n", "code": "values = []\n\nif value is not None:\n    reader = csv.reader(StringIO(str(value)), **self.csv_params)\n    for item in reader:\n        values.append(item)\n\nreturn values", "path": "app\\appengine\\gaefy\\db\\properties.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"\nReturn a static text analysation function that\nreturns float values.\n\"\"\"\n", "func_signal": "def make_analysator(f):\n", "code": "def text_analyse(text):\n    rv = f(text)\n    if not rv:\n        return 0.0\n    return min(1.0, max(0.0, float(rv)))\ntext_analyse.__doc__ = f.__doc__\nreturn staticmethod(text_analyse)", "path": "app\\console\\app\\pygments\\util.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Converts the list to CSV.\"\"\"\n", "func_signal": "def get_value_for_datastore(self, model_instance):\n", "code": "value = super(CsvProperty, self).get_value_for_datastore(model_instance)\n\nif value is not None:\n    csvfile = StringIO()\n    writer = csv.writer(csvfile, **self.csv_params)\n    writer.writerows(value)\n    value = csvfile.getvalue().strip()\n    csvfile.close()\n    return db.Text(value)", "path": "app\\appengine\\gaefy\\db\\properties.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Got n tokens ahead.\"\"\"\n", "func_signal": "def skip(self, n=1):\n", "code": "for x in xrange(n):\n    next(self)", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Look at the next token.\"\"\"\n", "func_signal": "def look(self):\n", "code": "old_token = next(self)\nresult = self.current\nself.push(result)\nself.current = old_token\nreturn result", "path": "app\\jinja2\\jinja2\\lexer.py", "repo_name": "yesudeep/tisshrmlr", "stars": 4, "license": "other", "language": "python", "size": 3012}
{"docstring": "\"\"\"Parse through the contents of a given file-like object.\"\"\"\n", "func_signal": "def parse_file(self, fin):\n", "code": "self.reset()\nwhile True:\n    try:\n        data = fin.read(self.CHUNKSIZE)\n        if len(data) == 0: break\n        self.feed(data)\n    except _ScraperFinishedException:\n        break\nreturn self.feed_entries", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Scrape the page and return an RSS feed.\"\"\"\n", "func_signal": "def scrape_rss(self):\n", "code": "return self.scrape(self.RSS_ENTRY_TMPL, \n        self.RSS_FEED_TMPL, self.RSS_DATE_FMT)", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\nClose the databases used to track feeds and entries seen.\n\"\"\"\n", "func_signal": "def closeDBs(feed_db, entry_db):\n", "code": "feed_db.close()\nentry_db.close()", "path": "agglib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Use xpaths to extract feed entries and entry attributes.\"\"\"\n# Fetch the HTML source, tidy it up, parse it.\n", "func_signal": "def produce_entries(self):\n", "code": "src      = urlopen(self.SCRAPE_URL).read()\ntidy_src = tidy_string(src)\ndoc      = NonvalidatingReader.parseString(tidy_src, self.SCRAPE_URL)\n\nentries = []\n\n# Iterate through the parts identified as feed entry nodes.\nfor entry_node in doc.xpath(self.ENTRIES_XPATH, self.NSS):\n\n    # For each entry attribute path, attempt to extract the value\n    data = {}\n    for k,v in self.ENTRY_XPATHS.items():\n        nodes   = entry_node.xpath(v, self.NSS)\n        vals    = [x.nodeValue for x in nodes if x.nodeValue]\n        data[k] = \" \".join(vals)\n        \n    # Create and append the FeedEntryDict for this extraction\n    entries.append(FeedEntryDict(data, self.date_fmt))\n\nreturn entries", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Initialize the scraper, compile the regex\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.entry_re = re.compile(self.ENTRY_RE, \n    re.DOTALL | re.MULTILINE | re.IGNORECASE)", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\nOpen the databases used to track feeds and entries seen.\n\"\"\"\n", "func_signal": "def openDBs(feed_db_fn, entry_db_fn):\n", "code": "feed_db  = shelve.open(feed_db_fn)\nentry_db = shelve.open(entry_db_fn)\nreturn (feed_db, entry_db)", "path": "agglib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\nGiven a list of feeds and a URI at which to find feeds, try\nadding this feeds to the list.\n\"\"\"\n", "func_signal": "def subscribeFeed(feeds, uri):\n", "code": "feeds_found = feedfinder.getFeeds(uri)\n\nif len(feeds_found) == 0:  \n    raise SubsNoFeedsFound(uri)\nelif len(feeds_found) > 1: \n    raise SubsMultipleFeedsFound(uri, feeds_found)\nelse:\n    feed_uri = feeds_found[0]\n    if feed_uri in feeds:\n        raise SubsAlreadySubscribed(feed_uri)\n    feeds.append(feed_uri)\n    return feed_uri", "path": "agglib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\n\"\"\"\n# Handle access to feed data on keys starting with \"feed.\"\n", "func_signal": "def __getitem__(self, name):\n", "code": "if name.startswith(\"feed.\"):\n    return self.feed.get(name[5:], \"\").encode(UNICODE_ENC)\n# Handle access to entry data on keys starting with \"entry.\"\nif name.startswith(\"entry.\"):\n    return self.entry.get(name[6:], \"\").encode(UNICODE_ENC)\n# Handle a few more special-case keys.\nif name == \"date\":\n    return time.strftime(\"%Y-%m-%d\", time.localtime(self.date))\nif name == \"time\": \n    return time.strftime(\"%H:%M:%S\", time.localtime(self.date))\nif name == \"content\":\n    if self.entry.has_key(\"content\"):\n        return self.entry.content[0].value.encode(UNICODE_ENC)\n    return \"\"\n\n# If all else fails, return an empty string.\nreturn \"\"", "path": "agglib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Handle start of feed entry scraping\"\"\"\n", "func_signal": "def start_feed_entry(self):\n", "code": "self.curr_entry = FeedEntryDict({}, self.date_fmt)\nself.in_entry   = True", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Initialize the parser state.\"\"\"\n", "func_signal": "def reset(self):\n", "code": "HTMLParser.reset(self)\nself.feed_entries = []\nself.in_feed      = False\nself.in_entry     = False\nself.curr_data    = ''", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Come up with a state DB ID for this entry.\"\"\"\n# Try to use entry GUID, first.\n", "func_signal": "def id(self):\n", "code": "id = self['id']\n\n# If no GUID available, hash entry contents.\nif not len(id) > 0:\n    m = md5.md5()\n    for v in self.data.values():\n        if type(v) is unicode:\n            v = v.encode(self.UNICODE_ENC)\n        m.update('%s' % v)\n    id = m.hexdigest()\n    \nreturn id", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\nGiven an entry and feed string templates, scrape an HTML page for \ncontent and use the templates to return a feed.\n\"\"\"\n", "func_signal": "def scrape(self, entry_tmpl, feed_tmpl, date_fmt):\n", "code": "self.date_fmt = date_fmt\nself.state_db = shelve.open(self.STATE_FN)\n\n# Scrape the source data for FeedEntryDict instances\nentries = self.produce_entries()\n\n# Make a polishing-up run through the extracted entries.\nfor e in entries:\n\n    # Come up with ID for state db\n    state_id = e.id()\n    \n    # Make sure the entry link is absolute\n    if e.data.has_key('link'):\n        e['link'] = urljoin(self.BASE_HREF, e['link'])\n    \n    # Try to get state for this ID, creating a new record\n    # if needed.\n    if not self.state_db.has_key(state_id): \n        self.state_db[state_id] = {}\n    entry_state = self.state_db[state_id]\n    \n    # Manage remembered values for datestamps when entry data \n    # first found, unless dates were extracted.\n    if e.data.get('modified','') == '':\n        if not entry_state.has_key('modified'): \n            entry_state['modified'] = time.time()\n        e.data['modified'] = entry_state['modified']\n    \n    if e.data.get('issued','') == '':\n        e.data['issued'] = e.data['modified']\n        \n    for n in ('issued', 'modified'):\n        if e.data.get(n, '') != '': \n            continue\n    \n    # Construct a canonical tag URI for the entry if none set\n    if not len(e.data.get('id', '')) > 0:\n        (scheme, addr, path, params, query, frag) = \\\n            urlparse(e['link'])\n        now = e.data.has_key('modified') and time.gmtime(e.data['modified']) or time.gmtime()\n        ymd = time.strftime(\"%Y-%m-%d\", now)\n        e['id'] = \"tag:%s,%s:%s\" % (addr, ymd, quote(path,''))\n        \n    # Update the state database record\n    self.state_db[state_id] = entry_state\n\n# Close the state database\nself.state_db.close()\n            \n# Sort the entries, now that they all should have dates.\nif self.SORT_ENTRIES: entries.sort()\n\n# Build the entries from template, and populate the feed data\nentries_out = [entry_tmpl % e for e in entries[:self.MAX_ENTRIES]]\nfeed = { 'feed.entries' : \"\\n\".join(entries_out) }\n\n# Add all the feed metadata into the feed, ensuring \n# Unicode encoding happens.\nfor k, v in self.FEED_META.items():\n    if type(v) is unicode:\n        v = v.encode(self.UNICODE_ENC)\n    feed[k] = v\n\n# Return the built feed\nreturn feed_tmpl % feed", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\nAttempt to remove a URI from the give list of subscriptions.\nThrows a SubsNotSubscribed exception if the URI wasn't found in the\nsubscriptions.\n\"\"\"\n", "func_signal": "def unsubscribeFeed(feeds, uri):\n", "code": "if uri not in feeds: raise SubsNotSubscribed(uri)\nfeeds.remove(uri)", "path": "agglib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\nGiven a query string, perform an Amazon search.\n\"\"\"\n# Construct an Amazon search URL and fetch it.\n", "func_signal": "def amazon_search(self, query):\n", "code": "args = {\n    'SubscriptionId' : self.AMAZON_KEY,\n    'AssociateTag'   : self.ASSOCIATE_TAG,\n    'Service'        : 'AWSECommerceService',\n    'Operation'      : 'ItemSearch',\n    'ResponseGroup'  : 'Medium,ItemAttributes',\n    'SearchIndex'    : 'Books',\n    'TextStream'     : query\n}\nurl  = \"http://webservices.amazon.com/onca/xml?%s\" % \\\n    urllib.urlencode(args)\n\n# Parse and return the results of the search\ndata = HTTPCache(url).content()\ndoc  = xmltramp.parse(data)\nreturn doc", "path": "ch16_feed_amazon_ads.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Use regex to extract entries from source\"\"\"\n# Fetch the source for scraping.\n", "func_signal": "def produce_entries(self):\n", "code": "src = urlopen(self.SCRAPE_URL).read()\n\n# Iterate through all the matches of the regex found.\nentries, pos = [], 0\nwhile True:\n    \n    # Find the latest match, stop if none found.\n    m = self.entry_re.search(src, pos)\n    if not m: break\n    \n    # Advance the search position to end of previous search.\n    pos = m.end()\n    \n    # Create and append the FeedEntryDict for this extraction.\n    entries.append(FeedEntryDict(m.groupdict(), self.date_fmt))\n\nreturn entries", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Handle the detected end of a feed entry scraped\"\"\"\n", "func_signal": "def end_feed_entry(self):\n", "code": "self.feed_entries.append(self.curr_entry)\nself.in_entry = False", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Scrape the page and return an Atom feed.\"\"\"\n", "func_signal": "def scrape_atom(self):\n", "code": "self.FEED_META['feed.modified'] = \\\n    time.strftime(self.ATOM_DATE_FMT, time.gmtime(time.time()))\nreturn self.scrape(self.ATOM_ENTRY_TMPL, \n        self.ATOM_FEED_TMPL, self.ATOM_DATE_FMT)", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\nUse FeedNormalizer to get feed entries, then merge\nthe lists together.\n\"\"\"\n# Grab and parse the feed\n", "func_signal": "def produce_entries(self):\n", "code": "feed = feedparser.parse(HTTPCache(self.main_feed).content())\n\n# Normalize feed meta data\nself.FEED_META = normalize_feed_meta(feed, self.date_fmt)\nself.FEED_META['feed.title'] += ' (with Amazon items)'\n\n# Normalize entries from the feed\nentries = normalize_entries(feed.entries)\n\n# Run through all the normalized entries...\nfor e in entries:\n    \n    # Perform a search on the entry title, extract the items\n    result = self.amazon_search(e['summary'])\n    items  = [ x for x in result.Items if 'Item' in x._name ]\n    \n    # Use each search result item to populate the templates.\n    insert_items = [ self.INSERT_ITEM_TMPL % {\n        'title' : i.ItemAttributes.Title,\n        'url'   : i.DetailPageURL,\n        'img'   : i.SmallImage.URL\n    } for i in items[:self.MAX_ITEMS] ]\n    insert_out = self.INSERT_TMPL % '\\n'.join(insert_items)\n\n    # Append the rendered search results onto the entry summary.\n    e.data['summary'] += insert_out.decode('utf-8', 'ignore')\n    \nreturn entries", "path": "ch16_feed_amazon_ads.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"Return a dict item, escaped and encoded for XML inclusion\"\"\"\n# Chop off the entry. prefix, if found.\n", "func_signal": "def __getitem__(self, name):\n", "code": "if name.startswith('entry.'): \n    name = name[6:]\n \n# If this key is a date, format accordingly.\nif name in self.DATE_KEYS:\n    date = self.data.get(name, time.time())\n    val  = time.strftime(self.date_fmt, time.gmtime(date))\n\n# Otherwise, try returning what was asked for.\nelse: \n    val = self.data.get(name, '')\n\n# Make sure the value is finally safe for inclusion in XML\nif type(val) is unicode:\n    val = val.encode(self.UNICODE_ENC)\nreturn escape(val.strip())", "path": "scraperlib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\nCome up with a unique identifier for this entry.\n\"\"\"\n", "func_signal": "def hash(self):\n", "code": "if self.entry.has_key('id'):\n    return self.entry['id'].encode(UNICODE_ENC)\nelse:\n    m = md5.md5()\n    for k in ('title', 'link', 'issued', 'modified', 'description'):\n        m.update(self.entry.get(k,'').encode(UNICODE_ENC))\n    return m.hexdigest()", "path": "agglib.py", "repo_name": "lmorchard/hacking_rss_and_atom", "stars": 4, "license": "None", "language": "python", "size": 165}
{"docstring": "\"\"\"\nRun the cleanups added with L{addCleanup} in order.\n\n@return: A C{Deferred} that fires when all cleanups are run.\n\"\"\"\n", "func_signal": "def _runCleanups(self):\n", "code": "def _makeFunction(f, args, kwargs):\n    return lambda: f(*args, **kwargs)\ncallables = []\nwhile len(self._cleanups) > 0:\n    f, args, kwargs = self._cleanups.pop()\n    callables.append(_makeFunction(f, args, kwargs))\nreturn util._runSequentially(callables)", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\n@param reason: A string explaining why the test is marked 'todo'\n\n@param errors: An iterable of exception types that the test is\nexpected to raise. If one of these errors is raised by the test, it\nwill be trapped. Raising any other kind of error will fail the test.\nIf C{None} is passed, then all errors will be trapped.\n\"\"\"\n", "func_signal": "def __init__(self, reason, errors=None):\n", "code": "self.reason = reason\nself.errors = errors", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nFail the test if C{containee} is found in C{container}.\n\n@param containee: the value that should not be in C{container}\n@param container: a sequence type, or in the case of a mapping type,\n                  will follow semantics of 'if key in dict.keys()'\n@param msg: if msg is None, then the failure message will be\n            '%r in %r' % (first, second)\n\"\"\"\n", "func_signal": "def failIfIn(self, containee, container, msg=None):\n", "code": "if containee in container:\n    raise self.failureException(msg or \"%r in %r\"\n                                % (containee, container))\nreturn containee", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nRun the unit test.\n\n@param result: A TestResult object.\n\"\"\"\n", "func_signal": "def run(self, result):\n", "code": "return self._originalTest.run(\n    reporter._AdaptedReporter(result, self.__class__))", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nFail the test if C{first} is not C{second}.  This is an\nobect-identity-equality test, not an object equality\n(i.e. C{__eq__}) test.\n\n@param msg: if msg is None, then the failure message will be\n'%r is not %r' % (first, second)\n\"\"\"\n", "func_signal": "def failUnlessIdentical(self, first, second, msg=None):\n", "code": "if first is not second:\n    raise self.failureException(msg or '%r is not %r' % (first, second))\nreturn first", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nTest that a standard log message doesn't go anywhere near the result.\n\"\"\"\n", "func_signal": "def test_msg(self):\n", "code": "self.observer.gotEvent({'message': ('some message',),\n                        'time': time.time(), 'isError': 0,\n                        'system': '-'})\nself.assertEqual(self.observer.getErrors(), [])", "path": "twisted\\trial\\test\\test_log.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nCall C{f} with C{args} positional arguments and C{kwargs} keyword arguments\nand collect all warnings which are emitted as a result in a list.\n\n@param observeWarning: A callable which will be invoked with a L{_Warning}\n    instance each time a warning is emitted.\n\n@return: The return value of C{f(*args, **kwargs)}.\n\"\"\"\n", "func_signal": "def _collectWarnings(observeWarning, f, *args, **kwargs):\n", "code": "def showWarning(message, category, filename, lineno, file=None, line=None):\n    assert isinstance(message, Warning)\n    observeWarning(_Warning(\n            message.args[0], category, filename, lineno))\n\n# Disable the per-module cache for every module otherwise if the warning\n# which the caller is expecting us to collect was already emitted it won't\n# be re-emitted by the call to f which happens below.\nfor v in sys.modules.itervalues():\n    if v is not None:\n        try:\n            v.__warningregistry__ = None\n        except:\n            # Don't specify a particular exception type to handle in case\n            # some wacky object raises some wacky exception in response to\n            # the setattr attempt.\n            pass\n\norigFilters = warnings.filters[:]\norigShow = warnings.showwarning\nwarnings.simplefilter('always')\ntry:\n    warnings.showwarning = showWarning\n    result = f(*args, **kwargs)\nfinally:\n    warnings.filters[:] = origFilters\n    warnings.showwarning = origShow\nreturn result", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nTest that when two errors get logged, they both get reported as test\nerrors.\n\"\"\"\n", "func_signal": "def test_twoErrors(self):\n", "code": "test = Mask.MockTest('test_double')\ntest(self.result)\nself.assertEqual(len(self.result.errors), 2)", "path": "twisted\\trial\\test\\test_log.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nCheck that C{_ignoreErrors} actually causes errors to be ignored.\n\"\"\"\n", "func_signal": "def test_ignoreErrors(self):\n", "code": "self.observer._ignoreErrors(ZeroDivisionError)\nf = makeFailure()\nself.observer.gotEvent({'message': (),\n                        'time': time.time(), 'isError': 1,\n                        'system': '-', 'failure': f,\n                        'why': None})\nself.assertEqual(self.observer.getErrors(), [])", "path": "twisted\\trial\\test\\test_log.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nCall a function that was deprecated at a specific version.\n\n@param version: The version that the function was deprecated in.\n@param f: The deprecated function to call.\n@return: Whatever the function returns.\n\"\"\"\n", "func_signal": "def callDeprecated(self, version, f, *args, **kwargs):\n", "code": "result = f(*args, **kwargs)\nwarningsShown = self.flushWarnings([self.callDeprecated])\n\nif len(warningsShown) == 0:\n    self.fail('%r is not deprecated.' % (f,))\n\nobservedWarning = warningsShown[0]['message']\nexpectedWarning = getDeprecationWarningString(f, version)\nself.assertEqual(expectedWarning, observedWarning)\n\nreturn result", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nIterate through all of the test cases in C{testSuiteOrCase}.\n\"\"\"\n", "func_signal": "def _iterateTests(testSuiteOrCase):\n", "code": "try:\n    suite = iter(testSuiteOrCase)\nexcept TypeError:\n    yield testSuiteOrCase\nelse:\n    for test in suite:\n        for subtest in _iterateTests(test):\n            yield subtest", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\n@param failure: A L{twisted.python.failure.Failure}.\n\n@return: C{True} if C{failure} is expected, C{False} otherwise.\n\"\"\"\n", "func_signal": "def expected(self, failure):\n", "code": "if self.errors is None:\n    return True\nfor error in self.errors:\n    if failure.check(error):\n        return True\nreturn False", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nReturn a new, realistic failure.\n\"\"\"\n", "func_signal": "def makeFailure():\n", "code": "try:\n    1/0\nexcept ZeroDivisionError:\n    f = failure.Failure()\nreturn f", "path": "twisted\\trial\\test\\test_log.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nVisit this test case. Call C{visitor} with C{self} as a parameter.\n\nDeprecated in Twisted 8.0.\n\n@param visitor: A callable which expects a single parameter: a test\ncase.\n\n@return: None\n\"\"\"\n", "func_signal": "def visit(self, visitor):\n", "code": "warnings.warn(\"Test visitors deprecated in Twisted 8.0\",\n              category=DeprecationWarning)\nvisitor(self)", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nDeprecate C{iterate}, C{crash} and C{stop} on C{reactor}. That is,\neach method is wrapped in a function that issues a deprecation\nwarning, then calls the original.\n\n@param reactor: The Twisted reactor.\n\"\"\"\n", "func_signal": "def _deprecateReactor(self, reactor):\n", "code": "self._reactorMethods = {}\nfor name in ['crash', 'iterate', 'stop']:\n    self._reactorMethods[name] = getattr(reactor, name)\n    setattr(reactor, name, self._makeReactorMethod(name))", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nReturns the timeout value set on this test. Checks on the instance\nfirst, then the class, then the module, then packages. As soon as it\nfinds something with a C{timeout} attribute, returns that. Returns\nL{util.DEFAULT_TIMEOUT_DURATION} if it cannot find anything. See\nL{TestCase} docstring for more details.\n\"\"\"\n", "func_signal": "def getTimeout(self):\n", "code": "timeout =  util.acquireAttribute(self._parents, 'timeout',\n                                 util.DEFAULT_TIMEOUT_DURATION)\ntry:\n    return float(timeout)\nexcept (ValueError, TypeError):\n    # XXX -- this is here because sometimes people will have methods\n    # called 'timeout', or set timeout to 'orange', or something\n    # Particularly, test_news.NewsTestCase and ReactorCoreTestCase\n    # both do this.\n    warnings.warn(\"'timeout' attribute needs to be a number.\",\n                  category=DeprecationWarning)\n    return util.DEFAULT_TIMEOUT_DURATION", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nRestore the deprecated reactor methods. Undoes what\nL{_deprecateReactor} did.\n\n@param reactor: The Twisted reactor.\n\"\"\"\n", "func_signal": "def _undeprecateReactor(self, reactor):\n", "code": "for name, method in self._reactorMethods.iteritems():\n    setattr(reactor, name, method)\nself._reactorMethods = {}", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nCall C{run} on every member of the suite.\n\"\"\"\n# we implement this because Python 2.3 unittest defines this code\n# in __call__, whereas 2.4 defines the code in run.\n", "func_signal": "def run(self, result):\n", "code": "for test in self._tests:\n    if result.shouldStop:\n        break\n    test(result)\nreturn result", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nFail if C{instance} is not an instance of the given class or of\none of the given classes.\n\n@param instance: the object to test the type (first argument of the\n    C{isinstance} call).\n@type instance: any.\n@param classOrTuple: the class or classes to test against (second\n    argument of the C{isinstance} call).\n@type classOrTuple: class, type, or tuple.\n\"\"\"\n", "func_signal": "def failUnlessIsInstance(self, instance, classOrTuple):\n", "code": "if not isinstance(instance, classOrTuple):\n    self.fail(\"%r is not an instance of %s\" % (instance, classOrTuple))", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nFail the test if C{condition} evaluates to True.\n\n@param condition: any object that defines __nonzero__\n\"\"\"\n", "func_signal": "def failIf(self, condition, msg=None):\n", "code": "if condition:\n    raise self.failureException(msg)\nreturn condition", "path": "twisted\\trial\\unittest.py", "repo_name": "tzuryby/freespeech", "stars": 4, "license": "other", "language": "python", "size": 2324}
{"docstring": "\"\"\"\nMake a string safe for Javascript\n\"\"\"\n", "func_signal": "def _js_dq_safe(string, depth=1):\n", "code": "repl_str = \"\\\\\" * depth + '\"'\nstring = string.replace('\"',repl_str)\nreturn string", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nprepares the values to insert.\nBy overriding prepare_sql_values, a child class can format a few things.\n\"\"\"\n", "func_signal": "def _prepare_sql_values(self):\n", "code": "values = self.__dict__.copy()\n\nself.prepare_sql_values(values)\nreturn values", "path": "python\\base\\DBObject.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nParses a URL and truncates it after the domain part\n\"\"\"\n\n", "func_signal": "def url_truncate(url):\n", "code": "url_tuple = urlparse.urlparse(url)\nreturn url_tuple[0] + '://' + url_tuple[1]", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"Send an email.\n\nAll arguments should be Unicode strings (plain ASCII works as well).\n\nOnly the real name part of sender and recipient addresses may contain\nnon-ASCII characters.\n\nThe email will be properly MIME encoded and delivered though SMTP to\nlocalhost port 25.  This is easy to change if you want something different.\n\nThe charset of the email will be the first one out of US-ASCII, ISO-8859-1\nand UTF-8 that can represent all the characters occurring in the email.\n\"\"\"\n\n", "func_signal": "def simple_send(recipient, sender, subject, body, reply_to=None):\n", "code": "msg = createMessage(recipient, sender, subject, body, reply_to)\n\n# Send the message via SMTP to localhost:25\nsmtp = SMTP(SMTP_SERVER)\nif config.SMTP_USER:\n    smtp.login(config.SMTP_USER, config.SMTP_PASSWORD)\n    \nsmtp.sendmail(sender, recipient, msg.as_string())\nsmtp.quit()", "path": "python\\base\\mail.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nDelete an object\n\"\"\"\n", "func_signal": "def delete(self):\n", "code": "sql = \"delete from \" + self.TABLE_NAME\nsql += \" where \" + self.PRIMARY_KEY + \" = %(pkval)s\"\n\npkval = self.__dict__[self.PRIMARY_KEY]\nDB.perform(sql)", "path": "python\\base\\DBObject.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nURL encode everything even unresreved chars\n\"\"\"\n", "func_signal": "def urlencodeall(str):\n", "code": "if not str:\n    return \"\"\n\nreturn string.join(['%' + s.encode('hex') for s in str], '')", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nGoes up the indicated number of levels and returns the equivalent of calling locals()\nin that scope\n\"\"\"\n", "func_signal": "def parent_vars(level, extra_vars = None):\n", "code": "try: 1/0\nexcept: frame = sys.exc_traceback.tb_frame\n\n# Go up in the frame stack\nfor i in range(level+1): frame = frame.f_back\n\nloc, glob = frame.f_locals, frame.f_globals\n\n\nif extra_vars != None:\n    loc = loc.copy()\n    for key in extra_vars.keys():\n        loc[key] = extra_vars[key]\n        \nreturn loc", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nMake a string safe for a CSV field\n\"\"\"\n# let's backslash all the quotation marks anyways\n", "func_signal": "def csv_safe(string):\n", "code": "string = str(string)\nstring = string.replace('\"','\\\\\"')\n\nif \",\" not in string and \"\\n\" not in string:\n    return string\n\nreturn '\"' + string + '\"'", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nMake a string short enough for display\n\"\"\"\n", "func_signal": "def trunc_string(string, length=50):\n", "code": "if len(string)>length:\n    return \"%s...\" % string[:length-3]\nelse:\n    return string", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nMake a string or a list safe for Javascript\n\"\"\"\n", "func_signal": "def js_sq_safe(sol, depth=1, escape_newlines = True):\n", "code": "if not sol:\n    return ''\n       \nif isinstance(sol, list):\n    l = []\n    for el in sol:\n        l.append(_js_sq_safe(el, depth, escape_newlines))\n    return l\nelse:\n    return _js_sq_safe(sol, depth, escape_newlines)", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nLoads and returns the content of a URL.\nNo streaming for now.\n\"\"\"\n\n", "func_signal": "def load_url(url):\n", "code": "req = urllib2.Request(url = url)\nf = urllib2.urlopen(req)\nreturn f.read()", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "# called for each end tag, e.g. for </pre>, tag will be 'pre'\n# Reconstruct the original end tag.\n", "func_signal": "def unknown_endtag(self, tag):\n", "code": "if tag not in self.elements_no_end_tag:\n    self.pieces.append(\"</%(tag)s>\" % locals())", "path": "python\\base\\htmlsanitizer.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n# Reconstruct the original character reference.\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if ref.startswith('x'):\n    value = unichr(int(ref[1:],16))\nelse:\n    value = unichr(int(ref))\n\nif value in _cp1252.keys():\n    self.pieces.append('&#%s;' % hex(ord(_cp1252[value]))[1:])\nelse:\n    self.pieces.append('&#%(ref)s;' % locals())", "path": "python\\base\\htmlsanitizer.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nprepares the values after they come back from SQL\nto be set up correctly as object values.\n\"\"\"\n", "func_signal": "def _prepare_object_values(self, row):\n", "code": "prepared_row = dict()\nprepared_row.update(row)\nself.prepare_object_values(prepared_row)\nreturn prepared_row", "path": "python\\base\\DBObject.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"Encode Unicode strings, by default in UTF-8\"\"\"\n", "func_signal": "def filter(self, val, **kw):\n", "code": "if kw.has_key('encoding'):\n    encoding = kw['encoding']\nelse:\n    encoding='utf8'\n                    \nif type(val) == type(u''):\n    filtered = val.encode(encoding)\nelse:\n    filtered = str(val)\nreturn filtered", "path": "python\\base\\template.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nMake a string or a list safe for Javascript\n\"\"\"\n", "func_signal": "def js_dq_safe(sol, depth=1):\n", "code": "if not sol:\n    return ''\n       \nif isinstance(sol, list):\n    l = []\n    for el in sol:\n        l.append(_js_dq_safe(el))\n    return l\nelse:\n    return _js_dq_safe(sol, depth)", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nParses a URL and errors out if its not scheme http or https or has no net location\n\"\"\"\n\n", "func_signal": "def url_check(url):\n", "code": "url_tuple = urlparse.urlparse(url)\nif url_tuple[0] == 'http' or url_tuple[0] == 'https' and url_tuple[1] != \"\":\n  return url\nelse:\n  raise Exception('bad url')", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nURL encode\n\"\"\"\n", "func_signal": "def urlencode(str):\n", "code": "if not str:\n    return \"\"\n\nreturn urllib.quote(str)", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nInsert a new object, but only if it hasn't been inserted yet\n\"\"\"\n", "func_signal": "def insert(self, generate_new_pkey= True):\n", "code": "if generate_new_pkey:\n    if self.PRIMARY_KEY in self.__dict__:\n        if self.__dict__[self.PRIMARY_KEY] != None:\n            raise Exception('primary key already set')\n\nsql = \"insert into \" + self.TABLE_NAME\nsql += \"(\" + self.all_fields_sql(with_table_prefix=False) + \") \"\n\nif generate_new_pkey:\n    self.__dict__[self.PRIMARY_KEY] = int(DB.oneval(\"select nextval('\" + self.SEQ_NAME + \"')\"))\n\nsql += \"values (\" + \",\".join([self._sql_insert_value(f) for f in self.FIELDS]) + \")\"\n\nDB.perform(sql, extra_vars = self._prepare_sql_values())", "path": "python\\base\\DBObject.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nTakes a chunk of HTML, looks for a select, and outputs a list of options of that select.\n\nLooks only at value='', not at the pretty display.\n\"\"\"\n\n# a regular expression to match the select block\n", "func_signal": "def get_select_options(html, select_name):\n", "code": "pattern = re.compile('<select *name=\"%s\"[^>]*>(.*?)</select>' % select_name, re.MULTILINE | re.IGNORECASE | re.DOTALL)\nm = pattern.search(html)\nif (m == None):\n    # we have no match, try another pattern\n    pattern = re.compile('<select *name=%s[^>]*>(.*?)</select>' % select_name, re.MULTILINE | re.IGNORECASE | re.DOTALL)\n    m = pattern.search(html)\n\nselect_block = m.group()\n\n# extract the options from the select block\npattern = re.compile('<option[^>]*value=\"(.*?)\">(.*?)</option>', re.IGNORECASE)\noptions = pattern.findall(select_block)\n\nreturn options", "path": "python\\base\\utils.py", "repo_name": "benadida/sessionlock", "stars": 5, "license": "None", "language": "python", "size": 112}
{"docstring": "\"\"\"\nIterate the provided DOM Node, parsing the facet name and any child\nvalue counts.  Facet values are additionally merged into a tree\nstructure based on common name prefixes, and then flattened out again.\nThis allows for parent-child relationships and nested value counts.\nSee merge_values.\n\nParses the facet counts into this Result's facets list.\n\nTakes a parsed xml document.\n\"\"\"\n", "func_signal": "def __init__(self, node):\n", "code": "(self.name, self.values) = (xmlutils.get_attribute(node, \"name\"), [])\n\nfor c in xmlutils.get_child_nodes(node, \"int\"):\n    \n    value = xmlutils.get_attribute(c, \"name\")\n    count = xmlutils.get_int(c)\n    \n    self.values.append(FacetValue(value, count))\n\nself.merge_values()", "path": "solango\\solr\\facet.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "#if initial data exists then we parse it out and put it in the right fields\n# expects a list of tuples. Facets shouldn't be built directly, only of queries\n", "func_signal": "def __init__(self, *args, **kwargs):\n", "code": "self.simple = CleverDict(instance='simple')\nself.regex = CleverDict(instance='regex')\nself.fl = []\nCleverDict.__init__(self, *args, **kwargs)", "path": "solango\\solr\\query.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nExpects a list of tuples like:\n[('q', 'model'), ('sort', 'date desc'), ('facet.field', 'model')]\n\"\"\"\n", "func_signal": "def clean(self, *args, **kwargs):\n", "code": "params = []\nif args:\n    params = list(args[0].items())\nparams.extend(kwargs.items()) \n\nif not params:\n    return None\n\nfacet_params = settings.SEARCH_FACET_PARAMS\nhl_params = settings.SEARCH_HL_PARAMS\nfor key, value in params:\n    if key.startswith('facet'):\n        facet_params.append((key, value),)\n    elif key.startswith('hl'):\n        hl_params.append((key, value),)\n    else:\n        try:\n            v = self[key]\n            if isinstance(v, list):\n                self[key].append(value)\n            else:\n                self[key] = value\n        except KeyError:\n            self.q.append('%s:%s' % (key, value))\n\nself.facet = Facet(facet_params, instance='facet')\nself.hl = Highlight(hl_params, instance='hl')", "path": "solango\\solr\\query.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nTakes a model or a dict.\n\nfor a model it assumes that you are trying to create a document from the values\n\nfor a dict it assumes that you recieved results from solr and you want to make a \npython object representation of the model    \n    \n\"\"\"\n", "func_signal": "def __init__(self, model_or_dict):\n", "code": "self.fields = deepcopy(self.base_fields)\nself.pk_field = None\nself._model = None\nself.data_dict = {}\nself.highlight = \"\"\n\n# If it's a model, set the _model and create a dictionary from the fields\nif isinstance(model_or_dict, Model):\n    #make it into a dict.\n    self._model = model_or_dict\n    self.data_dict = model_to_dict(model_or_dict)\nelif isinstance(model_or_dict, dict):\n    self.data_dict = model_or_dict\nelse:\n    raise ValueError('Argument must be a Model or a dictionary')\n\n# Iterate through fields and get value\nfor field in self.fields.values():\n    #Save value\n    if isinstance(field, search_fields.PrimaryKeyField):\n        self.pk_field = field\n        break\n\nif not self.pk_field:\n    raise NoPrimaryKeyFieldException('Search Document needs a Primary Key Field')\n\nif self._model:\n    self.transform()\nelse:\n    self.clean()", "path": "solango\\solr\\documents.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nReturns True if the search system appears to be available and in good\nhealth, False otherwise.  A ping is periodically sent to the search\nserver to query its availability.\n\"\"\"\n", "func_signal": "def is_available(self):\n", "code": "(now, delta) = (datetime.now(), timedelta(0, 300))\n\nif now - self.heartbeat > delta:\n    try:\n        for url in self.ping_urls:\n            res = urllib2.urlopen(url).read()\n    except StandardError:\n        self.available = False\n    else:\n        self.available = True\n    \nreturn self.available", "path": "solango\\solr\\connection.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nIssues a select request to the search server and renders any results.\nThe query term is derived from the incoming URL, while additional\nparameters for pagination, faceting, filtering, sorting, etc come\nfrom the query string.\n\"\"\"\n", "func_signal": "def select(request, q=''):\n", "code": "if not connection.is_available():\n    return HttpResponseRedirect(reverse('solango_search_error'))\n\nparams = {}\nfacets = []\npaginator = None\nsort_links = []\n\nif q:\n    params['q'] = q\n\nif request.GET:\n    params.update(dict(request.GET.items()))\n\nif params:\n    paginator = SearchPaginator(params, request)\n    facets = utils.get_facets_links( request, paginator.results)\n    sort_links = utils.get_sort_links(request)\n    \nreturn render_to_response(\"solango/search.html\", {'paginator': paginator ,\n                                                  'facets' : facets,\n                                                  'q' : q,\n                                                  'sort_links' : sort_links } , RequestContext(request))", "path": "solango\\views.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nTakes the data dictionary and creates python values from it.\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "if not self.data_dict:\n    raise ValueError('No data to clean into a Search Document')\n\nfor name, field in self.fields.items():\n    value = None\n    field.value = self.data_dict[field.get_name()]\n    try:\n        value = getattr(self, 'clean_%s' % name, None)()\n        field.value = value\n    except:\n        #no transform rely on the field\n        field.clean()", "path": "solango\\solr\\documents.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nTakes an model instance and transforms it into a Search Document\n\"\"\"\n", "func_signal": "def transform(self):\n", "code": "if not self._model:\n    raise ValueError('No model to transform into a Search Document')\n\nfor name, field in self.fields.items():\n    value = None\n    try:\n        value = getattr(self, 'transform_%s' % name)(self._model)\n        field.value = value\n    except AttributeError:\n        #no transform rely on the field\n        field.transform(self._model)", "path": "solango\\solr\\documents.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nParses the provided XML body, including documents, facets, and\nhighlighting information.  See Results.__init__(self, xml).\n\"\"\"\n", "func_signal": "def __init__(self, xml):\n", "code": "Results.__init__(self, xml)\n\n(self.documents, self.facets, self.highlighting) = ([], [], {})\n\nself._parse_results()\n\nself._parse_facets()\n\nself._parse_highlighting()\n\nself._doc.unlink()", "path": "solango\\solr\\results.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nInstantiate the default logger.\n\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self.logger = logging.getLogger(globals()['__name__'])\nself.logger.setLevel(getattr(logging, settings.LOG_LEVEL))\n\nformatter = logging.Formatter(settings.LOG_FORMAT)\n\nfile_handler = logging.FileHandler(settings.LOG_FILENAME)\nfile_handler.setFormatter(formatter)\n\nself.logger.addHandler(file_handler)", "path": "solango\\log.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "#Delete looks like <id>1</id>\n", "func_signal": "def to_xml(self, delete=False):\n", "code": "if delete:\n    return \"<%s>%s</%s>\" % (self.pk_field.name, self.pk_field.value, self.pk_field.name)\n\ndoc = unicode(\"\", \"utf-8\")\n\nfor field in self.fields.values():\n    doc += unicode(field)\n\nreturn \"<doc>\\n\" + doc + \"</doc>\\n\"", "path": "solango\\solr\\documents.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nAppends value and all of its child values to this facet's values list\nusing depth-first recursion.  Value levels are also calculated for\ndisplay purposes.\n\"\"\"\n\n", "func_signal": "def recurse_children(self, value):\n", "code": "self.values.append(value)\n\nif value.parent:\n    value.level = value.parent.level + 1\n\nfor c in value.children:\n    self.recurse_children(c)", "path": "solango\\solr\\facet.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nMerges facet values which appear to be related to each other by\nparent/child relationships, based on the sharing of name prefixes.\nAfter merging the facets, depth-first recursion of the resulting\ntree is used to produce a linear, sorted list of values.\n\"\"\"\n", "func_signal": "def merge_values(self):\n", "code": "values = []\n\nfor v in self.values:\n    parent = self.get_parent(v)\n            \n    if not parent:\n        values.append(v)\n        continue\n    \n    self.add_to_parent(parent, v)\n\nself.values = []\n\nfor v in values:\n    self.recurse_children(v)", "path": "solango\\solr\\facet.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nOptimizes the search index.  Returns an UpdateResults instance.\n\"\"\"\n", "func_signal": "def optimize(self):\n", "code": "res = self.update(unicode(\"\\n<optimize/>\\n\", \"utf-8\"))\nreturn results.UpdateResults(res)", "path": "solango\\solr\\connection.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nAdds the specified list of objects to the search index.  Returns a\ntwo-element List of UpdateResults; the first element corresponds to\nthe add operation, the second to the subsequent commit operation.\n\"\"\"\n", "func_signal": "def add(self, documents):\n", "code": "if not documents:\n    raise ValueError        \n\nxml = self.get_document_xml(documents, ADD)\n\nif not len(xml):\n    return\n\nif not self.is_available():\n    logger.info(\"add: Search is unavailable.\")\n    return\n\nres = self.update(\"\\n<add>\\n\" + xml + \"</add>\\n\")\nreturn [results.UpdateResults(res), self.commit()]", "path": "solango\\solr\\connection.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nCommits any pending changes to the search index.  Returns an\nUpdateResults instance.\n\"\"\"\n", "func_signal": "def commit(self):\n", "code": "res = self.update(unicode(\"\\n<commit/>\\n\", \"utf-8\"))\nreturn results.UpdateResults(res)", "path": "solango\\solr\\connection.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nReturns Solr Document XML representation of the specified objects, \ntransformed according to mode, as a Unicode (utf-8) string\".\n\"\"\"\n", "func_signal": "def get_document_xml(self, documents, mode):\n", "code": "if not documents:\n    raise ValueError\n\nif not isinstance(documents, (list, tuple)):\n    documents = [documents]\n\nxml = unicode(\"\", \"utf-8\")\nfor d in documents:\n    if mode:\n        xml += d.add()\n    else:\n        xml += d.delete()\nreturn xml", "path": "solango\\solr\\connection.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nInitialize value and count, resolving name from the value.\n\"\"\"\n", "func_signal": "def __init__(self, value, count=0):\n", "code": "(self.value, self.count) = (value, count)\n(self.parent, self.children, self.level) = (None, [], 0)\n\nn = self.value.rfind(settings.SEARCH_SEPARATOR)\n\nif n == -1:\n    n = 0\nelse:\n    n += len(settings.SEARCH_SEPARATOR)\n\nself.name = self.value[n:].title()", "path": "solango\\solr\\facet.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nParses the highlighting list into this Result's highlighting dictionary.\nAlso iterate over this Result's documents, inserting highlighting\nelements to their owning documents.\n\"\"\"\n", "func_signal": "def _parse_highlighting(self):\n", "code": "highlighting = xmlutils.get_child_node(self._doc.firstChild, \"lst\", \"highlighting\")\n\nif not highlighting:\n    return\n\nself.highlighting = xmlutils.get_dictionary(highlighting)\nfor d in self.documents:\n    #TODO: Ugly\n    model_key = settings.SEARCH_SEPARATOR.join([d.fields['model'].value, d.pk_field.value])\n    for key, value in self.highlighting[model_key].items():\n        d.highlight += ' ' + ' '.join(value)\n        d.fields[key].highlight = ' '.join(value)", "path": "solango\\solr\\results.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nReturns a dictionary associating names to logging levels.\n\"\"\"\n", "func_signal": "def get_levels(self):\n", "code": "return {\n    'DEBUG': logging.DEBUG, 'INFO': logging.INFO, 'WARNING': logging.WARNING,\n    'ERROR': logging.ERROR, 'CRITICAL': logging.CRITICAL\n}", "path": "solango\\log.py", "repo_name": "brosner/solango", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "# Convert the document to Unicode.\n", "func_signal": "def _feed(self, inDocumentEncoding=None):\n", "code": "markup = self.markup\nif isinstance(markup, unicode):\n    if not hasattr(self, 'originalEncoding'):\n        self.originalEncoding = None\nelse:\n    dammit = UnicodeDammit\\\n             (markup, [self.fromEncoding, inDocumentEncoding],\n              smartQuotesTo=self.smartQuotesTo)\n    markup = dammit.unicode\n    self.originalEncoding = dammit.originalEncoding\nif markup:\n    if self.markupMassage:\n        if not isList(self.markupMassage):\n            self.markupMassage = self.MARKUP_MASSAGE            \n        for fix, m in self.markupMassage:\n            markup = fix.sub(m, markup)\nself.reset()\n\nSGMLParser.feed(self, markup)\n# Close out any unfinished strings and close all the open tags.\nself.endData()\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears before this Tag in the document.\"\"\"\n", "func_signal": "def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findPreviousSiblings, name, attrs, text,\n                     **kwargs)", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Handle entity references as data, possibly converting known\nHTML entity references to the corresponding Unicode\ncharacters.\"\"\"\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "data = None\nif self.convertEntities == self.HTML_ENTITIES or \\\n       (self.convertEntities == self.XML_ENTITIES and \\\n        self.XML_ENTITY_LIST.get(ref)):\n    try:\n        data = unichr(name2codepoint[ref])\n    except KeyError:\n        pass\nif not data:\n    data = '&%s;' % ref\nself.handle_data(data)", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Initializes a map representation of this tag's attributes,\nif not already initialized.\"\"\"\n", "func_signal": "def _getAttrMap(self):\n", "code": "if not getattr(self, 'attrMap'):\n    self.attrMap = {}\n    for (key, value) in self.attrs:\n        self.attrMap[key] = value \nreturn self.attrMap", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is listlike.\"\"\"\n", "func_signal": "def isList(l):\n", "code": "return hasattr(l, '__iter__') \\\n       or (type(l) in (types.ListType, types.TupleType))", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"\nGiven a list of tree items, produces doubles of a tree item and a\n``dict`` containing information about the tree structure around the\nitem, with the following contents:\n\n   new_level\n      ``True`` if the current item is the start of a new level in\n      the tree, ``False`` otherwise.\n\n   closed_levels\n      A list of levels which end after the current item. This will\n      be an empty list if the next item is at the same level as the\n      current item.\n\nUsing this filter with unpacking in a ``{% for %}`` tag, you should\nhave enough information about the tree structure to create a\nhierarchical representation of the tree.\n\nExample::\n\n   {% for genre,structure in genres|tree_info %}\n   {% if tree.new_level %}<ul><li>{% else %}</li><li>{% endif %}\n   {{ genre.name }}\n   {% for level in tree.closed_levels %}</li></ul>{% endfor %}\n   {% endfor %}\n\n\"\"\"\n", "func_signal": "def tree_info(items, features=None):\n", "code": "kwargs = {}\nif features:\n    feature_names = features.split(',')\n    if 'ancestors' in feature_names:\n        kwargs['ancestors'] = True\nreturn tree_item_iterator(items, **kwargs)", "path": "external_apps\\mptt\\templatetags\\mptt_tags.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\ndeclaration as a CData object.\"\"\"\n", "func_signal": "def parse_declaration(self, i):\n", "code": "j = None\nif self.rawdata[i:i+9] == '<![CDATA[':\n     k = self.rawdata.find(']]>', i)\n     if k == -1:\n         k = len(self.rawdata)\n     data = self.rawdata[i+9:k]\n     j = k+3\n     self._toStringSubclass(data, CData)\nelse:\n    try:\n        j = SGMLParser.parse_declaration(self, i)\n    except SGMLParseError:\n        toHandle = self.rawdata[i:]\n        self.handle_data(toHandle)\n        j = i + len(toHandle)\nreturn j", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Changes a MS smart quote character to an XML or HTML\nentity.\"\"\"\n", "func_signal": "def _subMSChar(self, orig):\n", "code": "sub = self.MS_CHARS.get(orig)\nif type(sub) == types.TupleType:\n    if self.smartQuotesTo == 'xml':\n        sub = '&#x%s;' % sub[1]\n    else:\n        sub = '&%s;' % sub[0]\nreturn sub", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Given a document, tries to detect its XML encoding.\"\"\"\n", "func_signal": "def _detectEncoding(self, xml_data):\n", "code": "xml_encoding = sniffed_xml_encoding = None\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = self._ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') \\\n             and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and \\\n             (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        sniffed_xml_encoding = 'ascii'\n        pass\n    xml_encoding_match = re.compile \\\n                         ('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>')\\\n                         .match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and \\\n       (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',\n                         'iso-10646-ucs-4', 'ucs-4', 'csucs4',\n                         'utf-16', 'utf-32', 'utf_16', 'utf_32',\n                         'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nreturn xml_data, xml_encoding, sniffed_xml_encoding", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"\nPopulates a template variable with a ``QuerySet`` containing the\nfull tree for a given model.\n\nUsage::\n\n   {% full_tree_for_model [model] as [varname] %}\n\nThe model is specified in ``[appname].[modelname]`` format.\n\nExample::\n\n   {% full_tree_for_model tests.Genre as genres %}\n\n\"\"\"\n", "func_signal": "def do_full_tree_for_model(parser, token):\n", "code": "bits = token.contents.split()\nif len(bits) != 4:\n    raise template.TemplateSyntaxError(_('%s tag requires three arguments') % bits[0])\nif bits[2] != 'as':\n    raise template.TemplateSyntaxError(_(\"second argument to %s tag must be 'as'\") % bits[0])\nreturn FullTreeForModelNode(bits[1], bits[3])", "path": "external_apps\\mptt\\templatetags\\mptt_tags.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"\nPopulates a template variable with the drilldown tree for a given\nnode, optionally counting the number of items associated with its\nchildren.\n\nA drilldown tree consists of a node's ancestors, itself and its\nimmediate children. For example, a drilldown tree for a book\ncategory \"Personal Finance\" might look something like::\n\n   Books\n      Business, Finance & Law\n         Personal Finance\n            Budgeting (220)\n            Financial Planning (670)\n\nUsage::\n\n   {% drilldown_tree_for_node [node] as [varname] %}\n\nExtended usage::\n\n   {% drilldown_tree_for_node [node] as [varname] count [foreign_key] in [count_attr] %}\n   {% drilldown_tree_for_node [node] as [varname] cumulative count [foreign_key] in [count_attr] %}\n\nThe foreign key is specified in ``[appname].[modelname].[fieldname]``\nformat, where ``fieldname`` is the name of a field in the specified\nmodel which relates it to the given node's model.\n\nWhen this form is used, a ``count_attr`` attribute on each child of\nthe given node in the drilldown tree will contain a count of the\nnumber of items associated with it through the given foreign key.\n\nIf cumulative is also specified, this count will be for items\nrelated to the child node and all of its descendants.\n\nExamples::\n\n   {% drilldown_tree_for_node genre as drilldown %}\n   {% drilldown_tree_for_node genre as drilldown count tests.Game.genre in game_count %}\n   {% drilldown_tree_for_node genre as drilldown cumulative count tests.Game.genre in game_count %}\n\n\"\"\"\n", "func_signal": "def do_drilldown_tree_for_node(parser, token):\n", "code": "bits = token.contents.split()\nlen_bits = len(bits)\nif len_bits not in (4, 8, 9):\n    raise TemplateSyntaxError(_('%s tag requires either three, seven or eight arguments') % bits[0])\nif bits[2] != 'as':\n    raise TemplateSyntaxError(_(\"second argument to %s tag must be 'as'\") % bits[0])\nif len_bits == 8:\n    if bits[4] != 'count':\n        raise TemplateSyntaxError(_(\"if seven arguments are given, fourth argument to %s tag must be 'with'\") % bits[0])\n    if bits[6] != 'in':\n        raise TemplateSyntaxError(_(\"if seven arguments are given, sixth argument to %s tag must be 'in'\") % bits[0])\n    return DrilldownTreeForNodeNode(bits[1], bits[3], bits[5], bits[7])\nelif len_bits == 9:\n    if bits[4] != 'cumulative':\n        raise TemplateSyntaxError(_(\"if eight arguments are given, fourth argument to %s tag must be 'cumulative'\") % bits[0])\n    if bits[5] != 'count':\n        raise TemplateSyntaxError(_(\"if eight arguments are given, fifth argument to %s tag must be 'count'\") % bits[0])\n    if bits[7] != 'in':\n        raise TemplateSyntaxError(_(\"if eight arguments are given, seventh argument to %s tag must be 'in'\") % bits[0])\n    return DrilldownTreeForNodeNode(bits[1], bits[3], bits[6], bits[8], cumulative=True)\nelse:\n    return DrilldownTreeForNodeNode(bits[1], bits[3])", "path": "external_apps\\mptt\\templatetags\\mptt_tags.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "'''Given a string and its encoding, decodes the string into Unicode.\n%encoding is a string recognized by encodings.aliases'''\n\n# strip Byte Order Mark (if present)\n", "func_signal": "def _toUnicode(self, data, encoding):\n", "code": "if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') \\\n       and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') \\\n         and (data[2:4] != '\\x00\\x00'):\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nreturn newdata", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Returns the closest sibling to this Tag that matches the\ngiven criteria and appears after this Tag in the document.\"\"\"\n", "func_signal": "def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):\n", "code": "return self._findOne(self.findNextSiblings, name, attrs, text,\n                     **kwargs)", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Returns the closest parent of this Tag that matches the given\ncriteria.\"\"\"\n# NOTE: We can't use _findOne because findParents takes a different\n# set of arguments.\n", "func_signal": "def findParent(self, name=None, attrs={}, **kwargs):\n", "code": "r = None\nl = self.findParents(name, attrs, 1)\nif l:\n    r = l[0]\nreturn r", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Handle a processing instruction as a ProcessingInstruction\nobject, possibly one with a %SOUP-ENCODING% slot into which an\nencoding will be plugged later.\"\"\"\n", "func_signal": "def handle_pi(self, text):\n", "code": "if text[:3] == \"xml\":\n    text = \"xml version='1.0' encoding='%SOUP-ENCODING%'\"\nself._toStringSubclass(text, ProcessingInstruction)", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Returns true iff the given string is the name of a\nself-closing tag according to this parser.\"\"\"\n", "func_signal": "def isSelfClosingTag(self, name):\n", "code": "return self.SELF_CLOSING_TAGS.has_key(name) \\\n       or self.instanceSelfClosingTags.has_key(name)", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n#print \"Popping to %s\" % name\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return            \n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "#print \"End tag %s\" % name\n", "func_signal": "def unknown_endtag(self, name):\n", "code": "if self.quoteStack and self.quoteStack[-1] != name:\n    #This is not a real end tag.\n    #print \"</%s> is not real!\" % name\n    self.handle_data('</%s>' % name)\n    return\nself.endData()\nself._popToTag(name)\nif self.quoteStack and self.quoteStack[-1] == name:\n    self.quoteStack.pop()\n    self.literal = (len(self.quoteStack) > 0)", "path": "libs\\BeautifulSoup.py", "repo_name": "spool/oxstu", "stars": 5, "license": "None", "language": "python", "size": 21116}
{"docstring": "\"\"\"Directory tree walk with callback function.\n\nFor each directory in the directory tree rooted at top (including top\nitself, but excluding '.' and '..'), call func(arg, dirname, fnames).\ndirname is the name of the directory, and fnames a list of the names of\nthe files and subdirectories in dirname (excluding '.' and '..').  func\nmay modify the fnames list in-place (e.g. via del or slice assignment),\nand walk will only recurse into the subdirectories whose names remain in\nfnames; this can be used to implement a filter, or to impose a specific\norder of visiting.  No semantics are defined for, or required of, arg,\nbeyond that arg is always passed to func.  It can be used, e.g., to pass\na filename pattern, or a mutable object designed to accumulate\nstatistics.  Passing None for arg is common.\"\"\"\n\n", "func_signal": "def walk(top, func, arg):\n", "code": "try:\n    names = os.listdir(top)\nexcept os.error:\n    return\nfunc(arg, top, names)\nexceptions = ('.', '..')\nfor name in names:\n    if name not in exceptions:\n        name = join(top, name)\n        if isdir(name):\n            walk(name, func, arg)", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Split a pathname into drive and path specifiers. Returns a 2-tuple\n\"(drive,path)\";  either part may be empty\"\"\"\n", "func_signal": "def splitdrive(p):\n", "code": "if p[1:2] == ':':\n    return p[0:2], p[2:]\nreturn '', p", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Split a pathname into UNC mount point and relative path specifiers.\n\nReturn a 2-tuple (unc, rest); either part may be empty.\nIf unc is not empty, it has the form '//host/mount' (or similar\nusing backslashes).  unc+rest is always the input path.\nPaths containing drive letters never have an UNC part.\n\"\"\"\n", "func_signal": "def splitunc(p):\n", "code": "if p[1:2] == ':':\n    return '', p # Drive letter present\nfirstTwo = p[0:2]\nif firstTwo == '//' or firstTwo == '\\\\\\\\':\n    # is a UNC path:\n    # vvvvvvvvvvvvvvvvvvvv equivalent to drive letter\n    # \\\\machine\\mountpoint\\directories...\n    #           directory ^^^^^^^^^^^^^^^\n    normp = normcase(p)\n    index = normp.find('\\\\', 2)\n    if index == -1:\n        ##raise RuntimeError, 'illegal UNC path: \"' + p + '\"'\n        return (\"\", p)\n    index = normp.find('\\\\', index + 1)\n    if index == -1:\n        index = len(p)\n    return p[:index], p[index:]\nreturn '', p", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Split the extension from a pathname.\n\nExtension is everything from the last dot to the end.\nReturn (root, ext), either part may be empty.\"\"\"\n\n", "func_signal": "def splitext(p):\n", "code": "i = p.rfind('.')\nif i<=max(p.rfind('/'), p.rfind('\\\\')):\n    return p, ''\nelse:\n    return p[:i], p[i:]", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Test whether a path is absolute\"\"\"\n", "func_signal": "def isabs(s):\n", "code": "s = splitdrive(s)[1]\nreturn s != '' and s[:1] in '/\\\\'", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Test whether a path is a mount point (defined as root of drive)\"\"\"\n", "func_signal": "def ismount(path):\n", "code": "unc, rest = splitunc(path)\nif unc:\n    return rest in (\"\", \"/\", \"\\\\\")\np = splitdrive(path)[1]\nreturn len(p) == 1 and p[0] in '/\\\\'", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Test whether a path exists\"\"\"\n", "func_signal": "def exists(path):\n", "code": "try:\n    st = os.stat(path)\nexcept os.error:\n    return False\nreturn True", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Split a pathname.\n\nReturn tuple (head, tail) where tail is everything after the final slash.\nEither part may be empty.\"\"\"\n\n", "func_signal": "def split(p):\n", "code": "d, p = splitdrive(p)\n# set i to index beyond p's last slash\ni = len(p)\nwhile i and p[i-1] not in '/\\\\':\n    i = i - 1\nhead, tail = p[:i], p[i:]  # now tail has no slashes\n# remove trailing slashes from head, unless it's all slashes\nhead2 = head\nwhile head2 and head2[-1] in '/\\\\':\n    head2 = head2[:-1]\nhead = head2 or head\nreturn d + head, tail", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Return the absolute version of a path.\"\"\"\n", "func_signal": "def abspath(path):\n", "code": "if not isabs(path):\n    path = join(os.getcwd(), path)\nreturn normpath(path)", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Test whether a path is a directory\"\"\"\n", "func_signal": "def isdir(path):\n", "code": "try:\n    st = os.stat(path)\nexcept os.error:\n    return False\nreturn stat.S_ISDIR(st.st_mode)", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Split a pathname.  Returns tuple \"(head, tail)\" where \"tail\" is\neverything after the final slash.  Either part may be empty.\"\"\"\n", "func_signal": "def split(p):\n", "code": "i = p.rfind('/') + 1\nhead, tail = p[:i], p[i:]\nif head and head != '/'*len(head):\n    head = head.rstrip('/')\nreturn head, tail", "path": "client\\posixpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Return the absolute version of a path.\"\"\"\n\n", "func_signal": "def abspath(path):\n", "code": "if path: # Empty path must return current working directory.\n    try:\n        path = _getfullpathname(path)\n    except WindowsError:\n        pass # Bad path - return unchanged.\nelse:\n    path = os.getcwd()\nreturn normpath(path)", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "# Make progressively weaker assumptions about \"other\"\n", "func_signal": "def update(self, other=None, **kwargs):\n", "code": "if other is None:\n    pass\nelif hasattr(other, 'iteritems'):  # iteritems saves memory and lookups\n    for k, v in other.iteritems():\n        self[k] = v\nelif hasattr(other, 'keys'):\n    for k in other.keys():\n        self[k] = other[k]\nelse:\n    for k, v in other:\n        self[k] = v\nif kwargs:\n    self.update(kwargs)", "path": "lib\\UserDict.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Expand ~ and ~user constructs.\n\nIf user or $HOME is unknown, do nothing.\"\"\"\n", "func_signal": "def expanduser(path):\n", "code": "if path[:1] != '~':\n    return path\ni, n = 1, len(path)\nwhile i < n and path[i] not in '/\\\\':\n    i = i + 1\nif i == 1:\n    if 'HOME' in os.environ:\n        userhome = os.environ['HOME']\n    elif not 'HOMEPATH' in os.environ:\n        return path\n    else:\n        try:\n            drive = os.environ['HOMEDRIVE']\n        except KeyError:\n            drive = ''\n        userhome = join(drive, os.environ['HOMEPATH'])\nelse:\n    return path\nreturn userhome + path[i:]", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "# Normalize newlines\n", "func_signal": "def fix_newlines(self, s):\n", "code": "s = s.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n\nif self._prepend_newline:\n    prefix = '\\n'\n    self._prepend_newline = False\nelse:\n    prefix = ''\n\nif s[-1] == '\\n':\n    self._prepend_newline = True\n    s = s[:-1]\n    \nreturn prefix + s", "path": "client\\silvershell\\stream.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Test whether a path is a regular file\"\"\"\n", "func_signal": "def isfile(path):\n", "code": "try:\n    st = os.stat(path)\nexcept os.error:\n    return False\nreturn stat.S_ISREG(st.st_mode)", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Split the extension from a pathname.  Extension is everything from the\nlast dot to the end.  Returns \"(root, ext)\", either part may be empty.\"\"\"\n", "func_signal": "def splitext(p):\n", "code": "i = p.rfind('.')\nif i<=p.rfind('/'):\n    return p, ''\nelse:\n    return p[:i], p[i:]", "path": "client\\posixpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n", "func_signal": "def normpath(path):\n", "code": "path = path.replace(\"/\", \"\\\\\")\nprefix, path = splitdrive(path)\n# We need to be careful here. If the prefix is empty, and the path starts\n# with a backslash, it could either be an absolute path on the current\n# drive (\\dir1\\dir2\\file) or a UNC filename (\\\\server\\mount\\dir1\\file). It\n# is therefore imperative NOT to collapse multiple backslashes blindly in\n# that case.\n# The code below preserves multiple backslashes when there is no drive\n# letter. This means that the invalid filename \\\\\\a\\b is preserved\n# unchanged, where a\\\\\\b is normalised to a\\b. It's not clear that there\n# is any better behaviour for such edge cases.\nif prefix == '':\n    # No drive letter - preserve initial backslashes\n    while path[:1] == \"\\\\\":\n        prefix = prefix + \"\\\\\"\n        path = path[1:]\nelse:\n    # We have a drive letter - collapse initial backslashes\n    if path.startswith(\"\\\\\"):\n        prefix = prefix + \"\\\\\"\n        path = path.lstrip(\"\\\\\")\ncomps = path.split(\"\\\\\")\ni = 0\nwhile i < len(comps):\n    if comps[i] in ('.', ''):\n        del comps[i]\n    elif comps[i] == '..':\n        if i > 0 and comps[i-1] != '..':\n            del comps[i-1:i+1]\n            i -= 1\n        elif i == 0 and prefix.endswith(\"\\\\\"):\n            del comps[i]\n        else:\n            i += 1\n    else:\n        i += 1\n# If the path is now empty, substitute '.'\nif not prefix and not comps:\n    comps.append('.')\nreturn prefix + \"\\\\\".join(comps)", "path": "lib\\ntpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n", "func_signal": "def normpath(path):\n", "code": "if path == '':\n    return '.'\ninitial_slashes = path.startswith('/')\n# POSIX allows one or two initial slashes, but treats three or more\n# as single slash.\nif (initial_slashes and\n    path.startswith('//') and not path.startswith('///')):\n    initial_slashes = 2\ncomps = path.split('/')\nnew_comps = []\nfor comp in comps:\n    if comp in ('', '.'):\n        continue\n    if (comp != '..' or (not initial_slashes and not new_comps) or\n         (new_comps and new_comps[-1] == '..')):\n        new_comps.append(comp)\n    elif new_comps:\n        new_comps.pop()\ncomps = new_comps\npath = '/'.join(comps)\nif initial_slashes:\n    path = '/'*initial_slashes + path\nreturn path or '.'", "path": "client\\posixpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"Join two or more pathname components, inserting '/' as needed\"\"\"\n", "func_signal": "def join(a, *p):\n", "code": "path = a\nfor b in p:\n    if b.startswith('/'):\n        path = b\n    elif path == '' or path.endswith('/'):\n        path +=  b\n    else:\n        path += '/' + b\nreturn path", "path": "client\\posixpath.py", "repo_name": "eloff/silvershell", "stars": 5, "license": "bsd-3-clause", "language": "python", "size": 3861}
{"docstring": "\"\"\"set locale\"\"\"\n", "func_signal": "def setLocale(locale):\n", "code": "global LOCALE\n_checkLocaleSupported(locale)\nLOCALE = locale", "path": "lib\\albumart\\amazon.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"\nConvert an C{int} into a 4 byte string encoded with the synchsafe scheme\n\"\"\"\n\n#bin2byte(bin2synchsafe(dec2bin(len(data), 28)))\n", "func_signal": "def dec2synchsafe(xx):\n", "code": "xx = bin2byte(bin2synchsafe(dec2bin(xx, 28)))\nassert len(xx) == 4\nreturn xx", "path": "lib\\albumart\\id3\\binfuncs.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"set HTTP proxy\"\"\"\n", "func_signal": "def setProxy(http_proxy):\n", "code": "global HTTP_PROXY\nHTTP_PROXY = http_proxy", "path": "lib\\albumart\\amazon.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"Cancels the active process\"\"\"\n", "func_signal": "def processCanceled(self):\n", "code": "if self.thread:\n  self.thread.cancel()", "path": "lib\\albumart\\albumart_unattended_ui.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "# delete the temporary file if needed\n", "func_signal": "def __del__(self):\n", "code": "try:\n  if self.delete:\n    os.unlink(self.path)\nexcept:\n  pass", "path": "lib\\albumart\\items.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"Add a new track to this album\"\"\"\n", "func_signal": "def addTrack(self, fileName):\n", "code": "if not fileName in self.tracks:\n  self.tracks.append(fileName)\nreturn TrackItem(self, fileName)", "path": "lib\\albumart\\items.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "# get the first APIC cover we find\n", "func_signal": "def getCover(self, path):\n", "code": "if self.enabled:\n  for f in self.getFileList(path):\n    id3v2 = id3.ID3v2(f)\n    try:\n      for frame in id3v2.frames:\n        if frame.id == \"APIC\":\n          file = open(self.tempfile, \"wb\")\n          file.write(frame.image)\n          file.close()\n          return albumart.Cover(self.tempfile)\n    except:\n      pass", "path": "lib\\albumart\\albumart_target_id3v2.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"\nConvert a 4 byte string encoded with the synchsafe scheme into a C{int}\n\"\"\"\n# chop our string into a list of bytes\n", "func_signal": "def synchsafe2dec(string):\n", "code": "bytelist = list(string)\n# chop our list of bytes into a list of bits\nbitlist = []\nfor byte in bytelist:\n    bitlist += dec2bin(ord(byte), 8)\nbitlist = synchsafe2bin(bitlist)\nx = bin2dec(bitlist)\nreturn x", "path": "lib\\albumart\\id3\\binfuncs.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"Runs the given process instance. @see process.Process\"\"\"\n", "func_signal": "def startProcess(self, process):\n", "code": "self.progressDialog = QProgressDialog(self, \"progress\", 1)\nself.progressDialog.setCaption(process.__doc__)\nself.thread = process\nself.progressDialog.connect(self.progressDialog, SIGNAL(\"canceled()\"), self.processCanceled)\n\nif not self.hidden:\n  self.progressDialog.show()\nself.thread.start()", "path": "lib\\albumart\\albumart_unattended_ui.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"Delete all cover images below the given path\"\"\"\n", "func_signal": "def deleteCovers(self, path):\n", "code": "items = self.getItems(path)\nself.startProcess(DeleteProcess(self, path, items))", "path": "lib\\albumart\\albumart_unattended_ui.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"set license key\"\"\"\n", "func_signal": "def setLicense(license_key):\n", "code": "global LICENSE_KEY\nLICENSE_KEY = license_key", "path": "lib\\albumart\\amazon.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"Reports the given exception to the user.\n\n   @param task Description of task during which the exception was raised\n   @param exception The exception that was raised\"\"\"\n", "func_signal": "def reportException(self, task, exception):\n", "code": "xcpt = traceback.format_exception(sys.exc_type, sys.exc_value, sys.exc_traceback)\nmsg = self.tr(\n        \"%(task)s was interrupted by the following exception:\\n%(xcpt)s\\n\") % \\\n        {\"task\" : task, \"xcpt\": \"\".join(xcpt)}\nsys.stderr.write(msg)", "path": "lib\\albumart\\albumart_unattended_ui.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"@returns a QPixmap representing the given path\"\"\"\n", "func_signal": "def getPixmapForPath(path):\n", "code": "def loadPixmap(name):\n  pixmap = QPixmap()\n  img = QImage()\n  QImageDrag.decode(albumart_images.MimeSourceFactory_albumart().data(QString(name)), img)\n  pixmap.convertFromImage(img)\n  return pixmap\n\nglobal noCoverPixmap\n\nif not noCoverPixmap:\n  noCoverPixmap = loadPixmap(\"nocover.png\")\n\nif albumart.hasCover(path):\n  cover = albumart.getCover(path)\n\n  if cover:\n    filename = cover.path\n    # if we're running on Qt 2, convert the image to a png.\n    if qVersion().split(\".\")[0]=='2' and imghdr.what(filename) != \"png\":\n      try:\n          i = Image.open(filename)\n          s = StringIO.StringIO()\n          i.save(s, \"PNG\")\n          pixmap = QPixmap()\n          pixmap.loadFromData(s.getvalue())\n      except IOError:\n          return noCoverPixmap\n    else:\n        pixmap = QPixmap(filename)\n        \n    if pixmap.width() > 0 and pixmap.height() > 0:\n      return pixmap\n    else:\n      return noCoverPixmap\nreturn noCoverPixmap", "path": "lib\\albumart\\pixmap.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"Returns a list of processable items under the given path\"\"\"\n", "func_signal": "def getItems(self, path):\n", "code": "items = []\nfor root, dirs, files in os.walk(unicode(path)):\n  items.append(root)\n  items += [os.path.join(root, f) for f in files if not f.startswith(\".\")]\nreturn items", "path": "lib\\albumart\\albumart_unattended_ui.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"Download images automatically for the given path\"\"\"\n", "func_signal": "def downloadCovers(self, path):\n", "code": "items = self.getItems(path)\nitems = filter(lambda i: not albumart.hasCover(i), items)\nself.startProcess(AutoDownloadProcess(self, path, items, self.requireExactMatch))", "path": "lib\\albumart\\albumart_unattended_ui.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"Add a new track to this album\"\"\"\n", "func_signal": "def addTrack(self, fileName):\n", "code": "if not fileName in self.tracks:\n  self.tracks.append(fileName)\nreturn TrackItem(self, fileName)", "path": "lib\\albumart\\items.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"Make sure all the albums have same images in all their targets\n  (fd.o, wxp, id3v2, etc.)\"\"\"\n", "func_signal": "def synchronizeCovers(self, path):\n", "code": "items = self.getItems(path)\nitems = filter(lambda i: albumart.hasCover(i), items)\nself.startProcess(SynchronizeProcess(self, path, items))", "path": "lib\\albumart\\albumart_unattended_ui.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "# open the album if the cursor is within the trigger\n", "func_signal": "def activate(self):\n", "code": "p = self.listView().mapFromGlobal(QCursor.pos())\np.setY(p.y() - self.itemPos() + self.listView().contentsY())\nif p.y() >= self.openTrigger.top() and \\\n   p.x() > self.openTrigger.left() - 4 and \\\n   p.x() < self.openTrigger.right() + 4:\n  self.setOpen(not self.isOpen())", "path": "lib\\albumart\\items.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "# some day we might find the plugins dynamically\n", "func_signal": "def configure(self, config):\n", "code": "for c in [\"sources\", \"targets\", \"recognizers\"]:\n  config[c] = defaultConfig[c]\n\nself.requireExactMatch = config[\"require_exact_match\"]\n\n# only load plugins at startup\nif not self.modules:\n  for id in config[\"sources\"]:\n    albumart.addSource(self.loadModule(id))\n  \n  for id in config[\"targets\"]:\n    albumart.addTarget(self.loadModule(id))\n  \n  for id in config[\"recognizers\"]:\n    albumart.addRecognizer(self.loadModule(id))", "path": "lib\\albumart\\albumart_unattended_ui.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "# read our contents if it hasn't been done before\n", "func_signal": "def activate(self):\n", "code": "if self.childCount() == 0:\n  for fileName in self.getChildren():\n    if os.path.isdir(fileName):\n      FolderItem(self, fileName, self.extensions)\n    elif os.path.isfile(fileName):\n      FileItem(self, fileName)\n\n# open the album if the cursor is within the trigger\np = self.listView().mapFromGlobal(QCursor.pos())\np.setY(p.y() - self.itemPos() + self.listView().contentsY())\nif p.y() >= self.openTrigger.top() and \\\n   p.x() > self.openTrigger.left() - 4 and \\\n   p.x() < self.openTrigger.right() + 4:\n  self.setOpen(not self.isOpen())", "path": "lib\\albumart\\items.py", "repo_name": "skyostil/albumart", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 23476}
{"docstring": "\"\"\"\n>>> t = Textile()\n\n>>> t.glyphs(\"apostrophe's\")\n'apostrophe&#8217;s'\n\n>>> t.glyphs(\"back in '88\")\n'back in &#8217;88'\n\n>>> t.glyphs('foo ...')\n'foo &#8230;'\n\n>>> t.glyphs('--')\n'&#8212;'\n\n>>> t.glyphs('FooBar[tm]')\n'FooBar&#8482;'\n\n>>> t.glyphs(\"<p><cite>Cat's Cradle</cite> by Vonnegut</p>\")\n'<p><cite>Cat&#8217;s Cradle</cite> by Vonnegut</p>'\n\n\"\"\"\n # fix: hackish\n", "func_signal": "def glyphs(self, text):\n", "code": "text = re.sub(r'\"\\z', '\\\" ', text)\n\nglyph_search = (\n    re.compile(r\"(\\w)\\'(\\w)\"),                                      # apostrophe's\n    re.compile(r'(\\s)\\'(\\d+\\w?)\\b(?!\\')'),                          # back in '88\n    re.compile(r'(\\S)\\'(?=\\s|'+self.pnct+'|<|$)'),                       #  single closing\n    re.compile(r'\\'/'),                                             #  single opening\n    re.compile(r'(\\S)\\\"(?=\\s|'+self.pnct+'|<|$)'),                       #  double closing\n    re.compile(r'\"'),                                               #  double opening\n    re.compile(r'\\b([A-Z][A-Z0-9]{2,})\\b(?:[(]([^)]*)[)])'),        #  3+ uppercase acronym\n    re.compile(r'\\b([A-Z][A-Z\\'\\-]+[A-Z])(?=[\\s.,\\)>])'),           #  3+ uppercase\n    re.compile(r'\\b(\\s{0,1})?\\.{3}'),                                     #  ellipsis\n    re.compile(r'(\\s?)--(\\s?)'),                                    #  em dash\n    re.compile(r'\\s-(?:\\s|$)'),                                     #  en dash\n    re.compile(r'(\\d+)( ?)x( ?)(?=\\d+)'),                           #  dimension sign\n    re.compile(r'\\b ?[([]TM[])]', re.I),                            #  trademark\n    re.compile(r'\\b ?[([]R[])]', re.I),                             #  registered\n    re.compile(r'\\b ?[([]C[])]', re.I),                             #  copyright\n )\n\nglyph_replace = [x % dict(self.glyph_defaults) for x in (\n    r'\\1%(txt_apostrophe)s\\2',           # apostrophe's\n    r'\\1%(txt_apostrophe)s\\2',           # back in '88\n    r'\\1%(txt_quote_single_close)s',     #  single closing\n    r'%(txt_quote_single_open)s',         #  single opening\n    r'\\1%(txt_quote_double_close)s',        #  double closing\n    r'%(txt_quote_double_open)s',             #  double opening\n    r'<acronym title=\"\\2\">\\1</acronym>', #  3+ uppercase acronym\n    r'<span class=\"caps\">\\1</span>',     #  3+ uppercase\n    r'\\1%(txt_ellipsis)s',                  #  ellipsis\n    r'\\1%(txt_emdash)s\\2',               #  em dash\n    r' %(txt_endash)s ',                 #  en dash\n    r'\\1\\2%(txt_dimension)s\\3',          #  dimension sign\n    r'%(txt_trademark)s',                #  trademark\n    r'%(txt_registered)s',                #  registered\n    r'%(txt_copyright)s',                #  copyright\n)]\n\nresult = []\nfor line in re.compile(r'(<.*?>)', re.U).split(text):\n    if not re.search(r'<.*>', line):\n        for s, r in zip(glyph_search, glyph_replace):\n            line = s.sub(r, line)\n    result.append(line)\nreturn ''.join(result)", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\n>>> t = Textile()\n>>> t.fBlock(\"bq\", \"\", None, \"\", \"Hello BlockQuote\")\n('\\\\t<blockquote>\\\\n', '\\\\t\\\\t<p>', 'Hello BlockQuote', '</p>', '\\\\n\\\\t</blockquote>')\n\n>>> t.fBlock(\"bq\", \"\", None, \"http://google.com\", \"Hello BlockQuote\")\n('\\\\t<blockquote cite=\"http://google.com\">\\\\n', '\\\\t\\\\t<p>', 'Hello BlockQuote', '</p>', '\\\\n\\\\t</blockquote>')\n\n>>> t.fBlock(\"bc\", \"\", None, \"\", 'printf \"Hello, World\";') # doctest: +ELLIPSIS\n('<pre>', '<code>', ..., '</code>', '</pre>')\n\n>>> t.fBlock(\"h1\", \"\", None, \"\", \"foobar\")\n('', '\\\\t<h1>', 'foobar', '</h1>', '')\n\"\"\"\n", "func_signal": "def fBlock(self, tag, atts, ext, cite, content):\n", "code": "atts = self.pba(atts)\no1 = o2 = c2 = c1 = ''\n\nm = re.search(r'fn(\\d+)', tag)\nif m:\n    tag = 'p'\n    if m.group(1) in self.fn:\n        fnid = self.fn[m.group(1)]\n    else:\n        fnid = m.group(1)\n    atts = atts + ' id=\"fn%s\"' % fnid\n    if atts.find('class=') < 0:\n        atts = atts + ' class=\"footnote\"'\n    content = ('<sup>%s</sup>' % m.group(1)) + content\n\nif tag == 'bq':\n    cite = self.checkRefs(cite)\n    if cite:\n        cite = ' cite=\"%s\"' % cite\n    else:\n        cite = ''\n    o1 = \"\\t<blockquote%s%s>\\n\" % (cite, atts)\n    o2 = \"\\t\\t<p%s>\" % atts\n    c2 = \"</p>\"\n    c1 = \"\\n\\t</blockquote>\"\n\nelif tag == 'bc':\n    o1 = \"<pre%s>\" % atts\n    o2 = \"<code%s>\" % atts\n    c2 = \"</code>\"\n    c1 = \"</pre>\"\n    content = self.shelve(self.encode_html(content.rstrip(\"\\n\") + \"\\n\"))\n\nelif tag == 'notextile':\n    content = self.shelve(content)\n    o1 = o2 = ''\n    c1 = c2 = ''\n\nelif tag == 'pre':\n    content = self.shelve(self.encode_html(content.rstrip(\"\\n\") + \"\\n\"))\n    o1 = \"<pre%s>\" % atts\n    o2 = c2 = ''\n    c1 = '</pre>'\n\nelse:\n    o2 = \"\\t<%s%s>\" % (tag, atts)\n    c2 = \"</%s>\" % tag\n\ncontent = self.graf(content)\nreturn o1, o2, content, c2, c1", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nspecial blocks like notextile or code\n\"\"\"\n", "func_signal": "def fSpecial(self, match):\n", "code": "before, text, after = match.groups()\nif after == None: after = ''\nreturn ''.join([before, self.shelve(self.encode_html(text)), after])", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"mxTidy's XHTML validator.\n\nThis function is a wrapper to mxTidy's validator.\n\"\"\"\n", "func_signal": "def _tidy1(text):\n", "code": "nerrors, nwarnings, text, errortext = Tidy.tidy(text, output_xhtml=1, numeric_entities=1, wrap=0)\nreturn _in_tag(text, 'body')", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"docstring for __init__\"\"\"\n", "func_signal": "def __init__(self, restricted=False, lite=False):\n", "code": "self.restricted = restricted\nself.lite = lite\nself.fn = {}\nself.urlrefs = {}\nself.shelf = {}\nself.rel = ''", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "# called for each end tag, e.g. for </pre>, tag will be \"pre\"\n# Reconstruct the original end tag.\n", "func_signal": "def unknown_endtag(self, tag):\n", "code": "if tag not in self.elements_no_end_tag:\n    self.pieces.append(\"</%(tag)s>\" % locals())", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "# (None, '', '/imgs/myphoto.jpg', None, None)\n", "func_signal": "def fImage(self, match):\n", "code": "algn, atts, url, title, href = match.groups()\natts  = self.pba(atts)\nif algn:\n    atts = atts + ' align=\"%s\"' % self.iAlign(algn)\n\nif title:\n    atts = atts + ' title=\"%s\" alt=\"%s\"' % (title, title)\nelse:\n    atts = atts + ' alt=\"\"'\n\n# TODO how to do this in python?\n# size = @getimagesize(url)\n# if (size) atts .= \" size[3]\"\n\nif href:\n    href = self.checkRefs(href)\n\nurl = self.checkRefs(url)\nurl = self.relURL(url)\n\nout = []\nif href: out.append('<a href=\"%s\">' % href)\nout.append('<img src=\"%s\"%s />' % (url, atts))\nif href: out.append('</a>')\n\nreturn ''.join(out)", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\n>>> t = Textile()\n>>> t.lists(\"* one\\\\n* two\\\\n* three\")\n'\\\\t<ul>\\\\n\\\\t\\\\t<li>one</li>\\\\n\\\\t\\\\t<li>two</li>\\\\n\\\\t\\\\t<li>three</li>\\\\n\\\\t</ul>'\n\"\"\"\n", "func_signal": "def lists(self, text):\n", "code": "pattern = re.compile(r'^([#*]+%s .*)$(?![^#*])' % self.c, re.U|re.M|re.S)\nreturn pattern.sub(self.fList, text)", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\n>>> t = Textile()\n>>> t.pba(r'\\3')\n''\n>>> t.pba(r'\\\\3', element='td')\n' colspan=\"3\"'\n>>> t.pba(r'/4', element='td')\n' rowspan=\"4\"'\n>>> t.pba(r'\\\\3/4', element='td')\n' colspan=\"3\" rowspan=\"4\"'\n\n>>> t.vAlign('^')\n'top'\n\n>>> t.pba('^', element='td')\n' style=\"vertical-align:top;\"'\n\n>>> t.pba('{line-height:18px}')\n' style=\"line-height:18px;\"'\n\n>>> t.pba('(foo-bar)')\n' class=\"foo-bar\"'\n\n>>> t.pba('(#myid)')\n' id=\"myid\"'\n\n>>> t.pba('(foo-bar#myid)')\n' class=\"foo-bar\" id=\"myid\"'\n\n>>> t.pba('((((')\n' style=\"padding-left:4em;\"'\n\n>>> t.pba(')))')\n' style=\"padding-right:3em;\"'\n\n>>> t.pba('[fr]')\n' lang=\"fr\"'\n\n\"\"\"\n", "func_signal": "def pba(self, input, element=None):\n", "code": "style = []\naclass = ''\nlang = ''\ncolspan = ''\nrowspan = ''\nid = ''\natts = ''\n\nif not input: return ''\n\nmatched = input\nif element == 'td':\n    m = re.search(r'\\\\(\\d+)', matched)\n    if m:\n        colspan = m.group(1)\n\n    m = re.search(r'/(\\d+)', matched)\n    if m:\n        rowspan = m.group(1)\n\nif element == 'td' or element == 'tr':\n    m = re.search(r'(%s)' % self.vlgn, matched)\n    if m: style.append(\"vertical-align:%s;\" % self.vAlign(m.group(1)))\n\nm = re.search(r'\\{([^}]*)\\}', matched)\nif m:\n    style.append(m.group(1).rstrip(';') + ';')\n    matched = matched.replace(m.group(0), '')\n\nm = re.search(r'\\[([^\\]]+)\\]', matched, re.U)\nif m:\n    lang = m.group(1)\n    matched = matched.replace(m.group(0), '')\n\nm = re.search(r'\\(([^()]+)\\)', matched, re.U)\nif m:\n    aclass = m.group(1)\n    matched = matched.replace(m.group(0), '')\n\nm = re.search(r'([(]+)', matched)\nif m:\n    style.append(\"padding-left:%sem;\" % len(m.group(1)))\n    matched = matched.replace(m.group(0), '')\n\nm = re.search(r'([)]+)', matched)\nif m:\n    style.append(\"padding-right:%sem;\" % len(m.group(1)))\n    matched = matched.replace(m.group(0), '')\n\nm = re.search(r'(%s)' % self.hlgn, matched)\nif m:\n    style.append(\"text-align:%s;\" % self.hAlign(m.group(1)))\n\nm = re.search(r'^(.*)#(.*)$', aclass)\nif m:\n    id = m.group(2)\n    aclass = m.group(1)\n\nif self.restricted:\n    if lang: return ' lang=\"%s\"'\n    else: return ''\n\nresult = []\nif style: result.append(' style=\"%s\"' % \"\".join(style))\nif aclass: result.append(' class=\"%s\"' % aclass)\nif lang: result.append(' lang=\"%s\"' % lang)\nif id: result.append(' id=\"%s\"' % id)\nif colspan: result.append(' colspan=\"%s\"' % colspan)\nif rowspan: result.append(' rowspan=\"%s\"' % rowspan)\nreturn ''.join(result)", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "# utility method to be called by descendants\n", "func_signal": "def normalize_attrs(self, attrs):\n", "code": "attrs = [(k.lower(), sgmllib.charref.sub(lambda m: unichr(int(m.groups()[0])), v).strip()) for k, v in attrs]\nattrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]\nreturn attrs", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "# called for each start tag\n# attrs is a list of (attr, value) tuples\n# e.g. for <pre class=\"screen\">, tag=\"pre\", attrs=[(\"class\", \"screen\")]\n", "func_signal": "def unknown_starttag(self, tag, attrs):\n", "code": "strattrs = \"\".join([' %s=\"%s\"' % (key, value) for key, value in attrs])\nif tag in self.elements_no_end_tag:\n    self.pieces.append(\"<%(tag)s%(strattrs)s />\" % locals())\nelse:\n    self.pieces.append(\"<%(tag)s%(strattrs)s>\" % locals())", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\n>>> t = Textile()\n>>> t.block('h1. foobar baby')\n'\\\\t<h1>foobar baby</h1>'\n\"\"\"\n", "func_signal": "def block(self, text):\n", "code": "tre = '|'.join(self.btag)\ntext = text.split('\\n\\n')\n\ntag = 'p'\natts = cite = graf = ext = ''\n\nout = []\n\nanon = False\nfor line in text:\n    pattern = r'^(%s)(%s%s)\\.(\\.?)(?::(\\S+))? (.*)$' % (tre, self.a, self.c)\n    match = re.search(pattern, line, re.S)\n    if match:\n        if ext:\n            out.append(out.pop() + c1)\n\n        tag,atts,ext,cite,graf = match.groups()\n        o1, o2, content, c2, c1 = self.fBlock(tag, atts, ext, cite, graf)\n        # leave off c1 if this block is extended, we'll close it at the start of the next block\n        if ext:\n            line = \"%s%s%s%s\" % (o1, o2, content, c2)\n        else:\n            line = \"%s%s%s%s%s\" % (o1, o2, content, c2, c1)\n\n    else:\n        anon = True\n        if ext or not re.search(r'^\\s', line):\n            o1, o2, content, c2, c1 = self.fBlock(tag, atts, ext, cite, line)\n            # skip $o1/$c1 because this is part of a continuing extended block\n            if tag == 'p' and not self.hasRawText(content):\n                line = content\n            else:\n                line = \"%s%s%s\" % (o2, content, c2)\n        else:\n            line = self.graf(line)\n\n    line = self.doPBr(line)\n    line = re.sub(r'<br>', '<br />', line)\n\n    if ext and anon:\n        out.append(out.pop() + \"\\n\" + line)\n    else:\n        out.append(line)\n\n    if not ext:\n        tag = 'p'\n        atts = ''\n        cite = ''\n        graf = ''\n\nif ext:\n    out.append(out.pop() + c1)\nreturn '\\n\\n'.join(out)", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\n>>> t = Textile()\n>>> t.image('!/imgs/myphoto.jpg!:http://jsamsa.com')\n'<a href=\"http://jsamsa.com\"><img src=\"/imgs/myphoto.jpg\" alt=\"\" /></a>'\n\"\"\"\n", "func_signal": "def image(self, text):\n", "code": "pattern = re.compile(r\"\"\"\n    (?:[\\[{])?          # pre\n    \\!                 # opening !\n    (\\<|\\=|\\>)??       # optional alignment atts\n    (%s)               # optional style,class atts\n    (?:\\. )?           # optional dot-space\n    ([^\\s(!]+)         # presume this is the src\n    \\s?                # optional space\n    (?:\\(([^\\)]+)\\))?  # optional title\n    \\!                 # closing\n    (?::(\\S+))?        # optional href\n    (?:[\\]}]|(?=\\s|$)) # lookahead: space or end of string\n\"\"\" % self.c, re.U|re.X)\nreturn pattern.sub(self.fImage, text)", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\n>>> t = Textile()\n>>> id = t.shelve(\"foobar\")\n>>> t.retrieve(id)\n'foobar'\n\"\"\"\n", "func_signal": "def retrieve(self, text):\n", "code": "while True:\n    old = text\n    for k,v in self.shelf.items():\n        text = text.replace(k,v)\n    if text == old: break\nreturn text", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"uTidyLib's XHTML validator.\n\nThis function is a wrapper to uTidyLib's validator.\n\"\"\"\n", "func_signal": "def _tidy2(text):\n", "code": "text = tidy.parseString(text,  output_xhtml=1, add_xml_decl=0, indent=0, tidy_mark=0)\nreturn _in_tag(str(text), 'body')", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\n>>> import textile\n>>> textile.textile('some textile')\n'\\\\t<p>some textile</p>'\n\"\"\"\n", "func_signal": "def textile(self, text, rel=None, encoding='utf8', output='utf8', validate=False, sanitize=False, head_offset='ignored'):\n", "code": "text = _normalize_newlines(text)\n\nif rel:\n    self.rel = ' rel=\"%s\"' % rel\n\ntext = self.getRefs(text)\n\nif not self.lite:\n    text = self.block(text)\n\ntext = self.retrieve(text)\n\n# Convert to desired output.\nif isinstance(text, str):\n    text = unicode(text, encoding)\n \ntext = text.encode(output, 'xmlcharrefreplace')\n\n# Sanitize?\nif sanitize:\n    p = _HTMLSanitizer()\n    p.feed(text)\n    text = p.output()\n\n# Validate output.\nif _tidy and validate:\n    text = _tidy(text)\n\nreturn text", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\n>>> t = Textile()\n>>> t.span(r\"hello %(bob)span *strong* and **bold**% goodbye\")\n'hello <span class=\"bob\">span <strong>strong</strong> and <b>bold</b></span> goodbye'\n\"\"\"\n", "func_signal": "def span(self, text):\n", "code": "qtags = (r'\\*\\*', r'\\*', r'\\?\\?', r'\\-', r'__', r'_', r'%', r'\\+', r'~', r'\\^')\npnct = \".,\\\"'?!;:\"\n\nfor qtag in qtags:\n    pattern = re.compile(r\"\"\"\n        (?:^|(?<=[\\s>%(pnct)s])|([\\]}]))\n        (%(qtag)s)(?!%(qtag)s)\n        (%(c)s)\n        (?::(\\S+))?\n        ([^\\s%(qtag)s]+|\\S[^%(qtag)s\\n]*[^\\s%(qtag)s\\n])\n        ([%(pnct)s]*)\n        %(qtag)s\n        (?:$|([\\]}])|(?=%(selfpnct)s{1,2}|\\s))\n    \"\"\" % {'qtag':qtag,'c':self.c,'pnct':pnct,'selfpnct':self.pnct}, re.X)\n    text = pattern.sub(self.fSpan, text)\nreturn text", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\n>>> t = Textile()\n>>> t.links('fooobar \"Google\":http://google.com/foobar/ and hello world \"flickr\":http://flickr.com/photos/jsamsa/ ') # doctest: +ELLIPSIS\n'fooobar ... and hello world ...'\n\"\"\"\n\n", "func_signal": "def links(self, text):\n", "code": "punct = '!\"#$%&\\'*+,-./:;=?@\\\\^_`|~'\n\npattern = r'''\n    ([\\s\\[{(]|[%s])?     # $pre\n    \"                          # start\n    (%s)                     # $atts\n    ([^\"]+?)                   # $text\n    \\s?\n    (?:\\(([^)]+?)\\)(?=\"))?     # $title\n    \":\n    (\\S+?)                     # $url\n    (\\/)?                      # $slash\n    ([^\\w\\/;]*?)               # $post\n    (?=<|\\s|$)\n''' % (re.escape(punct), self.c)\n\ntext = re.compile(pattern, re.X).sub(self.fLink, text)\n\nreturn text", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nwhat is this for?\n\"\"\"\n", "func_signal": "def getRefs(self, text):\n", "code": "pattern = re.compile(r'(?:(?<=^)|(?<=\\s))\\[(.+)\\]((?:http:\\/\\/|\\/)\\S+)(?=\\s|$)', re.U)\ntext = pattern.sub(self.refs, text)\nreturn text", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nchecks whether the text has text not already enclosed by a block tag\n\n>>> t = Textile()\n>>> t.hasRawText('<p>foo bar biz baz</p>')\nFalse\n\n>>> t.hasRawText(' why yes, yes it does')\nTrue\n\n\"\"\"\n", "func_signal": "def hasRawText(self, text):\n", "code": "r = re.compile(r'<(p|blockquote|div|form|table|ul|ol|pre|h\\d)[^>]*?>.*</\\1>', re.S).sub('', text.strip()).strip()\nr = re.compile(r'<(hr|br)[^>]*?/>').sub('', r)\nreturn '' != r", "path": "textile.py", "repo_name": "aurelian/gaeskel", "stars": 4, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nMethod returns back the sha1 sum of the all certs\nin it. Actually it is a little bit symbolic method\nbecause it appends the sha1 sums of all certs inthe\nchain and then gets back the sum of all that sums. Nothing\nspecial or magical ...\n\"\"\"\n", "func_signal": "def get_chain_hash(self):\n", "code": "from imzaci.digest.digest_util import DigestUtil\nif not self.__chain_valid or not self.__final_chain:\n    return None\n\nfinal_str = \"\"\nfor cert in self.__final_chain:\n    final_str = \"\".join([final_str,cert.cert_hash()])\n\nfinal_str = DigestUtil.digest_from_buffer(final_str)\nreturn final_str", "path": "cert\\chain_manager.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nA similar approach like above ones but we need\nit for our project because all the time we have \ndifferent diectories for different certs so may\nneed to scan them. The scanning depth is 1 we dont do\nrecursive things because we may have some private keys :)\n\"\"\"\n", "func_signal": "def load_chain_from_dirs(list_of_dirs):\n", "code": "chain_place = load_certs_from_dirs(list_of_dirs)\nresult=chain_manager_factory(chain_place,X509ChainManager.X509_CERT)\nreturn result", "path": "cert\\cert_tools.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\" To show the digest nicely...\"\"\"\n", "func_signal": "def nice_digest(dgst):\n", "code": "ng=string.replace(dgst, \":\", \"\")\nreturn string.lower(ng)", "path": "digest\\cryptUtility.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nIt tells if we need to initialize the structures useful \nfor reusing the X509ChainManager objects ...\n\"\"\"\n", "func_signal": "def __should_initialize(self):\n", "code": "if self.__final_chain or self.__ca_cert_stack or self.__subject_cert or self.__chain_valid:\n    return True\nelse:\n    return False", "path": "cert\\chain_manager.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nA simple wrapper method for check_ca because \nit returns some error codes which are not very\nPythonic .. if it is return True else False\n\"\"\"\n\n", "func_signal": "def is_ca(self):\n", "code": "if self.cert.check_ca() == 0:\n    return False\nelse:\n    return True", "path": "cert\\cert.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nTries to construct the chain with starting point at\ncurrent_cert so :\n1) We search for the issuer of the current_cert if find it\n2) When find the isuuer check check if it signed the cert here\n3) If all goes good we will have a fresh good chain :)\n\"\"\"\n", "func_signal": "def __construct_chain(self,current_cert):\n", "code": "find_issuer = current_cert.person_info(\"issuer\")\n#sometimes the passed cert is also a CA so\n#that breakes the normal behaviour if it is\n#a CA we should add it immediately to the list...\nif current_cert.is_ca():\n    self.__final_chain.append(current_cert)\n\nwhile not current_cert is None:\n    #did we found a match ?\n    found = False\n    for cert_check in self.__ca_cert_stack:\n        #check if subject info equeals to issuer info\n        #print \"What we try is :\"\n        #print \"The issuer to find :\",find_issuer\n        #print \"The subject we check :\",cert_check.person_info(\"subject\")\n        if cert_check.person_info(\"subject\") == find_issuer:\n\n            #check if that CA has signed our current cert\n            if current_cert.verify_issuer(cert_check.get_public_key()):\n                self.__final_chain.append(cert_check)\n                current_cert = cert_check\n                find_issuer = current_cert.person_info(\"issuer\")\n                found = True\n                break\n            else:\n                raise ChainValidationException(\"The cert with subject %s didnt sign the cert with subject %s the chain can not be constructed\"%(cert_check.person_info(\"subject\"),find_issuer))\n                return False\n    \n    if found == False and len(self.__ca_cert_stack) != len(self.__final_chain):\n        raise ChainValidationException(\"There is no signer for cert with subject %s \"%(current_cert.person_info(\"subject\")))\n        return False\n        \n    elif len(self.__ca_cert_stack) == len(self.__final_chain):\n        #The chain is ok should exit from main while loop\n        current_cert = None\n        return True", "path": "cert\\chain_manager.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\" Return Back the certs public key\"\"\"\n", "func_signal": "def get_public_key(self):\n", "code": "self.pkey=self.cert.get_pubkey()\nreturn self.pkey", "path": "cert\\cert.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nAfter you construct the chain get the subject\n\"\"\"\n", "func_signal": "def get_final_subject(self):\n", "code": "if self.__chain_valid:\n    return self.__subject_cert[0]\nelse:\n    raise ChainValidationException(\"It seems that chain is not constructed try running create_chain method first\")\n    return None", "path": "cert\\chain_manager.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nThe mentioned private key here is the one that is\nin the latest part of the chain,what we mean is the child\ncert's private key ...\n\"\"\"\n", "func_signal": "def load_private_key(chain_dir):\n", "code": "if os.path.exists(os.path.join(chain_dir,INTERNAL_DB_FILE)):\n    #continue by scanning the file ...\n    store = get_index_data(chain_dir)\n    #print store\n    if not store.has_key(\"private\") or not store['private']:\n        #print \"No cert wa found into the INTERNAL_DB_FILE\"\n        return None\n    else:\n        return os.path.join(chain_dir,\"private\",store['private'])\nelse:\n    return None", "path": "cert\\cert_tools.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\" Returns the latest constructed chain\"\"\"\n", "func_signal": "def get_final(self):\n", "code": "if self.__chain_valid:\n\n    return self.__final_chain\nelse:\n    raise ChainValidationException(\"It seems that chain is not constructed try running create_chain method first\")\n    return None", "path": "cert\\chain_manager.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\" Makes some options clearer in the certificate\"\"\"\n\n", "func_signal": "def optionClear(self):\n", "code": "opClr={'C'                      : 'Country Name', \n 'SP'                     : 'State or Province', \n 'ST'                     : 'Province or State', \n 'stateOrProvinceName'    : 'State Province', \n 'L'                      : 'Locality Name', \n 'localityName'           : 'Local Name', \n 'O'                      : 'Organization', \n 'organizationName'       : 'Organization Name', \n 'OU'                     : 'Org Unit Name', \n 'organizationUnitName'   : 'Organization Unit Name', \n 'CN'                     : 'The Common Name', \n 'commonName'             : 'Common Name', \n 'Email'                  : 'Email', \n 'emailAddress'           : 'email', \n 'serialNumber'           : 'Serial Number', \n 'SN'                     : 'Surname', \n 'surname'                : 'The Surname', \n 'GN'                     : 'Given Name', \n 'givenName'              : 'GivenName' \n      }\nreturn opClr", "path": "cert\\cert.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nThat method checks if its signature when\nis decrypted by public key of the issuer\nwill give us the fingerprint of that cert\nA very useful method to check if some other cert\nis the issuer of that ! (in chain validation)\n\"\"\"\n", "func_signal": "def verify_issuer(self,issuer_public_key):\n", "code": "result =  self.cert.verify(issuer_public_key)\n#ah it is C style :)\nif result == 0:\n    return False\nelse:\n    return True", "path": "cert\\cert.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\" From files that are uploaded by args creates a valid chain\nTrue if created,else False\nThere are a few things we try here when constructing a chain\n1) Check if there is nothhing into the self.__ca_cert_stack if empty False\n2) Check if there is more into the self.__subject_cert if False\n3) Then if there is sth into self.__subject_cert search into the\nself.__cert_stack and find its signer if any continue if no False\n4) If there is no self.__subject_cert find a starting point and try\nto construct the cert\nSimple is good :)\n\"\"\"\n#print \"The current CA list is like :\"\n#for ca in self.__ca_cert_stack:\n#    ca.list_info()\n#print \"The subject list is like :\"\n#for subj in self.__subject_cert:\n#    subj.list_info()\n\n", "func_signal": "def create_chain(self):\n", "code": "if self.__final_chain:\n    self.__final_chain = []\n\nif len(self.__ca_cert_stack) == 0:\n    raise ChainValidationException(\"There is no CA certs into the chain you try to build.\")\n    return False\n\nif len(self.__subject_cert) > 1:\n    raise ChainValidationException(\"There should be only one non CA cert into the chain\")\n    return False\n\nif len(self.__ca_cert_stack) == 1 and len(self.__subject_cert)==0:\n    raise ChainValidationException(\"Only one cert was found we can not  construct the chain with one cert\")\n    return False\n\nif len(self.__subject_cert)>0:\n    result = self.__construct_chain(self.__subject_cert[0])\n    if not result :\n        return False\n    else:\n        self.__chain_valid = True\n        return True\n    #continue while there is cert object to check\n                  \nelse:#if all of our certs are CA which is notvery wise but ...\n    cert_index = self.__find_starting_index()\n    if cert_index == -1:\n        raise ChainValidationException(\"Chain can not be constructed no starting point found\")\n        return False\n    current_cert = self.__ca_cert_stack[cert_index]\n    result=self.__construct_chain(current_cert)\n    \n    if not result :\n        return False\n    else:\n        self.__chain_valid = True\n        return True", "path": "cert\\chain_manager.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"Format 0 is pem,format 1 is der \nExtracts the encrypted base64 text from certificate\"\"\"\n#print self.cert.verify()\n#print self.cert.check_ca()\n#get it according to its format\n", "func_signal": "def get_cert_text(self,format=0):\n", "code": "if format==0:\n    return self.cert.as_pem()\nelif format==1:\n    return self.cert.as_der()", "path": "cert\\cert.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nThat method should not be called directly \nThe purpose is to find a certificate which is not a signer\nso the way to do that is find a subject that doesnt match to\nthe issuer part of any of the self.__ca_cert_stack ... if find\nany just return the index otherwise return -1 for Failure\n\"\"\"\n\n#first pickup a subject \n", "func_signal": "def __find_starting_index(self):\n", "code": "for index,subject_pick_cert in enumerate(self.__ca_cert_stack):\n    subj = subject_pick_cert.person_info(\"subject\")\n    found = False\n    for sub_search_cert in self.__ca_cert_stack:\n\n        if sub_search_cert.person_info(\"issuer\") == subj:\n            #print \"Index is \",index\n            found = True\n            break\n    #yep that is our starting point\n    if found == False :\n        return index \n\nreturn -1", "path": "cert\\chain_manager.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nIt is all of the certs hash\n\"\"\"\n", "func_signal": "def cert_hash(self):\n", "code": "import string\nreturn string.lower(str(self.cert.get_fingerprint(\"sha1\")))", "path": "cert\\cert.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\" The method sets the cert with data from a buffer\nformat 0 is for pem and 1 for der\"\"\"\n", "func_signal": "def set_from_buf(self,certData,format=0):\n", "code": "self.cert=x.load_cert_string(certData)\nreturn True\n #to check if it was loaded", "path": "cert\\cert.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nCleares the data structures for maybe reusing the object for different\nobjects and places ...\n\"\"\"\n", "func_signal": "def __clear_structures(self):\n", "code": "self.__final_chain=[] #Will store the final CA stack\nself.__ca_cert_stack=[] #The latest chain with CA's in it #the root is the latest one\nself.__subject_cert = [] #that will be the latest one in the chain which is\n#not a CA\n    \n#sometimes we may need to know if the loaded certs are\n#controlled before ...\nself.__chain_valid = False", "path": "cert\\chain_manager.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nLoads a chain of certs from a dir\nOnly gets the .pem files from it\n\"\"\"\n", "func_signal": "def load_chain_dir(chain_dir):\n", "code": "chain_place=load_cert_from_dir(chain_dir,get_all=True)\nresult=chain_manager_factory(chain_place,X509ChainManager.X509_CERT)\nreturn result", "path": "cert\\cert_tools.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\" Checks if the certificate is still valid and is not expired...\"\"\"\n", "func_signal": "def is_valid(self):\n", "code": "self.expDate=str(self.cert.get_not_after())\n\n#Get rid off GMT thing\nself.expDate=string.replace(self.expDate, \"GMT\", \"\").strip()\nself.expDate=string.replace(self.expDate, \"UTC\", \"\").strip()\n#print self.expDate\n\n#Convert to a valid type for easy comparison\nself.expDate=strptime(self.expDate,\"%b %d %H:%M:%S %Y\")\n#print self.expDate\n\n#Get the current time in same format\ncurTime=strptime(strftime(\"%b %d %H:%M:%S %Y\"),\"%b %d %H:%M:%S %Y\")\n#print type(curTime)\n\nif self.expDate>curTime:\n    #print \"It is still valid\"\n    return True\nelse:\n    #print \"It expired\"\n    return False", "path": "cert\\cert.py", "repo_name": "makkalot/pysign", "stars": 5, "license": "None", "language": "python", "size": 204}
{"docstring": "\"\"\"\nReturn True if Records are the same, based on either UUIDs (if available) or title \n\"\"\"\n", "func_signal": "def is_corresponding(self, record):\n", "code": "if not self.uuid or not record.uuid:\n    return self.title == record.title\nreturn self.uuid == record.uuid", "path": "src\\loxodo\\vault.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nGenerate the SHA-256 value of a password after several rounds of stretching.\n\nThe algorithm is described in the following paper:\n[KEYSTRETCH Section 4.1] http://www.schneier.com/paper-low-entropy.pdf\n\"\"\"\n", "func_signal": "def _stretch_password(password, salt, iterations):\n", "code": "sha = hashlib.sha256()\nsha.update(password)\nsha.update(salt)\nstretched_password = sha.digest()\nfor dummy in range(iterations):\n    stretched_password = hashlib.sha256(stretched_password).digest()\nreturn stretched_password", "path": "src\\loxodo\\vault.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nStore contents of this Vault into a file.\n\"\"\"\n\n", "func_signal": "def write_to_file(self, filename, password):\n", "code": "assert type(password) != unicode\n\n_last_save = struct.pack(\"<L\", int(time.time()))\nself.header.raw_fields[0x04] = self.Field(0x04, len(_last_save), _last_save)\n_what_saved = \"Loxodo 0.0-git\".encode(\"utf_8\", \"replace\")\nself.header.raw_fields[0x06] = self.Field(0x06, len(_what_saved), _what_saved)\n\n# write to temporary file first\n(osfilehandle, tmpfilename) = tempfile.mkstemp('.part', os.path.basename(filename) + \".\", os.path.dirname(filename), text=False)\nfilehandle = os.fdopen(osfilehandle, \"wb\")\n\n# FIXME: choose new SALT, B1-B4, IV values on each file write? Conflicting Specs!\n\n# write boilerplate\n\nfilehandle.write(self.f_tag)\nfilehandle.write(self.f_salt)\nfilehandle.write(struct.pack(\"<L\", self.f_iter))\n\nstretched_password = self._stretch_password(password, self.f_salt, self.f_iter)\nself.f_sha_ps = hashlib.sha256(stretched_password).digest()\nfilehandle.write(self.f_sha_ps)\n\nfilehandle.write(self.f_b1)\nfilehandle.write(self.f_b2)\nfilehandle.write(self.f_b3)\nfilehandle.write(self.f_b4)\n\ncipher = TwofishECB(stretched_password)\nkey_k = cipher.decrypt(self.f_b1) + cipher.decrypt(self.f_b2)\nkey_l = cipher.decrypt(self.f_b3) + cipher.decrypt(self.f_b4)\n\nfilehandle.write(self.f_iv)\n\nhmac_checker = HMAC(key_l, \"\", hashlib.sha256)\ncipher = TwofishCBC(key_k, self.f_iv)\n\nend_of_record = self.Field(0xff, 0, \"\")\n\nfor field in self.header.raw_fields.values():\n    self._write_field_tlv(filehandle, cipher, field)\n    hmac_checker.update(field.raw_value)\nself._write_field_tlv(filehandle, cipher, end_of_record)\nhmac_checker.update(end_of_record.raw_value)\n\nfor record in self.records:\n    for field in record.raw_fields.values():\n        self._write_field_tlv(filehandle, cipher, field)\n        hmac_checker.update(field.raw_value)\n    self._write_field_tlv(filehandle, cipher, end_of_record)\n    hmac_checker.update(end_of_record.raw_value)\n\nself._write_field_tlv(filehandle, cipher, None)\n\nself.f_hmac = hmac_checker.digest()\nfilehandle.write(self.f_hmac)\nfilehandle.close()\n\ntry:\n    tmpvault = Vault(password, filename=tmpfilename)\nexcept RuntimeError:\n    os.remove(tmpfilename)\n    raise self.VaultFormatError(\"File integrity check failed\")\n\n# after writing the temporary file, replace the original file with it\ntry:\n    os.remove(filename)\nexcept OSError:\n    pass\nos.rename(tmpfilename, filename)", "path": "src\\loxodo\\vault.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "# update vault\n", "func_signal": "def addRecord_(self, record):\n", "code": "self.vault.records.append(record)\n\n# update outline view\ngroupNode = self._getGroupNodeWithTitle_(record._get_group())\nif not groupNode:\n    # create new group\n    groupNode = RecordGroupNode.alloc().initWithTitle_(record._get_group())\n    self.nodes[groupNode] = []\nnewNode = RecordNode.alloc().initWithRecord_(record)\nself.nodes[groupNode].append(newNode)\nself._sortNodes()\nself._refreshOutlineViewAndFocusRecordNode_(newNode)\n\nself.document.updateChangeCount_(NSChangeDone)", "path": "src\\PCDataSource.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nDecrypt the given string using Twofish CBC.\n\"\"\"\n", "func_signal": "def decrypt(self, ciphertext):\n", "code": "if len(ciphertext) % 16:\n    raise RuntimeError(\"Twofish ciphertext length must be a multiple of 16\")\nplaintext = \"\"\nwhile len(ciphertext) >= 16:\n    block = ciphertext[0:16]\n    plaintext += self._xor_block(self.twofish.decrypt(block), self.state)\n    ciphertext = ciphertext[16:]\n    self.state = block\nreturn plaintext", "path": "src\\loxodo\\twofish\\twofish_cbc.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nSet the key to be used for en-/de-cryption and optionally specify an initialization vector (aka seed/salt).\n\"\"\"\n", "func_signal": "def __init__(self, key, init_vec=0):\n", "code": "self.twofish = twofish.Twofish()\nself.twofish.set_key(key)\nself.state = init_vec", "path": "src\\loxodo\\twofish\\twofish_cbc.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "# update vault\n", "func_signal": "def removeRecord_(self, record):\n", "code": "self.vault.records.remove(record)\n\n# update outline view\ngroupNode = None\nfor g in self.nodes.keys():\n    if record._get_group() == g._get_title():\n        groupNode = g\n        break\nif len(self.nodes[groupNode]) == 1:\n    # the only element in this group -> remove group completely\n    self.nodes.pop(groupNode)\nelse:\n    for r in self.nodes[groupNode]:\n        if r.record._get_uuid() == record._get_uuid():\n            self.nodes[groupNode].remove(r)\n            break\nself._refreshOutlineView()\n\nself.document.updateChangeCount_(NSChangeDone)", "path": "src\\PCDataSource.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nReturn the bitwise xor of two arbitrary-length blocks of data\n\"\"\"\n", "func_signal": "def _xor_block(text1, text2):\n", "code": "return \"\".join(\n               map(\n                   lambda c1, c2: chr(operator.xor(ord(c1), ord(c2))),\n                   text1,\n                   text2\n                   )\n               )", "path": "src\\loxodo\\twofish\\twofish_cbc.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nMerge in fields from another Record, replacing existing ones\n\"\"\"\n", "func_signal": "def merge(self, record):\n", "code": "self.raw_fields = {}\nfor field in record.raw_fields.values():\n    self.add_raw_field(field)", "path": "src\\loxodo\\vault.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nWrite one field of a vault record using the given file handle.\n\"\"\"\n", "func_signal": "def _write_field_tlv(self, filehandle, cipher, field):\n", "code": "if (field is None):\n    filehandle.write(\"PWS3-EOFPWS3-EOF\")\n    return\n\nassert len(field.raw_value) == field.raw_len\n\nraw_len = struct.pack(\"<L\", field.raw_len)\nraw_type = struct.pack(\"<B\", field.raw_type)\nraw_value = field.raw_value\n\n# Assemble TLV block and pad to 16-byte boundary\ndata = raw_len + raw_type + raw_value\nif (len(data) % 16 != 0):\n    pad_count = 16 - (len(data) % 16)\n    data += self._urandom(pad_count)\n\ndata = cipher.encrypt(data)\n\nfilehandle.write(data)", "path": "src\\loxodo\\vault.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nReturn one field of a vault record by reading from the given file handle.\n\"\"\"\n", "func_signal": "def _read_field_tlv(self, filehandle, cipher):\n", "code": "data = filehandle.read(16)\nif (not data) or (len(data) < 16):\n    raise self.VaultFormatError(\"EOF encountered when parsing record field\")\nif data == \"PWS3-EOFPWS3-EOF\":\n    return None\ndata = cipher.decrypt(data)\nraw_len = struct.unpack(\"<L\", data[0:4])[0]\nraw_type = struct.unpack(\"<B\", data[4])[0]\nraw_value = data[5:]\nif (raw_len > 11):\n    for dummy in range((raw_len+4)//16):\n        data = filehandle.read(16)\n        if (not data) or (len(data) < 16):\n            raise self.VaultFormatError(\"EOF encountered when parsing record field\")\n        raw_value += cipher.decrypt(data)\nraw_value = raw_value[:raw_len]\nreturn self.Field(raw_type, raw_len, raw_value)", "path": "src\\loxodo\\vault.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nDecrypt the given string using Twofish ECB.\n\"\"\"\n", "func_signal": "def decrypt(self, ciphertext):\n", "code": "if len(ciphertext) % 16:\n    raise RuntimeError(\"Twofish ciphertext length must be a multiple of 16\")\nplaintext = \"\"\nwhile len(ciphertext) >= 16:\n    plaintext += self.twofish.decrypt(ciphertext[0:16])\n    ciphertext = ciphertext[16:]\nreturn plaintext", "path": "src\\loxodo\\twofish\\twofish_ecb.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "# update outline view\n", "func_signal": "def recordUpdated_(self, record):\n", "code": "recordNode, groupNode = self._getRecordAndGroupNodeForUUID_(record._get_uuid())\nif record._get_group() != groupNode._get_title():\n    # group changed\n    if len(self.nodes[groupNode]) == 1:\n        # the only element in this group -> remove group completely\n        self.nodes.pop(groupNode)\n    else:\n        self.nodes[groupNode].remove(recordNode)\n    # check if new group exists\n    groupNode = self._getGroupNodeWithTitle_(record._get_group())\n    if not groupNode:\n        # create new group\n        groupNode = RecordGroupNode.alloc().initWithTitle_(record._get_group())\n        self.nodes[groupNode] = []\n    recordNode = RecordNode.alloc().initWithRecord_(record)\n    self.nodes[groupNode].append(recordNode)\nself._sortNodes()\nself._refreshOutlineViewAndFocusRecordNode_(recordNode)\nself.document.updateInfo()\n\nself.document.updateChangeCount_(NSChangeDone)", "path": "src\\PCDataSource.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"Twofish.\"\"\"\n\n", "func_signal": "def __init__(self, key=None):\n", "code": "if key:\n    self.set_key(key)", "path": "src\\loxodo\\twofish\\twofish.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nEncrypt the given string using Twofish ECB.\n\"\"\"\n", "func_signal": "def encrypt(self, plaintext):\n", "code": "if len(plaintext) % 16:\n    raise RuntimeError(\"Twofish plaintext length must be a multiple of 16\")\nciphertext = \"\"\nwhile len(plaintext) >= 16:\n    ciphertext += self.twofish.encrypt(plaintext[0:16])\n    plaintext = plaintext[16:]\nreturn ciphertext", "path": "src\\loxodo\\twofish\\twofish_ecb.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nEncrypt the given string using Twofish CBC.\n\"\"\"\n", "func_signal": "def encrypt(self, plaintext):\n", "code": "if len(plaintext) % 16:\n    raise RuntimeError(\"Twofish ciphertext length must be a multiple of 16\")\nciphertext = \"\"\nwhile len(plaintext) >= 16:\n    block = self.twofish.encrypt(self._xor_block(plaintext[0:16], self.state))\n    ciphertext += block\n    plaintext = plaintext[16:]\n    self.state = block\nreturn ciphertext", "path": "src\\loxodo\\twofish\\twofish_cbc.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nInitialize all class members by loading the contents of a Vault stored in the given file.\n\"\"\"\n\n", "func_signal": "def _read_from_file(self, filename, password):\n", "code": "assert type(password) != unicode\n\nfilehandle = file(filename, 'rb')\n\n# read boilerplate\n\nself.f_tag = filehandle.read(4)  # TAG: magic tag\nif (self.f_tag != 'PWS3'):\n    raise self.VaultVersionError(\"Not a PasswordSafe V3 file\")\n\nself.f_salt = filehandle.read(32)  # SALT: SHA-256 salt\nself.f_iter = struct.unpack(\"<L\", filehandle.read(4))[0]  # ITER: SHA-256 keystretch iterations\nstretched_password = self._stretch_password(password, self.f_salt, self.f_iter)  # P': the stretched key\nmy_sha_ps = hashlib.sha256(stretched_password).digest()\n\nself.f_sha_ps = filehandle.read(32) # H(P'): SHA-256 hash of stretched passphrase\nif (self.f_sha_ps != my_sha_ps):\n    raise self.BadPasswordError(\"Wrong password\")\n\nself.f_b1 = filehandle.read(16)  # B1\nself.f_b2 = filehandle.read(16)  # B2\nself.f_b3 = filehandle.read(16)  # B3\nself.f_b4 = filehandle.read(16)  # B4\n\ncipher = TwofishECB(stretched_password)\nkey_k = cipher.decrypt(self.f_b1) + cipher.decrypt(self.f_b2)\nkey_l = cipher.decrypt(self.f_b3) + cipher.decrypt(self.f_b4)\n\nself.f_iv = filehandle.read(16)  # IV: initialization vector of Twofish CBC\n\nhmac_checker = HMAC(key_l, \"\", hashlib.sha256)\ncipher = TwofishCBC(key_k, self.f_iv)\n\n# read header\n\nwhile (True):\n    field = self._read_field_tlv(filehandle, cipher)\n    if not field:\n        break\n    if field.raw_type == 0xff:\n        break\n    self.header.add_raw_field(field)\n    hmac_checker.update(field.raw_value)\n\n# read fields\n\ncurrent_record = self.Record()\nwhile (True):\n    field = self._read_field_tlv(filehandle, cipher)\n    if not field:\n        break\n    if field.raw_type == 0xff:\n        self.records.append(current_record)\n        current_record = self.Record()\n    else:\n        hmac_checker.update(field.raw_value)\n        current_record.add_raw_field(field)\n\n\n# read HMAC\n\nself.f_hmac = filehandle.read(32)  # HMAC: used to verify Vault's integrity\n\nmy_hmac = hmac_checker.digest()\nif (self.f_hmac != my_hmac):\n    raise self.VaultFormatError(\"File integrity check failed\")\n\nfilehandle.close()", "path": "src\\loxodo\\vault.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "# sort vault records by group then by title\n", "func_signal": "def _sortVaultRecords(self):\n", "code": "def comp(r1, r2):\n    if r1._get_group() == r2._get_group():\n        return cmp(r1._get_title(), r2._get_title())\n    return cmp(r1._get_group(), r2._get_group())\n\nself.vault.records.sort(comp)", "path": "src\\PCDataSource.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nConverts text to URL if possible.\n\nCopyright (c) Django Software Foundation and individual contributors.\nAll rights reserved.\nModified by Mathis Hofer.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n    1. Redistributions of source code must retain the above copyright notice, \n       this list of conditions and the following disclaimer.\n    \n    2. Redistributions in binary form must reproduce the above copyright \n       notice, this list of conditions and the following disclaimer in the\n       documentation and/or other materials provided with the distribution.\n\n    3. Neither the name of Django nor the names of its contributors may be used\n       to endorse or promote products derived from this software without\n       specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n", "func_signal": "def urlize(text):\n", "code": "LEADING_PUNCTUATION  = ['(', '<', '&lt;']\nTRAILING_PUNCTUATION = ['.', ',', ')', '>', '\\n', '&gt;']\npunctuation_re = re.compile('^(?P<lead>(?:%s)*)(?P<middle>.*?)(?P<trail>(?:%s)*)$' % \\\n    ('|'.join([re.escape(x) for x in LEADING_PUNCTUATION]),\n    '|'.join([re.escape(x) for x in TRAILING_PUNCTUATION])))\nsimple_email_re = re.compile(r'^\\S+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9._-]+$')\n\ntext = text.strip()\nmatch = None\nif '.' in text or '@' in text or ':' in text:\n    match = punctuation_re.match(text)\nif match:\n    lead, middle, trail = match.groups()\n    # Make URL we want to point to.\n    url = None\n    if middle.startswith('http://') or middle.startswith('https://'):\n        url = urllib.quote(middle, safe='/&=:;#?+*')\n    elif middle.startswith('www.') or ('@' not in middle and \\\n            middle and middle[0] in string.ascii_letters + string.digits and \\\n            (middle.endswith('.org') or middle.endswith('.net') or middle.endswith('.com'))):\n        url = urllib.quote('http://%s' % middle, safe='/&=:;#?+*')\n    elif '@' in middle and not ':' in middle and simple_email_re.match(middle):\n        url = 'mailto:%s' % middle\n    return url\nreturn text", "path": "src\\util.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"\nSet the key to be used for en-/de-cryption.\n\"\"\"\n", "func_signal": "def __init__(self, key):\n", "code": "self.twofish = twofish.Twofish()\nself.twofish.set_key(key)", "path": "src\\loxodo\\twofish\\twofish_ecb.py", "repo_name": "hupf/passwordchest", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 5963}
{"docstring": "\"\"\"Return a new environ dict targeting the given wsgi.version\"\"\"\n", "func_signal": "def get_environ(self):\n", "code": "req = self.req\nenv_10 = WSGIGateway_10.get_environ(self)\nenv = dict([(k.decode('ISO-8859-1'), v) for k, v in env_10.iteritems()])\nenv[u'wsgi.version'] = ('u', 0)\n\n# Request-URI\nenv.setdefault(u'wsgi.url_encoding', u'utf-8')\ntry:\n    for key in [u\"PATH_INFO\", u\"SCRIPT_NAME\", u\"QUERY_STRING\"]:\n        env[key] = env_10[str(key)].decode(env[u'wsgi.url_encoding'])\nexcept UnicodeDecodeError:\n    # Fall back to latin 1 so apps can transcode if needed.\n    env[u'wsgi.url_encoding'] = u'ISO-8859-1'\n    for key in [u\"PATH_INFO\", u\"SCRIPT_NAME\", u\"QUERY_STRING\"]:\n        env[key] = env_10[str(key)].decode(env[u'wsgi.url_encoding'])\n\nfor k, v in sorted(env.items()):\n    if isinstance(v, str) and k not in ('REQUEST_URI', 'wsgi.input'):\n        env[k] = v.decode('ISO-8859-1')\n\nreturn env", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Mark the given socket fd as non-inheritable (Windows).\"\"\"\n", "func_signal": "def prevent_socket_inheritance(sock):\n", "code": "if not windll.kernel32.SetHandleInformation(sock.fileno(), 1, 0):\n    raise WinError()", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Read headers from the given stream into the given header dict.\n\nIf hdict is None, a new header dict is created. Returns the populated\nheader dict.\n\nHeaders which are repeated are folded together using a comma if their\nspecification so dictates.\n\nThis function raises ValueError when the read bytes violate the HTTP spec.\nYou should probably return \"400 Bad Request\" if this happens.\n\"\"\"\n", "func_signal": "def read_headers(rfile, hdict=None):\n", "code": "if hdict is None:\n    hdict = {}\n\nwhile True:\n    line = rfile.readline()\n    if not line:\n        # No more data--illegal end of headers\n        raise ValueError(\"Illegal end of headers.\")\n    \n    if line == CRLF:\n        # Normal end of headers\n        break\n    if not line.endswith(CRLF):\n        raise ValueError(\"HTTP requires CRLF terminators\")\n    \n    if line[0] in ' \\t':\n        # It's a continuation line.\n        v = line.strip()\n    else:\n        try:\n            k, v = line.split(\":\", 1)\n        except ValueError:\n            raise ValueError(\"Illegal header line.\")\n        # TODO: what about TE and WWW-Authenticate?\n        k = k.strip().title()\n        v = v.strip()\n        hname = k\n    \n    if k in comma_separated_headers:\n        existing = hdict.get(hname)\n        if existing:\n            v = \", \".join((existing, v))\n    hdict[hname] = v\n\nreturn hdict", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "# Use max, disallow tiny reads in a loop as they are very inefficient.\n# We never leave read() with any leftover data from a new recv() call\n# in our internal buffer.\n", "func_signal": "def read(self, size=-1):\n", "code": "rbufsize = max(self._rbufsize, self.default_bufsize)\n# Our use of StringIO rather than lists of string objects returned by\n# recv() minimizes memory usage and fragmentation that occurs when\n# rbufsize is large compared to the typical return value of recv().\nbuf = self._rbuf\nbuf.seek(0, 2)  # seek end\nif size < 0:\n    # Read until EOF\n    self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.\n    while True:\n        data = self.recv(rbufsize)\n        if not data:\n            break\n        buf.write(data)\n    return buf.getvalue()\nelse:\n    # Read until size bytes or EOF seen, whichever comes first\n    buf_len = buf.tell()\n    if buf_len >= size:\n        # Already have size bytes in our buffer?  Extract and return.\n        buf.seek(0)\n        rv = buf.read(size)\n        self._rbuf = StringIO.StringIO()\n        self._rbuf.write(buf.read())\n        return rv\n\n    self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.\n    while True:\n        left = size - buf_len\n        # recv() will malloc the amount of memory given as its\n        # parameter even though it often returns much less data\n        # than that.  The returned data string is short lived\n        # as we copy it into a StringIO and free it.  This avoids\n        # fragmentation issues on many platforms.\n        data = self.recv(left)\n        if not data:\n            break\n        n = len(data)\n        if n == size and not buf_len:\n            # Shortcut.  Avoid buffer data copies when:\n            # - We have no data in our buffer.\n            # AND\n            # - Our call to recv returned exactly the\n            #   number of bytes we were asked to read.\n            return data\n        if n == left:\n            buf.write(data)\n            del data  # explicit free\n            break\n        assert n <= left, \"recv(%d) returned %d bytes\" % (left, n)\n        buf.write(data)\n        buf_len += n\n        del data  # explicit free\n        #assert buf_len == buf.tell()\n    return buf.getvalue()", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Start the pool of threads.\"\"\"\n", "func_signal": "def start(self):\n", "code": "for i in range(self.min):\n    self._threads.append(WorkerThread(self.server))\nfor worker in self._threads:\n    worker.setName(\"CP Server \" + worker.getName())\n    worker.start()\nfor worker in self._threads:\n    while not worker.ready:\n        time.sleep(.1)", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "# Shamelessly stolen from StringIO\n", "func_signal": "def readlines(self, sizehint=0):\n", "code": "total = 0\nlines = []\nline = self.readline(sizehint)\nwhile line:\n    lines.append(line)\n    total += len(line)\n    if 0 < sizehint <= total:\n        break\n    line = self.readline(sizehint)\nreturn lines", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Spawn new worker threads (not above self.max).\"\"\"\n", "func_signal": "def grow(self, amount):\n", "code": "for i in range(amount):\n    if self.max > 0 and len(self._threads) >= self.max:\n        break\n    worker = WorkerThread(self.server)\n    worker.setName(\"CP Server \" + worker.getName())\n    self._threads.append(worker)\n    worker.start()", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Mark the given socket fd as non-inheritable (POSIX).\"\"\"\n", "func_signal": "def prevent_socket_inheritance(sock):\n", "code": "fd = sock.fileno()\nold_flags = fcntl.fcntl(fd, fcntl.F_GETFD)\nfcntl.fcntl(fd, fcntl.F_SETFD, old_flags | fcntl.FD_CLOEXEC)", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Return a new environ dict targeting the given wsgi.version\"\"\"\n", "func_signal": "def get_environ(self):\n", "code": "req = self.req\nenv = {\n    # set a non-standard environ entry so the WSGI app can know what\n    # the *real* server protocol is (and what features to support).\n    # See http://www.faqs.org/rfcs/rfc2145.html.\n    'ACTUAL_SERVER_PROTOCOL': req.server.protocol,\n    'PATH_INFO': req.path,\n    'QUERY_STRING': req.qs,\n    'REMOTE_ADDR': req.conn.remote_addr or '',\n    'REMOTE_PORT': str(req.conn.remote_port or ''),\n    'REQUEST_METHOD': req.method,\n    'REQUEST_URI': req.uri,\n    'SCRIPT_NAME': '',\n    'SERVER_NAME': req.server.server_name,\n    # Bah. \"SERVER_PROTOCOL\" is actually the REQUEST protocol.\n    'SERVER_PROTOCOL': req.request_protocol,\n    'wsgi.errors': sys.stderr,\n    'wsgi.input': req.rfile,\n    'wsgi.multiprocess': False,\n    'wsgi.multithread': True,\n    'wsgi.run_once': False,\n    'wsgi.url_scheme': req.scheme,\n    'wsgi.version': (1, 0),\n    }\n\nif isinstance(req.server.bind_addr, basestring):\n    # AF_UNIX. This isn't really allowed by WSGI, which doesn't\n    # address unix domain sockets. But it's better than nothing.\n    env[\"SERVER_PORT\"] = \"\"\nelse:\n    env[\"SERVER_PORT\"] = str(req.server.bind_addr[1])\n\n# CONTENT_TYPE/CONTENT_LENGTH\nfor k, v in req.inheaders.iteritems():\n    env[\"HTTP_\" + k.upper().replace(\"-\", \"_\")] = v\nct = env.pop(\"HTTP_CONTENT_TYPE\", None)\nif ct is not None:\n    env[\"CONTENT_TYPE\"] = ct\ncl = env.pop(\"HTTP_CONTENT_LENGTH\", None)\nif cl is not None:\n    env[\"CONTENT_LENGTH\"] = cl\n\nif req.conn.ssl_env:\n    env.update(req.conn.ssl_env)\n\nreturn env", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Kill off worker threads (not below self.min).\"\"\"\n# Grow/shrink the pool if necessary.\n# Remove any dead threads from our list\n", "func_signal": "def shrink(self, amount):\n", "code": "for t in self._threads:\n    if not t.isAlive():\n        self._threads.remove(t)\n        amount -= 1\n\nif amount > 0:\n    for i in range(min(amount, len(self._threads) - self.min)):\n        # Put a number of shutdown requests on the queue equal\n        # to 'amount'. Once each of those is processed by a worker,\n        # that worker will terminate and be culled from our list\n        # in self.put.\n        self._queue.put(_SHUTDOWNREQUEST)", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Parse the next HTTP request start-line and message-headers.\"\"\"\n", "func_signal": "def parse_request(self):\n", "code": "self.rfile = SizeCheckWrapper(self.conn.rfile,\n                              self.server.max_request_header_size)\ntry:\n    self._parse_request()\nexcept MaxSizeExceeded:\n    self.simple_response(\"413 Request Entity Too Large\")\n    return", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"WSGI callable to write unbuffered data to the client.\n\nThis method is also used internally by start_response (to write\ndata from the iterable returned by the WSGI application).\n\"\"\"\n", "func_signal": "def write(self, chunk):\n", "code": "if not self.started_response:\n    raise AssertionError(\"WSGI write called before start_response.\")\n\nif not self.req.sent_headers:\n    self.req.sent_headers = True\n    self.req.send_headers()\n\nself.req.write(chunk)", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "# Shamelessly stolen from StringIO\n", "func_signal": "def readlines(self, sizehint=0):\n", "code": "total = 0\nlines = []\nline = self.readline()\nwhile line:\n    lines.append(line)\n    total += len(line)\n    if 0 < sizehint <= total:\n        break\n    line = self.readline()\nreturn lines", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "# Shamelessly stolen from StringIO\n", "func_signal": "def readlines(self, sizehint=0):\n", "code": "total = 0\nlines = []\nline = self.readline(sizehint)\nwhile line:\n    lines.append(line)\n    total += len(line)\n    if 0 < sizehint <= total:\n        break\n    line = self.readline(sizehint)\nreturn lines", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Call the gateway and write its iterable output.\"\"\"\n", "func_signal": "def respond(self):\n", "code": "mrbs = self.server.max_request_body_size\nif self.chunked_read:\n    self.rfile = ChunkedRFile(self.conn.rfile, mrbs)\nelse:\n    cl = int(self.inheaders.get(\"Content-Length\", 0))\n    if mrbs and mrbs < cl:\n        if not self.sent_headers:\n            self.simple_response(\"413 Request Entity Too Large\")\n        return\n    self.rfile = KnownLengthRFile(self.conn.rfile, cl)\n\nself.server.gateway(self).respond()\n\nif (self.ready and not self.sent_headers):\n    self.sent_headers = True\n    self.send_headers()\nif self.chunked_write:\n    self.conn.wfile.sendall(\"0\\r\\n\\r\\n\")", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Write unbuffered data to the client.\"\"\"\n", "func_signal": "def write(self, chunk):\n", "code": "if self.chunked_write and chunk:\n    buf = [hex(len(chunk))[2:], CRLF, chunk, CRLF]\n    self.conn.wfile.sendall(\"\".join(buf))\nelse:\n    self.conn.wfile.sendall(chunk)", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Close the socket underlying this connection.\"\"\"\n", "func_signal": "def close(self):\n", "code": "self.rfile.close()\n\nif not self.linger:\n    # Python's socket module does NOT call close on the kernel socket\n    # when you call socket.close(). We do so manually here because we\n    # want this server to send a FIN TCP segment immediately. Note this\n    # must be called *before* calling socket.close(), because the latter\n    # drops its reference to the kernel socket.\n    if hasattr(self.socket, '_sock'):\n        self.socket._sock.close()\n    self.socket.close()\nelse:\n    # On the other hand, sometimes we want to hang around for a bit\n    # to make sure the client has a chance to read our entire\n    # response. Skipping the close() calls here delays the FIN\n    # packet until the socket object is garbage-collected later.\n    # Someday, perhaps, we'll do the full lingering_close that\n    # Apache does, but not today.\n    pass", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Parse a Request-URI into (scheme, authority, path).\n\nNote that Request-URI's must be one of:\n    \n    Request-URI    = \"*\" | absoluteURI | abs_path | authority\n\nTherefore, a Request-URI which starts with a double forward-slash\ncannot be a \"net_path\":\n\n    net_path      = \"//\" authority [ abs_path ]\n\nInstead, it must be interpreted as an \"abs_path\" with an empty first\npath segment:\n\n    abs_path      = \"/\"  path_segments\n    path_segments = segment *( \"/\" segment )\n    segment       = *pchar *( \";\" param )\n    param         = *pchar\n\"\"\"\n", "func_signal": "def parse_request_uri(self, uri):\n", "code": "if uri == \"*\":\n    return None, None, uri\n\ni = uri.find('://')\nif i > 0 and '?' not in uri[:i]:\n    # An absoluteURI.\n    # If there's a scheme (and it must be http or https), then:\n    # http_URL = \"http:\" \"//\" host [ \":\" port ] [ abs_path [ \"?\" query ]]\n    scheme, remainder = uri[:i].lower(), uri[i + 3:]\n    authority, path = remainder.split(\"/\", 1)\n    return scheme, authority, path\n\nif uri.startswith('/'):\n    # An abs_path.\n    return None, None, uri\nelse:\n    # An authority.\n    return None, uri, None", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Assert, process, and send the HTTP response message-headers.\n\nYou must set self.status, and self.outheaders before calling this.\n\"\"\"\n", "func_signal": "def send_headers(self):\n", "code": "hkeys = [key.lower() for key, value in self.outheaders]\nstatus = int(self.status[:3])\n\nif status == 413:\n    # Request Entity Too Large. Close conn to avoid garbage.\n    self.close_connection = True\nelif \"content-length\" not in hkeys:\n    # \"All 1xx (informational), 204 (no content),\n    # and 304 (not modified) responses MUST NOT\n    # include a message-body.\" So no point chunking.\n    if status < 200 or status in (204, 205, 304):\n        pass\n    else:\n        if (self.response_protocol == 'HTTP/1.1'\n            and self.method != 'HEAD'):\n            # Use the chunked transfer-coding\n            self.chunked_write = True\n            self.outheaders.append((\"Transfer-Encoding\", \"chunked\"))\n        else:\n            # Closing the conn is the only way to determine len.\n            self.close_connection = True\n\nif \"connection\" not in hkeys:\n    if self.response_protocol == 'HTTP/1.1':\n        # Both server and client are HTTP/1.1 or better\n        if self.close_connection:\n            self.outheaders.append((\"Connection\", \"close\"))\n    else:\n        # Server and/or client are HTTP/1.0\n        if not self.close_connection:\n            self.outheaders.append((\"Connection\", \"Keep-Alive\"))\n\nif (not self.close_connection) and (not self.chunked_read):\n    # Read any remaining request body data on the socket.\n    # \"If an origin server receives a request that does not include an\n    # Expect request-header field with the \"100-continue\" expectation,\n    # the request includes a request body, and the server responds\n    # with a final status code before reading the entire request body\n    # from the transport connection, then the server SHOULD NOT close\n    # the transport connection until it has read the entire request,\n    # or until the client closes the connection. Otherwise, the client\n    # might not reliably receive the response message. However, this\n    # requirement is not be construed as preventing a server from\n    # defending itself against denial-of-service attacks, or from\n    # badly broken client implementations.\"\n    remaining = getattr(self.rfile, 'remaining', 0)\n    if remaining > 0:\n        self.rfile.read(remaining)\n\nif \"date\" not in hkeys:\n    self.outheaders.append((\"Date\", rfc822.formatdate()))\n\nif \"server\" not in hkeys:\n    self.outheaders.append((\"Server\", self.server.server_name))\n\nbuf = [self.server.protocol + \" \" + self.status + CRLF]\nfor k, v in self.outheaders:\n    buf.append(k + \": \" + v + CRLF)\nbuf.append(CRLF)\nself.conn.wfile.sendall(\"\".join(buf))", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Return error numbers for all errors in errnames on this platform.\n\nThe 'errno' module contains different global constants depending on\nthe specific platform (OS). This function will return the list of\nnumeric values for a given list of potential names.\n\"\"\"\n", "func_signal": "def plat_specific_errors(*errnames):\n", "code": "errno_names = dir(errno)\nnums = [getattr(errno, k) for k in errnames if k in errno_names]\n# de-dupe the list\nreturn dict.fromkeys(nums).keys()", "path": "snow\\wsgiserver.py", "repo_name": "brosner/snow", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 124}
{"docstring": "\"\"\"Like getattr(), but can go down a hierarchy like 'attr.subattr'\"\"\"\n", "func_signal": "def getattr_by_path(obj, attr, *default):\n", "code": "value = obj\nfor part in attr.split('.'):\n    if not hasattr(value, part) and len(default):\n        return default[0]\n    value = getattr(value, part)\n    if callable(value):\n        value = value()\nreturn value", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"\nConverts objects to table-style list of rows with heading:\n\nExample:\nx.a = 1\nx.b = 2\nx.c = 3\ny.a = 11\ny.b = 12\ny.c = 13\nobject_list_to_table(('a', 'b', 'c'), [x, y])\nresults in the following (dict keys reordered for better readability):\n[\n    ('a', 'b', 'c'),\n    (1, 2, 3),\n    (11, 12, 13),\n]\n\"\"\"\n", "func_signal": "def object_list_to_table(headings, dict_list):\n", "code": "return [headings] + [tuple([getattr_by_path(row, heading, None)\n                            for heading in headings])\n                     for row in dict_list]", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"Returns a subset of the keys of a dictionary.\"\"\"\n", "func_signal": "def subdict(data, *attrs):\n", "code": "result = {}\nresult.update([(key, data[key]) for key in attrs])\nreturn result", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"\nRemove expired instances of ``RegistrationProfile`` and their\nassociated ``User``s.\n\nAccounts to be deleted are identified by searching for\ninstances of ``RegistrationProfile`` with expired activation\nkeys, and then checking to see if their associated ``User``\ninstances have the field ``is_active`` set to ``False``; any\n``User`` who is both inactive and has an expired activation\nkey will be deleted.\n\nIt is recommended that this method be executed regularly as\npart of your routine site maintenance; this application\nprovides a custom management command which will call this\nmethod, accessible as ``manage.py cleanupregistration``.\n\nRegularly clearing out accounts which have never been\nactivated serves two useful purposes:\n\n1. It alleviates the ocasional need to reset a\n   ``RegistrationProfile`` and/or re-send an activation email\n   when a user does not receive or does not act upon the\n   initial activation email; since the account will be\n   deleted, the user will be able to simply re-register and\n   receive a new activation key.\n\n2. It prevents the possibility of a malicious user registering\n   one or more accounts and never activating them (thus\n   denying the use of those usernames to anyone else); since\n   those accounts will be deleted, the usernames will become\n   available for use again.\n\nIf you have a troublesome ``User`` and wish to disable their\naccount while keeping it in the database, simply delete the\nassociated ``RegistrationProfile``; an inactive ``User`` which\ndoes not have an associated ``RegistrationProfile`` will not\nbe deleted.\n\n\"\"\"\n", "func_signal": "def delete_expired_users(self):\n", "code": "for profile in RegistrationProfile.all():\n    if profile.activation_key_expired():\n        user = profile.user\n        if not user.is_active:\n            user.delete()\n            profile.delete()", "path": "registration\\models.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"\nAppends '?' or '&' to an url, so you can easily add extra GET parameters.\n\"\"\"\n", "func_signal": "def urlquerybase(url):\n", "code": "if url:\n    if '?' in url:\n        url += '&'\n    else:\n        url += '?'\nreturn url", "path": "common\\appenginepatch\\ragendja\\templatetags\\ragendjatags.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"\nValidate an activation key and activate the corresponding\n``User`` if valid.\n\nIf the key is valid and has not expired, return the ``User``\nafter activating.\n\nIf the key is not valid or has expired, return ``False``.\n\nIf the key is valid but the ``User`` is already active,\nreturn ``False``.\n\nTo prevent reactivation of an account which has been\ndeactivated by site administrators, the activation key is\nreset to the string constant ``RegistrationProfile.ACTIVATED``\nafter successful activation.\n\nTo execute customized logic when a ``User`` is activated,\nconnect a function to the signal\n``registration.signals.user_activated``; this signal will be\nsent (with the ``User`` as the value of the keyword argument\n``user``) after a successful activation.\n\n\"\"\"\n", "func_signal": "def activate_user(self, activation_key):\n", "code": "from registration.signals import user_activated\n\n# Make sure the key we're trying conforms to the pattern of a\n# SHA1 hash; if it doesn't, no point trying to look it up in\n# the database.\nif SHA1_RE.search(activation_key):\n    profile = RegistrationProfile.get_by_key_name(\"key_\"+activation_key)\n    if not profile:\n        return False\n    if not profile.activation_key_expired():\n        user = profile.user\n        user.is_active = True\n        user.put()\n        profile.activation_key = RegistrationProfile.ACTIVATED\n        profile.put()\n        user_activated.send(sender=self.model, user=user)\n        return user\nreturn False", "path": "registration\\models.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"\nDetermine whether this ``RegistrationProfile``'s activation\nkey has expired, returning a boolean -- ``True`` if the key\nhas expired.\n\nKey expiration is determined by a two-step process:\n\n1. If the user has already activated, the key will have been\n   reset to the string constant ``ACTIVATED``. Re-activating\n   is not permitted, and so this method returns ``True`` in\n   this case.\n\n2. Otherwise, the date the user signed up is incremented by\n   the number of days specified in the setting\n   ``ACCOUNT_ACTIVATION_DAYS`` (which should be the number of\n   days after signup during which a user is allowed to\n   activate their account); if the result is less than or\n   equal to the current date, the key has expired and this\n   method returns ``True``.\n\n\"\"\"\n", "func_signal": "def activation_key_expired(self):\n", "code": "expiration_date = datetime.timedelta(days=settings.ACCOUNT_ACTIVATION_DAYS)\nreturn self.activation_key == RegistrationProfile.ACTIVATED or \\\n       (self.user.date_joined + expiration_date <= datetime.datetime.now())", "path": "registration\\models.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"\nCreate a ``RegistrationProfile`` for a given\n``User``, and return the ``RegistrationProfile``.\n\nThe activation key for the ``RegistrationProfile`` will be a\nSHA1 hash, generated from a combination of the ``User``'s\nusername and a random salt.\n\n\"\"\"\n", "func_signal": "def create_profile(self, user):\n", "code": "salt = sha.new(str(random.random())).hexdigest()[:5]\nactivation_key = sha.new(salt+user.username).hexdigest()", "path": "registration\\models.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "# Add username to POST data, so it gets set in the created model\n# You could also use a hidden form field for example, but this is more secure\n", "func_signal": "def create_entry(request):\n", "code": "request.POST = request.POST.copy()\nrequest.POST['author'] = str(request.user.key())\nreturn create_object(request, Greeting, post_save_redirect='/guestbook')", "path": "guestbook\\views.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"Creates a class-wide instance property with a thread-specific value.\"\"\"\n", "func_signal": "def make_tls_property(default=None):\n", "code": "class TLSProperty(object):\n    def __init__(self):\n        self.local = local()\n\n    def __get__(self, instance, cls):\n        if not instance:\n            return self\n        return self.value\n\n    def __set__(self, instance, value):\n        self.value = value\n\n    def _get_value(self):\n        return getattr(self.local, 'value', default)\n    def _set_value(self, value):\n        self.local.value = value\n    value = property(_get_value, _set_value)\n\nreturn TLSProperty()", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "# Allow for using a RegisterVars instance as a context processor\n", "func_signal": "def __call__(self, item=None, name=None):\n", "code": "if isinstance(item, HttpRequest):\n    return self\nif name is None and isinstance(item, basestring):\n    # @register('as_name') # first param (item) contains the name\n    name, item = item, name\nelif name is None and isinstance(item, dict):\n    # register(somedict)  or  register(othermodule.register)\n    return self.update(item)\nif item is None and isinstance(name, basestring):\n    # @register(name='as_name')\n    return lambda func: self(func, name)\nself[name or item.__name__] = item\nreturn item", "path": "common\\appenginepatch\\ragendja\\registervars.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"Returns paths of files that must be copied directly.\"\"\"\n# Some files types (MUST_COMBINE) never get copied.\n# They must always be combined.\n", "func_signal": "def get_copy_targets(media_dirs, **kwargs):\n", "code": "targets = {}\nfor app, media_dir in media_dirs.items():\n    for root, dirs, files in os.walk(media_dir):\n        for name in dirs:\n            if name.startswith('.'):\n                dirs.remove(name)\n        for file in files:\n            if file.startswith('.') or file.endswith(tuple(MUST_COMBINE)):\n                continue\n            path = os.path.abspath(os.path.join(root, file))\n            base = app + path[len(media_dir):]\n            targets[base.replace(os.sep, '/')] = path\nreturn targets", "path": "common\\appenginepatch\\mediautils\\generatemedia.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"\nCompares two lists and returs True if they contain the same elements, but\ndoesn't require that they have the same order.\n\"\"\"\n", "func_signal": "def equal_lists(left, right):\n", "code": "right = list(right)\nif len(left) != len(right):\n    return False\nfor item in left:\n    if item in right:\n        del right[right.index(item)]\n    else:\n        return False\nreturn True", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"Returns all files that must be combined.\"\"\"\n", "func_signal": "def get_targets(combine_media=settings.COMBINE_MEDIA, **kwargs):\n", "code": "targets = []\nfor target in sorted(combine_media.keys()):\n    group = combine_media[target]\n    if '.site_data.js' in group:\n        index = list(group).index('.site_data.js')\n        group = group[:index] + (site_data,) + group[index+1:]\n    group = tuple(group)\n    if '%(LANGUAGE_CODE)s' in target:\n        # This file uses i18n, so generate a separate file per language.\n        # The language data is always added before all other files.\n        for LANGUAGE_CODE in LANGUAGES:\n            data = kwargs.copy()\n            data['LANGUAGE_CODE'] = LANGUAGE_CODE\n            filename = target % data\n            data['target'] = filename\n            group = (lang_data,) + group\n            targets.append((filename, data, group))\n    elif '%(LANGUAGE_DIR)s' in target:\n        # Generate CSS files for both text directions\n        for LANGUAGE_DIR in ('ltr', 'rtl'):\n            data = kwargs.copy()\n            data['LANGUAGE_DIR'] = LANGUAGE_DIR\n            filename = target % data\n            data['target'] = filename\n            targets.append((filename, data, group))\n    else:\n        data = kwargs.copy()\n        filename = target % data\n        data['target'] = filename\n        targets.append((filename, data, group))\nreturn targets", "path": "common\\appenginepatch\\mediautils\\generatemedia.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"Provide site_data variable with settings (currently only MEDIA_URL).\"\"\"\n", "func_signal": "def site_data(**kwargs):\n", "code": "content = 'window.site_data = {};'\ncontent += 'window.site_data.settings = %s;' % dumps({\n    'MEDIA_URL': settings.MEDIA_URL\n})\nreturn content", "path": "common\\appenginepatch\\mediautils\\generatemedia.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "# Remove modules that we want to override\n", "func_signal": "def patch_python():\n", "code": "for module in ('memcache',):\n    if module in sys.modules:\n        del sys.modules[module]\n\n# For some reason the imp module can't be replaced via sys.path\nfrom appenginepatcher import have_appserver\nif have_appserver:\n    from appenginepatcher import imp\n    sys.modules['imp'] = imp\n\nif have_appserver:\n    def unlink(_):\n        raise NotImplementedError('App Engine does not support FS writes!')\n    os.unlink = unlink", "path": "common\\appenginepatch\\appenginepatcher\\patch.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"\nConverts dict to table-style list of rows with heading:\n\nExample:\ndict_list_to_table(('a', 'b', 'c'),\n    [{'a': 1, 'b': 2, 'c': 3}, {'a': 11, 'b': 12, 'c': 13}])\nresults in the following (dict keys reordered for better readability):\n[\n    ('a', 'b', 'c'),\n    (1, 2, 3),\n    (11, 12, 13),\n]\n\"\"\"\n", "func_signal": "def dict_list_to_table(headings, dict_list):\n", "code": "return [headings] + [tuple([row[heading] for heading in headings])\n                     for row in dict_list]", "path": "common\\appenginepatch\\ragendja\\pyutils.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "# Fix handling of verbose_name. Google resolves lazy translation objects\n# immedately which of course breaks translation support.\n# http://code.google.com/p/googleappengine/issues/detail?id=583\n", "func_signal": "def fix_app_engine_bugs():\n", "code": "from django import forms\nfrom django.utils.text import capfirst\n# This import is needed, so the djangoforms patch can do its work, first\nfrom google.appengine.ext.db import djangoforms\ndef get_form_field(self, form_class=forms.CharField, **kwargs):\n    defaults = {'required': self.required}\n    defaults['label'] = capfirst(self.verbose_name)\n    if self.choices:\n        choices = []\n        if not self.required or (self.default is None and\n                                 'initial' not in kwargs):\n            choices.append(('', '---------'))\n        for choice in self.choices:\n            choices.append((unicode(choice), unicode(choice)))\n        defaults['widget'] = forms.Select(choices=choices)\n    if self.default is not None:\n        defaults['initial'] = self.default\n    defaults.update(kwargs)\n    return form_class(**defaults)\ndb.Property.get_form_field = get_form_field\n\n# Extend ModelForm with support for EmailProperty\n# http://code.google.com/p/googleappengine/issues/detail?id=880\ndef get_form_field(self, **kwargs):\n    \"\"\"Return a Django form field appropriate for an email property.\"\"\"\n    defaults = {'form_class': forms.EmailField}\n    defaults.update(kwargs)\n    return super(db.EmailProperty, self).get_form_field(**defaults)\ndb.EmailProperty.get_form_field = get_form_field\n\n# Fix DateTimeProperty, so it returns a property even for auto_now and\n# auto_now_add.\n# http://code.google.com/p/googleappengine/issues/detail?id=994\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.DateTimeField}\n    defaults.update(kwargs)\n    return super(db.DateTimeProperty, self).get_form_field(**defaults)\ndb.DateTimeProperty.get_form_field = get_form_field\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.DateField}\n    defaults.update(kwargs)\n    return super(db.DateProperty, self).get_form_field(**defaults)\ndb.DateProperty.get_form_field = get_form_field\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.TimeField}\n    defaults.update(kwargs)\n    return super(db.TimeProperty, self).get_form_field(**defaults)\ndb.TimeProperty.get_form_field = get_form_field\n\n# Fix default value of UserProperty (Google resolves the user too early)\n# http://code.google.com/p/googleappengine/issues/detail?id=879\nfrom django.utils.functional import lazy\nfrom google.appengine.api import users\ndef get_form_field(self, **kwargs):\n    defaults = {'initial': lazy(users.GetCurrentUser, users.User)()}\n    defaults.update(kwargs)\n    return super(db.UserProperty, self).get_form_field(**defaults)\ndb.UserProperty.get_form_field = get_form_field\n\n# Fix file uploads via BlobProperty\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.FileField}\n    defaults.update(kwargs)\n    return super(db.BlobProperty, self).get_form_field(**defaults)\ndb.BlobProperty.get_form_field = get_form_field\ndef get_value_for_form(self, instance):\n    return getattr(instance, self.name)\ndb.BlobProperty.get_value_for_form = get_value_for_form\nfrom django.core.files.uploadedfile import UploadedFile\ndef make_value_from_form(self, value):\n    if isinstance(value, UploadedFile):\n        return db.Blob(value.read())\n    return super(db.BlobProperty, self).make_value_from_form(value)\ndb.BlobProperty.make_value_from_form = make_value_from_form\n\n# Optimize ReferenceProperty, so it returns the key directly\n# http://code.google.com/p/googleappengine/issues/detail?id=993\ndef get_value_for_form(self, instance):\n    return self.get_value_for_datastore(instance)\ndb.ReferenceProperty.get_value_for_form = get_value_for_form\n# Use our ModelChoiceField instead of Google's\ndef get_form_field(self, **kwargs):\n    defaults = {'form_class': forms.ModelChoiceField,\n                'queryset': self.reference_class.all()}\n    defaults.update(kwargs)\n    return super(db.ReferenceProperty, self).get_form_field(**defaults)\ndb.ReferenceProperty.get_form_field = get_form_field", "path": "common\\appenginepatch\\appenginepatcher\\patch.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "# Remove old generated files\n", "func_signal": "def cleanup_dir(dir, paths):\n", "code": "keep = []\ndir = os.path.abspath(dir)\nfor path in paths:\n    if not os.path.isabs(path):\n        path = os.path.join(dir, path)\n    path = os.path.abspath(path)\n    while path not in keep and path != dir:\n        keep.append(path)\n        path = os.path.dirname(path)\nfor root, dirs, files in os.walk(dir):\n    for name in dirs:\n        path = os.path.abspath(os.path.join(root, name))\n        if path not in keep:\n            shutil.rmtree(path)\n            dirs.remove(name)\n    for file in files:\n        path = os.path.abspath(os.path.join(root, file))\n        if path not in keep:\n            os.remove(path)", "path": "common\\appenginepatch\\mediautils\\generatemedia.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "# These are needed for i18n\n", "func_signal": "def lang_data(LANGUAGE_CODE, **kwargs):\n", "code": "from django.http import HttpRequest\nfrom django.views.i18n import javascript_catalog\n\nLANGUAGE_BIDI = LANGUAGE_CODE.split('-')[0] in \\\n    settings.LANGUAGES_BIDI\n\nrequest = HttpRequest()\nrequest.GET['language'] = LANGUAGE_CODE\n\n# Add some JavaScript data\ncontent = 'var LANGUAGE_CODE = \"%s\";\\n' % LANGUAGE_CODE\ncontent += 'var LANGUAGE_BIDI = ' + \\\n    (LANGUAGE_BIDI and 'true' or 'false') + ';\\n'\ncontent += javascript_catalog(request,\n    packages=settings.INSTALLED_APPS).content\n\n# The hgettext() function just calls gettext() internally, but\n# it won't get indexed by makemessages.\ncontent += '\\nwindow.hgettext = function(text) { return gettext(text); };\\n'\n# Add a similar hngettext() function\ncontent += 'window.hngettext = function(singular, plural, count) { return ngettext(singular, plural, count); };\\n'\n\nreturn content", "path": "common\\appenginepatch\\mediautils\\generatemedia.py", "repo_name": "lehrblogger/delvicious", "stars": 6, "license": "None", "language": "python", "size": 3576}
{"docstring": "\"\"\"\nCheck that if a CSRF cookie is present but no token, the middleware\nrejects the incoming request.\n\"\"\"\n", "func_signal": "def test_process_request_csrf_cookie_no_token(self):\n", "code": "req = self._get_POST_csrf_cookie_request()\nreq2 = CsrfMiddleware().process_view(req, post_form_view, (), {})\nself.assertEquals(403, req2.status_code)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nTest that a POST HTTPS request with a good referer is accepted\n\"\"\"\n", "func_signal": "def test_https_good_referer(self):\n", "code": "req = self._get_POST_request_with_token()\nreq._is_secure = True\nreq.META['HTTP_HOST'] = 'www.example.com'\nreq.META['HTTP_REFERER'] = 'https://www.example.com/somepage'\nreq2 = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\nself.assertEquals(None, req2)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck that CsrfTokenNode works when no CSRF cookie is set\n\"\"\"\n", "func_signal": "def test_token_node_no_csrf_cookie(self):\n", "code": "req = self._get_GET_no_csrf_cookie_request()\nresp = token_view(req)\nself.assertEquals(u\"\", resp.content)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nA view that doesn't use the token, but does use the csrf view processor.\n\"\"\"\n", "func_signal": "def non_token_view_using_request_processor(request):\n", "code": "context = RequestContext(request, processors=[csrf])\ntemplate = Template(\"\")\nreturn HttpResponse(template.render(context))", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck that CsrfTokenNode works when a CSRF cookie is created by\nthe middleware (when one was not already present)\n\"\"\"\n", "func_signal": "def test_token_node_with_new_csrf_cookie(self):\n", "code": "req = self._get_GET_no_csrf_cookie_request()\nCsrfViewMiddleware().process_view(req, token_view, (), {})\nresp = token_view(req)\nresp2 = CsrfViewMiddleware().process_response(req, resp)\ncsrf_cookie = resp2.cookies[settings.CSRF_COOKIE_NAME]\nself._check_token_present(resp, csrf_id=csrf_cookie.value)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck that AJAX requests are automatically exempted.\n\"\"\"\n", "func_signal": "def test_ajax_exemption(self):\n", "code": "req = self._get_POST_csrf_cookie_request()\nreq.META['HTTP_X_REQUESTED_WITH'] = 'XMLHttpRequest'\nreq2 = CsrfMiddleware().process_view(req, post_form_view, (), {})\nself.assertEquals(None, req2)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nModifies a view function so that its response is exempt\nfrom the post-processing of the CSRF middleware.\n\"\"\"\n", "func_signal": "def csrf_response_exempt(view_func):\n", "code": "def wrapped_view(*args, **kwargs):\n    resp = view_func(*args, **kwargs)\n    resp.csrf_exempt = True\n    return resp\nreturn wraps(view_func)(wrapped_view)", "path": "django_csrf\\decorators.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck that if a CSRF cookie is present and no token, but the csrf_exempt\ndecorator has been applied to the view, the middleware lets it through\n\"\"\"\n", "func_signal": "def test_process_request_csrf_cookie_no_token_exempt_view(self):\n", "code": "req = self._get_POST_csrf_cookie_request()\nreq2 = CsrfMiddleware().process_view(req, csrf_exempt(post_form_view), (), {})\nself.assertEquals(None, req2)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"Returns the matched <form> tag plus the added <input> element\"\"\"\n", "func_signal": "def add_csrf_field(match):\n", "code": "return mark_safe(match.group() + \"<div style='display:none;'>\" + \\\n\"<input type='hidden' \" + idattributes.next() + \\\n\" name='csrfmiddlewaretoken' value='\" + csrf_token + \\\n\"' /></div>\")", "path": "django_csrf\\middleware.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nWhen no prior CSRF cookie exists, check that the cookie is created, even\nif only CsrfViewMiddleware is used.\n\"\"\"\n# This is checking that CsrfViewMiddleware has the cookie setting\n# code. Most of the other tests use CsrfMiddleware.\n", "func_signal": "def test_process_response_no_csrf_cookie_view_only_get_token_used(self):\n", "code": "req = self._get_GET_no_csrf_cookie_request()\n# token_view calls get_token() indirectly\nCsrfViewMiddleware().process_view(req, token_view, (), {})\nresp = token_view(req)\nresp2 = CsrfViewMiddleware().process_response(req, resp)\n\ncsrf_cookie = resp2.cookies.get(settings.CSRF_COOKIE_NAME, False)\nself.assertNotEqual(csrf_cookie, False)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck the the post-processor does nothing for content-types not in _HTML_TYPES.\n\"\"\"\n", "func_signal": "def test_process_response_non_html(self):\n", "code": "req = self._get_GET_no_csrf_cookie_request()\nCsrfMiddleware().process_view(req, post_form_view, (), {})\nresp = post_form_response_non_html()\nresp_content = resp.content # needed because process_response modifies resp\nresp2 = CsrfMiddleware().process_response(req, resp)\nself.assertEquals(resp_content, resp2.content)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"Return the absolute name of the module to be imported.\"\"\"\n", "func_signal": "def _resolve_name(name, package, level):\n", "code": "if not hasattr(package, 'rindex'):\n    raise ValueError(\"'package' not set to a string\")\ndot = len(package)\nfor x in xrange(level, 1, -1):\n    try:\n        dot = package.rindex('.', 0, dot)\n    except ValueError:\n        raise ValueError(\"attempted relative import beyond top-level \"\n                          \"package\")\nreturn \"%s.%s\" % (package[:dot], name)", "path": "django_csrf\\utils\\importlib.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nWhen no CSRF cookie exists, but the user has a session, check that a token\nusing the session cookie as a legacy CSRF cookie is accepted.\n\"\"\"\n", "func_signal": "def test_process_request_session_cookie_no_csrf_cookie_token(self):\n", "code": "orig_secret_key = settings.SECRET_KEY\nsettings.SECRET_KEY = self._secret_key_for_session_test\ntry:\n    req = self._get_POST_session_request_with_token()\n    req2 = CsrfMiddleware().process_view(req, post_form_view, (), {})\n    self.assertEquals(None, req2)\nfinally:\n    settings.SECRET_KEY = orig_secret_key", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck that if neither a CSRF cookie nor a session cookie are present,\nthe middleware rejects the incoming request.  This will stop login CSRF.\n\"\"\"\n", "func_signal": "def test_process_request_no_session_no_csrf_cookie(self):\n", "code": "req = self._get_POST_no_csrf_cookie_request()\nreq2 = CsrfMiddleware().process_view(req, post_form_view, (), {})\nself.assertEquals(403, req2.status_code)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck that if both a cookie and a token is present, the middleware lets it through.\n\"\"\"\n", "func_signal": "def test_process_request_csrf_cookie_and_token(self):\n", "code": "req = self._get_POST_request_with_token()\nreq2 = CsrfMiddleware().process_view(req, post_form_view, (), {})\nself.assertEquals(None, req2)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck that no post processing is done for an exempt view\n\"\"\"\n", "func_signal": "def test_process_response_exempt_view(self):\n", "code": "req = self._get_POST_csrf_cookie_request()\nresp = csrf_exempt(post_form_view)(req)\nresp_content = resp.content\nresp2 = CsrfMiddleware().process_response(req, resp)\nself.assertEquals(resp_content, resp2.content)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "# We must do the response post-processing first, because that calls\n# get_token(), which triggers a flag saying that the CSRF cookie needs\n# to be sent (done in CsrfViewMiddleware.process_response)\n", "func_signal": "def process_response(self, request, resp):\n", "code": "resp2 = self.response_middleware.process_response(request, resp)\nreturn self.view_middleware.process_response(request, resp2)", "path": "django_csrf\\middleware.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck that CsrfResponseMiddleware finishes without error if the view middleware\nhas not been called, as is the case if a request middleware returns a response.\n\"\"\"\n", "func_signal": "def test_response_middleware_without_view_middleware(self):\n", "code": "req = self._get_GET_no_csrf_cookie_request()\nresp = post_form_view(req)\nCsrfMiddleware().process_response(req, resp)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nWhen no prior CSRF cookie exists, check that the cookie is created and a\ntoken is inserted.\n\"\"\"\n", "func_signal": "def test_process_response_no_csrf_cookie(self):\n", "code": "req = self._get_GET_no_csrf_cookie_request()\nCsrfMiddleware().process_view(req, post_form_view, (), {})\n\nresp = post_form_response()\nresp_content = resp.content # needed because process_response modifies resp\nresp2 = CsrfMiddleware().process_response(req, resp)\n\ncsrf_cookie = resp2.cookies.get(settings.CSRF_COOKIE_NAME, False)\nself.assertNotEqual(csrf_cookie, False)\nself.assertNotEqual(resp_content, resp2.content)\nself._check_token_present(resp2, csrf_cookie.value)\n# Check the Vary header got patched correctly\nself.assert_('Cookie' in resp2.get('Vary',''))", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCheck that if get_token() is not called, the view middleware does not\nadd a cookie.\n\"\"\"\n# This is important to make pages cacheable.  Pages which do call\n# get_token(), assuming they use the token, are not cacheable because\n# the token is specific to the user\n", "func_signal": "def test_process_response_get_token_not_used(self):\n", "code": "req = self._get_GET_no_csrf_cookie_request()\n# non_token_view_using_request_processor does not call get_token(), but\n# does use the csrf request processor.  By using this, we are testing\n# that the view processor is properly lazy and doesn't call get_token()\n# until needed.\nCsrfViewMiddleware().process_view(req, non_token_view_using_request_processor, (), {})\nresp = non_token_view_using_request_processor(req)\nresp2 = CsrfViewMiddleware().process_response(req, resp)\n\ncsrf_cookie = resp2.cookies.get(settings.CSRF_COOKIE_NAME, False)\nself.assertEqual(csrf_cookie, False)", "path": "django_csrf\\tests.py", "repo_name": "brosner/django-csrf", "stars": 5, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"Generates the code for a Namespace definition.\n\nThis function will recursively generate the code for all the definitions\ninside the namespace.\n\nArgs:\n  parent_section: the main section of the parent scope.\n  scope: the parent scope.\n  obj: the Namespace definition.\n\"\"\"\n", "func_signal": "def Namespace(self, parent_section, scope, obj):\n", "code": "scope = scope  # silence gpylint.\nself.Documentation(parent_section, obj, '')\nparent_section.PushNamespace(obj.name)\nself.DefinitionList(parent_section, obj, obj.defn_list)\nparent_section.PopNamespace()", "path": "nixysa\\cpp_header_generator.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Generates the code for a Function definition.\n\nArgs:\n  parent_section: the main section of the parent scope.\n  scope: the parent scope.\n  obj: the Function definition.\n\"\"\"\n", "func_signal": "def Function(self, parent_section, scope, obj):\n", "code": "section = self.GetSectionFromAttributes(parent_section, obj)\nself.Documentation(section, obj, '')\nprototype = java_utils.GetFunctionPrototype(scope, obj)\nsection.EmitCode(prototype + ';')", "path": "nixysa\\cpp_header_generator.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the representation of a type when used as a C++ function return value.\n\nArgs:\n  scope: a Definition for the scope in which the expression will be written.\n  type_defn: a Definition for the type.\n\nReturns:\n  a (string, boolean) pair, the first element being the representation of\n  the type, the second element indicating whether or not the definition of\n  the type is needed for the expression to be valid.\n\"\"\"\n", "func_signal": "def CppReturnValueString(scope, type_defn):\n", "code": "data_type = type_defn.GetFinalType().data_type\ndata_type_bm = data_type.binding_model\nreturn data_type_bm.CppReturnValueString(scope, data_type)", "path": "nixysa\\nullable_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Generates the code for an Enum definition.\n\nArgs:\n  parent_section: the main section of the parent scope.\n  scope: the parent scope.\n  obj: the Enum definition.\n\"\"\"\n", "func_signal": "def Enum(self, parent_section, scope, obj):\n", "code": "scope = scope  # silence gpylint.\nsection = self.GetSectionFromAttributes(parent_section, obj)\nself.Documentation(parent_section, obj, '')\nsection.EmitCode('enum %s {' % obj.name)\nfor value in obj.values:\n  if value.value is None:\n    section.EmitCode('%s,' % value.name)\n  else:\n    section.EmitCode('%s = %s,' % (value.name, value.value))\nsection.EmitCode('};')", "path": "nixysa\\cpp_header_generator.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the representation of a member name in Java.\n\nArgs:\n  scope: a Definition for the scope in which the expression will be written.\n  type_defn: a Definition for the type.\n\nReturns:\n  a string representing the type\n\"\"\"\n", "func_signal": "def JavaMemberString(scope, type_defn):\n", "code": "data_type = type_defn.GetFinalType().data_type\ndata_type_bm = data_type.binding_model\nreturn data_type_bm.JavaMemberString(scope, data_type)", "path": "nixysa\\nullable_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Generates the headers for all input files.\n\nArgs:\n  output_dir: the output directory.\n  pairs: a list of (idl_parser.File, syntax_tree.Definition list) describing\n    the list of top-level definitions in each source file.\n  namespace: a syntax_tree.Namespace for the global namespace.\n\nReturns:\n  a list of cpp_utils.CppFileWriter, one for each output header file.\n\"\"\"\n", "func_signal": "def ProcessFiles(output_dir, pairs, namespace):\n", "code": "generator = CPPHeaderGenerator(output_dir)\nwriter_list = []\nfor (f, defn) in pairs:\n  writer_list.append(generator.Generate(f, namespace, defn))\nreturn writer_list", "path": "nixysa\\cpp_header_generator.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Generates the header file.\n\nArgs:\n  idl_file: the source IDL file containing the definitions, as a\n    idl_parser.File instance.\n  namespace: a Definition for the global namespace.\n  defn_list: the list of top-level definitions.\n\nReturns:\n  a cpp_utils.CppFileWriter that contains the C++ header file code.\n\nRaises:\n  CircularDefinition: circular definitions were found in the file.\n\"\"\"\n", "func_signal": "def Generate(self, idl_file, namespace, defn_list):\n", "code": "writer = cpp_utils.CppFileWriter('%s/%s' % (self._output_dir,\n                                            idl_file.header), True)\ncode_section = writer.CreateSection('defns')\nself.DefinitionList(code_section, namespace, defn_list)\nreturn writer", "path": "nixysa\\cpp_header_generator.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Generates the code for a Variable definition.\n\nThis function will generate the member/global variable declaration, as well\nas the setter/getter functions if specified in the attributes.\n\nArgs:\n  parent_section: the main section of the parent scope.\n  scope: the parent scope.\n  obj: the Variable definition.\n\"\"\"\n", "func_signal": "def Variable(self, parent_section, scope, obj):\n", "code": "member_section = self.GetSectionFromAttributes(parent_section, obj)\n\nbm = obj.type_defn.binding_model\ntype_string = bm.JavaMemberString(scope, obj.type_defn)\n# Note: There is no static in javascript\nfield_name = naming.Normalize(obj.name, naming.Java)\nif 'getter' in obj.attributes and 'setter' not in obj.attributes:\n  self.Documentation(member_section, obj,\n                     'This property is read-only.')\nelif 'getter' not in obj.attributes and 'setter' in obj.attributes:\n  self.Documentation(member_section, obj,\n                     'This property is write-only.')\nelse:\n  self.Documentation(member_section, obj, '')\nmember_section.EmitCode('%s %s;' % (type_string, field_name))\n# Note: There are no getter/setter in javascript", "path": "nixysa\\cpp_header_generator.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Generates the code for a Class definition.\n\nThis function will recursively generate the code for all the definitions\ninside the class. These definitions will be output into one of 3 sections\n(private, protected, public), depending on their attributes. These\nindividual sections will only be output if they are not empty.\n\nArgs:\n  parent_section: the main section of the parent scope.\n  scope: the parent scope.\n  obj: the Class definition.\n\"\"\"\n", "func_signal": "def Class(self, parent_section, scope, obj):\n", "code": "self.Documentation(parent_section, obj, '')\nsection = self.GetSectionFromAttributes(parent_section, obj).CreateSection(\n    obj.name)\nif obj.base_type:\n  bm = obj.base_type.binding_model\n  type_string = bm.JavaMemberString(scope, obj.base_type)\n  section.EmitCode('class %s : public %s {' % (obj.name, type_string))\nelse:\n  section.EmitCode('class %s {' % obj.name)\npublic_section = section.CreateSection('public:')\nprotected_section = section.CreateSection('protected:')\nprivate_section = section.CreateSection('private:')\nself.DefinitionList(section, obj, obj.defn_list)\nif not public_section.IsEmpty():\n  public_section.AddPrefix('public:')\nif not protected_section.IsEmpty():\n  protected_section.AddPrefix('protected:')\nif not private_section.IsEmpty():\n  private_section.AddPrefix('private:')\nsection.EmitCode('};')", "path": "nixysa\\cpp_header_generator.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the representation of a type when used as a C++ class member.\n\nArgs:\n  scope: a Definition for the scope in which the expression will be written.\n  type_defn: a Definition for the type.\n\nReturns:\n  a (string, boolean) pair, the first element being the representation of\n  the type, the second element indicating whether or not the definition of\n  the type is needed for the expression to be valid.\n\"\"\"\n", "func_signal": "def CppMemberString(scope, type_defn):\n", "code": "data_type = type_defn.GetFinalType().data_type\ndata_type_bm = data_type.binding_model\nreturn data_type_bm.CppMemberString(scope, data_type)", "path": "nixysa\\nullable_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the representation of a type in JSDoc notation.\n\nArgs:\n  type_defn: a Definition for the type.\n\nReturns:\n  a string that is the JSDoc notation of type_defn.\n\"\"\"\n", "func_signal": "def JSDocTypeString(type_defn):\n", "code": "type_defn = type_defn.GetFinalType()\nelement_type_defn = type_defn.data_type.GetFinalType()\ntype = element_type_defn.binding_model.JSDocTypeString(element_type_defn)\nif type[0] == '!':\n  type = type[1:]\nreturn type", "path": "nixysa\\nullable_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the string converting a mutable expression to a non-mutable one.\n\nArgs:\n  scope: a Definition for the scope in which the expression will be written.\n  type_defn: a Definition for the type.\n  expr: a string for the mutable expression.\n\nReturns:\n  a string, which is the non-mutable expression.\n\"\"\"\n", "func_signal": "def CppMutableToNonMutable(scope, type_defn, expr):\n", "code": "(scope, type_defn) = (scope, type_defn)  # silence gpylint.\nreturn '*(%s)' % expr", "path": "nixysa\\pod_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the representation of a type for a mutable function parameter.\n\nArgs:\n  scope: a Definition for the scope in which the expression will be written.\n  type_defn: a Definition for the type.\n\nReturns:\n  a (string, boolean) pair, the first element being the string representing\n  the type, the second element indicating whether or not the definition of\n  the type is needed for the expression to be valid.\n\"\"\"\n", "func_signal": "def CppMutableParameterString(scope, type_defn):\n", "code": "data_type = type_defn.GetFinalType().data_type\ndata_type_bm = data_type.binding_model\nreturn data_type_bm.CppMutableParameterString(scope, data_type)", "path": "nixysa\\nullable_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Transforms a input_string into a list of lower-case components.\n\nArgs:\n  input_string: the input string.\n\nReturns:\n  a list of lower-case words.\n\"\"\"\n", "func_signal": "def SplitWords(input_string):\n", "code": "if input_string.find('_') > -1:\n  # 'some_TEXT_' -> 'some text'\n  return input_string.replace('_', ' ').strip().lower().split()\nelse:\n  if re.search('[A-Z]', input_string) and re.search('[a-z]', input_string):\n    # mixed case.\n    # look for capitalization to cut input_strings\n    # 'SomeText' -> 'Some Text'\n    input_string = re.sub('([A-Z])', r' \\1', input_string).strip()\n    # 'Vector3' -> 'Vector 3'\n    input_string = re.sub('([^0-9])([0-9])', r'\\1 \\2', input_string)\n  return input_string.lower().split()", "path": "nixysa\\naming.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the representation of a type when used in a C++ typedef.\n\nArgs:\n  scope: a Definition for the scope in which the expression will be written.\n  type_defn: a Definition for the type.\n\nReturns:\n  a (string, boolean) pair, the first element being the representation of\n  the type, the second element indicating whether or not the definition of\n  the type is needed for the expression to be valid.\n\"\"\"\n", "func_signal": "def CppTypedefString(scope, type_defn):\n", "code": "data_type = type_defn.GetFinalType().data_type\ndata_type_bm = data_type.binding_model\nreturn data_type_bm.CppTypedefString(scope, data_type)", "path": "nixysa\\nullable_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the representation of a type when used for a function parameter.\n\nArgs:\n  scope: a Definition for the scope in which the expression will be written.\n  type_defn: a Definition for the type.\n\nReturns:\n  a (string, boolean) pair, the first element being the representation of\n  the type, the second element indicating whether or not the definition of\n  the type is needed for the expression to be valid.\n\"\"\"\n", "func_signal": "def CppParameterString(scope, type_defn):\n", "code": "data_type = type_defn.GetFinalType().data_type\ndata_type_bm = data_type.binding_model\nreturn data_type_bm.CppParameterString(scope, data_type)", "path": "nixysa\\nullable_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the representation of a type when used for a function parameter.\n\nArgs:\n  scope: a Definition for the scope in which the expression will be written.\n  type_defn: a Definition for the type.\n\nReturns:\n  a (string, boolean) pair, the first element being the representation of\n  the type, the second element indicating whether or not the definition of\n  the type is needed for the expression to be valid.\n\nRaises:\n  BadVoidUsage: type_defn is a 'void' POD type.\n\"\"\"\n", "func_signal": "def CppParameterString(scope, type_defn):\n", "code": "final_type = type_defn.GetFinalType()\nif final_type.podtype == 'void':\n  raise BadVoidUsage\nelif final_type.podtype == 'string' or final_type.podtype == 'wstring':\n  return 'const %s&' % cpp_utils.GetScopedName(scope, type_defn), True\nelse:\n  return cpp_utils.GetScopedName(scope, type_defn), True", "path": "nixysa\\pod_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the representation of a type when used as a C++ class member.\n\nArgs:\n  scope: a Definition for the scope in which the expression will be written.\n  type_defn: a Definition for the type.\n\nReturns:\n  a (string, boolean) pair, the first element being the representation of\n  the type, the second element indicating whether or not the definition of\n  the type is needed for the expression to be valid.\n\nRaises:\n  BadVoidUsage: type_defn is a 'void' POD type.\n\"\"\"\n", "func_signal": "def CppMemberString(scope, type_defn):\n", "code": "if type_defn.GetFinalType().podtype == 'void':\n  raise BadVoidUsage\nreturn cpp_utils.GetScopedName(scope, type_defn), True", "path": "nixysa\\pod_binding.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Generates the documentation code.\n\nArgs:\n  parent_section: the main section of the parent scope.\n  obj: the object to be documented; may be class, function, enum or field.\n  extra_doc: extra documenation information to be put in comments\nRaises:\n  UndocumentedError: an error if there is no documentation\n\"\"\"\n", "func_signal": "def Documentation(self, parent_section, obj, extra_doc):\n", "code": "try:\n  section = self.GetSectionFromAttributes(parent_section, obj)\n  comment_lines = obj.attributes['__docs'].splitlines()\n  # Break up text and insert comment formatting\n  section.EmitCode('/*! ')\n  for line in comment_lines:\n    section.EmitCode('%s' % (line))\n  section.EmitCode('%s' % (extra_doc))\n  section.EmitCode('*/')\n\nexcept KeyError:\n  # catch the error when __docs does not exist\n  if self.force_documentation:\n    source = obj.source\n    print ('%s:%d Documentation not found' % (source.file.source,\n                                              source.line))\n    raise UndocumentedError('Documentation not found.')", "path": "nixysa\\cpp_header_generator.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Gets the code section appropriate for a given definition.\n\nClasses have 3 definition sections: private, protected and public. This\nfunction will pick one of the sections, based on the attributes of the\ndefinition, if its parent is a class. For other scopes (namespaces) it will\nreturn the parent scope main section.\n\nArgs:\n  parent_section: the main section for the parent scope.\n  defn: the definition.\n\nReturns:\n  the appropriate section.\n\"\"\"\n", "func_signal": "def GetSectionFromAttributes(self, parent_section, defn):\n", "code": "if defn.parent and defn.parent.defn_type == 'Class':\n  if 'private' in defn.attributes:\n    return parent_section.GetSection('private:') or parent_section\n  elif 'protected' in defn.attributes:\n    return parent_section.GetSection('protected:') or parent_section\n  else:\n    return parent_section.GetSection('public:') or parent_section\nelse:\n  return parent_section", "path": "nixysa\\cpp_header_generator.py", "repo_name": "AlexSc/HelloWorld", "stars": 5, "license": "mit", "language": "python", "size": 367}
{"docstring": "\"\"\"Doing some simple assertions based off states\"\"\"\n\n# TODO: Add more state checks\n\n# Task is just created\n", "func_signal": "def test_allowable_states(self):\n", "code": "states = self.task.allowable_states(self.user_admin)\nself.assertEquals(states,\n    [('1', 'leave open')])\n\n# Now we assign it. This is what the assignee sees\nself.task.assignee = self.user_joe\nself.task.save()\nstates = self.task.allowable_states(self.user_joe)\nself.assertEquals(states, [('1', 'leave open')])\n\n# this is what the creator sees\nstates = self.task.allowable_states(self.user_admin)\nself.assertEquals(states, [('1', 'leave open')])\n\n# Task is now moved to in-progress. this is what the assignee can see.\nself.task.state = \"4\"\nself.task.save()\nstates = self.task.allowable_states(self.user_joe)\nself.assertEquals(states,\n    [('4', 'still in progress'), ('5', 'discussion needed'),\n    ('8', 'fix needs review')] )", "path": "apps\\tasks\\tests\\test_models.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\" Called when a user nudges a ticket \"\"\"\n\n", "func_signal": "def nudge(request, id, group_slug=None, parent_slug=None, bridge=None):\n", "code": "if bridge:\n    try:\n        group = bridge.get_group(group_slug)\n    except ObjectDoesNotExist:\n        raise Http404\nelse:\n    group = None\n\nif group:\n    tasks = group.content_objects(Task)\nelse:\n    tasks = Task.objects.filter(object_id=None)\n\ntask = get_object_or_404(tasks, id=id)\ntask_url = task.get_absolute_url(group)\n\nnudged = Nudge.objects.filter(task__exact=task, nudger__exact=request.user)\nif nudged:\n    # you've already nudged this task.\n    nudge = nudged[0]\n    nudge.delete()\n    message = \"You've removed your nudge from this task\"\n    request.user.message_set.create(message=message)\n    return HttpResponseRedirect(task_url)\n\n\nnudge = Nudge(nudger=request.user, task=task)\nnudge.save()\n\ncount = Nudge.objects.filter(task__exact=task).count()\n\n# send the message to the user\nmessage = \"%s has been nudged about this task\" % task.assignee\nrequest.user.message_set.create(message=message)\n\n# send out the nudge notification\nif notification:\n    notify_list = [task.assignee]\n    notification.send(notify_list, \"tasks_nudge\", {\"nudger\": request.user, \"task\": task, \"count\": count})\n\nreturn HttpResponseRedirect(task_url)", "path": "apps\\tasks\\views.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "# mod_python fakes the environ, and thus doesn't process SetEnv.\n# This fixes that. Django will call this again since there is no way\n# of overriding __call__ to just process the request.\n", "func_signal": "def __call__(self, req):\n", "code": "os.environ.update(req.subprocess_env)\nfrom django.conf import settings\n\nsys.path.insert(0, abspath(join(dirname(__file__), \"../../\")))\n\nsys.path.insert(0, join(settings.PINAX_ROOT, \"apps\"))\nsys.path.insert(0, join(settings.PROJECT_ROOT, \"apps\"))\n\nreturn super(PinaxModPythonHandler, self).__call__(req)", "path": "deploy\\modpython.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\" In CPC task 173 non-comment changes by users besides the task\ncreator don't save the current user. This checks to see that the history\nis changed accurately.\n\"\"\"\n\n# we have joe assign himself to a task that admin created\n", "func_signal": "def test_change_history_by_non_creator(self):\n", "code": "self.task.assignee = self.user_joe\nself.task.save()\nself.task.save_history(change_owner=self.user_joe)\n\n# fetch the history\nhistory = self.task.history_task.all()[0]\n\n# the person who made the change was joe\nself.assertEquals(history.owner, self.user_joe)", "path": "apps\\tasks\\tests\\test_models.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "'''Create a new ChangeSet with the old content.'''\n\n# get the task history object\n", "func_signal": "def save_history(self, comment_instance=None, change_owner=None):\n", "code": "th = TaskHistory()\nth.task = self\n\nif self.group:\n    self.group.associate(th, commit=False)\n\n# save the simple fields\n\nfor field in self.fields:\n    value = getattr(self, field)\n    setattr(th, field, value)\n\n\nif change_owner:\n    # If a user is provided then we are editing a record.\n    # So the owner of the change is the editor.\n    th.owner = change_owner\nelse:\n    # This record is being created right now, hence the assignment\n    # of the creator to the task history object's owner field.\n    th.owner = self.creator\n\n# handle the comments\nif comment_instance:\n    th.comment = comment_instance.comment\n    \nth.save()", "path": "apps\\tasks\\models.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "# current user is assignee or there is no assignee\n", "func_signal": "def is_assignee_or_none(task, user):\n", "code": "if task.assignee == user or not task.assignee:\n    return True\nreturn False", "path": "apps\\tasks\\workflow.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "# normal test\n", "func_signal": "def test_is_assignee_or_none(self):\n", "code": "self.assertEquals(False, is_assignee_or_none(self.task, self.user_admin))\nself.assertEquals(True, is_assignee_or_none(self.task, self.user_joe))\nself.assertEquals(False, is_assignee_or_none(self.task, None))\n\n# now lets remove the assignee and see what happens\nself.task.assignee = None\nself.task.save()\nself.assertEquals(True, is_assignee_or_none(self.task, self.user_admin))\nself.assertEquals(True, is_assignee_or_none(self.task, self.user_joe))\nself.assertEquals(True, is_assignee_or_none(self.task, None))", "path": "apps\\tasks\\tests\\test_workflow.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "#kwargs = {\"id\": self.pk, 'group_slug': self.slug}\n", "func_signal": "def get_absolute_url(self, group=None):\n", "code": "if not group:\n    group = self.group\nif group:\n    return group.content_bridge.reverse(\"project_detail\", group, {\"id\": self.pk})\nreturn reverse(\"project_detail\", kwargs={'project_slug': self.slug})", "path": "apps\\projects\\models.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\"Anonymous users should not edit tasks\"\"\"\n", "func_signal": "def testAnonymousCannotEdit(self):\n", "code": "response = self.client.get('/tasks/task/1/')\nself.failUnlessEqual(response.status_code, 200)\nself.failUnless(response.content.find('<h2>Edit</h2>') == -1,\n    'Anonymous user is able to edit tasks.')", "path": "apps\\tasks\\tests\\test_authentication.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\"\nreturn state choices allowed given current state and user\n\"\"\"\n\n# I'm the relevant state choices.\n", "func_signal": "def allowable_states(self, user):\n", "code": "choices = []\n\n# I'm the states already allowed for the users\nexisting_states = []\n\nfor transition in workflow.STATE_TRANSITIONS:\n    \n    if self.state != str(transition[0]):\n        # if the current state does not match a first element in the\n        # state transitions we skip to the next transition\n        continue\n    \n    # Fire the validation function.\n    if transition[2](self, user):\n        \n        # grab the new state and state description\n        new_state = str(transition[1])\n        description = transition[3]\n\n        # build new element\n        element = (new_state, description)\n        \n        # append new element to choices\n        choices.append(element)\n\nreturn choices", "path": "apps\\tasks\\models.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "# ugly cowboy code that really needs refactoring\n", "func_signal": "def export_state_transitions(format='csv'):\n", "code": "rows = []\nfor row in STATE_TRANSITIONS:\n    record = ''\n    current_state = STATE_CHOICES_DICT[str(row[0])]\n    new_state = STATE_CHOICES_DICT[str(row[1])]\n    permission = str(row[2]).split()[1]\n    transition_name = row[3]\n    record = \"\"\" \"%s\",\"%s\",\"%s\",\"%s\" \"\"\" % (current_state, new_state, permission, transition_name)\n    rows.append(record.strip())\n\n# ick turn this into a string.\n# TODO: to this right!\ntext = ''\nfor row in rows:\n    text += row + '\\n'\nreturn text", "path": "apps\\tasks\\workflow.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\" lets see if history tracks user changes if done against the task\"\"\"\n\n# we have admin assign joe to the task.\n", "func_signal": "def test_history(self):\n", "code": "self.task.assignee = self.user_joe\nself.task.save()\nself.task.save_history(change_owner=self.user_admin)\n\n# fetch the history\nhistory = self.task.history_task.all()[0]\n\n# The task assignee should be joe\nself.assertEquals(history.assignee, self.user_joe)\n\n# the person who made the change was admin\nself.assertEquals(history.owner, self.user_admin)", "path": "apps\\tasks\\tests\\test_models.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "# we remove all nudges for this Task\n\n", "func_signal": "def denudge(self):\n", "code": "for nudge in Nudge.objects.filter(task__exact=self):\n    nudge.delete()", "path": "apps\\tasks\\models.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\"Member users should be able to edit tasks\"\"\"\n", "func_signal": "def testMemberCanEdit(self):\n", "code": "self.client.login(username='admin', password='test')\nresponse = self.client.get('/tasks/task/1/')\nself.failUnlessEqual(response.status_code, 200)\nself.failUnless(response.content.find('<h2>Edit</h2>') != -1,\n    'Authenticated users cannot edit tasks.')\nself.client.logout()", "path": "apps\\tasks\\tests\\test_authentication.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "# test first with assignee\n", "func_signal": "def test_is_no_assignee(self):\n", "code": "self.assertEquals(False, no_assignee(self.task, self.user_admin))\nself.assertEquals(False, no_assignee(self.task, self.user_joe))\nself.assertEquals(False, no_assignee(self.task, None))\n\n# test again without assignee\nself.task.assignee = None\nself.task.save()\nself.assertEquals(True, no_assignee(self.task, self.user_admin))\nself.assertEquals(True, no_assignee(self.task, self.user_joe))\nself.assertEquals(True, no_assignee(self.task, None))", "path": "apps\\tasks\\tests\\test_workflow.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\"\nGenerates the most comment keyword arguments for a given ``content_object``.\n\"\"\"\n", "func_signal": "def _generate_object_kwarg_dict(self, content_object, **kwargs):\n", "code": "kwargs['content_type'] = ContentType.objects.get_for_model(content_object)\nkwargs['object_id'] = getattr(content_object, 'pk', getattr(content_object, 'id'))\nreturn kwargs", "path": "apps\\economic_resources\\models.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\"\n    {% content_objects group \"tasks.Task\" as tasks %}\n\"\"\"\n", "func_signal": "def content_objects(parser, token):\n", "code": "bits = token.split_contents()\nif len(bits) != 5:\n    raise template.TemplateSyntaxError(\"'%s' requires five arguments.\" % bits[0])\nreturn ContentObjectsNode(bits[1], bits[2], bits[4])", "path": "apps\\groups\\templatetags\\group_tags.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\" We check that:\n    1. We have nudges across multiple tasks.\n    2. After denudging the task, the task has no more nudges\n    3. After denudging the task, the other task still has nudges\n\"\"\"\n\n# we have nudges across multiple tasks including our sample task\n", "func_signal": "def test_denudge(self):\n", "code": "self.assertEquals(len(self.task.task_nudge.all()), self.task_nudge_count)\nself.assertEquals(len(self.other_task.task_nudge.all()), self.other_task_nudge_count)\n\n# now we denudge our task\nself.task.denudge()\n\n# Our task should have no nudges\nself.assertEquals(len(self.task.task_nudge.all()), 0)\n\n# The other task should have its original number of nudges\nself.assertEquals(len(self.other_task.task_nudge.all()), self.other_task_nudge_count)", "path": "apps\\tasks\\tests\\test_models.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "# create some sample form data\n", "func_signal": "def test_markup(self):\n", "code": "        form_data = {'summary': 'my simple test',\n                'detail': rst_markup,\n                'markup':'rst',\n                'assignee':'',\n                'tags':''\n                }\n# post the form\n        response = self.client.post('/tasks/add/', form_data)\n# display the resultant task\n        response = self.client.get('/tasks/task/3/')\n# test the markup\n        self.assertContains(response, '<h1 class=\"title\">Sample Header</h1>')", "path": "apps\\tasks\\tests\\test_client.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "\"\"\"\nReturns a QuerySet from a Model, Manager, or QuerySet. Created to make\nget_object_or_404 and get_list_or_404 more DRY.\n\nPulled from django.shortcuts\n\"\"\"\n\n", "func_signal": "def _get_queryset(klass):\n", "code": "if isinstance(klass, QuerySet):\n    return klass\nelif isinstance(klass, models.Manager):\n    manager = klass\nelse:\n    manager = klass._default_manager\nreturn manager.all()", "path": "apps\\groups\\base.py", "repo_name": "bhaugen/pinax-groups-experiments", "stars": 4, "license": "None", "language": "python", "size": 168}
{"docstring": "# if sign should be drawn, do it!\n", "func_signal": "def draw_clicked_pos(self, widget, event):\n", "code": "if self.sign:\n    self.draw_sign()", "path": "client\\gui\\mapscreen.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nUpdate the signal strength.\n'''\n# return signal strength as string\n", "func_signal": "def check_signal_strength(self):\n", "code": "if self.signal_strength == 0 or self.signal_strength == None:\n    return \"offline\"\nelif -self.critical_signal_strength < -self.signal_strength:\n    return \"low\"\nelse:\n    return \"high\"", "path": "client\\qosmanager.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nPack this message to a simplejson string.\n'''\n", "func_signal": "def pack(self):\n", "code": "dict = {}\ndict[\"type\"] = self.type\ndict[\"subtype\"] = self.subtype\ndict[\"prio\"] = self.prio\ndict[\"timestamp\"] = self.timestamp.strftime(\"%s\")\ndict[\"sender\"] = self.sender\ndict[\"receiver\"] = self.receiver\ndict[\"response_to\"] = self.response_to\nif self.message_id is not None:\n    dict[\"message_id\"] = self.message_id\ntry:\n    dict[\"packed_data\"] = self.unpacked_data.to_dict()\nexcept:\n    dict[\"packed_data\"] = self.unpacked_data\nself.packed_data = json.dumps(dict)\nreturn unicode(self.packed_data)", "path": "shared\\data.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "''' Creates a label and en entry in a HBox and adds the HBox to\n    parent. Returns entry.\n'''\n", "func_signal": "def create_entry(self, labeltext, parent):\n", "code": "hbox = gtk.HBox(True,0)\nlabel = gtk.Label(labeltext)\nlabel.set_alignment(0, 0.5)\nentry = gtk.Entry()\nentry.set_max_length(300)\nentry.set_text(\"\")\n# TODO: usuable?\nentry.select_region(0, len(entry.get_text()))\nhbox.add(label)\nhbox.add(entry)\nparent.add(hbox)\nreturn entry", "path": "client\\gui\\gui.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "# Lagrar basbildens bredd och h\u00f6jd\n", "func_signal": "def __init__(self, width, height):\n", "code": "self.__width = int(width)\nself.__height = int(height)\n\n# Beh\u00f6vs f\u00f6r att l\u00e4gga in tiles\nself.__col_pos = 0\nself.__row_pos = 0\n    \n# Beh\u00f6vs f\u00f6r matematiska ber\u00e4kningar\nself.__cols = 0\nself.__rows = 0", "path": "testing\\kartkomponent\\data_storage.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "''' Construct a new queue item.\n\nprio is not used, it's only to have same __init__ interface as NetworkOutQueueItem\n'''\n", "func_signal": "def __init__(self, data, prio=0):\n", "code": "self.data = data\nself.processed = False", "path": "shared\\data.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "# H\u00e4mtar alla tiles f\u00f6r en niv\u00e5\n", "func_signal": "def draw(self):\n", "code": "level = self.mapdata.get_level(self.zoom_level)\n# Plockar ur de tiles vi s\u00f6ker fr\u00e5n niv\u00e5n\ntiles, cols, rows = level.get_tiles(self.mapdata.focus)\nself.cols = cols\nself.rows = rows\n\nself.bounds[\"min_longitude\"] = tiles[0].bounds[\"min_longitude\"]\nself.bounds[\"min_latitude\"] = tiles[0].bounds[\"min_latitude\"]\nself.bounds[\"max_longitude\"] = tiles[-1].bounds[\"max_longitude\"]\nself.bounds[\"max_latitude\"] = tiles[-1].bounds[\"max_latitude\"]\n\n# Ritar kartan\nfor tile in tiles:\n    #img = tile.get_picture()\n    x, y = self.gps_to_pixel(tile.bounds[\"min_longitude\"],\n                             tile.bounds[\"min_latitude\"])\n    tile.picture.draw(self.context, x, y)\n\n# Ritar ut eventuella objekt\n\nobjects = self.mapdata.objects\n    \nfor item in objects:\n    x, y = self.gps_to_pixel(objects[item].map_object_data.coords[0],\n                             objects[item].map_object_data.coords[1])\n    if objects[item].map_object_data.name == config.client.name:\n        objects[item].picture.is_me = True\n\n    if x != 0 and y != 0:\n        objects[item].picture.draw(self.context, x, y)", "path": "client\\gui\\mapscreen.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nStarts an audio server\n@param myIp\n@param port\n'''\n\n", "func_signal": "def start_audio_recv(self,port):\n", "code": "self.audio_recv = gst.parse_launch(\"udpsrc port=\"+str(port)+\" ! audio/x-iLBC,rate=8000,channels=1,mode=20 ! dspilbcsink\")\nbus1 = self.audio_recv.get_bus()\nbus1.add_signal_watch()\nbus1.enable_sync_message_emission()\nbus1.connect(\"message\", self.on_message)\nbus1.connect(\"sync-message::element\", self.on_sync_message)\nself.audio_recv.set_state(gst.STATE_PLAYING)", "path": "client\\gui\\camerascreen.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nAdd all objects from the database to the dict with objects to draw.\n'''\n", "func_signal": "def add_all_mapobjects(self):\n", "code": "mapobjectdata = self.db.get_all_mapobjects()\nfor data in mapobjectdata:\n    if data.__class__ == shared.data.UnitData:\n        self.mapdata.objects[data.id] = Unit(data)\n    elif data.__class__ == shared.data.POIData:\n        self.mapdata.objects[data.id] = POI(data)\nself.queue_draw()", "path": "client\\gui\\mapscreen.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nConstructor.\n'''\n", "func_signal": "def __init__(self):\n", "code": "dbus.mainloop.glib.DBusGMainLoop(set_as_default=True)\nself.session_bus = dbus.SessionBus()\nself.name = dbus.service.BusName(\"included.errors.QoSManager\", self.session_bus)\ndbus.service.Object.__init__(self, self.session_bus, '/QoSManager')\nself.bus = dbus.SystemBus()\n\ntry:\n    hal_obj = self.bus.get_object('org.freedesktop.Hal', '/org/freedesktop/Hal/Manager')\n    hal = dbus.Interface(hal_obj, 'org.freedesktop.Hal.Manager')\n    uids = hal.FindDeviceByCapability('battery')\n    self.dev_obj = self.bus.get_object('org.freedesktop.Hal', uids[0])\nexcept:\n    pass\n\n# boolean used to determine if testing or not\nself.testing = False\n\n# the service level\nself.service_level = \"None\"\n\n# the variables updated by this class\nself.gps_coord = (0,0)\nself.signal_strength = 0\nself.battery_level = 0\n\n# gps update interval (every X seconds)\nself.gps_update_interval = 10\n# try this number of times every time ;P\nself.try_limit = 29\n\n# service level update interval (every X seconds)\nself.service_level_update_interval = 10\n\nself.signal_strength_update_interval = 10\n\n# lower service level if under those levels\nself.critical_battery_level = 20 # % of max charged\nself.critical_signal_strength = -80 # dB damp\n\nself.running = False\n\nself.iap_id = None\nself.wlan = None", "path": "client\\qosmanager.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "# H\u00e4mtar alla tiles f\u00f6r en niv\u00e5\n", "func_signal": "def pixel_to_gps(self, movement_x, movement_y):\n", "code": "level = self.mapdata.get_level(self.zoom_level)\n# Plockar ur de tiles vi s\u00f6ker fr\u00e5n niv\u00e5n\ntiles, cols, rows = level.get_tiles(self.mapdata.focus)\n\n# Gps per pixlar\nwidth = self.bounds[\"max_longitude\"] - self.bounds[\"min_longitude\"]\nheight = self.bounds[\"min_latitude\"] - self.bounds[\"max_latitude\"]\ngps_per_pix_width = width / (cols * 300)\ngps_per_pix_height = height / (rows * 160)\n\n# Observera att kartans GPS-koordinatsystem b\u00f6rjar i v\u00e4nstra nedre\n# h\u00f6rnet, medan cairo b\u00f6rjar i v\u00e4nstra \u00f6vre h\u00f6rnet! P\u00e5 grund av detta\n# inverterar vi v\u00e4rdet vi r\u00e4knar fram s\u00e5 b\u00e5da koordinatsystemen\n# \u00f6verensst\u00e4mmer.\nreturn [gps_per_pix_width * movement_x,\n        gps_per_pix_height * movement_y]", "path": "client\\gui\\mapscreen.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "# Frig\u00f6r minnet genom att ladda ur alla tiles f\u00f6r f\u00f6reg\u00e5ende niv\u00e5\n#        level = self.mapdata.get_level(self.zoom_level)\n#        level.unload_tiles(\"all\")\n      \n", "func_signal": "def zoom(self, change):\n", "code": "if change == \"+\":\n    if self.zoom_level < 3:\n        self.zoom_level += 1\nelse:\n    if self.zoom_level > 1:\n        self.zoom_level -= 1\n\n# Ritar ny niv\u00e5\nself.queue_draw()", "path": "client\\gui\\mapscreen.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nUpdate the battery level.\n'''\n# Battery left (mAh)\n", "func_signal": "def check_battery_level(self):\n", "code": "battery_left = self.dev_obj.GetProperty('battery.reporting.current')\n# Battery lifetime (mAh)\nbattery_lifetime = self.dev_obj.GetProperty('battery.reporting.design')\n# True if charging\ncharging = self.dev_obj.GetProperty('battery.rechargeable.is_charging')\n# Battery left in %\nself.battery_level = battery_left*100/battery_lifetime\n\n# return battery level\nif charging:\n    return \"charging\"\nelif self.battery_level < self.critical_battery_level:\n    return \"low\"\nelse:\n    return \"high\"", "path": "client\\qosmanager.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nConstructor. Creates a map object.\n@param coord: The coordinates of this object.\n@param name: The name of this object.\n@param timestamp: The time this object was created.\n'''\n", "func_signal": "def __init__(self, coordx, coordy, name, timestamp, id):\n", "code": "self.coordx = coordx\nself.coordy = coordy\nself.name = name\nself.timestamp = timestamp\nself.id = id", "path": "shared\\data.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "#Show the incoming video\n", "func_signal": "def start_video_recv(self,port):\n", "code": "self.video_recv = gst.parse_launch(\"udpsrc port=\"+str(port)+ \" caps=application/x-rtp,clock-rate=90000 ! rtph263depay ! hantro4100dec ! xvimagesink\")\n\nbus3 = self.video_recv.get_bus()\nbus3.add_signal_watch()\nbus3.enable_sync_message_emission()\nbus3.connect(\"message\", self.on_message)\nbus3.connect(\"sync-message::element\", self.on_sync_message)\nself.video_recv.set_state(gst.STATE_PLAYING)", "path": "client\\gui\\camerascreen.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nCreate the database.\n'''\n\n# create tables and columns\n", "func_signal": "def create_database(db = Database()):\n", "code": "Base.metadata.create_all(db.engine)\nreturn db", "path": "shared\\data.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nDelete an object from the database.\n@param object: the object to delete.\n'''\n", "func_signal": "def delete(self, object):\n", "code": "session = self._Session()\nresult = session.query(object.__class__).filter_by(id=object.id).first()\nif result is not None:\n    session.delete(result)\n    session.commit()\nsession.close()\nself.emit(\"mapobject-deleted\", object)", "path": "shared\\data.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nUpdate the gps coordinates (own position).\n'''\n# start the GPS\n", "func_signal": "def update_gps_coord(self):\n", "code": "self.gps_context = gpsbt.start()\n\n# wait for the gps to start (Needed?)\ntime.sleep(2)\n\n# create the device\ngpsdevice = gpsbt.gps()\n\n# get the gps coordinates\n(x,y) = (0,0)\ntries = 0\nwhile (x,y) == (0,0):\n    coords = gpsdevice.get_position()\n    (x,y) = (coords[1],coords[0])\n    tries += 1\n    if tries >= self.try_limit:\n        break\n    time.sleep(1)\n\n# Stop the GPS\ngpsbt.stop(self.gps_context)\n\n# set gps coordinates\nif not (x,y) == (0,0):\n    self.gps_coord = (x,y)\n    self.signal_new_gps_coord(str(x), str(y))", "path": "client\\qosmanager.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nReturns this object as a list representation of this object with\nencapsulated objects first and self as last element.\n'''\n", "func_signal": "def to_list(self):\n", "code": "list = []\nfor var in self.__dict__.keys():\n    v = self.__dict__[var]\n    if isinstance(v, Packable):\n        list.append(v.to_dict())\nlist.append(self.to_dict())\nreturn list", "path": "shared\\data.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "'''\nConstructor. Creates a message.\n@param type: the type of this message\n@param unpacked_data: the unpacked data to pack\n'''\n", "func_signal": "def __init__(self, sender, receiver, type = None, subtype = None, response_to = 0, unpacked_data = None, prio = -1):\n", "code": "self.sender = sender\nself.receiver = receiver\nself.type = type\nself.subtype = subtype\nself.unpacked_data = unpacked_data\nself.timestamp = datetime.now()\nself.response_to = response_to\nif unpacked_data is not None and hasattr(unpacked_data, \"prio\"):\n    self.prio = unpacked_data.prio\nif prio != -1:\n    self.prio = prio\n\n# pack the unpacked data\nself.pack()", "path": "shared\\data.py", "repo_name": "Loffe/errors-included", "stars": 6, "license": "None", "language": "python", "size": 4872}
{"docstring": "\"\"\"locate Table objects within the given expression.\"\"\"\n\n", "func_signal": "def find_tables(clause, check_columns=False, include_aliases=False):\n", "code": "tables = []\nkwargs = {}\nif include_aliases:\n    def visit_alias(alias):\n        tables.append(alias)\n    kwargs['visit_alias']  = visit_alias\n\nif check_columns:\n    def visit_column(column):\n        tables.append(column.table)\n    kwargs['visit_column'] = visit_column\n\ndef visit_table(table):\n    tables.append(table)\nkwargs['visit_table'] = visit_table\n\nvisitors.traverse(clause, traverse_options= {'column_collections':False}, **kwargs)\nreturn tables", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"label columns present in a select().\"\"\"\n\n", "func_signal": "def label_select_column(self, select, column, asfrom):\n", "code": "if isinstance(column, sql._Label):\n    return column\n\nif select.use_labels and getattr(column, '_label', None):\n    return column.label(column._label)\n\nif \\\n    asfrom and \\\n    isinstance(column, sql._ColumnClause) and \\\n    not column.is_literal and \\\n    column.table is not None and \\\n    not isinstance(column.table, sql.Select):\n    return column.label(column.name)\nelif not isinstance(column, (sql._UnaryExpression, sql._TextClause, sql._BindParamClause)) and (not hasattr(column, 'name') or isinstance(column, sql._Function)):\n    return column.label(column.anon_label)\nelse:\n    return column", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"return a dictionary of bind parameter keys and values\"\"\"\n\n", "func_signal": "def construct_params(self, params=None):\n", "code": "if params:\n    pd = {}\n    for bindparam, name in self.bind_names.iteritems():\n        for paramname in (bindparam, bindparam.key, bindparam.shortname, name):\n            if paramname in params:\n                pd[name] = params[paramname]\n                break\n        else:\n            if callable(bindparam.value):\n                pd[name] = bindparam.value()\n            else:\n                pd[name] = bindparam.value\n    return pd\nelse:\n    pd = {}\n    for bindparam in self.bind_names:\n        if callable(bindparam.value):\n            pd[self.bind_names[bindparam]] = bindparam.value()\n        else:\n            pd[self.bind_names[bindparam]] = bindparam.value\n    return pd", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"traverse an expression and locate binary criterion pairs.\"\"\"\n\n", "func_signal": "def criterion_as_pairs(expression, consider_as_foreign_keys=None, consider_as_referenced_keys=None, any_operator=False):\n", "code": "if consider_as_foreign_keys and consider_as_referenced_keys:\n    raise exceptions.ArgumentError(\"Can only specify one of 'consider_as_foreign_keys' or 'consider_as_referenced_keys'\")\n    \ndef visit_binary(binary):\n    if not any_operator and binary.operator != operators.eq:\n        return\n    if not isinstance(binary.left, sql.ColumnElement) or not isinstance(binary.right, sql.ColumnElement):\n        return\n\n    if consider_as_foreign_keys:\n        if binary.left in consider_as_foreign_keys:\n            pairs.append((binary.right, binary.left))\n        elif binary.right in consider_as_foreign_keys:\n            pairs.append((binary.left, binary.right))\n    elif consider_as_referenced_keys:\n        if binary.left in consider_as_referenced_keys:\n            pairs.append((binary.left, binary.right))\n        elif binary.right in consider_as_referenced_keys:\n            pairs.append((binary.right, binary.left))\n    else:\n        if isinstance(binary.left, schema.Column) and isinstance(binary.right, schema.Column):\n            if binary.left.references(binary.right):\n                pairs.append((binary.right, binary.left))\n            elif binary.right.references(binary.left):\n                pairs.append((binary.left, binary.right))\npairs = []\nvisitors.traverse(expression, visit_binary=visit_binary)\nreturn pairs", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "# AliasedRow objects don't nest, so un-nest\n# if another AliasedRow was passed\n", "func_signal": "def __init__(self, row, map):\n", "code": "if isinstance(row, AliasedRow):\n    self.row = row.row\nelse:\n    self.row = row\nself.map = map", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"Construct a new ``IdentifierPreparer`` object.\n\ninitial_quote\n  Character that begins a delimited identifier.\n\nfinal_quote\n  Character that ends a delimited identifier. Defaults to `initial_quote`.\n\nomit_schema\n  Prevent prepending schema name. Useful for databases that do\n  not support schemae.\n\"\"\"\n\n", "func_signal": "def __init__(self, dialect, initial_quote='\"', final_quote=None, omit_schema=False):\n", "code": "self.dialect = dialect\nself.initial_quote = initial_quote\nself.final_quote = final_quote or self.initial_quote\nself.omit_schema = omit_schema\nself.__strings = {}", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"create a row adapter callable against a selectable.\"\"\"\n\n", "func_signal": "def row_adapter(from_, equivalent_columns=None):\n", "code": "if equivalent_columns is None:\n    equivalent_columns = {}\n\ndef locate_col(col):\n    c = from_.corresponding_column(col)\n    if c:\n        return c\n    elif col in equivalent_columns:\n        for c2 in equivalent_columns[col]:\n            corr = from_.corresponding_column(c2)\n            if corr:\n                return corr\n    return col\n    \nmap = util.PopulateDict(locate_col)\n\ndef adapt(row):\n    return AliasedRow(row, map)\nreturn adapt", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"compile the given string/parameters using this SchemaGenerator's dialect.\"\"\"\n", "func_signal": "def _compile(self, tocompile, parameters):\n", "code": "compiler = self.dialect.statement_compiler(self.dialect, tocompile, parameters)\ncompiler.compile()\nreturn compiler", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"create a copy of this adapter and chain to the given adapter.\n\ncurrently this adapter must be unchained to start, raises\nan exception if it's already chained.\n\nDoes not modify the given adapter.\n\"\"\"\n\n", "func_signal": "def copy_and_chain(self, adapter):\n", "code": "if adapter is None:\n    return self\n\nif hasattr(self, '_next'):\n    raise NotImplementedError(\"Can't chain_to on an already chained ClauseAdapter (yet)\")\n\nca = ClauseAdapter(self.selectable, self.include, self.exclude, self.equivalents)\nca._next = adapter\nreturn ca", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"Construct a new ``DefaultCompiler`` object.\n\ndialect\n  Dialect to be used\n\nstatement\n  ClauseElement to be compiled\n\ncolumn_keys\n  a list of column names to be compiled into an INSERT or UPDATE\n  statement.\n\"\"\"\n\n", "func_signal": "def __init__(self, dialect, statement, column_keys=None, inline=False, **kwargs):\n", "code": "super(DefaultCompiler, self).__init__(dialect, statement, column_keys, **kwargs)\n\n# if we are insert/update/delete.  set to true when we visit an INSERT, UPDATE or DELETE\nself.isdelete = self.isinsert = self.isupdate = False\n\n# compile INSERT/UPDATE defaults/sequences inlined (no pre-execute)\nself.inline = inline or getattr(statement, 'inline', False)\n\n# a dictionary of bind parameter keys to _BindParamClause instances.\nself.binds = {}\n\n# a dictionary of _BindParamClause instances to \"compiled\" names that are\n# actually present in the generated SQL\nself.bind_names = {}\n\n# a stack.  what recursive compiler doesn't have a stack ? :)\nself.stack = []\n\n# relates label names in the final SQL to\n# a tuple of local column/label name, ColumnElement object (if any) and TypeEngine.\n# ResultProxy uses this for type processing and column targeting\nself.result_map = {}\n\n# a dictionary of ClauseElement subclasses to counters, which are used to\n# generate truncated identifier names or \"anonymous\" identifiers such as\n# for aliases\nself.generated_ids = {}\n\n# paramstyle from the dialect (comes from DB-API)\nself.paramstyle = self.dialect.paramstyle\n\n# true if the paramstyle is positional\nself.positional = self.dialect.positional\n\nself.bindtemplate = BIND_TEMPLATES[self.paramstyle]\n\n# a list of the compiled's bind parameter names, used to help\n# formulate a positional argument list\nself.positiontup = []\n\n# an IdentifierPreparer that formats the quoting of identifiers\nself.preparer = self.dialect.identifier_preparer", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"Unpack 'schema.table.column'-like strings into components.\"\"\"\n\n", "func_signal": "def unformat_identifiers(self, identifiers):\n", "code": "try:\n    r = self._r_identifiers\nexcept AttributeError:\n    initial, final, escaped_final = \\\n             [re.escape(s) for s in\n              (self.initial_quote, self.final_quote,\n               self._escape_identifier(self.final_quote))]\n    r = re.compile(\n        r'(?:'\n        r'(?:%(initial)s((?:%(escaped)s|[^%(final)s])+)%(final)s'\n        r'|([^\\.]+))(?=\\.|$))+' %\n        { 'initial': initial,\n          'final': final,\n          'escaped': escaped_final })\n    self._r_identifiers = r\n\nreturn [self._unescape_identifier(i)\n        for i in [a or b for a, b in r.findall(identifiers)]]", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"sort a collection of Table objects in order of their foreign-key dependency.\"\"\"\n\n", "func_signal": "def sort_tables(tables, reverse=False):\n", "code": "tuples = []\nclass TVisitor(schema.SchemaVisitor):\n    def visit_foreign_key(_self, fkey):\n        if fkey.use_alter:\n            return\n        parent_table = fkey.column.table\n        if parent_table in tables:\n            child_table = fkey.parent.table\n            tuples.append( ( parent_table, child_table ) )\nvis = TVisitor()\nfor table in tables:\n    vis.traverse(table)\nsequence = topological.sort(tuples, tables)\nif reverse:\n    return util.reversed(sequence)\nelse:\n    return sequence", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"create a join condition between two tables.\n\nignore_nonexistent_tables=True allows a join condition to be\ndetermined between two tables which may contain references to\nother not-yet-defined tables.  In general the NoSuchTableError\nraised is only required if the user is trying to join selectables\nacross multiple MetaData objects (which is an extremely rare use \ncase).\n\n\"\"\"\n", "func_signal": "def join_condition(a, b, ignore_nonexistent_tables=False):\n", "code": "crit = []\nconstraints = util.Set()\nfor fk in b.foreign_keys:\n    try:\n        col = fk.get_referent(a)\n    except exceptions.NoReferencedTableError:\n        if ignore_nonexistent_tables:\n            continue\n        else:\n            raise\n            \n    if col:\n        crit.append(col == fk.parent)\n        constraints.add(fk.constraint)\n\nif a is not b:\n    for fk in a.foreign_keys:\n        try:\n            col = fk.get_referent(b)\n        except exceptions.NoReferencedTableError:\n            if ignore_nonexistent_tables:\n                continue\n            else:\n                raise\n        \n        if col:\n            crit.append(col == fk.parent)\n            constraints.add(fk.constraint)\n\nif len(crit) == 0:\n    raise exceptions.ArgumentError(\n        \"Can't find any foreign key relationships \"\n        \"between '%s' and '%s'\" % (a.description, b.description))\nelif len(constraints) > 1:\n    raise exceptions.ArgumentError(\n        \"Can't determine join between '%s' and '%s'; \"\n        \"tables have more than one foreign key \"\n        \"constraint relationship between them. \"\n        \"Please specify the 'onclause' of this \"\n        \"join explicitly.\" % (a.description, b.description))\nelif len(crit) == 1:\n    return (crit[0])\nelse:\n    return sql.and_(*crit)", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"create a set of tuples representing column/string pairs for use\nin an INSERT or UPDATE statement.\n\n\"\"\"\n\n", "func_signal": "def _get_colparams(self, stmt):\n", "code": "def create_bind_param(col, value):\n    bindparam = sql.bindparam(col.key, value, type_=col.type)\n    self.binds[col.key] = bindparam\n    return self.bindparam_string(self._truncate_bindparam(bindparam))\n\nself.postfetch = []\nself.prefetch = []\n\n# no parameters in the statement, no parameters in the\n# compiled params - return binds for all columns\nif self.column_keys is None and stmt.parameters is None:\n    return [(c, create_bind_param(c, None)) for c in stmt.table.columns]\n\n# if we have statement parameters - set defaults in the\n# compiled params\nif self.column_keys is None:\n    parameters = {}\nelse:\n    parameters = dict([(getattr(key, 'key', key), None) for key in self.column_keys])\n\nif stmt.parameters is not None:\n    for k, v in stmt.parameters.iteritems():\n        parameters.setdefault(getattr(k, 'key', k), v)\n\n# create a list of column assignment clauses as tuples\nvalues = []\nfor c in stmt.table.columns:\n    if c.key in parameters:\n        value = parameters[c.key]\n        if sql._is_literal(value):\n            value = create_bind_param(c, value)\n        else:\n            self.postfetch.append(c)\n            value = self.process(value.self_group())\n        values.append((c, value))\n    elif isinstance(c, schema.Column):\n        if self.isinsert:\n            if (c.primary_key and self.dialect.preexecute_pk_sequences and not self.inline):\n                if (((isinstance(c.default, schema.Sequence) and\n                      not c.default.optional) or\n                     not self.dialect.supports_pk_autoincrement) or\n                    (c.default is not None and\n                     not isinstance(c.default, schema.Sequence))):\n                    values.append((c, create_bind_param(c, None)))\n                    self.prefetch.append(c)\n            elif isinstance(c.default, schema.ColumnDefault):\n                if isinstance(c.default.arg, sql.ClauseElement):\n                    values.append((c, self.process(c.default.arg.self_group())))\n                    if not c.primary_key:\n                        # dont add primary key column to postfetch\n                        self.postfetch.append(c)\n                else:\n                    values.append((c, create_bind_param(c, None)))\n                    self.prefetch.append(c)\n            elif isinstance(c.default, schema.PassiveDefault):\n                if not c.primary_key:\n                    self.postfetch.append(c)\n            elif isinstance(c.default, schema.Sequence):\n                proc = self.process(c.default)\n                if proc is not None:\n                    values.append((c, proc))\n                    if not c.primary_key:\n                        self.postfetch.append(c)\n        elif self.isupdate:\n            if isinstance(c.onupdate, schema.ColumnDefault):\n                if isinstance(c.onupdate.arg, sql.ClauseElement):\n                    values.append((c, self.process(c.onupdate.arg.self_group())))\n                    self.postfetch.append(c)\n                else:\n                    values.append((c, create_bind_param(c, None)))\n                    self.prefetch.append(c)\n            elif isinstance(c.onupdate, schema.PassiveDefault):\n                self.postfetch.append(c)\nreturn values", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"Returns the column list of the given Join with all equivalently-named,\nequated columns folded into one column, where 'equated' means they are\nequated to each other in the ON clause of this join.\n\nThis function is used by Join.select(fold_equivalents=True).\n\nTODO: deprecate ?\n\"\"\"\n\n", "func_signal": "def folded_equivalents(join, equivs=None):\n", "code": "if equivs is None:\n    equivs = util.Set()\ndef visit_binary(binary):\n    if binary.operator == operators.eq and binary.left.name == binary.right.name:\n        equivs.add(binary.right)\n        equivs.add(binary.left)\nvisitors.traverse(join.onclause, visit_binary=visit_binary)\ncollist = []\nif isinstance(join.left, expression.Join):\n    left = folded_equivalents(join.left, equivs)\nelse:\n    left = list(join.left.columns)\nif isinstance(join.right, expression.Join):\n    right = folded_equivalents(join.right, equivs)\nelse:\n    right = list(join.right.columns)\nused = util.Set()\nfor c in left + right:\n    if c in equivs:\n        if c.name not in used:\n            collist.append(c)\n            used.add(c.name)\n    else:\n        collist.append(c)\nreturn collist", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"Prepare a quoted column name.\n\ndeprecated.  use preparer.quote(col, column.name) or combine with format_table()\n\"\"\"\n\n", "func_signal": "def format_column(self, column, use_table=False, name=None, table_name=None):\n", "code": "if name is None:\n    name = column.name\nif not getattr(column, 'is_literal', False):\n    if use_table:\n        return self.format_table(column.table, use_schema=False, name=table_name) + \".\" + self.quote(column, name)\n    else:\n        return self.quote(column, name)\nelse:\n    # literal textual elements get stuck into ColumnClause alot, which shouldnt get quoted\n    if use_table:\n        return self.format_table(column.table, use_schema=False, name=table_name) + \".\" + name\n    else:\n        return name", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"locate Column objects within the given expression.\"\"\"\n\n", "func_signal": "def find_columns(clause):\n", "code": "cols = util.Set()\ndef visit_column(col):\n    cols.add(col)\nvisitors.traverse(clause, visit_column=visit_column)\nreturn cols", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"given a list of columns, return a 'reduced' set based on natural equivalents.\n\nthe set is reduced to the smallest list of columns which have no natural\nequivalent present in the list.  A \"natural equivalent\" means that two columns\nwill ultimately represent the same value because they are related by a foreign key.\n\n\\*clauses is an optional list of join clauses which will be traversed\nto further identify columns that are \"equivalent\".\n\nThis function is primarily used to determine the most minimal \"primary key\"\nfrom a selectable, by reducing the set of primary key columns present\nin the the selectable to just those that are not repeated.\n\n\"\"\"\n\n", "func_signal": "def reduce_columns(columns, *clauses):\n", "code": "columns = util.OrderedSet(columns)\n\nomit = util.Set()\nfor col in columns:\n    for fk in col.foreign_keys:\n        for c in columns:\n            if c is col:\n                continue\n            if fk.column.shares_lineage(c):\n                omit.add(col)\n                break\n\nif clauses:\n    def visit_binary(binary):\n        if binary.operator == operators.eq:\n            cols = util.Set(chain(*[c.proxy_set for c in columns.difference(omit)]))\n            if binary.left in cols and binary.right in cols:\n                for c in columns:\n                    if c.shares_lineage(binary.right):\n                        omit.add(c)\n                        break\n    for clause in clauses:\n        visitors.traverse(clause, visit_binary=visit_binary)\n\nreturn expression.ColumnSet(columns.difference(omit))", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\util.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"Prepare a quoted table and schema name.\"\"\"\n\n", "func_signal": "def format_table(self, table, use_schema=True, name=None):\n", "code": "if name is None:\n    name = table.name\nresult = self.quote(table, name)\nif not self.omit_schema and use_schema and getattr(table, \"schema\", None):\n    result = self.quote(table, table.schema) + \".\" + result\nreturn result", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"Return True if the given identifier requires quoting.\"\"\"\n", "func_signal": "def _requires_quotes(self, value):\n", "code": "lc_value = value.lower()\nreturn (lc_value in self.reserved_words\n        or self.illegal_initial_characters.match(value[0])\n        or not self.legal_characters.match(unicode(value))\n        or (lc_value != value))", "path": "iphone-build\\directory-template\\usr\\lib\\python2.5\\site-packages\\SQLAlchemy-0.4.7p1-py2.5.egg\\sqlalchemy\\sql\\compiler.py", "repo_name": "typerlc/ankimini", "stars": 4, "license": "None", "language": "python", "size": 1100}
{"docstring": "\"\"\"\nReturn an instantiated filter. Options are passed to the filter\ninitializer if wanted. Raise a ClassNotFound if not found.\n\"\"\"\n", "func_signal": "def get_filter_by_name(filtername, **options):\n", "code": "cls = find_filter_class(filtername)\nif cls:\n    return cls(**options)\nelse:\n    raise ClassNotFound('filter %r not found' % filtername)", "path": "app\\console\\app\\pygments\\filters\\__init__.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Check if an object is safely callable.  Per default a function is\nconsidered safe unless the `unsafe_callable` attribute exists and is\nTrue.  Override this method to alter the behavior, but this won't\naffect the `unsafe` decorator from this module.\n\"\"\"\n", "func_signal": "def is_safe_callable(self, obj):\n", "code": "return not (getattr(obj, 'unsafe_callable', False) or \\\n            getattr(obj, 'alters_data', False))", "path": "app\\jinja2\\jinja2\\sandbox.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Process indentation spaces in a plain scalar.\"\"\"\n", "func_signal": "def parse_plain_scalar_indent(token_class):\n", "code": "def callback(lexer, match, context):\n    text = match.group()\n    if len(text) <= context.indent:\n        context.stack.pop()\n        context.stack.pop()\n        return\n    if text:\n        yield match.start(), token_class, text\n        context.pos = match.end()\nreturn callback", "path": "app\\console\\app\\pygments\\lexers\\text.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Process indentation spaces in a block scalar.\"\"\"\n", "func_signal": "def parse_block_scalar_indent(token_class):\n", "code": "def callback(lexer, match, context):\n    text = match.group()\n    if context.block_scalar_indent is None:\n        if len(text) <= max(context.indent, 0):\n            context.stack.pop()\n            context.stack.pop()\n            return\n        context.block_scalar_indent = len(text)\n    else:\n        if len(text) < context.block_scalar_indent:\n            context.stack.pop()\n            context.stack.pop()\n            return\n    if text:\n        yield match.start(), token_class, text\n        context.pos = match.end()\nreturn callback", "path": "app\\console\\app\\pygments\\lexers\\text.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"\nLookup a filter by name. Return None if not found.\n\"\"\"\n", "func_signal": "def find_filter_class(filtername):\n", "code": "if filtername in FILTERS:\n    return FILTERS[filtername]\nfor name, cls in find_plugin_filters():\n    if name == filtername:\n        return cls\nreturn None", "path": "app\\console\\app\\pygments\\filters\\__init__.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"\nReturn a generator of all filter names.\n\"\"\"\n", "func_signal": "def get_all_filters():\n", "code": "for name in FILTERS:\n    yield name\nfor name, _ in find_plugin_filters():\n    yield name", "path": "app\\console\\app\\pygments\\filters\\__init__.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"A range that can't generate ranges with a length of more than\nMAX_RANGE items.\n\"\"\"\n", "func_signal": "def safe_range(*args):\n", "code": "rng = xrange(*args)\nif len(rng) > MAX_RANGE:\n    raise OverflowError('range too big, maximum size for range is %d' %\n                        MAX_RANGE)\nreturn rng", "path": "app\\jinja2\\jinja2\\sandbox.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Reset the indentation levels.\"\"\"\n", "func_signal": "def reset_indent(token_class):\n", "code": "def callback(lexer, match, context):\n    text = match.group()\n    context.indent_stack = []\n    context.indent = -1\n    context.next_indent = 0\n    context.block_scalar_indent = None\n    yield match.start(), token_class, text\n    context.pos = match.end()\nreturn callback", "path": "app\\console\\app\\pygments\\lexers\\text.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Do not produce empty tokens.\"\"\"\n", "func_signal": "def something(token_class):\n", "code": "def callback(lexer, match, context):\n    text = match.group()\n    if not text:\n        return\n    yield match.start(), token_class, text\n    context.pos = match.end()\nreturn callback", "path": "app\\console\\app\\pygments\\lexers\\text.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Return an undefined object for unsafe attributes.\"\"\"\n", "func_signal": "def unsafe_undefined(self, obj, attribute):\n", "code": "return self.undefined('access to attribute %r of %r '\n                      'object is unsafe.' % (\n    attribute,\n    obj.__class__.__name__\n), name=attribute, obj=obj, exc=SecurityError)", "path": "app\\jinja2\\jinja2\\sandbox.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Set the previously saved indentation level.\"\"\"\n", "func_signal": "def set_indent(token_class, implicit=False):\n", "code": "def callback(lexer, match, context):\n    text = match.group()\n    if context.indent < context.next_indent:\n        context.indent_stack.append(context.indent)\n        context.indent = context.next_indent\n    if not implicit:\n        context.next_indent += len(text)\n    yield match.start(), token_class, text\n    context.pos = match.end()\nreturn callback", "path": "app\\console\\app\\pygments\\lexers\\text.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"\nMark a function or method as unsafe::\n\n    @unsafe\n    def delete(self):\n        pass\n\"\"\"\n", "func_signal": "def unsafe(f):\n", "code": "f.unsafe_callable = True\nreturn f", "path": "app\\jinja2\\jinja2\\sandbox.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Process an empty line in a block scalar.\"\"\"\n", "func_signal": "def parse_block_scalar_empty_line(indent_token_class, content_token_class):\n", "code": "def callback(lexer, match, context):\n    text = match.group()\n    if (context.block_scalar_indent is None or\n            len(text) <= context.block_scalar_indent):\n        if text:\n            yield match.start(), indent_token_class, text\n    else:\n        indentation = text[:context.block_scalar_indent]\n        content = text[context.block_scalar_indent:]\n        yield match.start(), indent_token_class, indentation\n        yield (match.start()+context.block_scalar_indent,\n                content_token_class, content)\n    context.pos = match.end()\nreturn callback", "path": "app\\console\\app\\pygments\\lexers\\text.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "# this test only tests the c extension\n", "func_signal": "def test_markup_leaks():\n", "code": "if hasattr(escape, 'func_code'):\n    raise SkipTest()\ncounts = set()\nfor count in xrange(20):\n    for item in xrange(1000):\n        escape(\"foo\")\n        escape(\"<foo>\")\n        escape(u\"foo\")\n        escape(u\"<foo>\")\n    counts.add(len(gc.get_objects()))\nassert len(counts) == 1, 'ouch, c extension seems to leak objects'", "path": "app\\jinja2\\tests\\test_various.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "# TODO: builtins are only subsequent tokens on lines\n#       and 'keywords' only happen at the beginning except\n#       for :au ones\n", "func_signal": "def get_tokens_unprocessed(self, text):\n", "code": "for index, token, value in \\\n    RegexLexer.get_tokens_unprocessed(self, text):\n    if token is Name.Other:\n        if self.is_in(value, self._cmd):\n            yield index, Keyword, value\n        elif self.is_in(value, self._opt) or \\\n             self.is_in(value, self._aut):\n            yield index, Name.Builtin, value\n        else:\n            yield index, Text, value\n    else:\n        yield index, token, value", "path": "app\\console\\app\\pygments\\lexers\\text.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Subscribe an object from sandboxed code.\"\"\"\n", "func_signal": "def getitem(self, obj, argument):\n", "code": "try:\n    return obj[argument]\nexcept (TypeError, LookupError):\n    if isinstance(argument, basestring):\n        try:\n            attr = str(argument)\n        except:\n            pass\n        else:\n            try:\n                value = getattr(obj, attr)\n            except AttributeError:\n                pass\n            else:\n                if self.is_safe_attribute(obj, argument, value):\n                    return value\n                return self.unsafe_undefined(obj, argument)\nreturn self.undefined(obj=obj, name=argument)", "path": "app\\jinja2\\jinja2\\sandbox.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"This function checks if an attribute on a builtin mutable object\n(list, dict, set or deque) would modify it if called.  It also supports\nthe \"user\"-versions of the objects (`sets.Set`, `UserDict.*` etc.) and\nwith Python 2.6 onwards the abstract base classes `MutableSet`,\n`MutableMapping`, and `MutableSequence`.\n\n>>> modifies_known_mutable({}, \"clear\")\nTrue\n>>> modifies_known_mutable({}, \"keys\")\nFalse\n>>> modifies_known_mutable([], \"append\")\nTrue\n>>> modifies_known_mutable([], \"index\")\nFalse\n\nIf called with an unsupported object (such as unicode) `False` is\nreturned.\n\n>>> modifies_known_mutable(\"foo\", \"upper\")\nFalse\n\"\"\"\n", "func_signal": "def modifies_known_mutable(obj, attr):\n", "code": "for typespec, unsafe in _mutable_spec:\n    if isinstance(obj, typespec):\n        return attr in unsafe\nreturn False", "path": "app\\jinja2\\jinja2\\sandbox.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Call an object from sandboxed code.\"\"\"\n# the double prefixes are to avoid double keyword argument\n# errors when proxying the call.\n", "func_signal": "def call(__self, __context, __obj, *args, **kwargs):\n", "code": "if not __self.is_safe_callable(__obj):\n    raise SecurityError('%r is not safely callable' % (__obj,))\nreturn __context.call(__obj, *args, **kwargs)", "path": "app\\jinja2\\jinja2\\sandbox.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Subscribe an object from sandboxed code and prefer the\nattribute.  The attribute passed *must* be a bytestring.\n\"\"\"\n", "func_signal": "def getattr(self, obj, attribute):\n", "code": "try:\n    value = getattr(obj, attribute)\nexcept AttributeError:\n    try:\n        return obj[attribute]\n    except (TypeError, LookupError):\n        pass\nelse:\n    if self.is_safe_attribute(obj, attribute, value):\n        return value\n    return self.unsafe_undefined(obj, attribute)\nreturn self.undefined(obj=obj, name=attribute)", "path": "app\\jinja2\\jinja2\\sandbox.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Save a possible indentation level.\"\"\"\n", "func_signal": "def save_indent(token_class, start=False):\n", "code": "def callback(lexer, match, context):\n    text = match.group()\n    extra = ''\n    if start:\n        context.next_indent = len(text)\n        if context.next_indent < context.indent:\n            while context.next_indent < context.indent:\n                context.indent = context.indent_stack.pop()\n            if context.next_indent > context.indent:\n                extra = text[context.indent:]\n                text = text[:context.indent]\n    else:\n        context.next_indent += len(text)\n    if text:\n        yield match.start(), token_class, text\n    if extra:\n        yield match.start()+len(text), token_class.Error, extra\n    context.pos = match.end()\nreturn callback", "path": "app\\console\\app\\pygments\\lexers\\text.py", "repo_name": "yesudeep/tiss-secure", "stars": 5, "license": "other", "language": "python", "size": 46141}
{"docstring": "\"\"\"Process a <thatstar> AIML element.\n\nOptional element attributes:\n    index: Specifies which \"*\" in the <that> pattern to match.\n\n<thatstar> elements are similar to <star> elements, except\nthat where <star/> returns the portion of the input string\nmatched by a \"*\" character in the pattern, <thatstar/> returns\nthe portion of the previous input string that was matched by a\n\"*\" in the current category's <that> pattern.\n\n\"\"\"\n", "func_signal": "def _processThatstar(self, elem, sessionID):\n", "code": "try: index = int(elem[1]['index'])\nexcept KeyError: index = 1\n# fetch the user's last input\ninputStack = self.getPredicate(self._inputStack, sessionID)\ninput = self._subbers['normal'].sub(inputStack[-1])\n# fetch the Kernel's last response (for 'that' context)\noutputHistory = self.getPredicate(self._outputHistory, sessionID)\ntry: that = self._subbers['normal'].sub(outputHistory[-1])\nexcept: that = \"\" # there might not be any output yet\ntopic = self.getPredicate(\"topic\", sessionID)\nresponse = self._brain.star(\"thatstar\", input, that, topic, index)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Create a new session with the specified ID string.\"\"\"\n", "func_signal": "def _addSession(self, sessionID):\n", "code": "if self._sessions.has_key(sessionID):\n    return\n# Create the session.\nself._sessions[sessionID] = {\n    # Initialize the special reserved predicates\n    self._inputHistory: [],\n    self._outputHistory: [],\n    self._inputStack: []\n}", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process a <that> AIML element.\n\nOptional element attributes:\n    index: Specifies which element from the output history to\n    return.  1 is the most recent response, 2 is the next most\n    recent, and so on.\n\n<that> elements (when they appear inside <template> elements)\nare the output equivilant of <input> elements; they return one\nof the Kernel's previous responses.\n\n\"\"\"\n", "func_signal": "def _processThat(self,elem, sessionID):\n", "code": "outputHistory = self.getPredicate(self._outputHistory, sessionID)\nindex = 1\ntry:\n    # According to the AIML spec, the optional index attribute\n    # can either have the form \"x\" or \"x,y\". x refers to how\n    # far back in the output history to go.  y refers to which\n    # sentence of the specified response to return.\n    index = int(elem[1]['index'].split(',')[0])\nexcept:\n    pass\ntry: return outputHistory[-index]\nexcept IndexError:\n    if self._verboseMode:\n        err = \"No such index %d while processing <that> element.\\n\" % index\n        sys.stderr.write(err)\n    return \"\"", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"\nIf input matches the form of a legal Googlism query (e.g. \"Who is X?\"),\nreturns a random answer from the Googlism server.  None is returned\nif an error occurs.\n\"\"\"\n\n# Make sure the input is a Googlism-type question.\n", "func_signal": "def respondTo(self, input):\n", "code": "result = self.__cmdRegex.search(input)\nif result is None:\n\treturn None\n\n# Fetch a list of Googlism answers to the input question, and return\n# a random selection.\ngoogs = self.__googlisms(result.group('query'), result.group('qtype'))\nif len(googs) != 0:\n\treturn \"%s %s %s\" % (result.group('query'), string.lower(result.group('verb')),\n\t\t\t\t\t\t random.choice(googs))\nelse:\n\treturn \"I don't know %s you're talking about.\" % string.lower(result.group('qtype'))", "path": "scripts\\googlism.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Retrieve the value of the specified bot predicate.\n\nIf name is not a valid bot predicate, the empty string is returned.        \n\n\"\"\"\n", "func_signal": "def getBotPredicate(self, name):\n", "code": "try: return self._botPredicates[name]\nexcept KeyError: return \"\"", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Return the Kernel's response to the input string.\"\"\"\n", "func_signal": "def respond(self, input, sessionID = _globalSessionID):\n", "code": "if len(input) == 0:\n    return \"\"\n\n#ensure that input is a unicode string\ntry: input = input.decode(self._textEncoding, 'replace')\nexcept UnicodeError: pass\nexcept AttributeError: pass\n\n# prevent other threads from stomping all over us.\nself._respondLock.acquire()\n\n# Add the session, if it doesn't already exist\nself._addSession(sessionID)\n\n# split the input into discrete sentences\nsentences = Utils.sentences(input)\nfinalResponse = \"\"\nfor s in sentences:\n    # Add the input to the history list before fetching the\n    # response, so that <input/> tags work properly.\n    inputHistory = self.getPredicate(self._inputHistory, sessionID)\n    inputHistory.append(s)\n    while len(inputHistory) > self._maxHistorySize:\n        inputHistory.pop(0)\n    self.setPredicate(self._inputHistory, inputHistory, sessionID)\n    \n    # Fetch the response\n    response = self._respond(s, sessionID)\n\n    # add the data from this exchange to the history lists\n    outputHistory = self.getPredicate(self._outputHistory, sessionID)\n    outputHistory.append(response)\n    while len(outputHistory) > self._maxHistorySize:\n        outputHistory.pop(0)\n    self.setPredicate(self._outputHistory, outputHistory, sessionID)\n\n    # append this response to the final response.\n    finalResponse += (response + \"  \")\nfinalResponse = finalResponse.strip()\n\nassert(len(self.getPredicate(self._inputStack, sessionID)) == 0)\n\n# release the lock and return\nself._respondLock.release()\ntry: return finalResponse.encode(self._textEncoding)\nexcept UnicodeError: return finalResponse", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process a <star> AIML element.\n\nOptional attribute elements:\n    index: Which \"*\" character in the current pattern should\n    be matched?\n\n<star> elements return the text fragment matched by the \"*\"\ncharacter in the current input pattern.  For example, if the\ninput \"Hello Tom Smith, how are you?\" matched the pattern\n\"HELLO * HOW ARE YOU\", then a <star> element in the template\nwould evaluate to \"Tom Smith\".\n\n\"\"\"\n", "func_signal": "def _processStar(self, elem, sessionID):\n", "code": "try: index = int(elem[1]['index'])\nexcept KeyError: index = 1\n# fetch the user's last input\ninputStack = self.getPredicate(self._inputStack, sessionID)\ninput = self._subbers['normal'].sub(inputStack[-1])\n# fetch the Kernel's last response (for 'that' context)\noutputHistory = self.getPredicate(self._outputHistory, sessionID)\ntry: that = self._subbers['normal'].sub(outputHistory[-1])\nexcept: that = \"\" # there might not be any output yet\ntopic = self.getPredicate(\"topic\", sessionID)\nresponse = self._brain.star(\"star\", input, that, topic, index)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process an <li> AIML element.\n\nOptional attribute elements:\n    name: the name of a predicate to query.\n    value: the value to check that predicate for.\n\n<li> elements process their contents recursively and return\nthe results. They can only appear inside <condition> and\n<random> elements.  See _processCondition() and\n_processRandom() for details of their usage.\n \n\"\"\"\n", "func_signal": "def _processLi(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process an <sr> AIML element.\n\n<sr> elements are shortcuts for <srai><star/></srai>.\n\n\"\"\"\n", "func_signal": "def _processSr(self,elem,sessionID):\n", "code": "star = self._processElement(['star',{}], sessionID)\nresponse = self._respond(star, sessionID)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process a <lowercase> AIML element.\n\n<lowercase> elements process their contents recursively, and\nthen convert the results to all-lowercase.\n\n\"\"\"\n", "func_signal": "def _processLowercase(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nreturn string.lower(response)", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process a <person> AIML element.\n\n<person> elements process their contents recursively, and then\nconvert all pronouns in the results from 1st person to 2nd\nperson, and vice versa.  This subsitution is handled by the\naiml.WordSub module.\n\nIf the <person> tag is used atomically (e.g. <person/>), it is\na shortcut for <person><star/></person>.\n\n\"\"\"\n", "func_signal": "def _processPerson(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nif len(elem[2:]) == 0:  # atomic <person/> = <person><star/></person>\n    response = self._processElement(['star',{}], sessionID)    \nreturn self._subbers['person'].sub(response)", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process a <random> AIML element.\n\n<random> elements contain zero or more <li> elements.  If\nnone, the empty string is returned.  If one or more <li>\nelements are present, one of them is selected randomly to be\nprocessed recursively and have its results returned.  Only the\nchosen <li> element's contents are processed.  Any non-<li> contents are\nignored.\n\n\"\"\"\n", "func_signal": "def _processRandom(self, elem, sessionID):\n", "code": "listitems = []\nfor e in elem[2:]:\n    if e[0] == 'li':\n        listitems.append(e)\nif len(listitems) == 0:\n    return \"\"\n        \n# select and process a random listitem.\nrandom.shuffle(listitems)\nreturn self._processElement(listitems[0], sessionID)", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process a <srai> AIML element.\n\n<srai> elements recursively process their contents, and then\npass the results right back into the AIML interpreter as a new\npiece of input.  The results of this new input string are\nreturned.\n\n\"\"\"\n", "func_signal": "def _processSrai(self,elem, sessionID):\n", "code": "newInput = \"\"\nfor e in elem[2:]:\n    newInput += self._processElement(e, sessionID)\nreturn self._respond(newInput, sessionID)", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Retrieve the current value of the predicate 'name' from the\nspecified session.\n\nIf name is not a valid predicate in the session, the empty\nstring is returned.\n\n\"\"\"\n", "func_signal": "def getPredicate(self, name, sessionID = _globalSessionID):\n", "code": "try: return self._sessions[sessionID][name]\nexcept KeyError: return \"\"", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Private version of respond(), does the real work.\"\"\"\n", "func_signal": "def _respond(self, input, sessionID):\n", "code": "if len(input) == 0:\n    return \"\"\n\n# guard against infinite recursion\ninputStack = self.getPredicate(self._inputStack, sessionID)\nif len(inputStack) > self._maxRecursionDepth:\n    if self._verboseMode:\n        err = \"WARNING: maximum recursion depth exceeded (input='%s')\" % input.encode(self._textEncoding, 'replace')\n        sys.stderr.write(err)\n    return \"\"\n\n# push the input onto the input stack\ninputStack = self.getPredicate(self._inputStack, sessionID)\ninputStack.append(input)\nself.setPredicate(self._inputStack, inputStack, sessionID)\n\n# run the input through the 'normal' subber\nsubbedInput = self._subbers['normal'].sub(input)\n\n# fetch the bot's previous response, to pass to the match()\n# function as 'that'.\noutputHistory = self.getPredicate(self._outputHistory, sessionID)\ntry: that = outputHistory[-1]\nexcept IndexError: that = \"\"\nsubbedThat = self._subbers['normal'].sub(that)\n\n# fetch the current topic\ntopic = self.getPredicate(\"topic\", sessionID)\nsubbedTopic = self._subbers['normal'].sub(topic)\n\n# Determine the final response.\nresponse = \"\"\nelem = self._brain.match(subbedInput, subbedThat, subbedTopic)\nif elem is None:\n    if self._verboseMode:\n        err = \"WARNING: No match found for input: %s\\n\" % input.encode(self._textEncoding)\n        sys.stderr.write(err)\nelse:\n    # Process the element into a response string.\n    response += self._processElement(elem, sessionID).strip()\n    response += \" \"\nresponse = response.strip()\n\n# pop the top entry off the input stack.\ninputStack = self.getPredicate(self._inputStack, sessionID)\ninputStack.pop()\nself.setPredicate(self._inputStack, inputStack, sessionID)\n\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Return the template which is the closest match to pattern. The\n'that' parameter contains the bot's previous response. The 'topic'\nparameter contains the current topic of conversation.\n\nReturns None if no template is found.\n\n\"\"\"\n", "func_signal": "def match(self, pattern, that, topic):\n", "code": "if len(pattern) == 0:\n\treturn None\n# Mutilate the input.  Remove all punctuation and convert the\n# text to all caps.\ninput = string.upper(pattern)\ninput = re.sub(self._puncStripRE, \"\", input)\nif that.strip() == u\"\": that = u\"ULTRABOGUSDUMMYTHAT\" # 'that' must never be empty\nthatInput = string.upper(that)\nthatInput = re.sub(self._whitespaceRE, \" \", thatInput)\nthatInput = re.sub(self._puncStripRE, \"\", thatInput)\nif topic.strip() == u\"\": topic = u\"ULTRABOGUSDUMMYTOPIC\" # 'topic' must never be empty\ntopicInput = string.upper(topic)\ntopicInput = re.sub(self._puncStripRE, \"\", topicInput)\n\n# Pass the input off to the recursive call\npatMatch, template = self._match(input.split(), thatInput.split(), topicInput.split(), self._root)\nreturn template", "path": "aiml\\PatternMgr.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process a <learn> AIML element.\n\n<learn> elements process their contents recursively, and then\ntreat the result as an AIML file to open and learn.\n\n\"\"\"\n", "func_signal": "def _processLearn(self, elem, sessionID):\n", "code": "filename = \"\"\nfor e in elem[2:]:\n    filename += self._processElement(e, sessionID)\nself.learn(filename)\nreturn \"\"", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process a <topicstar> AIML element.\n\nOptional element attributes:\n    index: Specifies which \"*\" in the <topic> pattern to match.\n\n<topicstar> elements are similar to <star> elements, except\nthat where <star/> returns the portion of the input string\nmatched by a \"*\" character in the pattern, <topicstar/>\nreturns the portion of current topic string that was matched\nby a \"*\" in the current category's <topic> pattern.\n\n\"\"\"\n", "func_signal": "def _processTopicstar(self, elem, sessionID):\n", "code": "try: index = int(elem[1]['index'])\nexcept KeyError: index = 1\n# fetch the user's last input\ninputStack = self.getPredicate(self._inputStack, sessionID)\ninput = self._subbers['normal'].sub(inputStack[-1])\n# fetch the Kernel's last response (for 'that' context)\noutputHistory = self.getPredicate(self._outputHistory, sessionID)\ntry: that = self._subbers['normal'].sub(outputHistory[-1])\nexcept: that = \"\" # there might not be any output yet\ntopic = self.getPredicate(\"topic\", sessionID)\nresponse = self._brain.star(\"topicstar\", input, that, topic, index)\nreturn response", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Set the value of the predicate 'name' in the specified\nsession.\n\nIf sessionID is not a valid session, it will be created. If\nname is not a valid predicate in the session, it will be\ncreated.\n\n\"\"\"\n", "func_signal": "def setPredicate(self, name, value, sessionID = _globalSessionID):\n", "code": "self._addSession(sessionID) # add the session, if it doesn't already exist.\nself._sessions[sessionID][name] = value", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\"Process a <person2> AIML element.\n\n<person2> elements process their contents recursively, and then\nconvert all pronouns in the results from 1st person to 3rd\nperson, and vice versa.  This subsitution is handled by the\naiml.WordSub module.\n\nIf the <person2> tag is used atomically (e.g. <person2/>), it is\na shortcut for <person2><star/></person2>.\n\n\"\"\"\n", "func_signal": "def _processPerson2(self,elem, sessionID):\n", "code": "response = \"\"\nfor e in elem[2:]:\n    response += self._processElement(e, sessionID)\nif len(elem[2:]) == 0:  # atomic <person2/> = <person2><star/></person2>\n    response = self._processElement(['star',{}], sessionID)\nreturn self._subbers['person2'].sub(response)", "path": "aiml\\Kernel.py", "repo_name": "russellhaering/ansr8r", "stars": 4, "license": "None", "language": "python", "size": 5070}
{"docstring": "\"\"\" A ValueError should be raised when sort_order is other\nthan \"asc\" or \"desc\".\n\"\"\"\n", "func_signal": "def test_exception_invalid_sort_order(self):\n", "code": "self.assertRaises(ValueError, self.conn.query, \"id:\" + \"abc\",\n                    **{\"sort\":\"id\", \"sort_order\":\"invalid_sort_order\"})", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Try to add one document object and commit changes in one operation\n\"\"\"\n\n", "func_signal": "def test_add_one_document_object_implicit_commit(self):\n", "code": "user_id = get_rand_string()\ndata = get_rand_string()\nid = get_rand_string()\n\ndoc = Document()\ndoc[\"user_id\"] = user_id\ndoc[\"data\"] = data\ndoc[\"id\"] = id\n\n# Commit the changes\nself.conn.add(True, doc)\nresults = self.conn.query(\"id:\" + id).results\n\nself.assertEquals(len(results), 1,\n    \"Could not find expected data (id:%s)\" % id)\n\ndoc = results[0]\nself.assertEquals(doc[\"user_id\"], user_id)\nself.assertEquals(doc[\"data\"], data)", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"Try to add one document object.\n\"\"\"\n", "func_signal": "def test_add_one_document_object(self):\n", "code": "user_id = get_rand_string()\ndata = get_rand_string()\nid = get_rand_string()\n\ndoc = Document()\ndoc[\"user_id\"] = user_id\ndoc[\"data\"] = data\ndoc[\"id\"] = id\n\n#raise Exception, doc.as_xml\nself.conn.add(doc)\nself.conn.commit()\nresults = self.conn.query(\"id:\" + id).results\n\nself.assertEquals(len(results), 1,\n    \"Could not find expected data (id:%s)\" % id)", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "# Connections may as well not exist.\n", "func_signal": "def tearDown(self):\n", "code": "for conn in [\"conn\", \"conn2\"]:\n    if hasattr(self, conn):\n        getattr(self, conn).close()", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Try to add one document.\n\"\"\"\n", "func_signal": "def test_add_one_document(self):\n", "code": "user_id = get_rand_string()\ndata = get_rand_string()\nid = get_rand_string()\n\ndoc = {}\ndoc[\"user_id\"] = user_id\ndoc[\"data\"] = data\ndoc[\"id\"] = id\n\nself.conn.add(**doc)\nself.conn.commit()\nresults = self.conn.query(\"id:\" + id).results\n\nself.assertEquals(len(results), 1,\n    \"Could not find expected data (id:%s)\" % id)\n\ndoc = results[0]\nself.assertEquals(doc[\"user_id\"], user_id)\nself.assertEquals(doc[\"data\"], data)", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Check whether the score is not being returned when explicitly\ntold not to do so.\n\"\"\"\n", "func_signal": "def test_query_no_score(self):\n", "code": "id = get_rand_string()\n\n# Same data and user_id\nuser_id = data = get_rand_string()\n\nself.conn.add(id=id, user_id=user_id, data=data)\nself.conn.commit()\n\nresults = self.conn.query(\"id:\" + id, score=False).results\n\nself.assertEquals(len(results), 1,\n    \"No documents fetched, expected id:%s\" % (id))\n\ndoc = results[0]\n\nself.assertTrue(\"score\" not in doc,\n    \"No score should be returned, doc:%s\" % repr(doc))", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"Update our built-in md5 registry\"\"\"\n\n", "func_signal": "def update_md5(filenames):\n", "code": "import re\n\nfor name in filenames:\n    base = os.path.basename(name)\n    f = open(name,'rb')\n    md5_data[base] = md5(f.read()).hexdigest()\n    f.close()\n\ndata = [\"    %r: %r,\\n\" % it for it in md5_data.items()]\ndata.sort()\nrepl = \"\".join(data)\n\nimport inspect\nsrcfile = inspect.getsourcefile(sys.modules[__name__])\nf = open(srcfile, 'rb'); src = f.read(); f.close()\n\nmatch = re.search(\"\\nmd5_data = {\\n([^}]+)}\", src)\nif not match:\n    print >>sys.stderr, \"Internal error!\"\n    sys.exit(2)\n\nsrc = src[:match.start(1)] + repl + src[match.end(1):]\nf = open(srcfile,'w')\nf.write(src)\nf.close()", "path": "ez_setup.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Try to add one document and commit changes in one operation.\n\"\"\"\n\n# That one fails in r5 (<commit/> must be made on its own)\n\n", "func_signal": "def test_add_one_document_implicit_commit(self):\n", "code": "user_id = get_rand_string()\ndata = get_rand_string()\nid = get_rand_string()\n\ndoc = {}\ndoc[\"user_id\"] = user_id\ndoc[\"data\"] = data\ndoc[\"id\"] = id\n\n# Commit the changes\nself.conn.add(True, **doc)\nresults = self.conn.query(\"id:\" + id).results\n\nself.assertEquals(len(results), 1,\n    \"Could not find expected data (id:%s)\" % id)\n\ndoc = results[0]\nself.assertEquals(doc[\"user_id\"], user_id)\nself.assertEquals(doc[\"data\"], data)", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nParse the XML results of a /select call.\n\"\"\"\n", "func_signal": "def parse_query_response(data, params, connection):\n", "code": "parser = make_parser()\nhandler = ResponseContentHandler()\nparser.setContentHandler(handler)\nparser.parse(data)\nif handler.stack[0].children:\n    response = handler.stack[0].children[0].final\n    response._params = params\n    response._connection = connection\n    return response\nelse:\n    return None", "path": "solr.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "# These are set in ResponseContentHandler.endElement()\n", "func_signal": "def __init__(self, connection):\n", "code": "self.header = {}\nself.results = []\n\n# These are set by parse_query_response().\n# Used only if .next_batch()/previous_batch() is called\nself._connection = connection\nself._params = {}", "path": "solr.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Test whether sorting works (using default, ascending, sort order).\n\"\"\"\n", "func_signal": "def test_query_sort_default_sort_order(self):\n", "code": "doc_count = 10\nfield_to_be_sorted_by = \"data\"\nprefix = get_rand_string()\n\ndata = [prefix + \"-\" + str(x) for x in range(10)]\n\n# Same user_id for all documents\nuser_id = get_rand_string()\n\nfor datum in data:\n    self.conn.add(id=get_rand_string(), user_id=user_id, data=datum)\nself.conn.commit()\n\nresults = self.conn.query(q=\"user_id:\" + user_id, sort=\"data\").results\n\nself.assertEquals(len(results), doc_count,\n    \"There should be %d documents returned, got:%d, results:%s\" % (\n        doc_count, len(results), results))\n\nquery_data = [doc[\"data\"] for doc in results]\n\nfor idx, datum in enumerate(sorted(data)):\n    self.assertEquals(datum, query_data[idx],\n        \"Expected %s instead of %s on position %s in query_data:%s\" % (\n            datum, query_data[idx], idx, query_data))", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Try to delete a single document matching a given query.\n\"\"\"\n", "func_signal": "def test_delete_one_document_by_query(self):\n", "code": "user_id = get_rand_string()\ndata = get_rand_string()\nid = get_rand_string()\n\ndoc = {}\ndoc[\"user_id\"] = user_id\ndoc[\"data\"] = data\ndoc[\"id\"] = data\n\nself.conn.add(**doc)\nself.conn.commit()\n\nresults = self.conn.query(\"id:\" + id).results\n\nself.conn.delete_query(\"id:\" + id)\nself.conn.commit()\n\nresults = self.conn.query(\"id:\" + id).results\nself.assertEquals(len(results), 0,\n    \"Document (id:%s) should've been deleted\" % id)", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nLoad the next set of matches.\n\nBy default, SOLR returns 10 at a time.\n\"\"\"\n", "func_signal": "def next_batch(self):\n", "code": "try:\n    start = int(self.results.start)\nexcept AttributeError:\n    start = 0\n\nstart += len(self.results)\nparams = dict(self._params)\nparams['start'] = start\nq = params['q']\ndel params['q']\nreturn self._connection.query(q, **params)", "path": "solr.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Check correctness of handling Unicode data when adding many\ndocuments.\n\"\"\"\n# Some Polish characters (UTF-8)\n", "func_signal": "def test_add_many_unicode(self):\n", "code": "chars = (\"\\xc4\\x99\\xc3\\xb3\\xc4\\x85\\xc5\\x9b\\xc5\\x82\"\n         \"\\xc4\\x98\\xc3\\x93\\xc4\\x84\\xc5\\x9a\\xc5\\x81\").decode(\"utf-8\")\n\ndocuments = []\nfor char in chars:\n    doc = {}\n    doc[\"data\"] = char\n    doc[\"user_id\"] = get_rand_string()\n    doc[\"id\"] = get_rand_string()\n    documents.append(doc)\n\nuser_ids = [doc[\"user_id\"] for doc in documents]\nids = [doc[\"id\"] for doc in documents]\n\nself.conn.add(documents)\nself.conn.commit()\n\nresults = []\nfor doc in documents:\n    id = doc[\"id\"]\n    res = self.conn.query(\"id:\" + id).results\n    if not res:\n        self.fail(\"Could not find document (id:%s)\" % id)\n    results.append(res[0])\n\nself.assertEquals(len(results), len(chars),\n    \"Query didn't return all documents. Expected: %d, got: %d\" % (\n        len(chars), len(results)))\n\n# Use sets' symmetric difference to check if we have all documents\n# (same way as in TestAddingDocuments.test_add_many)\n\nquery_user_ids = [doc[\"user_id\"] for doc in results]\nquery_data = [doc[\"data\"] for doc in results]\nquery_ids = [doc[\"id\"] for doc in results]\n\nuser_ids_symdiff = set(user_ids) ^ set(query_user_ids)\ndata_symdiff = set(chars) ^ set(query_data)\nids_symdiff = set(ids) ^ set(query_ids)\n\nself.assertEqual(user_ids_symdiff, set([]),\n    \"User IDs sets differ (difference:%s)\" % (user_ids_symdiff))\nself.assertEqual(data_symdiff, set([]),\n    \"Data sets differ (difference:%s)\" % (data_symdiff))\nself.assertEqual(ids_symdiff, set([]),\n    \"IDs sets differ (difference:%s)\" % (ids_symdiff))", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nConvert datetimes to the subset of ISO 8601 that SOLR expects.\n\"\"\"\n", "func_signal": "def utc_to_string(value):\n", "code": "value = value.astimezone(utc).isoformat()\nif '+' in value:\n    value = value.split('+')[0]\nvalue += 'Z'\nreturn value", "path": "solr.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nFinal will eventually be the \"final\" representation of\nthis node, whether an int, list, dict, etc.\n\"\"\"\n", "func_signal": "def __init__(self, name, attrs):\n", "code": "self.chars = []\nself.name = name\nself.attrs = attrs\nself.final = None\nself.children = []", "path": "solr.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Try to return only a specific field.\n\"\"\"\n", "func_signal": "def test_query_specific_field(self):\n", "code": "field_to_return = \"id\"\ndoc_count = 10\nids = [get_rand_string() for x in range(doc_count)]\nuser_ids = [get_rand_string() for x in range(doc_count)]\n\n# Same data for all documents\ndata = get_rand_string()\n\nfor idx, id in enumerate(ids):\n    self.conn.add(id=ids[idx], user_id=user_ids[idx], data=data)\nself.conn.commit()\n\n# We want to return only the \"id\" field\nresults = self.conn.query(\"data:\" + data, fields=field_to_return).results\nself.assertEquals(len(results), doc_count,\n    \"There should be exactly %d documents returned, got: %d\" % (\n        doc_count, len(results)))\n\n# Use the symmetric difference to check whether all IDs have been\n# fetched by a query.\n\nquery_ids = [doc[field_to_return] for doc in results]\nids_symdiff = set(ids) ^ set(query_ids)\n\nself.assertEquals(ids_symdiff, set([]),\n    \"Query didn't return expected fields (difference:%s)\" % (ids_symdiff))\n\n# Make sure no other field has been returned, note: by default\n# queries also return score for each document.\n\nfor result in results:\n    fields = result.keys()\n    fields.remove(field_to_return)\n\n    # Now there should only a score field\n    self.assertEquals(len(fields), 1,\n        (\"More fields returned than expected, \"\n        \"expected:%s and score, the result is:%s)\" % (\n            field_to_return,result)))\n\n    self.assertEquals(fields[0], \"score\",\n        \"Query returned some other fields then %s and score, result:%s\" % (\n            field_to_return,result))", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Try to add more than one document in a single operation.\n\"\"\"\n", "func_signal": "def test_add_many_objects(self):\n", "code": "doc_count = 10\nuser_ids = [get_rand_string() for x in range(doc_count)]\ndata =  [get_rand_string() for x in range(doc_count)]\nids =  [get_rand_string() for x in range(doc_count)]\ndocuments = []\nfor x in range(doc_count):\n    doc = Document()\n    doc['user_id'] = user_ids[x]\n    doc['data'] = data[x]\n    doc['id'] = ids[x]\n    documents.append(doc)\n\nself.conn.add(documents)\nself.conn.commit()\n\nresults = []\nfor id in ids:\n    res = self.conn.query(\"id:\" + id).results\n    if not res:\n        self.fail(\"Could not find document (id:%s)\" % id)\n    results.append(res[0])\n\nself.assertEquals(len(results), doc_count,\n    \"Query didn't return all documents. Expected: %d, got: %d\" % (\n        doc_count, len(results)))\n\nquery_user_ids = [doc[\"user_id\"] for doc in results]\nquery_data = [doc[\"data\"] for doc in results]\nquery_ids = [doc[\"id\"] for doc in results]\n\n# Symmetric difference will give us those documents which are neither\n# in original list nor in a fetched one. It's a handy way to check\n# whether all, and only those expected, documents have been returned.\n\nuser_ids_symdiff = set(user_ids) ^ set(query_user_ids)\ndata_symdiff = set(data) ^ set(query_data)\nids_symdiff = set(ids) ^ set(query_ids)\n\nself.assertEqual(user_ids_symdiff, set([]),\n    \"User IDs sets differ (difference:%s)\" % (user_ids_symdiff))\nself.assertEqual(data_symdiff, set([]),\n    \"Data sets differ (difference:%s)\" % (data_symdiff))\nself.assertEqual(ids_symdiff, set([]),\n    \"IDs sets differ (difference:%s)\" % (ids_symdiff))", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\" Try to add a document with a field of None\n\"\"\"\n", "func_signal": "def test_add_none_field(self):\n", "code": "user_id = get_rand_string()\ndata = get_rand_string()\nid = get_rand_string()\n\ndoc = {}\ndoc[\"user_id\"] = user_id\ndoc[\"data\"] = data\ndoc[\"id\"] = id\ndoc[\"num\"] = None\n\nself.conn.add(**doc)", "path": "tests\\test_all.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nAdd several documents to the SOLR server.\n\ndocs -- a list of dicts, where each dict is a document to add\n    to SOLR.\n\"\"\"\n", "func_signal": "def add_many(self, docs, _commit=False):\n", "code": "warnings.warn(\"add_many() is deprecated, lists of documents can now be passed to add()\", category=DeprecationWarning)\nlst = [u'<add>']\nself.__add_documents(lst, docs)\nlst.append(u'</add>')\nxstr = ''.join(lst)\nif not _commit:\n    return self._update(xstr)\nelse:\n    self._update(xstr)\n    return self.commit()", "path": "solr.py", "repo_name": "robyoung/solrpy", "stars": 7, "license": "None", "language": "python", "size": 350}
{"docstring": "\"\"\"\nGet filesize in a readable format.\n\"\"\"\n\n", "func_signal": "def _get_filesize(filesize_long):\n", "code": "filesize_str = ''\nif filesize_long < 1000:\n    filesize_str = str(filesize_long) + \"&nbsp;B\"\nelif filesize_long >= 1000 and filesize_long < 1000000:\n    filesize_str = str(filesize_long/1000) + \"&nbsp;kB\"\nelif filesize_long >= 1000000:\n    filesize_str = str(filesize_long/1000000) + \"&nbsp;MB\"\nreturn mark_safe(filesize_str)", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nGenerate Image Versions for existing singe Image or a whole Directory.\n    This is useful if someone uploads images via FTP, not using the\n    upload functionality of the FileBrowser.\n\"\"\"\n\n", "func_signal": "def generateimages(request, dir_name=None, file_name=None):\n", "code": "path = _get_path(request, dir_name)\nquery = _get_query(request.GET)\n\nif file_name:\n    # GENERATE IMAGES\n    if IMAGE_GENERATOR_LANDSCAPE != \"\" or IMAGE_GENERATOR_PORTRAIT != \"\":\n        _image_generator(PATH_SERVER, path, file_name)\n    # GENERATE CROPPED/RECTANGULAR IMAGE\n    if IMAGE_CROP_GENERATOR != \"\":\n        _image_crop_generator(PATH_SERVER, path, file_name)\nelse:\n    # GENERATE IMAGES FOR WHOLE DIRECTORY\n    dir_path = os.path.join(PATH_SERVER, path)\n    dir_list = os.listdir(dir_path)\n    for file in dir_list:\n        if os.path.isfile(os.path.join(PATH_SERVER, path, file)) and not re.compile(THUMB_PREFIX, re.M).search(file) and _get_file_type(file) == \"Image\":\n            # GENERATE IMAGES\n            if IMAGE_GENERATOR_LANDSCAPE != \"\" or IMAGE_GENERATOR_PORTRAIT != \"\":\n                _image_generator(PATH_SERVER, path, file)\n            # GENERATE CROPPED/RECTANGULAR IMAGE\n            if IMAGE_CROP_GENERATOR != \"\":\n                _image_crop_generator(PATH_SERVER, path, file)\n\n# MESSAGE & REDIRECT\nmsg = _('Successfully generated Images.')\nrequest.user.message_set.create(message=msg)\nreturn HttpResponseRedirect(URL_ADMIN + path + query['query_str_total'])\n\nreturn render_to_response('filebrowser/index.html', {\n    'dir': dir_name,\n    'query': query,\n    'settings_var': _get_settings_var(request.META['HTTP_HOST'], path),\n    'breadcrumbs': _get_breadcrumbs(_get_query(request.GET), dir_name, ''),\n    'root_path': URL_HOME,\n}, context_instance=Context(request))", "path": "filebrowser\\views.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nAdd and remove query parameters. From `django.contrib.admin`.\n\"\"\"\n", "func_signal": "def get_query_string(p, new_params=None, remove=None):\n", "code": "if new_params is None: new_params = {}\nif remove is None: remove = []\nfor r in remove:\n    for k in p.keys():\n        if k.startswith(r):\n            del p[k]\nfor k, v in new_params.items():\n    if k in p and v is None:\n        del p[k]\n    elif v is not None:\n        p[k] = v\nreturn mark_safe('?' + '&'.join([u'%s=%s' % (k, v) for k, v in p.items()]).replace(' ', '%20'))", "path": "filebrowser\\templatetags\\fb_tags.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nDelete existing File/Directory.\n    If file is an Image, also delete thumbnail.\n    When trying to delete a directory, the directory has to be empty.\n\"\"\"\n\n", "func_signal": "def delete(request, dir_name=None):\n", "code": "path = _get_path(request, dir_name)\nquery = _get_query(request.GET)\nmsg = \"\"\n\nif request.GET:\n    if request.GET.get('type') != \"Folder\":\n        server_path = os.path.join(PATH_SERVER, path, request.GET.get('filename'))\n        try:\n            \n            # DELETE FILE\n            os.unlink(server_path)\n            # TRY DELETING THUMBNAIL\n            path_thumb = os.path.join(PATH_SERVER, path, THUMB_PREFIX + request.GET.get('filename'))\n            try:\n                os.unlink(path_thumb)\n            except OSError:\n                pass\n            # TRY DELETING IMAGE_VERSIONS\n            versions_path = os.path.join(PATH_SERVER, path, request.GET.get('filename').replace(\".\", \"_\").lower() + IMAGE_GENERATOR_DIRECTORY)\n            try:\n                dir_list = os.listdir(versions_path)\n                for file in dir_list:\n                    file_path = os.path.join(versions_path, file)\n                    os.unlink(file_path)\n                os.rmdir(versions_path)\n            except OSError:\n                pass\n            \n            # MESSAGE & REDIRECT\n            msg = _('The file %s was successfully deleted.') % (request.GET.get('filename').lower())\n            request.user.message_set.create(message=msg)\n            return HttpResponseRedirect(URL_ADMIN + path + query['query_nodelete'])\n        except OSError:\n            # todo: define error message\n            msg = OSError\n    else:\n        server_path = os.path.join(PATH_SERVER, path, request.GET.get('filename'))\n        try:\n            os.rmdir(server_path)\n            \n            # MESSAGE & REDIRECT\n            msg = _('The directory %s was successfully deleted.') % (request.GET.get('filename').lower())\n            request.user.message_set.create(message=msg)\n            return HttpResponseRedirect(URL_ADMIN + path + query['query_nodelete'])\n        except OSError:\n            # todo: define error message\n            msg = OSError\n\nif msg:\n    request.user.message_set.create(message=msg)\n\nreturn render_to_response('filebrowser/index.html', {\n    'dir': dir_name,\n    'file': request.GET.get('filename', ''),\n    'query': query,\n    'settings_var': _get_settings_var(request.META['HTTP_HOST'], path),\n    'breadcrumbs': _get_breadcrumbs(_get_query(request.GET), dir_name, ''),\n    'root_path': URL_HOME,\n}, context_instance=Context(request))", "path": "filebrowser\\views.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nAllows the addition and removal of query string parameters.\n\n_response.html is just {{ response }}\n\nUsage:\nhttp://www.url.com/{% query_string \"param_to_add=value, param_to_add=value\" \"param_to_remove, params_to_remove\" %}\nhttp://www.url.com/{% query_string \"\" \"filter\" %}filter={{new_filter}}\nhttp://www.url.com/{% query_string \"sort=value\" \"sort\" %}\n\"\"\"\n# Written as an inclusion tag to simplify getting the context.\n", "func_signal": "def query_string(context, add=None, remove=None):\n", "code": "add = string_to_dict(add)\nremove = string_to_list(remove)\nparams = context['query'].copy()\nresponse = get_query_string(params, add, remove)\nreturn {'response': response }", "path": "filebrowser\\templatetags\\fb_tags.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nHandle File Upload.\n\"\"\"\n\n", "func_signal": "def _handle_file_upload(PATH_SERVER, path, file):\n", "code": "file_path = os.path.join(PATH_SERVER, path, file.name)\ndestination = open(file_path, 'wb+')\nfor chunk in file.chunks():\n    destination.write(chunk)", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nGet path.\n\"\"\"\n", "func_signal": "def _get_path(request, dir_name):\n", "code": "if request.user.is_superuser:\n    if dir_name:\n        return dir_name + \"/\"\n    else:\n        return \"\"\n\nif dir_name and dir_name.startswith('%s/' % request.user):\n    dir_name = dir_name.replace('%s/' % request.user, '')\nif dir_name == request.user.username:\n    dir_name = None\nif dir_name:\n    return '%s/%s/' % (request.user, dir_name)\nelse:\n    return '%s/' % request.user", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nHelper Function for use within views.\n\"\"\"\n", "func_signal": "def query_helper(query, add=None, remove=None):\n", "code": "add = string_to_dict(add)\nremove = string_to_list(remove)\nparams = query.copy()\nreturn get_query_string(params, add, remove)", "path": "filebrowser\\templatetags\\fb_tags.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nMultipe Upload.\n\"\"\"\n\n", "func_signal": "def upload(request, dir_name=None):\n", "code": "from django.forms.formsets import formset_factory\n\npath = _get_path(request, dir_name)\nquery = _get_query(request.GET)\n\n# PIL's Error \"Suspension not allowed here\" work around:\n# s. http://mail.python.org/pipermail/image-sig/1999-August/000816.html\nif STRICT_PIL:\n    from PIL import ImageFile\nelse:\n    try:\n        from PIL import ImageFile\n    except ImportError:\n        import ImageFile\nImageFile.MAXBLOCK = IMAGE_MAXBLOCK # default is 64k\n\nUploadFormSet = formset_factory(UploadForm, formset=BaseUploadFormSet, extra=5)\nif request.method == 'POST':\n    formset = UploadFormSet(data=request.POST, files=request.FILES, path_server=PATH_SERVER, path=path)\n    if formset.is_valid():\n        for cleaned_data in formset.cleaned_data:\n            if cleaned_data:\n                # UPLOAD FILE\n                _handle_file_upload(PATH_SERVER, path, cleaned_data['file'])\n                if _get_file_type(cleaned_data['file'].name) == \"Image\":\n                    # MAKE THUMBNAIL\n                    _make_image_thumbnail(PATH_SERVER, path, cleaned_data['file'].name)\n                    # IMAGE GENERATOR\n                    if FORCE_GENERATOR or (cleaned_data['use_image_generator'] and (IMAGE_GENERATOR_LANDSCAPE != \"\" or IMAGE_GENERATOR_PORTRAIT != \"\")):\n                        _image_generator(PATH_SERVER, path, cleaned_data['file'].name)\n                    # GENERATE CROPPED/RECTANGULAR IMAGE\n                    if FORCE_GENERATOR or (cleaned_data['use_image_generator'] and IMAGE_CROP_GENERATOR != \"\"):\n                        _image_crop_generator(PATH_SERVER, path, cleaned_data['file'].name)\n        # MESSAGE & REDIRECT\n        msg = _('Upload successful.')\n        request.user.message_set.create(message=msg)\n        # on redirect, sort by date desc to see the uploaded files on top of the list\n        redirect_url = URL_ADMIN + path + \"?&ot=desc&o=3&\" + query['pop']\n        return HttpResponseRedirect(redirect_url)\nelse:\n    formset = UploadFormSet(path_server=PATH_SERVER, path=path)\n\nreturn render_to_response('filebrowser/upload.html', {\n    'formset': formset,\n    'dir': dir_name,\n    'query': _get_query(request.GET),\n    'settings_var': _get_settings_var(request.META['HTTP_HOST'], path),\n    'breadcrumbs': _get_breadcrumbs(_get_query(request.GET), dir_name, _(u'Upload')),\n    'title': _(u'Select files to upload'),\n    'root_path': URL_HOME,\n}, context_instance=Context(request))", "path": "filebrowser\\views.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nUsage::\n\n    {{ url|thumbnail:\"width,height\" }}\n\"\"\"\n", "func_signal": "def string_to_list(string):\n", "code": "args = []\nif string:\n    string = str(string)\n    if ',' not in string:\n        # ensure at least one ','\n        string += ','\n    for arg in string.split(','):\n        arg = arg.strip()\n        if arg == '': continue\n        args.append(arg)\nreturn args", "path": "filebrowser\\templatetags\\fb_tags.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nMake Thumbnail(s) for existing Image or Directory\n    This is useful if someone uploads images via FTP, not using the\n    upload functionality of the FileBrowser.\n\"\"\"\n\n", "func_signal": "def makethumb(request, dir_name=None, file_name=None):\n", "code": "path = _get_path(request, dir_name)\nquery = _get_query(request.GET)\n\nif file_name:\n    # MAKE THUMB FOR SINGLE IMAGE\n    file_path = os.path.join(PATH_SERVER, path, file_name)\n    if os.path.isfile(file_path):\n        _make_image_thumbnail(PATH_SERVER, path, file_name)\nelse:\n    # MAKE THUMBS FOR WHOLE DIRECTORY\n    dir_path = os.path.join(PATH_SERVER, path)\n    dir_list = os.listdir(dir_path)\n    for file in dir_list:\n        if os.path.isfile(os.path.join(PATH_SERVER, path, file)) and not os.path.isfile(os.path.join(PATH_SERVER, path, THUMB_PREFIX + file)) and not re.compile(THUMB_PREFIX, re.M).search(file) and _get_file_type(file) == \"Image\":\n            _make_image_thumbnail(PATH_SERVER, path, file)\n\n# MESSAGE & REDIRECT\nmsg = _('Thumbnail creation successful.')\nrequest.user.message_set.create(message=msg)\nreturn HttpResponseRedirect(URL_ADMIN + path + query['query_str_total'])\n\nreturn render_to_response('filebrowser/index.html', {\n    'dir': dir_name,\n    'query': query,\n    'settings_var': _get_settings_var(request.META['HTTP_HOST'], path),\n    'breadcrumbs': _get_breadcrumbs(_get_query(request.GET), dir_name, ''),\n    'root_path': URL_HOME,\n}, context_instance=Context(request))", "path": "filebrowser\\views.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nMake Thumbnail for an Image.\n\"\"\"\n    \n", "func_signal": "def _make_image_thumbnail(PATH_SERVER, path, filename):\n", "code": "file_path = os.path.join(PATH_SERVER, path, filename)\nthumb_path = os.path.join(PATH_SERVER, path, THUMB_PREFIX + filename)\nmsg = \"\"\ntry:\n    im = Image.open(file_path)\n    im.thumbnail(THUMBNAIL_SIZE, Image.ANTIALIAS)\n    im.save(thumb_path)\nexcept IOError:\n    msg = \"%s: %s\" % (file.name, _('Thumbnail creation failed.'))\nreturn msg", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nGet a list of directories.\n\"\"\"\n\n", "func_signal": "def _get_dir_list(dir_name):\n", "code": "dir_list = []\nif dir_name:\n    dir_items = dir_name.split('/')\n    dirname = dir_items.pop()\n    dir_list.append(dirname)\n    dir_list.append(dir_name)\nreturn dir_list", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nGet all settings variables.\n\"\"\"\n\n", "func_signal": "def _get_settings_var(http_post, path):\n", "code": "settings_var = {}\nsettings_var['URL_WWW'] = URL_WWW\nsettings_var['URL_ADMIN'] = URL_ADMIN\nsettings_var['URL_HOME'] = URL_HOME\nsettings_var['URL_FILEBROWSER_MEDIA'] = URL_FILEBROWSER_MEDIA\nsettings_var['URL_TINYMCE'] = URL_TINYMCE\nsettings_var['PATH_SERVER'] = PATH_SERVER\n#settings_var['PATH_FILEBROWSER_MEDIA'] = PATH_FILEBROWSER_MEDIA\nsettings_var['PATH_TINYMCE'] = PATH_TINYMCE\nsettings_var['EXTENSIONS'] = EXTENSIONS\nsettings_var['MAX_UPLOAD_SIZE'] = _get_filesize(MAX_UPLOAD_SIZE)\nsettings_var['IMAGE_MAXBLOCK'] = IMAGE_MAXBLOCK\nsettings_var['THUMB_PREFIX'] = THUMB_PREFIX\nsettings_var['THUMBNAIL_SIZE'] = THUMBNAIL_SIZE\nsettings_var['USE_IMAGE_GENERATOR'] = USE_IMAGE_GENERATOR\nsettings_var['IMAGE_GENERATOR_DIRECTORY'] = IMAGE_GENERATOR_DIRECTORY\nsettings_var['IMAGE_GENERATOR_LANDSCAPE'] = IMAGE_GENERATOR_LANDSCAPE\nsettings_var['IMAGE_GENERATOR_PORTRAIT'] = IMAGE_GENERATOR_PORTRAIT\nsettings_var['FORCE_GENERATOR'] = FORCE_GENERATOR\nreturn settings_var", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nMake a dict out of the file_list.\n    This is for better readability in the templates.\n\"\"\"\n\n", "func_signal": "def _make_filedict(file_list):\n", "code": "file_dict = []\nfor item in file_list:\n    temp_list = {}\n    temp_list['filename'] = item[0]\n    temp_list['filesize_long'] = item[1]\n    temp_list['filesize_str'] = item[2]\n    temp_list['date'] = item[3]\n    temp_list['path_thumb'] = item[4]\n    temp_list['link'] = item[5]\n    temp_list['select_link'] = item[6]\n    temp_list['save_path'] = item[7]\n    temp_list['file_extension'] = item[8]\n    temp_list['file_type'] = item[9]\n    temp_list['image_dimensions'] = item[10]\n    temp_list['thumb_dimensions'] = item[11]\n    temp_list['filename_lower'] = item[12]\n    temp_list['flag_makethumb'] = item[13]\n    temp_list['flag_deletedir'] = item[14]\n    temp_list['flag_imageversion'] = item[15]\n    file_dict.append(temp_list)\nreturn file_dict", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "# this works because BaseFormSet._construct_form() passes **kwargs\n# to the form's __init__()\n", "func_signal": "def _construct_form(self, i, **kwargs):\n", "code": "kwargs[\"path_server\"] = self.path_server\nkwargs[\"path\"] = self.path\nreturn super(BaseUploadFormSet, self)._construct_form(i, **kwargs)", "path": "filebrowser\\forms.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nGet subquery.\n\"\"\"\n\n", "func_signal": "def _get_sub_query(items, var_1, var_2, var_3):\n", "code": "querystring= ''\nfor k,v in items:\n    if k != var_1 and k != var_2 and k != var_3:\n        querystring = querystring + \"&\" + k + \"=\" + v\nreturn querystring", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nGet file type as defined in EXTENSIONS.\n\"\"\"\n\n", "func_signal": "def _get_file_type(filename):\n", "code": "file_extension = os.path.splitext(filename)[1].lower()\nfile_type = ''\nfor k,v in EXTENSIONS.iteritems():\n    for extension in v:\n        if file_extension == extension.lower():\n            file_type = k\nreturn file_type", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "#if value is None: value = ''\n", "func_signal": "def render(self, name, value, attrs=None):\n", "code": "if value is None:\n    value = ''\nelif not isinstance(value, (str, unicode)):\n    value = value.original\nfinal_attrs = self.build_attrs(attrs, type=self.input_type, name=name)\ninit = final_attrs['initial_directory']\nfinal_attrs['initial_directory'] = _url_join(URL_ADMIN, init)\nif value != \"\":\n    # Open filebrowser to same folder as currently selected media\n    init = os.path.split(value)[0].replace(URL_WWW, \"\")\n    if value[0] != '/':\n        init = os.path.join(settings.MEDIA_ROOT, value).replace(PATH_SERVER, '')\n        init = os.path.split(init)[0].lstrip('/')\n        final_attrs['initial_directory'] = _url_join(URL_ADMIN, init)\n    else:\n        final_attrs['initial_directory'] = _url_join(URL_ADMIN, init)\nif value != '':\n    # Add the 'value' and 'preview' attribute if a value is non-empty.\n    final_attrs['value'] = force_unicode(value)\n    final_attrs['preview'] = final_attrs['value']\n    if value[0] != '/':\n        final_attrs['preview'] = os.path.join(settings.MEDIA_URL, value)\n    # Now sort out the thumbnail\n    file = os.path.split(value)[1]\n    if len(URL_WWW) < len(os.path.split(value)[0]):\n        path = os.path.split(value)[0].replace(URL_WWW, \"\")\n    else:\n        path = \"\"\n    file_type = _get_file_type(file)\n    path_thumb = \"\"\n    if file_type == 'Image':\n        # check if thumbnail exists\n        if os.path.isfile(os.path.join(PATH_SERVER, path, THUMB_PREFIX + file)):\n            path_thumb = os.path.join(os.path.split(value)[0], THUMB_PREFIX + file)\n        else:\n            path_thumb = URL_FILEBROWSER_MEDIA + 'img/filebrowser_type_image.gif'\n    elif file_type == \"Folder\":\n        path_thumb = URL_FILEBROWSER_MEDIA + 'img/filebrowser_type_folder.gif'\n    else:\n        # if file is not an image, display file-icon (which is linked to the file) instead\n        path_thumb = URL_FILEBROWSER_MEDIA + 'img/filebrowser_type_' + file_type.lower() + '.gif'\n    if path_thumb[0] != '/':\n        path_thumb = os.path.join(settings.MEDIA_URL, path_thumb)\n    final_attrs['thumbnail'] = path_thumb\npath_search_icon = URL_FILEBROWSER_MEDIA + 'img/filebrowser_icon_show.gif'\nfinal_attrs['search_icon'] = path_search_icon\nfinal_attrs['format'] = self.format\nreturn render_to_string(\"filebrowser/custom_field.html\", locals())", "path": "filebrowser\\fields.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "\"\"\"\nGet breadcrumbs.\n\"\"\"\n\n", "func_signal": "def _get_breadcrumbs(query, dir_name, page):\n", "code": "subdir_list = _get_subdir_list(dir_name)\ndir_list = _get_dir_list(dir_name)\n\nbreadcrumbs = \"\"\nif not query['pop']:\n    breadcrumbs = '<a href=\"%s\">%s</a>&nbsp;&rsaquo;&nbsp;' % (URL_HOME,_('Home'))\nbreadcrumbs = breadcrumbs + '<a href=\"%s%s\">%s</a>' % (URL_ADMIN, query['query_str_total'], 'FileBrowser')\nif subdir_list:\n    for item in subdir_list:\n        breadcrumbs = breadcrumbs + '&nbsp;&rsaquo;&nbsp;<a href=\"%s%s%s\">%s</a>' % (URL_ADMIN, item[1], query['query_str_total'], item[0])\nif page:\n    if dir_list:\n        breadcrumbs = breadcrumbs + '&nbsp;&rsaquo;&nbsp;<a href=\"%s%s/%s\">%s</a>&nbsp;&rsaquo;&nbsp;%s' % (URL_ADMIN, dir_list[1], query['query_str_total'], dir_list[0], _(page))\n    else:\n        breadcrumbs = breadcrumbs + '&nbsp;&rsaquo;&nbsp;%s' % (_(page))\nelif dir_list:\n    breadcrumbs = breadcrumbs + '&nbsp;&rsaquo;&nbsp;%s' % (dir_list[0])\nreturn mark_safe(breadcrumbs)", "path": "filebrowser\\functions.py", "repo_name": "callowayproject/django-filebrowser", "stars": 5, "license": "other", "language": "python", "size": 151}
{"docstring": "# Display a message in the message bar.\n\n", "func_signal": "def message(self, type, text):\n", "code": "(priority, showtime, bells, logmessage) = self['messagetypes'][type]\n\nif not self['silent']:\n    for i in range(bells):\n        if i != 0:\n            self.after(100)\n        self.bell()\n\nself._activemessage[priority] = 1\nif text is None:\n    text = ''\nself._messagetext[priority] = string.replace(text, '\\n', ' ')\nself._redisplayInfoMessage()\n\nif logmessage:\n    # Should log this text to a text widget.\n    pass\n\nif showtime > 0:\n    if self._timer[priority] is not None:\n        self.after_cancel(self._timer[priority])\n\n    # Define a callback to clear this message after a time.\n    def _clearmessage(self=self, priority=priority):\n        self._clearActivemessage(priority)\n\n    mseconds = int(showtime * 1000)\n    self._timer[priority] = self.after(mseconds, _clearmessage)", "path": "Pmw\\Pmw_0_0_0\\lib\\PmwMessageBar.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Script which demonstrates continuous flashing of dynamic scrollbars\n# in Pmw.ScrolledListBox.\n#\n# When this script is run, the two scrollbars will be continuously\n# mapped and unmapped and the window will continuously change size.\n\n", "func_signal": "def scrolledlistboxflashing_test():\n", "code": "frame = Tkinter.Frame(root)\nframe.pack(fill = 'both', expand = 1)\n\nsf = Pmw.ScrolledListBox(frame,\n        listbox_width = 20,\n        listbox_height = 10\n)\nsf.pack(fill = 'both', expand = 1)\nfor i in range(11):\n    sf.insert('end', '2' * 20)", "path": "Pmw\\Pmw_0_0_0\\tests\\ManualTests.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Define the megawidget options.\n", "func_signal": "def __init__(self, parent = None, **kw):\n", "code": "optiondefs = (\n    ('status', '', self._status),\n)\nself.defineoptions(kw, optiondefs)\n\n# Initialise the base class (after defining the options).\nPmw.MegaWidget.__init__(self, parent)\n\n# Create the components.\ninterior = self.interior()\nself._label = self.createcomponent('label',\n\t(), None,\n\tTkinter.Label, (interior,), text = 'test')\nself._label.pack(side='left', padx=2)\n\nself._statusList = []\n\n# Check keywords and initialise options.\nself.initialiseoptions()", "path": "Pmw\\Pmw_0_0_0\\tests\\PmwBase_test.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Create button to launch the dialog.\n", "func_signal": "def __init__(self, parent):\n", "code": "w = Tkinter.Button(parent, text = 'Show first dialog',\n        command = self.showFirstDialog)\nw.pack(padx = 8, pady = 8)", "path": "Pmw\\Pmw_0_0_0\\demos\\NestedDialogs.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Create and pack the EntryFields.\n", "func_signal": "def __init__(self, parent):\n", "code": "self._any = Pmw.EntryField(parent,\n\tlabelpos = 'w',\n\tlabel_text = 'Any:',\n\tvalidate = None,\n\tcommand = self.execute)\nself._real = Pmw.EntryField(parent,\n\tlabelpos = 'w',\n\tvalue = '55.5',\n\tlabel_text = 'Real (10.0 to 99.0):',\n\tvalidate = {'validator' : 'real',\n\t\t'min' : 10, 'max' : 99, 'minstrict' : 0},\n\tmodifiedcommand = self.changed)\nself._odd = Pmw.EntryField(parent,\n\tlabelpos = 'w',\n\tlabel_text = 'Odd length:',\n\tvalidate = self.custom_validate,\n\tvalue = 'ABC')\nself._date = Pmw.EntryField(parent,\n\tlabelpos = 'w',\n\tlabel_text = 'Date (in 2000):',\n\tvalue = '2000/2/29',\n\tvalidate = {'validator' : 'date',\n\t\t'min' : '2000/1/1', 'max' : '2000/12/31',\n\t\t'minstrict' : 0, 'maxstrict' : 0,\n\t\t'format' : 'ymd'},\n\t)\nnow = time.localtime(time.time())\nself._date2 = Pmw.EntryField(parent,\n\tlabelpos = 'w',\n\tlabel_text = 'Date (d.m.y):',\n\tvalue = '%d.%d.%d' % (now[2], now[1], now[0]),\n\tvalidate = {'validator' : 'date',\n\t\t'format' : 'dmy', 'separator' : '.'},\n\t)\nself._time = Pmw.EntryField(parent,\n\tlabelpos = 'w',\n\tlabel_text = 'Time (24hr clock):',\n\tvalue = '8:00:00',\n\tvalidate = {'validator' : 'time',\n\t\t'min' : '00:00:00', 'max' : '23:59:59',\n\t\t'minstrict' : 0, 'maxstrict' : 0},\n\t)\nself._comma = Pmw.EntryField(parent,\n\tlabelpos = 'w',\n\tlabel_text = 'Real (with comma):',\n\tvalue = '123,456',\n\tvalidate = {'validator' : 'real', 'separator' : ','},\n\t)\n\nentries = (self._any, self._real, self._odd, self._date, self._date2,\n\tself._time, self._comma)\n\nfor entry in entries:\n    entry.pack(fill='x', expand=1, padx=10, pady=5)\nPmw.alignlabels(entries)\n\nself._any.component('entry').focus_set()", "path": "Pmw\\Pmw_0_0_0\\demos\\EntryField.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "'''\\\nAn HTML structured text formatter.\nModel text as structured collection of paragraphs.\nStructure is implied by the indentation level.\n\nConvert a string containing structured text into a structured\ntext object.\n  structuredString -- The string to be parsed.\n\nReturn an HTML string representation of the structured text data.\n'''\n\n# Turn <, >, &, etc into html escape sequences and remove tabs.\n", "func_signal": "def gethtml(structuredString):\n", "code": "escapedText = cgi.escape(structuredString)\nnoTabstext = untabify(escapedText)\n\n#########\n# Not yet finished\n# # Convert '.. [ref1] \"Foo\" by Bar', A to a named link.\n# print '==='\n# print noTabstext\n# print '==='\n# #Find out what \\0 means inside []. Maybe convert to re module.\n# noTabstext = re.sub(\n#     '(^|\\n) *\\.\\. \\[([0-9_a-zA-Z-]+)\\]',\n#     '\\n  <a name=\"\\\\2\">[\\\\2]</a>',\n#     noTabstext)\n#             \n# # Convert '[ref1]' to a link within the document.\n# noTabstext = re.sub(\n#     '([\\0- ,])\\[([0-9_a-zA-Z-]+)\\]([\\0- ,.:])',\n#     '\\\\1<a href=\"#\\\\2\">[\\\\2]</a>\\\\3',\n#     noTabstext)\n#             \n# # Convert '[foo.html]' to an external link.\n# noTabstext = re.sub(\n#     '([\\0- ,])\\[([^]]+)\\.html\\]([\\0- ,.:])',\n#     '\\\\1<a href=\"\\\\2.html\">[\\\\2]</a>\\\\3',\n#     noTabstext)\n\n# Split the text into a list of paragraphs separted by a blank line.\nparasAndSpaces = reg_paragraph_divider.split(noTabstext)\n\n# Ignore white space between paragraphs\nrawParagraphs = []\ninPara = 1\nfor para in parasAndSpaces:\n    if inPara:\n        rawParagraphs.append(para)\n    inPara = not inPara\n\n# Create a list of (indent, para), where 'indent' is the number\n# of spaces on the first line and 'para' is the text'\nparagraphs = map(indent_level, rawParagraphs)\n\n# Create a list of (para, structure), where 'para' is the text of\n# a paragraph and 'structure' is a similar list of 'child'\n# paragraphs (ie, those indented further than 'para').\nstructure=parse_structure(paragraphs)\n#print '<pre>'\n#print_structure(structure)\n#print '</pre>'\n\ns=structure_to_string(structure, 1)\ns=reg_extra_dl.sub('\\n',s)\ns=reg_extra_ul.sub('\\n',s)\ns=reg_extra_ol.sub('\\n',s)\nreturn s", "path": "Pmw\\Pmw_0_0_0\\docsrc\\StructuredText.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# This performs the function of _scrollXNow and _scrollYNow.\n# If one is changed, the other should be updated to match.\n", "func_signal": "def _scrollBothNow(self):\n", "code": "self.scrollTimer = None\n\n# Call update_idletasks to make sure that the containing frame\n# has been resized before we attempt to set the scrollbars. \n# Otherwise the scrollbars may be mapped/unmapped continuously.\nself._scrollRecurse = self._scrollRecurse + 1\nself.update_idletasks()\nself._scrollRecurse = self._scrollRecurse - 1\nif self._scrollRecurse != 0:\n    return\n\nxview = self._canvas.xview()\nyview = self._canvas.yview()\nself._horizScrollbar.set(xview[0], xview[1])\nself._vertScrollbar.set(yview[0], yview[1])\n\nself._horizScrollbarNeeded = (xview != (0.0, 1.0))\nself._vertScrollbarNeeded = (yview != (0.0, 1.0))\n\n# If both horizontal and vertical scrollmodes are dynamic and\n# currently only one scrollbar is mapped and both should be\n# toggled, then unmap the mapped scrollbar.  This prevents a\n# continuous mapping and unmapping of the scrollbars. \nif (self['hscrollmode'] == self['vscrollmode'] == 'dynamic' and\n        self._horizScrollbarNeeded != self._horizScrollbarOn and\n        self._vertScrollbarNeeded != self._vertScrollbarOn and\n        self._vertScrollbarOn != self._horizScrollbarOn):\n    if self._horizScrollbarOn:\n        self._toggleHorizScrollbar()\n    else:\n        self._toggleVertScrollbar()\n    return\n\nif self['hscrollmode'] == 'dynamic':\n    if self._horizScrollbarNeeded != self._horizScrollbarOn:\n        self._toggleHorizScrollbar()\n\nif self['vscrollmode'] == 'dynamic':\n    if self._vertScrollbarNeeded != self._vertScrollbarOn:\n        self._toggleVertScrollbar()", "path": "Pmw\\Pmw_0_0_0\\lib\\PmwScrolledCanvas.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Read the file and modify it so that it can be bundled with the\n# other Pmw files.\n", "func_signal": "def mungeFile(file):\n", "code": "file = 'Pmw' + file + '.py'\ntext = open(os.path.join(srcdir, file)).read()\ntext = re.sub(r'import Pmw\\b', '', text)\ntext = re.sub(r'INITOPT = Pmw\\.INITOPT', '', text)\ntext = re.sub(r'\\bPmw\\.', '', text)\ntext = '\\n' + ('#' * 70) + '\\n' + '### File: ' + file + '\\n' + text\nreturn text", "path": "Pmw\\Pmw_0_0_0\\bin\\bundlepmw.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Another script which demonstrates continuous flashing of dynamic\n# scrollbars in Pmw.ScrolledListBox under Pmw.0.8.\n#\n# When this script is run, the two scrollbars will be continuously\n# mapped and unmapped and the window will continuously change size.\n#\n# (This did not display error when tried with Pmw.0.8, 99/8/3)\n\n", "func_signal": "def scrolledlistboxflashing2_test():\n", "code": "def insert():\n    sectionList = ['1', '2', '3', '4', '5', '6', '7', '8', '9',\n        '123456789012345678901']\n    for counter in sectionList: \n      slb.insert('end', counter)\n      \ndef clear():\n    slb.delete(0, 'end')\n    \nglobal slb\nslb = Pmw.ScrolledListBox(root)\nslb.pack()\n\nroot.after(2000,insert)\nroot.after(3000,clear) \nroot.after(4000,insert)\n\nroot.geometry('400x400')", "path": "Pmw\\Pmw_0_0_0\\tests\\ManualTests.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "'''\\\nConvert indentation tabs to spaces.\n'''\n", "func_signal": "def untabify(aString):\n", "code": "result=''\nrest=aString\nwhile 1:\n    match = reg_indent_tab.search(rest)\n    if match is not None:\n        start = match.start()", "path": "Pmw\\Pmw_0_0_0\\docsrc\\StructuredText.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Script which demonstrates continuous flashing of dynamic scrollbars\n# in Pmw.ScrolledCanvas.\n#\n# When this script is run, the two scrollbars will be continuously\n# mapped and unmapped and the window will continuously change size.\n\n", "func_signal": "def scrolledcanvasflashing_test():\n", "code": "frame = Tkinter.Frame(root)\nframe.pack(fill = 'both', expand = 1)\n\nsf = Pmw.ScrolledCanvas(frame,\n        canvas_scrollregion = (0, 0, 301, 200),\n        canvas_width=300,\n        canvas_height=200,\n        borderframe = 0\n)\nsf.pack(fill = 'both', expand = 1)", "path": "Pmw\\Pmw_0_0_0\\tests\\ManualTests.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Script which demonstrates continuous flashing of dynamic scrollbars\n# in Pmw.ScrolledText.\n#\n# When this script is run, the two scrollbars will be continuously\n# mapped and unmapped and the window will continuously change size.\n\n", "func_signal": "def scrolledtextflashing_test():\n", "code": "frame = Tkinter.Frame(root)\nframe.pack(fill = 'both', expand = 1)\n\nsf = Pmw.ScrolledText(frame,\n        text_width = 20,\n        text_height = 10,\n        text_wrap = 'none',\n        borderframe = 0\n)\nsf.pack(fill = 'both', expand = 1)\nfor i in range(11):\n    sf.insert('end', '2' * 20)\n    if i != 10:\n        sf.insert('end', '\\n')", "path": "Pmw\\Pmw_0_0_0\\tests\\ManualTests.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Do not call changefonts for preformatted text.\n", "func_signal": "def pre(p, after):\n", "code": "p = reg_remove_hash.sub('\\n', p)\nreturn '<dl><dd><pre>%s</pre></dd></dl>\\n%s\\n' % (p, after)", "path": "Pmw\\Pmw_0_0_0\\docsrc\\StructuredText.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Called by the canvas to set the horizontal or vertical\n# scrollbar when it has scrolled or changed scrollregion.\n\n", "func_signal": "def _scrollBothLater(self, first, last):\n", "code": "if self.scrollTimer is None:\n    self.scrollTimer = self.after_idle(self._scrollBothNow)", "path": "Pmw\\Pmw_0_0_0\\lib\\PmwScrolledCanvas.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Define the megawidget options.\n", "func_signal": "def __init__(self, parent = None, **kw):\n", "code": "optiondefs = ()\nself.defineoptions(kw, optiondefs)\n\n# Initialise the base class (after defining the options).\nPmw.MegaWidget.__init__(self, parent)", "path": "Pmw\\Pmw_0_0_0\\tests\\PmwBase_test.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Create and pack the ScrolledField.\n", "func_signal": "def __init__(self, parent):\n", "code": "self._field = Pmw.ScrolledField(parent, entry_width = 30,\n\tentry_relief='groove', labelpos = 'n',\n        label_text = 'Scroll the field using the\\nmiddle mouse button')\nself._field.pack(fill = 'x', expand = 1, padx = 10, pady = 10)\n\n# Create and pack a button to change the ScrolledField.\nself._button = Tkinter.Button(parent, text = 'Change field',\n        command = self.execute)\nself._button.pack(padx = 10, pady = 10)\n\nself._index = 0\nself.execute()", "path": "Pmw\\Pmw_0_0_0\\demos\\ScrolledField.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# The two scrollbars will be continuously mapped and unmapped, but\n# the toplevel window will remain the same size.\n\n", "func_signal": "def scrolledframeflashing2_test():\n", "code": "root.geometry('550x500')\n\nframe = Tkinter.Frame()\nframe.pack()\n\nsf = Pmw.ScrolledFrame(frame, borderframe = 0)\nsf.pack(fill = 'both')\n\ninner = Tkinter.Frame(sf.interior(),\n        width = 401,\n        height = 300,\n        borderwidth = 0,\n        highlightthickness = 0,\n)\ninner.pack()", "path": "Pmw\\Pmw_0_0_0\\tests\\ManualTests.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Create two buttons to generate errors.\n", "func_signal": "def __init__(self, parent):\n", "code": "w = Tkinter.Button(parent, text = 'Click here to generate\\n' +\n        'an error in a command callback.', command = self.execute)\nw.pack(padx = 8, pady = 8)\n\nw = Tkinter.Button(parent, text = 'Click here to generate\\n' +\n        'an error in a callback called\\nfrom an event binding.')\nw.pack(padx = 8, pady = 8)\nw.bind('<ButtonRelease-1>', self.execute)\nw.bind('<Key-space>', self.execute)", "path": "Pmw\\Pmw_0_0_0\\demos\\ErrorHandling.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "# Repeat random number sequence for each run.\n", "func_signal": "def __init__(self, parent, withTabs = 1):\n", "code": "        self.rand = 12345\n# Default demo is to display a tabbed notebook.\n        self.withTabs = withTabs\n# Create a frame to put everything in\n        self.mainframe = Tkinter.Frame(parent)\n        self.mainframe.pack(fill = 'both', expand = 1)\n# Find current default colors\n        button = Tkinter.Button()\n        defaultbg = button.cget('background')\n        defaultfg = button.cget('foreground')\n        button.destroy()\n# Create the list of colors to cycle through", "path": "Pmw\\Pmw_0_0_0\\demos\\NoteBook_2.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "'''\\\nFind the minimum indentation for a string, not counting blank lines.\n'''\n", "func_signal": "def indent_level(aString):\n", "code": "start=0\ntext='\\n'+aString\nindent=l=len(text)\nwhile 1:\n    match = reg_indent_space.search(text,start)\n    if match is not None:\n        start = match.start()", "path": "Pmw\\Pmw_0_0_0\\docsrc\\StructuredText.py", "repo_name": "nanotube/pmw_fixes", "stars": 6, "license": "None", "language": "python", "size": 768}
{"docstring": "\"\"\"set expected values for urlforward mock\nto http://example.com/a_hooks.php\n\"\"\"\n", "func_signal": "def mock_a_hooks(self, status_code, **args):\n", "code": "self.mock_forward(status_code, url=\"http://example.com/a_hooks.php\",\n                                                             **args)", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that param are forwarded even if not existing in default\nIf they are in only list and not in remove list\n\"\"\"\n", "func_signal": "def test_forward_existing_param_and_default_and_filter(self):\n", "code": "self.assert_transform(req={\"only1\": \"val_only1\"},\n                      fwd={\"only1\": \"val_only1\",\n                           \"def_only2\": \"val_def_only2\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that default param are present even if not in request\"\"\"\n", "func_signal": "def test_default_param(self):\n", "code": "self.assert_transform(req={}, fwd={\"def1\": \"val_def1\",\n                                   \"def2\": \"val_def2\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"disallow use of urlforward\"\"\"\n", "func_signal": "def mock_not_forward(self):\n", "code": "self.mock_fetch(KWARGS)\nself.mocker.throw(\"unhallowed call of urlforward\")\nself.mocker.replay()", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check forwarding of request to absent destination\"\"\"\n", "func_signal": "def test_absent_redirect(self):\n", "code": "self.mock_a_hooks(404)\nresponse = self.app.get('/request_url', expect_errors=True)\nself.assertEqual(\n    '404 Not Found', response.status)\nself.assertTrue(\n    'Houps: 404 for http://example.com/a_hooks.php' in response)", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check forwarding of request from '127.1.2.3'\"\"\"\n", "func_signal": "def test_ok_redirect(self):\n", "code": "self.mock_a_hooks(200)\nself.assert_a_hooks_get_ok(extra_environ={\"REMOTE_ADDR\": \"127.1.2.3\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that request param don't take over fixed value\"\"\"\n", "func_signal": "def test_set_over_request(self):\n", "code": "self.assert_transform(req={\"set1\": \"new_val_set1\"},\n                      fwd={\"set1\": \"val_set1\",\n                           \"set2\": \"val_set2\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that param are forwarded even if not existing in default\"\"\"\n", "func_signal": "def test_forward_existing_param_and_default(self):\n", "code": "self.assert_transform(req={\"pass1\": \"val_pass1\"},\n                      fwd={\"pass1\": \"val_pass1\",\n                           \"def1\": \"val_def1\",\n                           \"def2\": \"val_def2\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that param are forwarded if in only list\"\"\"\n", "func_signal": "def test_forward_existing_param(self):\n", "code": "self.assert_transform(req={\"only1\": \"val_only1\"},\n                      fwd={\"only1\": \"val_only1\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check value of post request parameter\"\"\"\n", "func_signal": "def test_value_of_post_parameter(self):\n", "code": "self.mock_a_hooks(200, param={\"foo\": \"bar\"})\nself.assert_a_hooks_ok(\n    self.app.post('/request_url', params={\"foo\": \"bar\"}))", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check forwarding of post request parameter\"\"\"\n", "func_signal": "def test_passing_of_post_parameter(self):\n", "code": "self.mock_a_hooks(200, param=CONTAINS(\"foo\"))\nself.assert_a_hooks_ok(\n    self.app.post('/request_url', params={\"foo\": \"bar\"}))", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that request param take over default value\"\"\"\n", "func_signal": "def test_request_over_default(self):\n", "code": "self.assert_transform(req={\"def1\": \"new_val_def1\"},\n                      fwd={\"def1\": \"new_val_def1\",\n                           \"def2\": \"val_def2\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that existing param are suppressed if not in only list\nAnd Check that param are forwarded if in only list\n\"\"\"\n", "func_signal": "def test_suppress_and_forward_existing_param(self):\n", "code": "self.assert_transform(req={\"pass1\": \"val_pass1\",\n                           \"only1\": \"val_only1\"},\n                      fwd={\"only1\": \"val_only1\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check forwarding of request\"\"\"\n", "func_signal": "def test_ok_redirect(self):\n", "code": "self.mock_a_hooks(200)\nself.assert_a_hooks_get_ok()", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that request param don't take over fixed value\nAnd that param are forwarded even if not fixed\n\"\"\"\n", "func_signal": "def test_forward_and_set_over_request(self):\n", "code": "self.assert_transform(req={\"pass1\": \"val_pass1\",\n                           \"set1\": \"new_val_set1\"},\n                      fwd={\"pass1\": \"val_pass1\",\n                           \"set1\": \"val_set1\",\n                           \"set2\": \"val_set2\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that existing param are forwarded if not in remove list\"\"\"\n", "func_signal": "def test_forward_existing_param(self):\n", "code": "self.assert_transform(req={\"pass1\": \"val_pass1\"},\n                      fwd={\"pass1\": \"val_pass1\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"set expected values for urlforward mock\"\"\"\n", "func_signal": "def mock_forward(self, status_code, **args):\n", "code": "self.mock_fetch(KWARGS, **args)\nself.mocker.result(status_code)\nself.mocker.replay()", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check unhallowed remote_addr\"\"\"\n", "func_signal": "def test_unhallowed_remote_addr(self):\n", "code": "self.mock_not_forward()\nresponse = self.app.get('/request_url', expect_errors=True,\n                        extra_environ={\"REMOTE_ADDR\": \"127.6.6.6\"})\nself.assertEqual('405 Method Not Allowed', response.status)", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check use of HTTP POST method\"\"\"\n", "func_signal": "def test_forward_http_method_post(self):\n", "code": "self.mock_a_hooks(200, method='POST')\nself.assert_a_hooks_get_ok()", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"Check that fixed param are present\nEven if in remove list or not in only list\n\"\"\"\n", "func_signal": "def test_default_param_and_filter(self):\n", "code": "self.assert_transform(req={},\n                      fwd={\"set_remove1\": \"val_set_remove1\",\n                           \"set_only2\": \"val_set_only2\",\n                           \"set_remove_only3\": \"val_set_remove_only3\"})", "path": "app_engine\\test\\test_main.py", "repo_name": "ppierre/simple-request-forwarding-app-engine", "stars": 4, "license": "None", "language": "python", "size": 180}
{"docstring": "\"\"\"\nScan the text for the given pattern and update pos/match\nand related fields. The return value is a boolen that\nindicates if the pattern matched. The matched value is\nstored on the instance as ``match``, the last value is\nstored as ``last``. ``start_pos`` is the position of the\npointer before the pattern was matched, ``pos`` is the\nend position.\n\"\"\"\n", "func_signal": "def scan(self, pattern):\n", "code": "if self.eos:\n    raise EndOfText()\nif pattern not in self._re_cache:\n    self._re_cache[pattern] = re.compile(pattern, self.flags)\nself.last = self.match\nm = self._re_cache[pattern].match(self.data, self.pos)\nif m is None:\n    return False\nself.start_pos = m.start()\nself.pos = m.end()\nself.match = m.group()\nreturn True", "path": "pygments_package\\pygments\\scanner.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nCheck if a doctype exists or if we have some tags.\n\"\"\"\n", "func_signal": "def looks_like_xml(text):\n", "code": "key = hash(text)\ntry:\n    return _looks_like_xml_cache[key]\nexcept KeyError:\n    m = doctype_lookup_re.match(text)\n    if m is not None:\n        return True\n    rv = tag_re.search(text[:1000]) is not None\n    _looks_like_xml_cache[key] = rv\n    return rv", "path": "pygments_package\\pygments\\util.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\n:param text:    The text which should be scanned\n:param flags:   default regular expression flags\n\"\"\"\n", "func_signal": "def __init__(self, text, flags=0):\n", "code": "self.data = text\nself.data_length = len(text)\nself.start_pos = 0\nself.pos = 0\nself.flags = flags\nself.last = None\nself.match = None\nself._re_cache = {}", "path": "pygments_package\\pygments\\scanner.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nReturn an instantiated filter. Options are passed to the filter\ninitializer if wanted. Raise a ClassNotFound if not found.\n\"\"\"\n", "func_signal": "def get_filter_by_name(filtername, **options):\n", "code": "cls = find_filter_class(filtername)\nif cls:\n    return cls(**options)\nelse:\n    raise ClassNotFound('filter %r not found' % filtername)", "path": "pygments_package\\pygments\\filters\\__init__.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nCheck if the given regular expression matches the last part of the\nshebang if one exists.\n\n    >>> from pygments.util import shebang_matches\n    >>> shebang_matches('#!/usr/bin/env python', r'python(2\\.\\d)?')\n    True\n    >>> shebang_matches('#!/usr/bin/python2.4', r'python(2\\.\\d)?')\n    True\n    >>> shebang_matches('#!/usr/bin/python-ruby', r'python(2\\.\\d)?')\n    False\n    >>> shebang_matches('#!/usr/bin/python/ruby', r'python(2\\.\\d)?')\n    False\n    >>> shebang_matches('#!/usr/bin/startsomethingwith python',\n    ...                 r'python(2\\.\\d)?')\n    True\n\nIt also checks for common windows executable file extensions::\n\n    >>> shebang_matches('#!C:\\\\Python2.4\\\\Python.exe', r'python(2\\.\\d)?')\n    True\n\nParameters (``'-f'`` or ``'--foo'`` are ignored so ``'perl'`` does\nthe same as ``'perl -e'``)\n\nNote that this method automatically searches the whole string (eg:\nthe regular expression is wrapped in ``'^$'``)\n\"\"\"\n", "func_signal": "def shebang_matches(text, regex):\n", "code": "index = text.find('\\n')\nif index >= 0:\n    first_line = text[:index].lower()\nelse:\n    first_line = text.lower()\nif first_line.startswith('#!'):\n    try:\n        found = [x for x in split_path_re.split(first_line[2:].strip())\n                 if x and not x.startswith('-')][-1]\n    except IndexError:\n        return False\n    regex = re.compile('^%s(\\.(exe|cmd|bat|bin))?$' % regex, re.IGNORECASE)\n    if regex.search(found) is not None:\n        return True\nreturn False", "path": "pygments_package\\pygments\\util.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "# rtf 1.8 header\n", "func_signal": "def format(self, tokensource, outfile):\n", "code": "outfile.write(r'{\\rtf1\\ansi\\deff0'\n              r'{\\fonttbl{\\f0\\fmodern\\fprq1\\fcharset0%s;}}'\n              r'{\\colortbl;' % (self.fontface and\n                                ' ' + self._escape(self.fontface) or\n                                ''))\n\n# convert colors and save them in a mapping to access them later.\ncolor_mapping = {}\noffset = 1\nfor _, style in self.style:\n    for color in style['color'], style['bgcolor'], style['border']:\n        if color and color not in color_mapping:\n            color_mapping[color] = offset\n            outfile.write(r'\\red%d\\green%d\\blue%d;' % (\n                int(color[0:2], 16),\n                int(color[2:4], 16),\n                int(color[4:6], 16)\n            ))\n            offset += 1\noutfile.write(r'}\\f0')\n\n# highlight stream\nfor ttype, value in tokensource:\n    while not self.style.styles_token(ttype) and ttype.parent:\n        ttype = ttype.parent\n    style = self.style.style_for_token(ttype)\n    buf = []\n    if style['bgcolor']:\n        buf.append(r'\\cb%d' % color_mapping[style['bgcolor']])\n    if style['color']:\n        buf.append(r'\\cf%d' % color_mapping[style['color']])\n    if style['bold']:\n        buf.append(r'\\b')\n    if style['italic']:\n        buf.append(r'\\i')\n    if style['underline']:\n        buf.append(r'\\ul')\n    if style['border']:\n        buf.append(r'\\chbrdr\\chcfpat%d' %\n                   color_mapping[style['border']])\n    start = ''.join(buf)\n    if start:\n        outfile.write('{%s ' % start)\n    outfile.write(self._escape_text(value))\n    if start:\n        outfile.write('}')\n\noutfile.write('}')", "path": "pygments_package\\pygments\\formatters\\rtf.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nCheck if the doctype matches a regular expression (if present).\nNote that this method only checks the first part of a DOCTYPE.\neg: 'html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"'\n\"\"\"\n", "func_signal": "def doctype_matches(text, regex):\n", "code": "m = doctype_lookup_re.match(text)\nif m is None:\n    return False\ndoctype = m.group(2)\nreturn re.compile(regex).match(doctype.strip()) is not None", "path": "pygments_package\\pygments\\util.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nLookup a filter by name. Return None if not found.\n\"\"\"\n", "func_signal": "def find_filter_class(filtername):\n", "code": "if filtername in FILTERS:\n    return FILTERS[filtername]\nfor name, cls in find_plugin_filters():\n    if name == filtername:\n        return cls\nreturn None", "path": "pygments_package\\pygments\\filters\\__init__.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "# Make progressively weaker assumptions about \"other\"\n", "func_signal": "def update(self, other=None, **kwargs):\n", "code": "if other is None:\n    pass\nelif hasattr(other, 'iteritems'):  # iteritems saves memory and lookups\n    for k, v in other.iteritems():\n        self[k] = v\nelif hasattr(other, 'keys'):\n    for k in other.keys():\n        self[k] = other[k]\nelse:\n    for k, v in other:\n        self[k] = v\nif kwargs:\n    self.update(kwargs)", "path": "pygments_package\\pygments_dependencies\\UserDict.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "# empty strings, should give a small performance improvment\n", "func_signal": "def _escape_text(self, text):\n", "code": "if not text:\n    return ''\n\n# escape text\ntext = self._escape(text)\nencoding = self.encoding or 'iso-8859-15'\n\nbuf = []\nfor c in text:\n    if ord(c) > 128:\n        ansic = c.encode(encoding, 'ignore') or '?'\n        if ord(ansic) > 128:\n            ansic = '\\\\\\'%x' % ord(ansic)\n        buf.append(r'\\ud{\\u%d%s}' % (ord(c), ansic))\n    else:\n        buf.append(str(c))\n\nreturn ''.join(buf).replace('\\n', '\\\\par\\n')", "path": "pygments_package\\pygments\\formatters\\rtf.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"Test whether FILENAME matches PATTERN, including case.\n\nThis is a version of fnmatch() which doesn't case-normalize\nits arguments.\n\"\"\"\n\n", "func_signal": "def fnmatchcase(name, pat):\n", "code": "if not pat in _cache:\n    res = translate(pat)\n    _cache[pat] = re.compile(res)\nreturn _cache[pat].match(name) is not None", "path": "pygments_package\\pygments_dependencies\\fnmatch.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nSince ERB doesn't allow \"<%\" and other tags inside of ruby\nblocks we have to use a split approach here that fails for\nthat too.\n\"\"\"\n", "func_signal": "def get_tokens_unprocessed(self, text):\n", "code": "tokens = self._block_re.split(text)\ntokens.reverse()\nstate = idx = 0\ntry:\n    while True:\n        # text\n        if state == 0:\n            val = tokens.pop()\n            yield idx, Other, val\n            idx += len(val)\n            state = 1\n        # block starts\n        elif state == 1:\n            tag = tokens.pop()\n            # literals\n            if tag in ('<%%', '%%>'):\n                yield idx, Other, tag\n                idx += 3\n                state = 0\n            # comment\n            elif tag == '<%#':\n                yield idx, Comment.Preproc, tag\n                val = tokens.pop()\n                yield idx + 3, Comment, val\n                idx += 3 + len(val)\n                state = 2\n            # blocks or output\n            elif tag in ('<%', '<%=', '<%-'):\n                yield idx, Comment.Preproc, tag\n                idx += len(tag)\n                data = tokens.pop()\n                r_idx = 0\n                for r_idx, r_token, r_value in \\\n                    self.ruby_lexer.get_tokens_unprocessed(data):\n                    yield r_idx + idx, r_token, r_value\n                idx += len(data)\n                state = 2\n            elif tag in ('%>', '-%>'):\n                yield idx, Error, tag\n                idx += len(tag)\n                state = 0\n            # % raw ruby statements\n            else:\n                yield idx, Comment.Preproc, tag[0]\n                r_idx = 0\n                for r_idx, r_token, r_value in \\\n                    self.ruby_lexer.get_tokens_unprocessed(tag[1:]):\n                    yield idx + 1 + r_idx, r_token, r_value\n                idx += len(tag)\n                state = 0\n        # block ends\n        elif state == 2:\n            tag = tokens.pop()\n            if tag not in ('%>', '-%>'):\n                yield idx, Other, tag\n            else:\n                yield idx, Comment.Preproc, tag\n            idx += len(tag)\n            state = 0\nexcept IndexError:\n    return", "path": "pygments_package\\pygments\\lexers\\templates.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nReturn a static text analysation function that\nreturns float values.\n\"\"\"\n", "func_signal": "def make_analysator(f):\n", "code": "def text_analyse(text):\n    rv = f(text)\n    if not rv:\n        return 0.0\n    return min(1.0, max(0.0, float(rv)))\ntext_analyse.__doc__ = f.__doc__\nreturn staticmethod(text_analyse)", "path": "pygments_package\\pygments\\util.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"Return the subset of the list NAMES that match PAT\"\"\"\n", "func_signal": "def filter(names, pat):\n", "code": "import os,posixpath\nresult=[]\npat=os.path.normcase(pat)\nif not pat in _cache:\n    res = translate(pat)\n    _cache[pat] = re.compile(res)\nmatch=_cache[pat].match\nif os.path is posixpath:\n    # normcase on posix is NOP. Optimize it away from the loop.\n    for name in names:\n        if match(name):\n            result.append(name)\nelse:\n    for name in names:\n        if match(os.path.normcase(name)):\n            result.append(name)\nreturn result", "path": "pygments_package\\pygments_dependencies\\fnmatch.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"Test whether FILENAME matches PATTERN.\n\nPatterns are Unix shell style:\n\n*       matches everything\n?       matches any single character\n[seq]   matches any character in seq\n[!seq]  matches any char not in seq\n\nAn initial period in FILENAME is not special.\nBoth FILENAME and PATTERN are first case-normalized\nif the operating system requires it.\nIf you don't want this, use fnmatchcase(FILENAME, PATTERN).\n\"\"\"\n\n", "func_signal": "def fnmatch(name, pat):\n", "code": "import os\nname = os.path.normcase(name)\npat = os.path.normcase(pat)\nreturn fnmatchcase(name, pat)", "path": "pygments_package\\pygments_dependencies\\fnmatch.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nApply `pattern` on the current position and return\nthe match object. (Doesn't touch pos). Use this for\nlookahead.\n\"\"\"\n", "func_signal": "def check(self, pattern):\n", "code": "if self.eos:\n    raise EndOfText()\nif pattern not in self._re_cache:\n    self._re_cache[pattern] = re.compile(pattern, self.flags)\nreturn self._re_cache[pattern].match(self.data, self.pos)", "path": "pygments_package\\pygments\\scanner.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "# TODO: add support for background colors\n", "func_signal": "def format(self, tokensource, outfile):\n", "code": "enc = self.encoding\n\nif self.full:\n    realoutfile = outfile\n    outfile = StringIO.StringIO()\n\noutfile.write(r'\\begin{Verbatim}[commandchars=@\\[\\]')\nif self.linenos:\n    start, step = self.linenostart, self.linenostep\n    outfile.write(',numbers=left' +\n                  (start and ',firstnumber=%d' % start or '') +\n                  (step and ',stepnumber=%d' % step or ''))\nif self.verboptions:\n    outfile.write(',' + self.verboptions)\noutfile.write(']\\n')\n\nfor ttype, value in tokensource:\n    if enc:\n        value = value.encode(enc)\n    value = escape_tex(value, self.commandprefix)\n    cmd = self.ttype2cmd.get(ttype)\n    while cmd is None:\n        ttype = ttype.parent\n        cmd = self.ttype2cmd.get(ttype)\n    if cmd:\n        spl = value.split('\\n')\n        for line in spl[:-1]:\n            if line:\n                outfile.write(\"@%s[%s]\" % (cmd, line))\n            outfile.write('\\n')\n        if spl[-1]:\n            outfile.write(\"@%s[%s]\" % (cmd, spl[-1]))\n    else:\n        outfile.write(value)\n\noutfile.write('\\\\end{Verbatim}\\n')\n\nif self.full:\n    realoutfile.write(DOC_TEMPLATE %\n        dict(docclass  = self.docclass,\n             preamble  = self.preamble,\n             title     = self.title,\n             encoding  = self.encoding or 'latin1',\n             styledefs = self.get_style_defs(),\n             code      = outfile.getvalue()))", "path": "pygments_package\\pygments\\formatters\\latex.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nReturn the \\\\newcommand sequences needed to define the commands\nused to format text in the verbatim environment. ``arg`` is ignored.\n\"\"\"\n", "func_signal": "def get_style_defs(self, arg=''):\n", "code": "nc = '\\\\newcommand'\ncp = self.commandprefix\nreturn (\n    '%s\\\\%sZat{@}\\n%s\\\\%sZlb{[}\\n%s\\\\%sZrb{]}\\n' % (nc, cp, nc, cp, nc, cp) +\n    '\\n'.join(['\\\\newcommand\\\\%s[1]{%s}' % (alias, cmndef)\n               for alias, cmndef in self.cmd2def.iteritems()\n               if cmndef != '#1']))", "path": "pygments_package\\pygments\\formatters\\latex.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nReturn a generator of all filter names.\n\"\"\"\n", "func_signal": "def get_all_filters():\n", "code": "for name in FILTERS:\n    yield name\nfor name, _ in find_plugin_filters():\n    yield name", "path": "pygments_package\\pygments\\filters\\__init__.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nAdditional options accepted:\n\n``fontface``\n    Name of the font used. Could for example be ``'Courier New'``\n    to further specify the default which is ``'\\fmodern'``. The RTF\n    specification claims that ``\\fmodern`` are \"Fixed-pitch serif\n    and sans serif fonts\". Hope every RTF implementation thinks\n    the same about modern...\n\"\"\"\n", "func_signal": "def __init__(self, **options):\n", "code": "Formatter.__init__(self, **options)\nself.fontface = options.get('fontface') or ''\nif self.encoding in ('utf-8', 'utf-16', 'utf-32'):\n    self.encoding = None", "path": "pygments_package\\pygments\\formatters\\rtf.py", "repo_name": "devhawk/pygments.wlwriter", "stars": 6, "license": "None", "language": "python", "size": 5713}
{"docstring": "\"\"\"\nFinds a multipart boundary in data.\n\nShould no boundry exist in the data None is returned instead. Otherwise\na tuple containing the indices of the following are returned:\n\n * the end of current encapsulation\n * the start of the next encapsulation\n\"\"\"\n", "func_signal": "def _find_boundary(self, data, eof = False):\n", "code": "index = self._fs(data)\nif index < 0:\n    return None\nelse:\n    end = index\n    next = index + len(self._boundary)\n    # backup over CRLF\n    if data[max(0,end-1)] == '\\n':\n        end -= 1\n    if data[max(0,end-1)] == '\\r':\n        end -= 1\n    return end, next", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"Return a strong reference to the bound method\n\nIf the target cannot be retrieved, then will\nreturn None, otherwise returns a bound instance\nmethod for our object and function.\n\nNote:\n    You may call this method any number of times,\n    as it does not invalidate the reference.\n\"\"\"\n", "func_signal": "def __call__(self):\n", "code": "target = self.weakSelf()\nif target is not None:\n    function = self.weakFunc()\n    if function is not None:\n        return function.__get__(target)\nreturn None", "path": "build\\libs\\django\\dispatch\\saferef.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"Instantiates the appropiate BoundMethodWeakRef, depending on the details of\nthe underlying class method implementation\"\"\"\n", "func_signal": "def get_bound_method_weakref(target, onDelete):\n", "code": "if hasattr(target, '__get__'):\n    # target method is a descriptor, so the default implementation works:\n    return BoundMethodWeakref(target=target, onDelete=onDelete)\nelse:\n    # no luck, use the alternative implementation:\n    return BoundNonDescriptorMethodWeakref(target=target, onDelete=onDelete)", "path": "build\\libs\\django\\dispatch\\saferef.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"Create new instance or return current instance\n\nBasically this method of construction allows us to\nshort-circuit creation of references to already-\nreferenced instance methods.  The key corresponding\nto the target is calculated, and if there is already\nan existing reference, that is returned, with its\ndeletionMethods attribute updated.  Otherwise the\nnew instance is created and registered in the table\nof already-referenced methods.\n\"\"\"\n", "func_signal": "def __new__( cls, target, onDelete=None, *arguments,**named ):\n", "code": "key = cls.calculateKey(target)\ncurrent =cls._allInstances.get(key)\nif current is not None:\n    current.deletionMethods.append( onDelete)\n    return current\nelse:\n    base = super( BoundMethodWeakref, cls).__new__( cls )\n    cls._allInstances[key] = base\n    base.__init__( target, onDelete, *arguments,**named)\n    return base", "path": "build\\libs\\django\\dispatch\\saferef.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nUpdates the unget history as a sanity check to see if we've pushed\nback the same number of bytes in one chunk. If we keep ungetting the\nsame number of bytes many times (here, 50), we're mostly likely in an\ninfinite loop of some sort. This is usually caused by a\nmaliciously-malformed MIME request.\n\"\"\"\n", "func_signal": "def _update_unget_history(self, num_bytes):\n", "code": "self._unget_history = [num_bytes] + self._unget_history[:49]\nnumber_equal = len([current_number for current_number in self._unget_history\n                    if current_number == num_bytes])\n\nif number_equal > 40:\n    raise SuspiciousOperation(\n        \"The multipart parser got stuck, which shouldn't happen with\"\n        \" normal uploaded files. Check for malicious upload activity;\"\n        \" if there is none, report this to the Django developers.\"\n    )", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nHandle all the signalling that takes place when a file is complete.\n\"\"\"\n", "func_signal": "def handle_file_complete(self, old_field_name, counters):\n", "code": "for i, handler in enumerate(self._upload_handlers):\n    file_obj = handler.file_complete(counters[i])\n    if file_obj:\n        # If it returns a file object, then set the files dict.\n        self._files.appendlist(force_unicode(old_field_name,\n                                             self._encoding,\n                                             errors='replace'),\n                               file_obj)\n        break", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nParses one and exactly one stream that encapsulates a boundary.\n\"\"\"\n# Stream at beginning of header, look for end of header\n# and parse it if found. The header must fit within one\n# chunk.\n", "func_signal": "def parse_boundary_stream(stream, max_header_size):\n", "code": "chunk = stream.read(max_header_size)\n\n# 'find' returns the top of these four bytes, so we'll\n# need to munch them later to prevent them from polluting\n# the payload.\nheader_end = chunk.find('\\r\\n\\r\\n')\n\ndef _parse_header(line):\n    main_value_pair, params = parse_header(line)\n    try:\n        name, value = main_value_pair.split(':', 1)\n    except:\n        raise ValueError(\"Invalid header: %r\" % line)\n    return name, (value, params)\n\nif header_end == -1:\n    # we find no header, so we just mark this fact and pass on\n    # the stream verbatim\n    stream.unget(chunk)\n    return (RAW, {}, stream)\n\nheader = chunk[:header_end]\n\n# here we place any excess chunk back onto the stream, as\n# well as throwing away the CRLFCRLF bytes from above.\nstream.unget(chunk[header_end + 4:])\n\nTYPE = RAW\noutdict = {}\n\n# Eliminate blank lines\nfor line in header.split('\\r\\n'):\n    # This terminology (\"main value\" and \"dictionary of\n    # parameters\") is from the Python docs.\n    try:\n        name, (value, params) = _parse_header(line)\n    except:\n        continue\n\n    if name == 'content-disposition':\n        TYPE = FIELD\n        if params.get('filename'):\n            TYPE = FILE\n\n    outdict[name] = value, params\n\nif TYPE == RAW:\n    stream.unget(chunk)\n\nreturn (TYPE, outdict, stream)", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\" Parse the header into a key-value. \"\"\"\n", "func_signal": "def parse_header(line):\n", "code": "plist = _parse_header_params(';' + line)\nkey = plist.pop(0).lower()\npdict = {}\nfor p in plist:\n    i = p.find('=')\n    if i >= 0:\n        name = p[:i].strip().lower()\n        value = p[i+1:].strip()\n        if len(value) >= 2 and value[0] == value[-1] == '\"':\n            value = value[1:-1]\n            value = value.replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n        pdict[name] = value\nreturn key, pdict", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nConvert a string version of a function name to the callable object.\n\nIf the lookup_view is not an import path, it is assumed to be a URL pattern\nlabel and the original string is returned.\n\nIf can_fail is True, lookup_view might be a URL pattern label, so errors\nduring the import fail and the string is returned.\n\"\"\"\n", "func_signal": "def get_callable(lookup_view, can_fail=False):\n", "code": "if not callable(lookup_view):\n    try:\n        # Bail early for non-ASCII strings (they can't be functions).\n        lookup_view = lookup_view.encode('ascii')\n        mod_name, func_name = get_mod_func(lookup_view)\n        if func_name != '':\n            lookup_view = getattr(__import__(mod_name, {}, {}, ['']), func_name)\n            if not callable(lookup_view):\n                raise AttributeError(\"'%s.%s' is not a callable.\" % (mod_name, func_name))\n    except (ImportError, AttributeError):\n        if not can_fail:\n            raise\n    except UnicodeEncodeError:\n        pass\nreturn lookup_view", "path": "build\\libs\\django\\core\\urlresolvers.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nRead data from the underlying file.\nIf you ask for too much or there isn't anything left,\nthis will raise an InputStreamExhausted error.\n\"\"\"\n", "func_signal": "def read(self, num_bytes=None):\n", "code": "if self.remaining <= 0:\n    raise InputStreamExhausted()\nif num_bytes is None:\n    num_bytes = self.remaining\nelse:\n    num_bytes = min(num_bytes, self.remaining)\nself.remaining -= num_bytes\nreturn self._file.read(num_bytes)", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nCompletely exhausts an iterator or stream.\n\nRaise a MultiPartParserError if the argument is not a stream or an iterable.\n\"\"\"\n", "func_signal": "def exhaust(stream_or_iterable):\n", "code": "iterator = None\ntry:\n    iterator = iter(stream_or_iterable)\nexcept TypeError:\n    iterator = ChunkIter(stream_or_iterable, 16384)\n\nif iterator is None:\n    raise MultiPartParserError('multipartparser.exhaust() was passed a non-iterable or stream parameter')\n\nfor __ in iterator:\n    pass", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nConvert the filename into an md5 string. We'll turn the first couple\nbits of the path into directory prefixes to be nice to filesystems\nthat have problems with large numbers of files in a directory.\n\nThus, a cache key of \"foo\" gets turnned into a file named\n``{cache-dir}ac/bd/18db4cc2f85cedef654fccc4a4d8``.\n\"\"\"\n", "func_signal": "def _key_to_file(self, key):\n", "code": "path = md5_constructor(key.encode('utf-8')).hexdigest()\npath = os.path.join(path[:2], path[2:4], path[4:])\nreturn os.path.join(self._dir, path)", "path": "build\\libs\\django\\core\\cache\\backends\\filebased.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"Compare with another reference\"\"\"\n", "func_signal": "def __cmp__( self, other ):\n", "code": "if not isinstance (other,self.__class__):\n    return cmp( self.__class__, type(other) )\nreturn cmp( self.key, other.key)", "path": "build\\libs\\django\\dispatch\\saferef.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nEvery LazyStream must have a producer when instantiated.\n\nA producer is an iterable that returns a string each time it\nis called.\n\"\"\"\n", "func_signal": "def __init__(self, producer, length=None):\n", "code": "self._producer = producer\nself._empty = False\nself._leftover = ''\nself.length = length\nself.position = 0\nself._remaining = length\nself._unget_history = []", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nInitialize the MultiPartParser object.\n\n:META:\n    The standard ``META`` dictionary in Django request objects.\n:input_data:\n    The raw post data, as a bytestring.\n:upload_handler:\n    An UploadHandler instance that performs operations on the uploaded\n    data.\n:encoding:\n    The encoding with which to treat the incoming data.\n\"\"\"\n\n#\n# Content-Type should containt multipart and the boundary information.\n#\n\n", "func_signal": "def __init__(self, META, input_data, upload_handlers, encoding=None):\n", "code": "content_type = META.get('HTTP_CONTENT_TYPE', META.get('CONTENT_TYPE', ''))\nif not content_type.startswith('multipart/'):\n    raise MultiPartParserError('Invalid Content-Type: %s' % content_type)\n\n# Parse the header to get the boundary to split the parts.\nctypes, opts = parse_header(content_type)\nboundary = opts.get('boundary')\nif not boundary or not cgi.valid_boundary(boundary):\n    raise MultiPartParserError('Invalid boundary in multipart: %s' % boundary)\n\n\n#\n# Content-Length should contain the length of the body we are about\n# to receive.\n#\ntry:\n    content_length = int(META.get('HTTP_CONTENT_LENGTH', META.get('CONTENT_LENGTH',0)))\nexcept (ValueError, TypeError):\n    # For now set it to 0; we'll try again later on down.\n    content_length = 0\n\nif content_length <= 0:\n    # This means we shouldn't continue...raise an error.\n    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\nself._boundary = boundary\nself._input_data = input_data\n\n# For compatibility with low-level network APIs (with 32-bit integers),\n# the chunk size should be < 2^31, but still divisible by 4.\nself._chunk_size = min(2**31-4, *[x.chunk_size for x in upload_handlers if x.chunk_size])\n\nself._meta = META\nself._encoding = encoding or settings.DEFAULT_CHARSET\nself._content_length = content_length\nself._upload_handlers = upload_handlers", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nUsed when the exact number of bytes to read is unimportant.\n\nThis procedure just returns whatever is chunk is conveniently returned\nfrom the iterator instead. Useful to avoid unnecessary bookkeeping if\nperformance is an issue.\n\"\"\"\n", "func_signal": "def next(self):\n", "code": "if self._leftover:\n    output = self._leftover\n    self._leftover = ''\nelse:\n    output = self._producer.next()\n    self._unget_history = []\nself.position += len(output)\nreturn output", "path": "build\\libs\\django\\http\\multipartparser.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "# regex is a string representing a regular expression.\n# callback is either a string like 'foo.views.news.stories.story_detail'\n# which represents the path to a module and a view function name, or a\n# callable object (view).\n", "func_signal": "def __init__(self, regex, callback, default_args=None, name=None):\n", "code": "self.regex = re.compile(regex, re.UNICODE)\nif callable(callback):\n    self._callback = callback\nelse:\n    self._callback = None\n    self._callback_str = callback\nself.default_args = default_args or {}\nself.name = name", "path": "build\\libs\\django\\core\\urlresolvers.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nSets the script prefix for the current thread.\n\"\"\"\n", "func_signal": "def set_script_prefix(prefix):\n", "code": "if not prefix.endswith('/'):\n    prefix += '/'\n_prefixes[currentThread()] = prefix", "path": "build\\libs\\django\\core\\urlresolvers.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"\nHelper to look up a model from an \"app_label.module_name\" string.\n\"\"\"\n", "func_signal": "def _get_model(model_identifier):\n", "code": "try:\n    Model = models.get_model(*model_identifier.split(\".\"))\nexcept TypeError:\n    Model = None\nif Model is None:\n    raise base.DeserializationError(u\"Invalid model identifier: '%s'\" % model_identifier)\nreturn Model", "path": "build\\libs\\django\\core\\serializers\\python.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"Return a *safe* weak reference to a callable target\n\ntarget -- the object to be weakly referenced, if it's a\n    bound method reference, will create a BoundMethodWeakref,\n    otherwise creates a simple weakref.\nonDelete -- if provided, will have a hard reference stored\n    to the callable to be called after the safe reference\n    goes out of scope with the reference object, (either a\n    weakref or a BoundMethodWeakref) as argument.\n\"\"\"\n", "func_signal": "def safeRef(target, onDelete = None):\n", "code": "if hasattr(target, 'im_self'):\n    if target.im_self is not None:\n        # Turn a bound method into a BoundMethodWeakref instance.\n        # Keep track of these instances for lookup by disconnect().\n        assert hasattr(target, 'im_func'), \"\"\"safeRef target %r has im_self, but no im_func, don't know how to create reference\"\"\"%( target,)\n        reference = get_bound_method_weakref(\n            target=target,\n            onDelete=onDelete\n        )\n        return reference\nif callable(onDelete):\n    return weakref.ref(target, onDelete)\nelse:\n    return weakref.ref( target )", "path": "build\\libs\\django\\dispatch\\saferef.py", "repo_name": "taylanpince/florence_and_brock", "stars": 5, "license": "None", "language": "python", "size": 10720}
{"docstring": "\"\"\"Returns a list of tuples (Tag, tag_count) of the most frequently used Tags on this artist. \"\"\"\n\n", "func_signal": "def getTopTagsWithCounts(self, limit = None):\n", "code": "params = self._getParams()\ndoc = _Request(self, 'artist.getTopTags', self.api_key, params).execute()\n\nif not doc:\n\treturn []\n\nelements = doc.getElementsByTagName('tag')\nlist = []\n\nfor element in elements:\n\tif limit and len(list) >= limit:\n\t\tbreak\n\ttag_name = self._extract(element, 'name')\n\ttag_count = self._extract(element, 'count')\n\t\n\tlist.append((Tag(tag_name, *self.auth_data), tag_count))\n\nreturn list", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a list of the most listened to Tracks by this artist. \"\"\"\n\n", "func_signal": "def getTopTracks(self):\n", "code": "params = self._getParams()\ndoc = _Request(self, 'artist.getTopTracks', self.api_key, params).execute()\n\nif not doc:\n\treturn None\n\nlist = []\n\nfor track in doc.getElementsByTagName('track'):\n\tt = {}\n\ttitle = self._extract(track, 'name')\n\tartist = self.getName()\n\t\n\tlist.append(Track(artist, title, *self.auth_data))\n\nreturn list", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns the matches sorted by relevance.\n* limit: Limit the number of artists returned at one time. Default (maximum) is 30.\n* page: Scan into the results by specifying a page number. Defaults to first page.\n\"\"\"\n\n", "func_signal": "def getResults(self, limit = 30, page = 1):\n", "code": "params = self._getParams()\nparams['limit'] = unicode(limit)\nparams['page'] = unicode(page)\n\ndoc = _Request(self, 'artist.Search', self.api_key, params).execute()\n\nif not doc:\n\treturn None\n\nself._total_result_count = self._extract(doc, 'opensearch:totalResults')\nself._page = page\nself._limit = limit\n\ne = doc.getElementsByTagName('artistmatches')[0]\n\nnames = self._extract_all(e, 'name')\n\nlist = []\nfor name in names:\n\tlist.append(Artist(name, *self.auth_data))\n\nreturn list", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns the track title.\n  * from_server: If set to True, the value will be retrieved from the server.\n\"\"\"\n\n", "func_signal": "def getTitle(self, from_server = False):\n", "code": "if from_server:\n\treturn self._getCachedInfo('title')\nelse:\n\treturn self.title", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns the number of pages you'd get when calling getArtists(). \"\"\"\n\n", "func_signal": "def getArtistsPageCount(self):\n", "code": "if self._artist_pages:\n\treturn self._artist_pages\n\nself._get_artists_info()\n\nreturn self._artist_pages", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a tuple of the most popular Artists in the country, ordered by popularity. \"\"\"\n\n", "func_signal": "def getTopArtists(self):\n", "code": "params = self._getParams()\ndoc = _Request(self, 'geo.getTopArtists', self.api_key, params).execute()\n\nif not doc:\n\treturn None\n\nnames = self._extract_all(doc, 'name')\n\nartists = []\nfor name in names:\n\tartists.append(Artist(name, *self.auth_data))\n\nreturn artists", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a paginated list of all events recommended to a user by Last.fm, based on their listening profile.\n* page: The page number of results to return.\n* limit: The limit of events to return.\n\"\"\"\n\n", "func_signal": "def getRecommendedEvents(self, page = None, limit = None):\n", "code": "params = self._getParams()\nif page:\n\tparams['page'] = unicode(page)\nif limit:\n\tparams['limit'] = unicode(limit)\n\ndoc = _Request(self, 'user.getRecommendedEvents', self.api_key, params, True, self.secret).execute()\n\nif not doc:\n\treturn []\n\nids = self._extract_all(doc, 'id')\nlist = []\nfor id in ids:\n\tlist.append(Event(id, *self.auth_data))\n\nreturn list", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a list of the participating Artists. \"\"\"\n\n", "func_signal": "def getArtists(self):\n", "code": "names = self._getCachedInfo('artists')\n\nartists = []\nfor name in names:\n\tartists.append(Artist(name, *self.auth_data))\n\nreturn artists", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns the album title.\n  * from_server: If set to True, the value will be retrieved from the server.\n\"\"\"\n\n", "func_signal": "def getTitle(self, from_server = False):\n", "code": "if from_server:\n\treturn self._getCachedInfo('name')\nelse:\n\treturn self.title", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns the tracks on this playlist.\"\"\"\n\n", "func_signal": "def fetch(self):\n", "code": "params = self._getParams()\n\ndoc = _Request(self, 'playlist.fetch', self.api_key, params).execute()\n\nif not doc:\n\treturn None\n\ndata = {}\n\ndata['title'] = self._extract(doc, 'title')\nlist = []\n\nfor n in doc.getElementsByTagName('track'):\n\ttitle = self._extract(n, 'title')\n\tartist = self._extract(n, 'creator')\n\t\n\tlist.append(Track(artist, title, *self.auth_data))\n\nreturn list", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Adds this track to a user playlist. \n* playlist_id: The unique playlist ID. \n\n[DEPRECATED]\nUse UserPlaylist.addTrack instead.\n\"\"\"\n\n", "func_signal": "def addToPlaylist(self, playlist_id):\n", "code": "warn_deprecated('Track.addToPlaylist', 'UserPlaylist.addTrack')\n\nparams = self._getParams()\nparams['playlistID'] = unicode(playlist_id)\n\n_Request(self, 'playlist.addTrack', self.api_key, params, True, self.secret).execute()", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a list of UserPlaylists that this user owns.\"\"\"\n\n", "func_signal": "def getPlaylists(self):\n", "code": "data = self.getPlaylistsData()\n\nif not data:\n\treturn []\n\nids = []\nfor p in data:\n\tids.append(p['id'])\n\nplaylists = []\n\nfor id in ids:\n\tplaylists.append(UserPlaylist(self.getName(), id, *self.auth_data))\n\nreturn playlists", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a tuple of the most frequently listened to Artists in a week range. If no date range is supplied, it will return the most recent week's data. You can obtain the available ranges from getWeeklyChartList. \n  \t\t* from_value: The value marking the beginning of a week.\n  \t\t* to_value: The value marking the end of a week. \n\"\"\"\n\n", "func_signal": "def getTopWeeklyArtists(self, from_value = None, to_value = None):\n", "code": "params = self._getParams()\nif from_value and to_value:\n\tparams['from'] = unicode(from_value)\n\tparams['to'] = unicode(to_value)\n\ndoc = _Request(self, 'group.getWeeklyArtistChart', self.api_key, params).execute()\n\nlist = []\n\nnames = self._extract_all(doc, 'name')\nfor name in names:\n\tlist.append(Artist(name, *self.auth_data))\n\nreturn list", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a list of the tracks on a playlist. \n* playlist_id: A unique last.fm playlist ID, can be retrieved from getPlaylistIDs().\n\n[DEPRECATED]\nUse UserPlaylist.getTracks() instead.\n\"\"\"\n\n", "func_signal": "def fetchPlaylist(self, playlist_id):\n", "code": "warn_deprecated('User.fetchPlaylist', 'UserPlaylist.getTracks')\n\nuri = u'lastfm://playlist/%s' %unicode(playlist_id)\n\nreturn Playlist(uri, *self.auth_data).fetch()", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Add an album to a user's Last.fm library. \"\"\"\n\n", "func_signal": "def addAlbum(self, album):\n", "code": "params = self._getParams()\nparams['artist'] = album.getArtist().getName()\nparams['album'] = album.getName()\n\n_Request(self, 'library.addAlbum', self.api_key, params, True, self.secret).execute()", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a tuple of the most frequently listened to Tracks in a week range. If no date range is supplied, it will return the most recent week's data. You can obtain the available ranges from getWeeklyChartList(). \n* from_value: The value marking the beginning of a week.\n* to_value: The value marking the end of a week. \n\"\"\"\n\n", "func_signal": "def getTopWeeklyTracks(self, from_value = None, to_value = None):\n", "code": "params = self._getParams()\nif from_value and to_value:\n\tparams['from'] = from_value\n\tparams['to'] = to_value\n\ndoc = _Request(self, 'user.getWeeklyTrackChart', self.api_key, params).execute()\n\nif not doc:\n\treturn []\n\nlist = []\nfor track in doc.getElementsByTagName('track'):\n\tartist = self._extract(track, 'artist')\n\ttitle = self._extract(track, 'name')\n\t\n\t\n\tlist.append(Track(artist, title, *self.auth_data))\n\nreturn list", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns the first match.\"\"\"\n\n", "func_signal": "def getFirstMatch(self):\n", "code": "matches = self.getResults(1)\nif matches:\n\treturn matches[0]", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns this user's recent listened-to tracks. \"\"\"\n\n", "func_signal": "def getRecentTracks(self, limit = None):\n", "code": "params = self._getParams()\nif limit:\n\tparams['limit'] = unicode(limit)\n\nlist = []\n\ndoc = _Request(self, 'user.getRecentTracks', self.api_key, params).execute()\n\nif not doc:\n\treturn None\n\nfor track in doc.getElementsByTagName('track'):\n\ttitle = self._extract(track, 'name')\n\tartist = self._extract(track, 'artist')\n\t\n\tif track.hasAttribute('nowplaying'):\n\t\tcontinue\t#to prevent the now playing track from sneaking in here\n\t\t\n\tlist.append(Track(artist, title, *self.auth_data))\n\nreturn list", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a list of the most-applied tags to this album. \"\"\"\n\n#Web services currently broken.\n#TODO: add getTopTagsWithCounts\n\n", "func_signal": "def getTopTags(self, limit = None):\n", "code": "l = []\nfor tag in self._getCachedInfo('top_tags'):\n\tif limit and len(l) >= limit:\n\t\tbreak\n\t\n\tl.append(Tag(tag, *self.auth_data))\n\nreturn l", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"Returns a list of the upcoming Events for this artist. \"\"\"\n\n", "func_signal": "def getEvents(self):\n", "code": "params = self._getParams()\ndoc = _Request(self, 'artist.getEvents', self.api_key, params).execute()\n\nids = self._extract_all(doc, 'id')\n\nevents = []\nfor id in ids:\n\tevents.append(Event(id, *self.auth_data))\n\nreturn events", "path": "ext\\pylast.py", "repo_name": "garethr/lastbot", "stars": 7, "license": "None", "language": "python", "size": 92}
{"docstring": "\"\"\"\norder = [cmd_type, [param1, param2, paramN]]\n\"\"\"\n", "func_signal": "def execute_order(self, robot, order):\n", "code": "command = order[0]\nif command == Controller.SHOOT:\n    return self.shoot_order(robot, order[1][0], order[1][1])\nelif command == Controller.SCAN:\n    return self.scan_order(robot, order[1][0], order[1][1])\nelif command == Controller.DRIVE:\n    return self.drive_order(robot, order[1][0], order[1][1])\nelif command == Controller.WAIT:\n    pass", "path": "simulator.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nReturns list of walls if current object collides with walls. Takes object's radius in consideration.\n1 = north wall, 2 = east wall, 3 = south wall, 4 = west wall.\nFor exaple [1,2] => collides with both eats and north walls (is in ne corner)\n\"\"\"\n", "func_signal": "def collides_with_walls(self):\n", "code": "walls = []\nx = self.location[0]\ny = self.location[1]\nif x < self.radius:\n    walls.append(4)\nif x > (ARENA_MAX_WIDTH - self.radius):\n    walls.append(2)\nif y < self.radius:\n    walls.append(1)\nif y > (ARENA_MAX_LENGTH - self.radius):\n    walls.append(3)\nreturn walls;", "path": "objects.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nComputes direction in degrees from current location to target_location\n\"\"\"\n", "func_signal": "def plot_course(self, target_location):\n", "code": "y_diff = target_location[1] - self.y_loc()\nx_diff = target_location[0] - self.x_loc()\n\nreturn degrees(atan2( y_diff, x_diff ))", "path": "controllers\\sniper.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"Called by simulator. Do not use!\"\"\"\n", "func_signal": "def __init_event_loop__(self, status):\n", "code": "self.__status__ = status.pop(0)\nself.execute()", "path": "controller.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "# check collisions with walls\n", "func_signal": "def handle_collisions(self):\n", "code": "walls = self.collides_with_walls()\nif walls:\n    collision_damage = int((self.speed / WALL_COLLISION_DAMAGE_DIVISOR  ) + 0.5)\n    self.damage += collision_damage\n    self.speed = 0\n    self.target_speed = 0\n    if 1 in walls:\n        self.location[1] = 0 + self.radius + 1\n    if 2 in walls:\n        self.location[0] = ARENA_MAX_WIDTH - self.radius - 1\n    if 3 in walls:\n        self.location[1] = ARENA_MAX_LENGTH - self.radius - 1\n    if 4 in walls:\n        self.location[0] = 0 + self.radius + 1", "path": "objects.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nSets target values for robot's drive. Note changes in speed and direction are not instantenous, but they take some time.\n\"\"\"\n", "func_signal": "def drive(self, direction, speed):\n", "code": "if speed < MAX_BACKWARD_SPEED:\n    speed = MAX_BACKWARD_SPEED\nif speed > MAX_FORWARD_SPEED:\n    speed = MAX_BACKWARD_SPEED\n\nspeed = int(speed + 0.5)\ndirection = int(direction + 0.5) % 360\nself.target_speed = speed\nself.target_direction = direction\n\ndirection_changed = False\nif not self.can_turn():\n    # can't turn now, going too fast\n    pass\nelif self.direction == self.target_direction:\n    # already going to correct direction\n    pass\nelse:\n    # change self.direction towards target_direction\n    # TODO this should be true only if actual direction changes\n    direction_changed = True\n    pass\n    \nreturn direction_changed", "path": "objects.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nComputes amount of degrees that is needed to add direction to get target_direction.\n\n>>> angle_difference(10, 10)\n0\n>>> angle_difference(1, 10)\n9\n>>> angle_difference(0, 90)\n90\n>>> angle_difference(90, 0)\n-90\n>>> angle_difference(19, 5)\n-14\n>>> angle_difference(1,359)\n-2\n>>> angle_difference(350, 10)\n20\n\"\"\"\n", "func_signal": "def angle_difference(direction, target_direction):\n", "code": "diff = target_direction - direction\nnorm = fabs(diff)\nlow = fabs(diff - 360)\nhigh = fabs(diff + 360)\n\nif norm < low and norm < high:\n    return diff\nif low < norm and low < high:\n    return diff -360\nreturn diff + 360", "path": "constants.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nScans an arc from direction-spread to direction+spread looking for other robots.\nIf robot is found within scanning arc, distance to nearest robot is returned.\nIf no robots are found, None is returned.\nSpreads larger than MAX_SCAN_SPREAD are truncated.\n\ndirection and spread are in degrees (integers only).\n\"\"\"\n", "func_signal": "def scan(self, direction, spread):\n", "code": "results = self.main_greenlet.switch( [self.SCAN, [direction, spread]] )\nreturn self.__set_status__(results).pop(0)", "path": "controller.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nScans a given direction with resolution.\n\"\"\"\n", "func_signal": "def scan_order(self, current_robot, scan_direction, resolution):\n", "code": "shortest_distance = None\nfor player in self.active_players:\n    robot = player.robot\n    if current_robot == robot:\n        continue\n    \n    direction_vector = direction(current_robot.location, robot.location)\n    if fabs(angle_difference(direction_vector, scan_direction)) > resolution:\n        continue\n    \n    dist = current_robot.distance_to(robot.location)\n    if not shortest_distance or shortest_distance > dist:\n        shortest_distance = dist\n# save command parameters\ncurrent_robot.scan_order = [scan_direction, resolution]\n\nreturn shortest_distance", "path": "simulator.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nSelects a random corner (different from current) and drives there.\n\"\"\"\n# select a random corner\n", "func_signal": "def new_corner(self):\n", "code": "new_corner = randint(0, 3)\n# if same as current, select another corner\nif new_corner == self.corner:\n    new_corner = (new_corner + 1) % 4\n\nself.corner = new_corner\nself.corner_location, self.scan_start = self.CORNERS[self.corner]\n\n# find heading for desired corner        \ndirection = self.plot_course(self.corner_location)\n\n# start driving there at full speed\nself.drive(direction, MAX_FORWARD_SPEED)\n\n# keep running until close to corner\nwhile(self.distance(self.location(), self.corner_location) > 1000 and self.speed() > 0):\n    self.wait()\n\n# compute a more accurate direction and slow down to a crawl => robot can be stopped in single timestep\ndirection = self.plot_course(self.corner_location)\nself.drive(direction, MAX_ACCELERATION);\n\nwhile(self.distance(self.location(), self.corner_location) > 200 and self.speed() > 0):\n    self.wait()\n\n# stop engine and point robot to arena center for faster turning to next corner\nself.drive(self.plot_course([ARENA_MAX_WIDTH / 2, ARENA_MAX_LENGTH / 2]), 0)", "path": "controllers\\sniper.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "# current scan direction\n", "func_signal": "def execute(self):\n", "code": "scan_dir = 0\n# current direction\ndir = 90\n# range to enemy\nrange = 0\n# fix on opponent from the last scan\nhadfix = False\n\n# start moving immediately\nSPEED = MAX_FORWARD_SPEED\nself.drive(dir, SPEED)\n\nwhile True:\n    tdir = dir\n    location, damage, speed, direction = self.status()\n    cx, cy = location\n    \n    # IMPROVE direction comments are not corrent?\n    if cx > ARENA_MAX_WIDTH - self.BORDER:\n        if cy < ARENA_MAX_LENGTH - self.BORDER:\n            # near east wall\n            tdir = 90\n        else:\n            # near northeast corner\n            tdir = 180\n    elif cx < self.BORDER:\n        if cy < self.BORDER:\n            # near southwest corner\n            tdir = 0\n        else:\n            # near west wall\n            tdir = 270\n    elif cy > ARENA_MAX_LENGTH - self.BORDER:\n        # near north wall\n        tdir = 180\n    elif cy < self.BORDER:\n        # near south wall\n        tdir = 0\n    \n    # if  speed = 0, restart drive unit\n    # if dir  != dir need to change direction\n    if not speed or dir != tdir:\n        dir = tdir\n        self.drive(dir, SPEED)\n\n    # scan for target\n    range = self.scan(scan_dir, 10)\n    if range:\n        # found one, start shooting it\n        self.cannon(scan_dir, range)\n        # remember that we saw a target\n        hadfix = True\n    elif hadfix:\n        # had target but lost it, back up the scan\n        scan_dir += 40\n        # forget target\n        hadfix = False\n    else: \n        # no target found, increment scan\n        scan_dir -= 20", "path": "controllers\\tracker.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"Initialized Simulator\"\"\"\n", "func_signal": "def __init__(self, players, visualizer = None):\n", "code": "self.players = players\nself.active_players = list(players)\nself.shells = []\nself.exploding_shells = []\nself.logger = logging.getLogger('SIMULATOR')\nself.logger.info(\"initialized\")\nself.order_results = {}\nself.visualizer = visualizer\nself.timestep_count = 0", "path": "simulator.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nEntry point of controller. Override this in subclass. \nDo not exit or throw execptions from this method, since that counts as surrender ;-) \nMake an infinite loop and call methods provided by this class.\n\nCalling shoot, drive, scan or wait finishes turn for current timeslice. Calling status does not affect timeslice.\n\"\"\"\n\n# default behaviour is do nothing\n", "func_signal": "def execute(self):\n", "code": "while True:\n    self.wait()", "path": "controller.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nMove flying shells and handle explosions.\n\"\"\"\n", "func_signal": "def move_shells(self):\n", "code": "finished_shells = []        \nfor shell in self.shells:\n    if shell.move():\n        shell.explode()\n        self.exploding_shells.append(shell)\n        self.shells.remove(shell)\n\nfor shell in self.exploding_shells:\n    for player in self.active_players:\n        robot = player.robot\n        distance = shell.distance_to(robot.location)\n        damage = 0\n        for blast in BLAST_DAMAGE:\n            if distance > blast[0]:\n                break;\n            damage = blast[1]\n        \n        robot.damage += damage\n            \n        if robot.damage > MAX_DAMAGE:\n            # robot died\n            player.active = False\n            self.active_players.remove(player)\n            self.logger.info(\"%s destroyed at %d\" % (robot.name, self.timestep_count))", "path": "simulator.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "# TODO kill game after N steps if no resolution before\n", "func_signal": "def stopping(self):\n", "code": "if self.timestep_count >= 30000:\n    self.logger.info(\"Stopping via timeout at %d \" % self.timestep_count)\n    return True", "path": "simulator.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nSkips a turn.\n\"\"\"\n", "func_signal": "def wait(self):\n", "code": "results = self.main_greenlet.switch( [self.WAIT] )\nreturn self.__set_status__(results)", "path": "controller.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nShoots the cannon to given direction and distance.\nThere may be up to 3 cannon shells in the air at same time.\nReturn True, if shooting was succesful, False otherwise.\n\ndirection is in degrees, distance is in pixels (integers only).\n\nTODO speed affects accuracy?\n\"\"\"\n", "func_signal": "def shoot(self, direction, distance):\n", "code": "results = self.main_greenlet.switch( [self.SHOOT, [direction, distance]] )\nreturn self.__set_status__(results).pop(0)", "path": "controller.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nChanges direction and speed of the robot. Robot will continue moving until collision or new drive command.\nMaximum forward speed is MAX_FORWARD_SPEED pixels/time-unit.\nMaximum backward speed is MAX_BACKWARD_SPEED pixels/time-unit.\nMaximum acceleration is MAX_ACCELERATION pixels/time-unit^2.\nMaximum speed where turning is allowed is MAX_TURN_SPEED pixels/time-unit. \nIf speed is too high for turning, robot will slow down, turn and go back to its target speed.\n\nReturns True if direction was changed, False if direction was not changed due to high speed.\n\ndirection is in degrees, speed is in pixels/time-unit (integers only).\n\"\"\"\n", "func_signal": "def drive(self, direction, speed):\n", "code": "results = self.main_greenlet.switch( [self.DRIVE, [direction, speed]] )\nreturn self.__set_status__(results).pop(0)", "path": "controller.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nScan 90 angle back and forth, shoot targets, stop sniping when getting damage.\n\"\"\"\n", "func_signal": "def snipe(self):\n", "code": "self.last_damage = self.damage()\nscan_dir = self.scan_start\nclosest = None\nwhile True:\n    while scan_dir < self.scan_start + 90:\n        # look at direction\n        range = self.scan(scan_dir, 1)\n        if range and range <= MAX_CANNON_RANGE:\n            # found someone to shoot at, keep shooting while it is in our sight\n            while range:\n                closest = range\n                self.shoot(scan_dir, range)\n                range = self.scan(scan_dir, 1)\n                if self.last_damage + 15 > self.damage():\n                    # getting too much damage => give up shooting and head for a new corner\n                    range = None\n            # back up scan, in case the target moved        \n            scan_dir -= 10\n        # scan slowly forward\n        scan_dir += 2\n        \n        if self.last_damage != self.damage():\n            # not shooting anyone, but somebody is shooting us, time to move\n            self.new_corner()\n            self.last_damage = self.damage()\n            scan_dir = self.scan_start\n            closest = None\n            \n    if not closest:\n        # nobody to shoot at from this corner, so try another corner\n        self.new_corner()\n        self.last_damage = self.damage()\n        scan_dir = self.scan_start\n    else:\n        # there was someone to shoot in range, but lost it, so scan again\n        scan_dir = self.scan_start\n        self.closest = None", "path": "controllers\\sniper.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "\"\"\"\nReturns direction (in float degrees) from location to target.\n\"\"\"\n\n", "func_signal": "def direction(location, target):\n", "code": "vector = [target[0] - location[0], target[1] - location[1]]\nreturn direction_of_vector(vector)", "path": "constants.py", "repo_name": "jsyrjala/pyrobots", "stars": 4, "license": "None", "language": "python", "size": 103}
{"docstring": "# alias all queue methods for faster lookup\n", "func_signal": "def _postinit(self):\n", "code": "self._popleft = self._queue.popleft\nself._pop = self._queue.pop\nif hasattr(self._queue, 'remove'):\n    self._remove = self._queue.remove\nself._wlock = allocate_lock()\nself._append = self._queue.append", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Return a list of items.\"\"\"\n", "func_signal": "def items(self):\n", "code": "result = [(key, self._mapping[key]) for key in list(self._queue)]\nresult.reverse()\nreturn result", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Check if the object passed is undefined.  This does nothing more than\nperforming an instance check against :class:`Undefined` but looks nicer.\nThis can be used for custom filters or tests that want to react to\nundefined variables.  For example a custom default filter can look like\nthis::\n\n    def default(var, default=''):\n        if is_undefined(var):\n            return default\n        return var\n\"\"\"\n", "func_signal": "def is_undefined(obj):\n", "code": "from jinja2.runtime import Undefined\nreturn isinstance(obj, Undefined)", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Marks the function as internally used\"\"\"\n", "func_signal": "def internalcode(f):\n", "code": "internal_code.add(f.func_code)\nreturn f", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Remove an item from the cache dict.\nRaise an `KeyError` if it does not exist.\n\"\"\"\n", "func_signal": "def __delitem__(self, key):\n", "code": "self._wlock.acquire()\ntry:\n    del self._mapping[key]\n    try:\n        self._remove(key)\n    except ValueError:\n        # __getitem__ is not locked, it might happen\n        pass\nfinally:\n    self._wlock.release()", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Python 2.4 compatibility.\"\"\"\n", "func_signal": "def _remove(self, obj):\n", "code": "for idx, item in enumerate(self._queue):\n    if item == obj:\n        del self._queue[idx]\n        break", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Consumes an iterable without doing anything with it.\"\"\"\n", "func_signal": "def consume(iterable):\n", "code": "for event in iterable:\n    pass", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Goes one item ahead and returns it.\"\"\"\n", "func_signal": "def next(self):\n", "code": "rv = self.current\nself.pos = (self.pos + 1) % len(self.items)\nreturn rv", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Helper for various string-wrapped functions.\"\"\"\n", "func_signal": "def _escape_argspec(obj, iterable):\n", "code": "for key, value in iterable:\n    if hasattr(value, '__html__') or isinstance(value, basestring):\n        obj[key] = escape(value)\nreturn obj", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Get an item from the cache. Moves the item up so that it has the\nhighest priority then.\n\nRaise an `KeyError` if it does not exist.\n\"\"\"\n", "func_signal": "def __getitem__(self, key):\n", "code": "rv = self._mapping[key]\nif self._queue[-1] != key:\n    try:\n        self._remove(key)\n    except ValueError:\n        # if something removed the key from the container\n        # when we read, ignore the ValueError that we would\n        # get otherwise.\n        pass\n    self._append(key)\nreturn rv", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Make a string unicode if it isn't already.  That way a markup\nstring is not converted back to unicode.\n\"\"\"\n", "func_signal": "def soft_unicode(s):\n", "code": "if not isinstance(s, unicode):\n    s = unicode(s)\nreturn s", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Clear the cache.\"\"\"\n", "func_signal": "def clear(self):\n", "code": "self._wlock.acquire()\ntry:\n    self._mapping.clear()\n    self._queue.clear()\nfinally:\n    self._wlock.release()", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Prettyprint an object.  Either use the `pretty` library or the\nbuiltin `pprint`.\n\"\"\"\n", "func_signal": "def pformat(obj, verbose=False):\n", "code": "try:\n    from pretty import pretty\n    return pretty(obj, verbose=verbose)\nexcept ImportError:\n    from pprint import pformat\n    return pformat(obj)", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Converts any URLs in text into clickable links. Works on http://,\nhttps:// and www. links. Links can have trailing punctuation (periods,\ncommas, close-parens) and leading punctuation (opening parens) and\nit'll still do the right thing.\n\nIf trim_url_limit is not None, the URLs in link text will be limited\nto trim_url_limit characters.\n\nIf nofollow is True, the URLs in link text will get a rel=\"nofollow\"\nattribute.\n\"\"\"\n", "func_signal": "def urlize(text, trim_url_limit=None, nofollow=False):\n", "code": "trim_url = lambda x, limit=trim_url_limit: limit is not None \\\n                     and (x[:limit] + (len(x) >=limit and '...'\n                     or '')) or x\nwords = _word_split_re.split(unicode(escape(text)))\nnofollow_attr = nofollow and ' rel=\"nofollow\"' or ''\nfor i, word in enumerate(words):\n    match = _punctuation_re.match(word)\n    if match:\n        lead, middle, trail = match.groups()\n        if middle.startswith('www.') or (\n            '@' not in middle and\n            not middle.startswith('http://') and\n            len(middle) > 0 and\n            middle[0] in _letters + _digits and (\n                middle.endswith('.org') or\n                middle.endswith('.net') or\n                middle.endswith('.com')\n            )):\n            middle = '<a href=\"http://%s\"%s>%s</a>' % (middle,\n                nofollow_attr, trim_url(middle))\n        if middle.startswith('http://') or \\\n           middle.startswith('https://'):\n            middle = '<a href=\"%s\"%s>%s</a>' % (middle,\n                nofollow_attr, trim_url(middle))\n        if '@' in middle and not middle.startswith('www.') and \\\n           not ':' in middle and _simple_email_re.match(middle):\n            middle = '<a href=\"mailto:%s\">%s</a>' % (middle, middle)\n        if lead + middle + trail != word:\n            words[i] = lead + middle + trail\nreturn u''.join(words)", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Return an item from the cache dict or `default`\"\"\"\n", "func_signal": "def get(self, key, default=None):\n", "code": "try:\n    return self[key]\nexcept KeyError:\n    return default", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Jinja2 keeps internal caches for environments and lexers.  These are\nused so that Jinja2 doesn't have to recreate environments and lexers all\nthe time.  Normally you don't have to care about that but if you are\nmessuring memory consumption you may want to clean the caches.\n\"\"\"\n", "func_signal": "def clear_caches():\n", "code": "from jinja2.environment import _spontaneous_environments\nfrom jinja2.lexer import _lexer_cache\n_spontaneous_environments.clear()\n_lexer_cache.clear()", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Generate some lorem impsum for the template.\"\"\"\n", "func_signal": "def generate_lorem_ipsum(n=5, html=True, min=20, max=100):\n", "code": "from jinja2.constants import LOREM_IPSUM_WORDS\nfrom random import choice, randrange\nwords = LOREM_IPSUM_WORDS.split()\nresult = []\n\nfor _ in xrange(n):\n    next_capitalized = True\n    last_comma = last_fullstop = 0\n    word = None\n    last = None\n    p = []\n\n    # each paragraph contains out of 20 to 100 words.\n    for idx, _ in enumerate(xrange(randrange(min, max))):\n        while True:\n            word = choice(words)\n            if word != last:\n                last = word\n                break\n        if next_capitalized:\n            word = word.capitalize()\n            next_capitalized = False\n        # add commas\n        if idx - randrange(3, 8) > last_comma:\n            last_comma = idx\n            last_fullstop += 2\n            word += ','\n        # add end of sentences\n        if idx - randrange(10, 20) > last_fullstop:\n            last_comma = last_fullstop = idx\n            word += '.'\n            next_capitalized = True\n        p.append(word)\n\n    # ensure that the paragraph ends with a dot.\n    p = u' '.join(p)\n    if p.endswith(','):\n        p = p[:-1] + '.'\n    elif not p.endswith('.'):\n        p += '.'\n    result.append(p)\n\nif not html:\n    return u'\\n\\n'.join(result)\nreturn Markup(u'\\n'.join(u'<p>%s</p>' % escape(x) for x in result))", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"This decorator can be used to mark a function or method context callable.\nA context callable is passed the active :class:`Context` as first argument when\ncalled from the template.  This is useful if a function wants to get access\nto the context or functions provided on the context object.  For example\na function that returns a sorted list of template variables the current\ntemplate exports could look like this::\n\n    @contextfunction\n    def get_exported_names(context):\n        return sorted(context.exported_vars)\n\"\"\"\n", "func_signal": "def contextfunction(f):\n", "code": "f.contextfunction = True\nreturn f", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Sets the value for an item. Moves the item up so that it\nhas the highest priority then.\n\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "self._wlock.acquire()\ntry:\n    if key in self._mapping:\n        try:\n            self._remove(key)\n        except ValueError:\n            # __getitem__ is not locked, it might happen\n            pass\n    elif len(self._mapping) == self.capacity:\n        del self._mapping[self._popleft()]\n    self._append(key)\n    self._mapping[key] = value\nfinally:\n    self._wlock.release()", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Set `default` if the key is not in the cache otherwise\nleave unchanged. Return the value of this key.\n\"\"\"\n", "func_signal": "def setdefault(self, key, default=None):\n", "code": "try:\n    return self[key]\nexcept KeyError:\n    self[key] = default\n    return default", "path": "app\\jinja2\\jinja2\\utils.py", "repo_name": "yesudeep/greatship", "stars": 4, "license": "other", "language": "python", "size": 5632}
{"docstring": "\"\"\"Get a latex table line representing configuration\nTakes a list of parameters in form\nsection.parameter, for example \"iperf.speed\"\n\"\"\"\n", "func_signal": "def latex_line(self, parameters):\n", "code": "def emph(el):\n    return (r\"\\textbf{%s}\" % el)\n\ndef make_line(els):\n    line = r\"\\hline\" + \"\\n%s\\t\" + r\"\\\\\" + \"\\n\"\n    result = []\n    for el in els:\n        result.append(el)\n    return line % (\" & \".join(result))\n\nels = [emph(self.codename)] + [self.conf[x][y].value for x, y in map(lambda x: x.split('.'), parameters)]\nreturn make_line(els)", "path": "src\\config.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Taking float numbers in a list of tuples\"\"\"\n", "func_signal": "def tuple_to_num(tup):\n", "code": "if tup[1]:\n    return float('.'.join([tup[0], tup[1]]))\nelse:\n    return int(tup[0])", "path": "src\\utils.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Substraction between configuration, equal values are eliminated\"\"\"\n", "func_signal": "def __sub__(self, other):\n", "code": "diff = deepcopy(self)\nfor key in diff.conf.keys():\n    if other.conf.has_key(key):\n        diff.conf[key] -= other.conf[key]\n        # CHANGED added this check to avoid empty keys\n        if diff.conf[key].is_empty():\n            diff.conf.pop(key)\ndiff.codename = other.codename\nreturn diff", "path": "src\\config.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"docstring for testPlotter\"\"\"\n", "func_signal": "def testPlotter():\n", "code": "p = Plotter(\"test\", \"band\")\np.add_data([50 + random.randrange(5)], \"random\")\nfor x in range(100):\n    p.update([50 + random.randrange(5)])\n    time.sleep(0.05)\np.add_data([30 + random.randrange(20)], \"random2\")\nfor x in range(100):\n    p.update([30 + random.randrange(20)])\n    time.sleep(0.05)\n    \np.add_data([20 + random.randrange(5)], \"random3\")\nfor x in range(100):\n    p.update([20 + random.randrange(5)])\n    time.sleep(0.05)", "path": "src\\test.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "# Just put in this list all the cases you want to examine\n", "func_signal": "def setUp(self):\n", "code": "self.equals = [\n                ((Size(1, 'MB'), Size(1024, 'KB')),\n                (Size(1, 'Mb'), Size(128, 'KB')))]", "path": "test\\utilsTest.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Gets the speed out of a string (1M, 2g for example)\nconverts it into unit if necessary\n\"\"\"\n", "func_signal": "def get_speed(speed, unit):\n", "code": "reg = re.compile(r'([\\d.]+)(\\w+)')\ns, u = reg.match(speed).groups()\nreturn Size(s, u).translate(unit)", "path": "src\\utils.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Gets the minimal Cnf, without choices and taking off null values\"\"\"\n", "func_signal": "def to_min(self):\n", "code": "not_nulls = filter(lambda x: self.conf[x].value != '', self.conf.keys())\nreturn dict(zip(not_nulls, [self.conf[key] for key in not_nulls]))", "path": "src\\config.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Plots and writes the graph generated to filename\"\"\"\n", "func_signal": "def plot(self, results, codename, filename):\n", "code": "plotter = Plotter(\"testing\", \"kbs\")\nfor key, val in results.iteritems():\n    plotter.add_data(val)\nplotter.plot()\nplotter.save(filename)", "path": "tester.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Setting some defaults confs\"\"\"\n", "func_signal": "def set_defaults(self):\n", "code": "self.plotter.set_string(\"title\", self.title)\nself.plotter.set_range('yrange', (0,\"*\"))\nself.plotter.set_label('xlabel', \"step\")\nself.plotter.set_label('ylabel', self.value)", "path": "src\\analyze.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\" Getting the diff of two configuration\"\"\"\n", "func_signal": "def __sub__(self, other):\n", "code": "subt = deepcopy(self)\nfor key, val in subt.conf.items():\n    if other.conf.has_key(key):\n        if other.conf[key] == subt.conf[key]:\n            subt.conf.pop(key)\n        else:\n            subt.conf[key] = other.conf[key]\nreturn subt", "path": "src\\config.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"\nGetting the codename from a whatever, using global variables\n\"\"\"\n", "func_signal": "def get_codename(test_file):\n", "code": "for el in RESULTS.values():\n    # Unreliable but working pretty well\n    reg = re.compile(\"(.*)\".join(el.split(\"%s\")))\n    found = r.search(test_file)\n    if found:\n        return found.groups()[0]\nreturn None", "path": "src\\utils.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Merge two configurations, the second one has the last word\nNote that of course this IS NOT symmetric\"\"\"\n", "func_signal": "def __add__(self, other):\n", "code": "merged = deepcopy(self)\nfor key in opt_conf.keys():\n    if merged.conf.has_key(key) and other.conf.has_key(key):\n        merged.conf[key] += other.conf[key]\n    elif other.conf.has_key(key):\n        merged.conf[key] = other.conf[key]\nmerged.codename = other.codename\nreturn merged", "path": "src\\config.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Returns the rounded translation in a different unit measure\nIt DOESN'T return a new Size object but the just the value\n\"\"\"\n", "func_signal": "def translate(self, unit):\n", "code": "self._check_unit(unit)\n# FIXME attention to equal named variables\noffset = self.units[self.unit] - self.units[unit]\nreturn round(self.value * (pow(2, offset)))", "path": "src\\utils.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "# last line not evaluated\n", "func_signal": "def parse_file(self, filename):\n", "code": "for line in filename.readlines()[:-1]:\n    self.parse_line(line)", "path": "src\\parse_iperf.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Returns the dictionary of results paths\"\"\"\n", "func_signal": "def get_res(root, code):\n", "code": "paths = [ os.path.join(root, k, val % code) for k, val in RESULTS.iteritems() ]\nreturn dict(zip(RESULTS.keys(), paths))", "path": "tester.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "# inverting the dictionary\n", "func_signal": "def __init__(self, format):\n", "code": "self.format = format\nself.fields = [\"avg\", \"missed\", \"total\", \"jitter\", \"transfer\"]\nself.result = dict(zip(self.fields, [None] * len(self.fields)))\nself.result['values'] = []", "path": "src\\parse_iperf.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"The other possible formats are defined in gnuplot configuration file.\nThe yaxis must be passed also\n\"\"\"\n", "func_signal": "def __init__(self, data, title, xaxis, yaxis, gnuplot_conf, format=\"default\"):\n", "code": "self.data = data\nself.title = title\nself.format = format\nself.xaxis = xaxis\nself.yaxis = yaxis\nself.gnuplot_conf = gnuplot_conf", "path": "src\\analyze.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"docstring for testSetting\"\"\"\n", "func_signal": "def testSetting(self):\n", "code": "b = BoolOpt(self.name)\nb.set(self.good)\nself.assertEqual(b.value, self.good)\nself.failUnlessRaises(ValueError, b.set, self.bad)", "path": "src\\test.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Automatic run of the tests\"\"\"\n", "func_signal": "def batch(self):\n", "code": "self.pre_run()\nself.run()", "path": "tester.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Write the configuration in ini format\nafter having minimized it\"\"\"\n", "func_signal": "def _write_conf(self, conf_file):\n", "code": "writer = ConfigParser.ConfigParser()\nconf = self.to_min()\nfor sec, opt in conf.items():\n    writer.add_section(sec)\n    for key, val in opt.items():\n        writer.set(sec, key, val.value)\nwriter.write(conf_file)", "path": "src\\config.py", "repo_name": "AndreaCrotti/nomadic-communications", "stars": 5, "license": "None", "language": "python", "size": 1148}
{"docstring": "\"\"\"Initialize a discovery object\"\"\"\n", "func_signal": "def __init__(self, session, url, session_key_suffix=None):\n", "code": "self.session = session\nself.url = url\nif session_key_suffix is None:\n    session_key_suffix = self.DEFAULT_SUFFIX\n\nself.session_key_suffix = session_key_suffix", "path": "pinax\\libs\\external_libs\\python-yadis-1.1.0\\yadis\\manager.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"Return the next service\n\nself.current() will continue to return that service until the\nnext call to this method.\"\"\"\n", "func_signal": "def next(self):\n", "code": "try:\n    self._current = self.services.pop(0)\nexcept IndexError:\n    raise StopIteration\nelse:\n    return self._current", "path": "pinax\\libs\\external_libs\\python-yadis-1.1.0\\yadis\\manager.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"There is no wildcard and the realm is the same as the return_to URL\n\"\"\"\n", "func_signal": "def test_trivial(self):\n", "code": "self.failUnlessDiscoURL('http://example.com/foo',\n                        'http://example.com/foo')", "path": "pinax\\libs\\external_libs\\python-openid-2.1.1\\openid\\test\\test_rpverify.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"Extract the YadisServiceManager for this object's URL and\nsuffix from the session.\n\n@return: The current YadisServiceManager, if it's for this\n    URL, or else None\n\"\"\"\n", "func_signal": "def getManager(self):\n", "code": "manager = self.session.get(self.getSessionKey())\nif (manager is not None and manager.forURL(self.url)):\n    return manager\nelse:\n    return None", "path": "pinax\\libs\\external_libs\\python-yadis-1.1.0\\yadis\\manager.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "# The URL that was used to initiate the Yadis protocol\n", "func_signal": "def __init__(self, starting_url, yadis_url, services, session_key):\n", "code": "self.starting_url = starting_url\n\n# The URL after following redirects (the identifier)\nself.yadis_url = yadis_url\n\n# List of service elements\nself.services = list(services)\n\nself.session_key = session_key\n\n# Reference to the current service object\nself._current = None", "path": "pinax\\libs\\external_libs\\python-yadis-1.1.0\\yadis\\manager.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = S.loads(JSON)\nout = S.dumps(res)\nself.assertEquals(res, S.loads(out))", "path": "pinax\\libs\\external_libs\\simplejson-1.9.1\\simplejson\\tests\\test_pass2.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"\nDeletes the specified comment, which can be either a ``FreeThreadedComment`` or a\n``ThreadedComment``.  If it is a POST request, then the comment will be deleted\noutright, however, if it is a GET request, a confirmation page will be shown.\n\"\"\"\n", "func_signal": "def comment_delete(request, object_id, model=ThreadedComment, extra_context = {}, context_processors = [], permission_callback=can_delete_comment):\n", "code": "tc = get_object_or_404(model, id=int(object_id))\nif not permission_callback(tc, request.user):\n    login_url = settings.LOGIN_URL\n    current_url = urlquote(request.get_full_path())\n    return HttpResponseRedirect(\"%s?next=%s\" % (login_url, current_url))\nif request.method == \"POST\":\n    tc.delete()\n    return HttpResponseRedirect(_get_next(request))\nelse:\n    if model == ThreadedComment:\n        is_free_threaded_comment = False\n        is_threaded_comment = True\n    else:\n        is_free_threaded_comment = True\n        is_threaded_comment = False\n    return render_to_response(\n        'threadedcomments/confirm_delete.html',\n        extra_context, \n        context_instance = RequestContext(\n            request, \n            {\n                'comment' : tc, \n                'is_free_threaded_comment' : is_free_threaded_comment,\n                'is_threaded_comment' : is_threaded_comment,\n                'next' : _get_next(request),\n            },\n            context_processors\n        )\n    )", "path": "pinax\\apps\\external_apps\\threadedcomments\\views.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"Return the next authentication service for the pair of\nuser_input and session.  This function handles fallback.\n\n\n@param discover: a callable that takes a URL and returns a\n    list of services\n\n@type discover: str -> [service]\n\n\n@return: the next available service\n\"\"\"\n", "func_signal": "def getNextService(self, discover):\n", "code": "manager = self.getManager()\nif manager is not None and not manager:\n    self.destroyManager()\n\nif not manager:\n    yadis_url, services = discover(self.url)\n    manager = self.createManager(services, yadis_url)\n\nif manager:\n    service = manager.next()\n    manager.store(self.session)\nelse:\n    service = None\n\nreturn service", "path": "pinax\\libs\\external_libs\\python-yadis-1.1.0\\yadis\\manager.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"\nReturns a preview of the comment so that the user may decide if he or she wants to\nedit it before submitting it permanently.\n\"\"\"\n", "func_signal": "def _preview(request, context_processors, extra_context, form_class=ThreadedCommentForm):\n", "code": "_adjust_max_comment_length(form_class)\nform = form_class(request.POST or None)\ncontext = {\n    'next' : _get_next(request),\n    'form' : form,\n}\nif form.is_valid():\n    new_comment = form.save(commit=False)\n    context['comment'] = new_comment\nelse:\n    context['comment'] = None\nreturn render_to_response(\n    'threadedcomments/preview_comment.html',\n    extra_context, \n    context_instance = RequestContext(request, context, context_processors)\n)", "path": "pinax\\apps\\external_apps\\threadedcomments\\views.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"copies the specified template directory to the copy_to location\"\"\"\n", "func_signal": "def copy_template(template_name, copy_to):\n", "code": "import django_extensions\nimport re\nimport shutil\n\ntemplate_dir = os.path.join(django_extensions.__path__[0], 'conf', template_name)\n\n# walks the template structure and copies it\nfor d, subdirs, files in os.walk(template_dir):\n    relative_dir = d[len(template_dir)+1:]\n    if relative_dir and not os.path.exists(os.path.join(copy_to, relative_dir)):\n        os.mkdir(os.path.join(copy_to, relative_dir))\n    for i, subdir in enumerate(subdirs):\n        if subdir.startswith('.'):\n            del subdirs[i]\n    for f in files:\n        if f.endswith('.pyc') or f.startswith('.DS_Store'):\n            continue\n        path_old = os.path.join(d, f)\n        path_new = os.path.join(copy_to, relative_dir, f)\n        if os.path.exists(path_new):\n            path_new = os.path.join(copy_to, relative_dir, f)\n            if os.path.exists(path_new):\n                continue\n        path_new = path_new.rstrip(\".tmpl\")\n        fp_old = open(path_old, 'r')\n        fp_new = open(path_new, 'w')\n        fp_new.write(fp_old.read())\n        fp_old.close()\n        fp_new.close()\n        try:\n            shutil.copymode(path_old, path_new)\n            _make_writeable(path_new)\n        except OSError:\n            sys.stderr.write(style.NOTICE(\"Notice: Couldn't set permission bits on %s. You're probably using an uncommon filesystem setup. No problem.\\n\" % path_new))", "path": "pinax\\apps\\external_apps\\django_extensions\\management\\commands\\create_jobs.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"Clean up Yadis-related services in the session and return\nthe most-recently-attempted service from the manager, if one\nexists.\n\n@return: current service endpoint object or None if there is\n    no current service\n\"\"\"\n", "func_signal": "def cleanup(self):\n", "code": "manager = self.getManager()\nif manager is not None:\n    service = manager.current()\n    self.destroyManager()\nelse:\n    service = None\n\nreturn service", "path": "pinax\\libs\\external_libs\\python-yadis-1.1.0\\yadis\\manager.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"\nThe part that's the least straightforward about views in this module is how they \ndetermine their redirects after they have finished computation.\n\nIn short, they will try and determine the next place to go in the following order:\n\n1. If there is a variable named ``next`` in the *POST* parameters, the view will\nredirect to that variable's value.\n2. If there is a variable named ``next`` in the *GET* parameters, the view will\nredirect to that variable's value.\n3. If Django can determine the previous page from the HTTP headers, the view will\nredirect to that previous page.\n4. Otherwise, the view raise a 404 Not Found.\n\"\"\"\n", "func_signal": "def _get_next(request):\n", "code": "next = request.POST.get('next', request.GET.get('next', request.META.get('HTTP_REFERER', None)))\nif not next or next == request.path:\n    raise Http404 # No next url was supplied in GET or POST.\nreturn next", "path": "pinax\\apps\\external_apps\\threadedcomments\\views.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"Delete any YadisServiceManager with this starting URL and\nsuffix from the session.\n\nIf there is no service manager or the service manager is for a\ndifferent URL, it silently does nothing.\n\"\"\"\n", "func_signal": "def destroyManager(self):\n", "code": "if self.getManager() is not None:\n    key = self.getSessionKey()\n    del self.session[key]", "path": "pinax\\libs\\external_libs\\python-yadis-1.1.0\\yadis\\manager.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"Build a discovery URL out of the realm and a return_to and\nmake sure that it matches the expected discovery URL\n\"\"\"\n", "func_signal": "def failUnlessDiscoURL(self, realm, expected_discovery_url):\n", "code": "realm_obj = trustroot.TrustRoot.parse(realm)\nactual_discovery_url = realm_obj.buildDiscoveryURL()\nself.failUnlessEqual(expected_discovery_url, actual_discovery_url)", "path": "pinax\\libs\\external_libs\\python-openid-2.1.1\\openid\\test\\test_rpverify.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "# instantiate every lexer, to see if the token type defs are correct\n", "func_signal": "def test_import_all(self):\n", "code": "for x in lexers.LEXERS.keys():\n    c = getattr(lexers, x)()", "path": "pinax\\libs\\external_libs\\Pygments-0.11.1\\tests\\test_basic_api.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "# test that the formatter supports encoding and Unicode\n", "func_signal": "def test_unicode_handling(self):\n", "code": "tokens = list(lexers.PythonLexer(encoding='utf-8').get_tokens(\"def f(): '\u00e4'\"))\nfor formatter, info in formatters.FORMATTERS.iteritems():\n    try:\n        inst = formatter(encoding=None)\n    except (ImportError, FontNotFound):\n        # some dependency or font not installed\n        continue\n    out = format(tokens, inst)\n    if formatter.unicodeoutput:\n        self.assert_(type(out) is unicode)\n\n    inst = formatter(encoding='utf-8')\n    out = format(tokens, inst)\n    self.assert_(type(out) is str)\n    # Cannot test for encoding, since formatters may have to escape\n    # non-ASCII characters.", "path": "pinax\\libs\\external_libs\\Pygments-0.11.1\\tests\\test_basic_api.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"Create a new YadisService Manager for this starting URL and\nsuffix, and store it in the session.\n\n@raises KeyError: When I already have a manager.\n\n@return: A new YadisServiceManager or None\n\"\"\"\n", "func_signal": "def createManager(self, services, yadis_url=None):\n", "code": "key = self.getSessionKey()\nif self.getManager():\n    raise KeyError('There is already a %r manager for %r' %\n                   (key, self.url))\n\nif not services:\n    return None\n\nmanager = YadisServiceManager(self.url, yadis_url, services, key)\nmanager.store(self.session)\nreturn manager", "path": "pinax\\libs\\external_libs\\python-yadis-1.1.0\\yadis\\manager.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"\nThin wrapper around free_comment which adds login_required status and also assigns\nthe ``model`` to be ``ThreadedComment``.\n\"\"\"\n", "func_signal": "def comment(*args, **kwargs):\n", "code": "kwargs['model'] = ThreadedComment\nkwargs['form_class'] = ThreadedCommentForm\nreturn free_comment(*args, **kwargs)", "path": "pinax\\apps\\external_apps\\threadedcomments\\views.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"\nDefault callback function to determine wether the given user has the\nability to delete the given comment.\n\"\"\"\n", "func_signal": "def can_delete_comment(comment, user):\n", "code": "if user.is_staff or user.is_superuser:\n    return True\nif hasattr(comment, 'user') and comment.user == user:\n    return True\nreturn False", "path": "pinax\\apps\\external_apps\\threadedcomments\\views.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"There is a wildcard\n\"\"\"\n", "func_signal": "def test_wildcard(self):\n", "code": "self.failUnlessDiscoURL('http://*.example.com/foo',\n                        'http://www.example.com/foo')", "path": "pinax\\libs\\external_libs\\python-openid-2.1.1\\openid\\test\\test_rpverify.py", "repo_name": "ericholscher/allfeeds", "stars": 4, "license": "None", "language": "python", "size": 6096}
{"docstring": "\"\"\"Given a list of individuals, get its genotype\n\nindividuals can be:\n- a single string (corresponding to an individual.id) or Individual object;\n- a list of strings or of Individual objects.\n\nformat can be:\n- c -> character \n- n -> numerical (0, 1, 2)\n\n>>> from connection import *\n>>> metadata.bind = 'mysql://guest:@localhost:3306/hgdp_test'\n>>> setup_all()\n\n>>> snp = SNP.get_by(id = 'rs13125929')\n>>> snp.get_genotype_by_individuals('HGDP00001')\n'0'\n\n>>> snp.get_genotype_by_individuals(individuals = ('HGDP00001', 'HGDP01419'), format = 'n')\n('0', '0')\n\n>>> snp.get_genotype_by_individuals(individuals = ('HGDP00001', 'HGDP01419'), format = 'c')\n('TT', 'TT')\n\n>>> session.close()\n\"\"\"\n\n", "func_signal": "def get_genotype_by_individuals(self, individuals, format='n'):\n", "code": "if isinstance(individuals, str) or isinstance(individuals, Individual):\n    # TODO: may refactored by using recursion\n    individuals = (individuals, )\nelif not (isinstance(individuals, list) or isinstance(individuals, tuple)):\n    raise TypeError(\"individuals should be a list of string or Individual objects, or a single individual/string\")\n\nind_indexes = Individual.query.filter(Individual.name.in_(individuals)).from_self(Individual.genotypes_index).all()\nind_indexes = [int(i[0]) for i in ind_indexes]\n\nif not ind_indexes:\n    genotypes = ()\nelse:\n    if format == 'c':\n        genotypes = tuple(map(self.get_genotype_char, ind_indexes))\n    else:\n        genotype_getter = operator.itemgetter(*ind_indexes)\n        genotypes = genotype_getter(self.genotypes) # TODO: convert to a list for backward compatibility?\n     \nreturn genotypes", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"Get transcripts in an interval of [upstream, downstream] from the snp position\n\n>>> from debug_database import *\n>>> metadata.bind = 'sqlite:///:memory:'\n>>> setup_all(); create_all()\n>>> print metadata\nMetaData(Engine(sqlite:///:memory:))\n>>> rs1333 = SNP('rs1333')\n>>> rs1333.chromosome = '11'\n>>> rs1333.physical_position = 900\n\n\n### INCLUSION CRITERIAS ###\nLet's say we want to get all transcripts 100 upstream or downstream the position,\n    so from 800 to 1000 on chromosome 11.\n\n>>> transcript1 = RefSeqTranscript('transcript1', 11, 700, 800)   # not included \n>>> transcript2 = RefSeqTranscript('transcript2', 11, 700, 810)   # not included, it ends within the interval but it starts before\n>>> transcript3 = RefSeqTranscript('transcript3', 11, 800, 900)   # included\n>>> transcript4 = RefSeqTranscript('transcript4', 11, 900, 1000)  # included\n>>> transcript5 = RefSeqTranscript('transcript5', 11, 1000, 1100) # not included \n\n>>> transcriptreverse = RefSeqTranscript('transcriptreverse', 11, 900, 800)   # included\n\n>>> transcripts = rs1333.get_transcripts(100, 100)\n>>> print [tr.transcript_id for tr in transcripts]\n['TRANSCRIPT3', 'TRANSCRIPT4', 'TRANSCRIPTREVERSE']\n\n>>> session.close()\n\"\"\"\n", "func_signal": "def get_transcripts(self, upstream = 0, downstream = 0):\n", "code": "if not isinstance(upstream, int) and not isinstance(downstream, int):\n    raise TypeError(\"SNP.get_transcripts requires two integers as input\")\n\nif (self.chromosome is None) or (self.physical_position is None):\n    raise ValueError('unknown coordinates for current snp')\n\n# get the proper interval where to find transcripts\nlower_limit = self.physical_position - upstream\nupper_limit = self.physical_position + downstream\n\ntranscripts = RefSeqTranscript.query().filter_by(chromosome = self.chromosome).\\\n                                filter(RefSeqTranscript.txStart >= lower_limit).\\\n                                filter(RefSeqTranscript.txEnd <= upper_limit).all()\nreturn transcripts", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nget all individuals belonging to a population working unit\n\n>>> from connection import *\n>>> metadata.bind = 'mysql://guest:@localhost:3306/hgdp_test'\n>>> setup_all()\n>>> Individual.get_by_working_unit('colombian')\n[Mrs. HGDP00704 (colombian), Mrs. HGDP00706 (colombian), Mrs. HGDP00702 (colombian), Mr. HGDP00710 (colombian), Mrs. HGDP00970 (colombian), Mr. HGDP00703 (colombian), Mrs. HGDP00708 (colombian)]\n\n>>> session.close()\n\"\"\"\n", "func_signal": "def get_by_working_unit(cls, popname):\n", "code": "inds = Individual.query.filter(Individual.population.has(working_unit = str(popname.lower()))).all()\nreturn inds", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\" test the current module\"\"\"\n#    from debug_database import *\n", "func_signal": "def _test():\n", "code": "import doctest\ndoctest.testmod()", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"get the next SNP on the chromosome\n\"\"\"\n#        return SNP.get_by(id = self.next_snp)\n", "func_signal": "def get_next_snp(self):\n", "code": "if self.physical_position is None:\n    raise ValueError('position of the snp in the chromosome has not been uploaded yet')\nreturn SNP.query().filter_by(chromosome = self.chromosome).\\\n        filter(SNP.physical_position > self.physical_position).order_by(SNP.physical_position).first()", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nFor every gene in list_genes, get the longest transcript\nOnly the longest transcript is returned\n\n>>> from connection import *\n>>> metadata.bind = 'mysql://guest:@localhost:3306/hgdp_test'\n>>> setup_all()\n                         \n>>> transcripts = RefSeqTranscript.get_by_geneids(['ALG2', 'ALG11'])\n>>> print [transcript.transcript_id for transcript in transcripts]\n['NM_001004127', 'NR_024532']\n\n>>> RefSeqTranscript.get_by_geneids('ALG2')\n[transcript NR_024532 on gene ALG2 on chromosome 9 (101018527-101024067)]\n\"\"\"\n", "func_signal": "def get_by_geneids(cls, genes_list):\n", "code": "if isinstance(genes_list, str):\n    genes_list = (genes_list, )     # this must be a list or a tuple", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nupload individuals from the rosenberg files.\n\"\"\"\n", "func_signal": "def upload_iHS_stats(iHS_filehandle, session, metadata):\n", "code": "session.query(iHS).delete()\nparsers.iHS_parser_new(iHS_filehandle, session, metadata)", "path": "src\\setupdb\\insert_iHS_all.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\ndetermine the position with respect to a transcript (in the coding region, #n upstream/downstream, etc)\n\n>>> from debug_database import *\n>>> metadata.bind = 'sqlite:///:memory:'\n>>> setup_all(); create_all()\n>>> print metadata\nMetaData(Engine(sqlite:///:memory:))\n>>> rs1333 = SNP('rs1333')\n>>> rs1333.chromosome = '11'\n>>> rs1333.physical_position = 900\n\n\n>>> transcript1 = RefSeqTranscript('transcript1', 11, 700, 800) # downstream\n>>> transcript2 = RefSeqTranscript('transcript2', 11, 800, 910) # within\n>>> transcript3 = RefSeqTranscript('transcript3', 11, 890, 930) # within\n>>> transcript4 = RefSeqTranscript('transcript4', 11, 910, 1000) # upstream\n\n>>> rs1333.get_position_from_transcript(transcript1)\n'downstream'\n>>> rs1333.get_position_from_transcript('transcript1')\n'downstream'\n>>> rs1333.get_position_from_transcript(transcript2)\n'inside_gene'\n>>> rs1333.get_position_from_transcript(transcript3)\n'inside_gene'\n>>> rs1333.get_position_from_transcript(transcript4)\n'upstream'\n\n\"\"\"\n", "func_signal": "def get_position_from_transcript(self, transcript_id):\n", "code": "outputs = ['upstream', 'downstream', 'inside_gene', 'coding', 'intronic', 'other_chromosome']\nposition = ''\n\nif isinstance(transcript_id, RefSeqTranscript):\n    transcript = transcript_id\nelif isinstance(transcript_id, str):   # todo: check for __str__ method?\n    transcript = RefSeqTranscript.get_by(alternateName = transcript_id.upper())\nelse:\n    transcript = None\nif transcript is None:\n    raise ValueError('transcript %s not present in database' % transcript_id)\n\nsnp_position = self.physical_position\nif transcript.chromosome != self.chromosome:\n    position = 'other_chromosome'\n\nelif snp_position < transcript.txStart:\n    position = 'upstream'\nelif transcript.txStart < snp_position < transcript.txEnd:\n    position = 'inside_gene'\nelif snp_position > transcript.txEnd:\n    position = 'downstream'\n\nreturn position", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nget all individuals belonging to a population working unit\n\n>>> from connection import *\n>>> metadata.bind = 'mysql://guest:@localhost:3306/hgdp_test'\n>>> setup_all()\n>>> Individual.get_by_continent_code('EUR')[0:5]\n[Mrs. HGDP01401 (adygei), Mrs. HGDP01388 (adygei), Mr. HGDP01383 (adygei), Mr. HGDP01403 (adygei), Mrs. HGDP01387 (adygei)]\n\n>>> session.close()\n\"\"\"\n", "func_signal": "def get_by_continent_code(cls, continent_code):\n", "code": "inds = Individual.query.filter(Individual.population.has(continent_code = str(continent_code.upper()))).all()\nreturn inds", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"get the url to dbSNP\"\"\"\n", "func_signal": "def get_dbSNP_url(self):\n", "code": "type = self.id[0:2]     # should always be rs\nid = self.id[2:]\nurl = \"http://www.ncbi.nlm.nih.gov/SNP/snp_ref.cgi?type=%s&rs=%s\" % (type, id)\nreturn url", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"Length of the trascript, from txStart to txEnd\n\nif feature == 't', return the transcript length (default)\nif feature == 'c', return the coding length\n\"\"\"\n", "func_signal": "def length(self, feature = 't'):\n", "code": "if feature.lower() in ('t', 'transcript', 'tr'):\n    return self.txEnd - self.txStart\nelif feature.lower() in ('c', 'coding', 'co'):\n    return self.cdsEnd - self.cdsStart\nelse:\n    raise TypeError('unknown value for parameter \"feature\"')", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"Given a list of snps, the their genotypes\n\"\"\"\n", "func_signal": "def get_genotypes(self, list_of_snps, format = 'n'):\n", "code": "genotypes = []\nif hasattr([], '__iter__') and not isinstance(list_of_snps, str):\n    logging.debug(\"list_of_snps is a list\")\n\n    for snp in list_of_snps:\n        if format == 'n':\n            genotypes.append(snp.genotypes[self.genotypes_index])\n        else:\n            genotype = snp.get_genotype_char(self.genotypes_index)\n            genotypes.append(genotype)\nreturn genotypes", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nGet all the snps in an interval of (transcript.txstart - downstream, transcript.txEnd + upstream)\n\n>>> from debug_database import *\n>>> metadata.bind = 'sqlite:///:memory:'\n>>> setup_all(); create_all()\n>>> print metadata\nMetaData(Engine(sqlite:///:memory:))\n\n\nExample: get all the snps with upstream=300, downstream=300\n\n>>> transcript1 = RefSeqTranscript('transcript1', 11, 900, 1100) # txCenter: 1000\n\n>>> snp1 = SNP('snp1', chromosome = 11, physical_position = 500L)    # not included\n>>> snp2 = SNP('snp2', chromosome = 11, physical_position = 700L)    # not included\n>>> snp3 = SNP('snp3', chromosome = 11, physical_position = 800L)    # included\n>>> snp4 = SNP('snp4', chromosome = 11, physical_position = 1000L)   # included, inside tr\n>>> snp5 = SNP('snp5', chromosome = 11, physical_position = 1200L)   # included\n>>> snp6 = SNP('snp6', chromosome = 11, physical_position = 1300L)   # not included\n>>> snp7 = SNP('snp7', chromosome = 11, physical_position = 1600L)   # not included\n>>> snp8 = SNP('snp8', chromosome = 1, physical_position = 1000L)    # not included (other chromosome)\n>>> snp4b = SNP('snp4b', chromosome = 11, physical_position = 1100L) # included and after snp4\n\n>>> transcript1.get_snps(300, 300, 'tr').all()\n[SNP snp2, SNP snp3, SNP snp4, SNP snp4b, SNP snp5, SNP snp6]\n\nGet all snps within the transcript (use 'tr' parameter)\n>>> transcript1.get_snps(0, 0, 'tr').all()\n[SNP snp4]\n\n>>> session.close()\n\"\"\"\n", "func_signal": "def get_snps(self, upstream = 0, downstream = 0, relative_to = 'tr'):\n", "code": "if not isinstance(upstream, int) and not isinstance(downstream, int):\n    raise TypeError(\"SNP.get_transcripts requires two integers as input\")\n\nif (self.chromosome is None):   # should check also for cdsStart, etc..\n    raise ValueError('unknown coordinates for current snp')\n\nif relative_to in ('tr', 'tx', 'transcript'):\n\n    lower_limit = self.txStart - upstream\n    upper_limit = self.txEnd + downstream", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"Given a snp id, get the genotype\n\nrequire 'snps' as input.\nsnps can be:\n- a snp id string\n- a list of snp id strings\n- a SNP instance\n- a list of SNP instances\n\n>>> from connection import *\n>>> metadata.bind = 'mysql://guest:@localhost:3306/hgdp_test'\n>>> setup_all()\n\n>>> snp1, snp2 = SNP.query().limit(2).all()\n>>> ind1 = Individual.query().first()\n>>> ind1.get_genotype('rs10009279')\n'1'\n\n>>> ind1.get_genotypes((snp1, snp2))\n['0', '0']\n\n>>> session.close()\n\"\"\"\n", "func_signal": "def get_genotype(self, snp):\n", "code": "genotype = ''\nif isinstance(snp, str):\n    snp_h = SNP.get_by(id = snp)\n    genotype = snp_h.genotypes[self.genotypes_index]\nif isinstance(snp, SNP):\n    genotype = snp.genotypes[self.genotypes_index]\nreturn genotype", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nGet a statistical value related to the snp\n\nFor the moment, it can only retrieve a stat at a time\n\n>>> from connection import *    # be careful - don't write anything to the db!\n>>> metadata.bind = 'mysql://guest:@localhost:3306/hgdp_test'\n>>> setup_all()\n>>> snp = SNP.get_by(id = 'rs2887286')\n\n>>> snp.get_stats_by_snps(snp, 'XPEHH', ['ame', 'csasia'])\n\"\"\"\n# Warning: this function makes use of eval\n# This function doesn't even try to validate user input\n\n", "func_signal": "def get_stats_by_snps(cls, snplist, stat, pops=None):\n", "code": "if snplist is None:\n    raise TypeError('select at least a snp')\nif isinstance(snplist, list) or isinstance(snplist, tuple):\n    snplist_cleaned = []\n    for snp in snplist:\n        if isinstance(snp, SNP):\n            snplist_cleaned.append(snp.id)\n        else:\n            snplist_cleaned.append(snp)\n    snplist = snplist_cleaned\nelse:   # assume snplist is a string or snp instance containing a single value\n    if isinstance(snplist, SNP):\n        snplist = (snplist.id, )\n    else:\n        snplist = (snplist.lower(), )\n\nif pops is None:\n    # return all the populations\n    pops = 'ame csasia easia eur mena oce ssafr'.split()\nif isinstance(pops, list) or isinstance(pops, tuple):\n    pops = [p.lower() for p in pops]\nelse:\n    pops = (pops.lower(), )\n\nqueryline = stat + \".query().filter(\" + stat + \".snp_id.in_(\" + str(snplist) + \")).from_self(\"\nfor pop in pops:\n    queryline += stat + '.' + pop + ', '\n\nqueryline += ')'", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nGet all snps in a window of (upstream, downstream) bases\n\n>>> from connection import *\n>>> metadata.bind = 'mysql://guest:@localhost:3306/hgdp_test'\n>>> setup_all()\n\n>>> snp = SNP.get_by(id = 'rs13125929')\n>>> print snp.get_nearby_snps(100000, 105000)[:7]\n[SNP rs4690284, SNP rs13114862, SNP rs11724335, SNP rs12509346, SNP rs10002444, SNP rs9884834, SNP rs2213704]\n\"\"\"\n", "func_signal": "def get_nearby_snps(self, upstream, downstream):\n", "code": "if not isinstance(upstream, int) and not isinstance(downstream, int):\n    raise TypeError(\"SNP.get_nearby_snps requires two integers as input\")\n\nlower_limit = self.physical_position - upstream\nupper_limit = self.physical_position + downstream\n\nsnps = SNP.get_snps_by_region(self.chromosome, lower_limit, upper_limit).filter(SNP.id != self.id)", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\" list all the continent codes in the table\"\"\"\n", "func_signal": "def all_continents(self):\n", "code": "units = Population.query().from_self(Population.continent_code).group_by(Population.continent_code).all()\nreturn map(lambda x:x[0], units)", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"get the previous SNP on the chromosome\n\"\"\"\n", "func_signal": "def get_previous_snp(self):\n", "code": "if self.physical_position is None:\n    raise ValueError('position of the snp in the chromosome has not been uploaded yet')\nreturn SNP.query().filter_by(chromosome = self.chromosome).\\\n        filter(SNP.physical_position > self.physical_position).order_by(SNP.physical_position).first()", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nGet the snps within a region\n>>> from connection import *    # be careful - don't write anything to the db!\n>>> metadata.bind = 'mysql://guest:@localhost:3306/hgdp_test'\n>>> setup_all()\n>>> print SNP.get_snps_by_region('1', 1000000, 1050000).all()\n[SNP rs9442372, SNP rs3737728, SNP rs11260588, SNP rs9442398, SNP rs6687776, SNP rs9651273, SNP rs4970405, SNP rs12726255]\n\nnote: if upper_limit == -1, get all the snps until the end of the chr.\n>>> SNP.get_snps_by_region('Y', -1, 13000000).all()\n[SNP rs2058276, SNP rs1865680]\n\n>>> session.close()\n\"\"\"\n# note: doesn't work if self.physical_position is none.\n", "func_signal": "def get_snps_by_region(cls, chromosome, lower_limit = 0, upper_limit = -1):\n", "code": "if upper_limit == -1:\n    snps = SNP.query.filter(SNP.chromosome == str(chromosome).upper()).\\\n        filter(SNP.physical_position > lower_limit).\\\n        order_by(SNP.physical_position)#.all()\nelse:\n    snps = SNP.query.filter(SNP.chromosome == str(chromosome).upper()).\\\n        filter(SNP.physical_position > lower_limit).\\\n        filter(SNP.physical_position < upper_limit).\\\n        order_by(SNP.physical_position)#.all()", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nget all individuals belonging to a population working unit\n\n>>> from connection import *\n>>> metadata.bind = 'mysql://guest:@localhost:3306/hgdp_test'\n>>> setup_all()\n>>> Individual.get_by_continent('Europe')[0:5]\n[Mrs. HGDP01401 (adygei), Mrs. HGDP01388 (adygei), Mr. HGDP01383 (adygei), Mr. HGDP01403 (adygei), Mrs. HGDP01387 (adygei)]\n\n>>> session.close()\n\"\"\"\n", "func_signal": "def get_by_continent(cls, continent):\n", "code": "inds = Individual.query.filter(Individual.population.has(continent_macroarea = str(continent.lower()))).all()\nreturn inds", "path": "src\\schema\\schema.py", "repo_name": "dalloliogm/snps-database", "stars": 5, "license": "None", "language": "python", "size": 4619}
{"docstring": "\"\"\"\nReturns whether the given address is a valid bluetooth address.\nFor example, \"00:0e:6d:7b:a2:0a\" is a valid address.\n\nReturns False if the argument is None or is not a string.\n\"\"\"\n# Define validity regex. Accept either \":\" or \"-\" as separators.    \n", "func_signal": "def _isbtaddr(address):\n", "code": "global _validbtaddr\nif _validbtaddr is None:\n    import re    \n    _validbtaddr = re.compile(\"((\\d|[a-f]){2}(:|-)){5}(\\d|[a-f]){2}\", \n            re.IGNORECASE)    \nimport types    \nif not isinstance(address, types.StringTypes):\n    return False\nreturn _validbtaddr.match(address) is not None", "path": "lib\\lightblue-0.3.2\\src\\linux\\_lightbluecommon.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "\"\"\"\nWaits until stopwaiting() returns True, or until the wait times out\n(according to the self.__timeout value).\n\nThis is to make a function wait until a buffer has been filled. i.e.\nstopwaiting() should return True when the buffer is no longer empty.\n\"\"\"\n", "func_signal": "def __waituntil(self, stopwaiting, timeoutmsg):\n", "code": "if not stopwaiting():\n    if self.__timeout == 0:\n        # in non-blocking mode (immediate timeout)\n        # push event loop to really be sure there is no data available\n        _macutil.looponce()\n        if not stopwaiting():\n            # trying to perform operation now would block\n            raise _socket.error(errno.EAGAIN, os.strerror(errno.EAGAIN))\n    else:\n        # block and wait until we get data, or time out\n        if not _macutil.waituntil(stopwaiting, self.__timeout):\n            raise _socket.timeout(timeoutmsg)", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# isOpen() check doesn't work for incoming (server-spawned) channels            \n", "func_signal": "def _isclosed(self):\n", "code": "if (self.__conn.proto == _lightbluecommon.RFCOMM and \n        self.__conn.channel is not None and\n        not self.__conn.channel.isIncoming()):\n    return not self.__conn.channel.isOpen()    \nreturn self.__closed", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# Number data\n", "func_signal": "def test_TINYINT(self):\n", "code": "def generator(row,col):\n    v = (row*row) % 256\n    if v > 127:\n        v = v-256\n    return v\nself.check_data_integrity(\n    ('col1 TINYINT',),\n    generator)", "path": "lib\\MySQL-python-1.2.2\\test_MySQLdb_capabilities.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "\"\"\"\nSplits the given class of device to return a 3-item tuple with the \nmajor service class, major device class and minor device class values.\n\nThese values indicate the device's major services and the type of the \ndevice (e.g. mobile phone, laptop, etc.). If you google for \n\"assigned numbers bluetooth baseband\" you might find some documents\nthat discuss how to extract this information from the class of device.\n\nExample:\n    >>> splitclass(1057036)\n    (129, 1, 3)\n    >>>     \n\"\"\"\n", "func_signal": "def splitclass(classofdevice):\n", "code": "if not isinstance(classofdevice, int):\n    try:   \n        classofdevice = int(classofdevice)\n    except (TypeError, ValueError):\n        raise TypeError(\"Given device class '%s' cannot be split\" % \\\n            str(classofdevice))\n\ndata = classofdevice >> 2   # skip over the 2 \"format\" bits\nservice = data >> 11\nmajor = (data >> 6) & 0x1F\nminor = data & 0x3F \nreturn (service, major, minor)", "path": "lib\\lightblue-0.3.2\\src\\linux\\_lightbluecommon.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "\"\"\"\nThe opposite of splitclass(). Joins a (service, major, minor) class-of-\ndevice tuple into a whole class of device value.\n\"\"\"\n", "func_signal": "def _joinclass(codtuple):\n", "code": "if not isinstance(codtuple, tuple):\n    raise TypeError(\"argument must be tuple, was %s\" % type(codtuple))\nif len(codtuple) != 3:\n    raise ValueError(\"tuple must have 3 items, has %d\" % len(codtuple))\n\nserviceclass = codtuple[0] << 2 << 11\nmajorclass = codtuple[1] << 2 << 6\nminorclass = codtuple[2] << 2\nreturn (serviceclass | majorclass | minorclass)", "path": "lib\\lightblue-0.3.2\\src\\linux\\_lightbluecommon.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# Character data\n", "func_signal": "def test_small_CHAR(self):\n", "code": "def generator(row,col):\n    i = (row*col+62)%256\n    if i == 62: return ''\n    if i == 63: return None\n    return chr(i)\nself.check_data_integrity(\n    ('col1 char(1)','col2 char(1)'),\n    generator)", "path": "lib\\MySQL-python-1.2.2\\test_MySQLdb_capabilities.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "\"\"\" \nHandle when a client connects to the server channel.\n(This method is called for both RFCOMM and L2CAP channels.)\n\"\"\"\n", "func_signal": "def newChannelOpened_channel_(self, notif, newChannel):\n", "code": "if newChannel is not None and newChannel.isIncoming():\n    # not sure if delegate really needs to be set\n    newChannel.setDelegate_(self) \n\n    if hasattr(self.__cb_obj, '_handle_channelopened'):          \n        self.__cb_obj._handle_channelopened(newChannel)", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# see what options on Linux+s60    \n# possibly have socket security option.        \n", "func_signal": "def setsockopt(self, level, optname, value):\n", "code": "raise _socket.error(\n    errno.ENOPROTOOPT, os.strerror(errno.ENOPROTOOPT))", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# oddly enough, sometimes the channelClosed: selector doesn't get called\n# (maybe if there's a lot of data being passed?) but this seems to work\n", "func_signal": "def registerclosenotif(self, channel):\n", "code": "notif = channel.registerForChannelCloseNotification_selector_(self, \n        \"channelClosedEvent:channel:\")\nif notif is not None:\n    self.__closenotif = notif", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# no type check, and assumes data is not empty!\n#append data to list, no need to \"\".join just yet.\n", "func_signal": "def write(self, data):\n", "code": "self.lock.acquire()\ntry:\n    self.l_buffer.append(data)\n    self.bufempty = False\nfinally:\n    self.lock.release()", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# beware that this value won't actually be set until the event loop\n# has been driven so that this method is actually called\n", "func_signal": "def _handle_channelclosed(self, channel):\n", "code": "self.__closed = True\n_macutil.interruptwait()", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "#build a new string out of list\n", "func_signal": "def _build_str(self):\n", "code": "new_string = \"\".join(self.l_buffer)\n#join string buffer and new string\nself.s_buffer = \"\".join((self.s_buffer, new_string))\n#clear list\nself.l_buffer = []", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "\"\"\"\nArguments:\n- cb_obj: An object that receives callbacks when events occur. This\n  object should have:\n    - a method '_handle_channeldata' which takes the related channel (a \n      IOBluetoothRFCOMMChannel or IOBluetoothL2CAPChannel) and the new\n      data (a string) as the arguments.\n    - a method '_handle_channelclosed' which takes the related channel\n      as the argument.\n\nIf this listener's delegate is passed to the openRFCOMMChannel... or\nopenL2CAPChannel... selectors as the delegate, the delegate (and\ntherefore this listener) will automatically start receiving events. \n\nOtherwise, call setDelegate_() on the channel with this listener's\ndelegate as the argument to allow this listener to start receiving\nchannel events. (This is the only option for server-spawned sockets.)\n\"\"\"\n", "func_signal": "def initWithDelegate_(self, cb_obj):\n", "code": "self = super(_ChannelEventListener, self).init()    \nif cb_obj is None:\n    raise TypeError(\"callback object is None\")\nself.__cb_obj = cb_obj\nself.__closenotif = None\nself.__channelDelegate = \\\n        BBBluetoothChannelDelegate.alloc().initWithDelegate_(self)\nreturn self", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# put new channels into a queue, which 'accept' can then pull out\n", "func_signal": "def _handle_channelopened(self, channel):\n", "code": "self.__queuedchannels_lock.acquire()\ntry:\n    # need to implement max connections\n    #if len(self.__queuedchannels) < self.__maxqueuedconns:\n    self.__queuedchannels.append(channel)\n    _macutil.interruptwait()\nfinally:\n    self.__queuedchannels_lock.release()", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# open RFCOMM channel (should timeout actually apply to opening of\n# channel as well? if so need to do timeout with async callbacks)\n", "func_signal": "def connect(self, device, port, listener):\n", "code": "try:\n    # pyobjc 2.0\n    result, self.channel = device.openRFCOMMChannelSync_withChannelID_delegate_(None, port, listener.delegate())\nexcept TypeError:\n    result, self.channel = device.openRFCOMMChannelSync_withChannelID_delegate_(port, listener.delegate())        \nif result == _macutil.kIOReturnSuccess:\n    self.channel.setDelegate_(listener.delegate())\n    listener.registerclosenotif(self.channel)\nelse:\n    self.channel = None\nreturn result", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "\"\"\"Update our built-in md5 registry\"\"\"\n\n", "func_signal": "def update_md5(filenames):\n", "code": "import re\nfrom md5 import md5\n\nfor name in filenames:\n    base = os.path.basename(name)\n    f = open(name,'rb')\n    md5_data[base] = md5(f.read()).hexdigest()\n    f.close()\n\ndata = [\"    %r: %r,\\n\" % it for it in md5_data.items()]\ndata.sort()\nrepl = \"\".join(data)\n\nimport inspect\nsrcfile = inspect.getsourcefile(sys.modules[__name__])\nf = open(srcfile, 'rb'); src = f.read(); f.close()\n\nmatch = re.search(\"\\nmd5_data = {\\n([^}]+)}\", src)\nif not match:\n    print >>sys.stderr, \"Internal error!\"\n    sys.exit(2)\n\nsrc = src[:match.start(1)] + repl + src[match.end(1):]\nf = open(srcfile,'w')\nf.write(src)\nf.close()", "path": "lib\\MySQL-python-1.2.2\\ez_setup.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# will want checkbtaddr=False if the address might be empty string \n# (for binding to a server address)\n\n", "func_signal": "def _checkaddrpair(address, checkbtaddr=True):\n", "code": "if not isinstance(address, tuple):\n    raise TypeError(\"address must be (address, port) tuple, was %s\" % \\\n        type(address))\n\nif len(address) != 2:\n    raise TypeError(\"address tuple must have 2 items (has %d)\" % \\\n        len(address))\n        \nif not isinstance(address[0], types.StringTypes):\n    raise TypeError(\"address host value must be string, was %s\" % \\\n        type(address[0]))\n    \nif checkbtaddr:\n    if not _lightbluecommon._isbtaddr(address[0]):\n        raise TypeError(\"address '%s' is not a bluetooth address\" % \\\n            address[0])\n\nif not isinstance(address[1], int):\n    raise TypeError(\"address port value must be int, was %s\" % \\\n        type(address[1]))", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# see what options on Linux+s60\n# possibly have socket security option.\n", "func_signal": "def getsockopt(self, level, optname, buflen=0):\n", "code": "raise _socket.error(\n    errno.ENOPROTOOPT, os.strerror(errno.ENOPROTOOPT))", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# Just advertise a service and see what channel it was assigned, then\n# stop advertising the service and return the channel.\n# It's a hacky way of doing it, but IOBluetooth doesn't seem to provide\n# functionality for just getting an available channel.\n\n", "func_signal": "def _getavailableport(proto):\n", "code": "if proto == _lightbluecommon.RFCOMM:\n    try:\n        result, channelID, servicerecordhandle = BBServiceAdvertiser.addRFCOMMServiceDictionary_withName_UUID_channelID_serviceRecordHandle_(BBServiceAdvertiser.serialPortProfileDictionary(), \"DummyService\", None, None, None)\n    except:\n        result, channelID, servicerecordhandle = BBServiceAdvertiser.addRFCOMMServiceDictionary_withName_UUID_channelID_serviceRecordHandle_(BBServiceAdvertiser.serialPortProfileDictionary(), \"DummyService\", None)\n    if result != _macutil.kIOReturnSuccess:\n        raise _lightbluecommon.BluetoothError(result, \\\n            \"Could not retrieve an available service channel\")\n    result = BBServiceAdvertiser.removeService_(servicerecordhandle)\n    if result != _macutil.kIOReturnSuccess:\n        raise _lightbluecommon.BluetoothError(result, \\\n            \"Could not retrieve an available service channel\")\n    return channelID\n\nelse:\n    raise NotImplementedError(\"L2CAP server sockets not currently supported\")", "path": "lib\\lightblue-0.3.2\\src\\mac\\_bluetoothsockets.py", "repo_name": "lhl/blueball", "stars": 4, "license": "None", "language": "python", "size": 3348}
{"docstring": "# Punctuation and spaces are trimmed, multiples reduced to 1\n", "func_signal": "def testNormalizeStringStrip(self):\n", "code": "self.assertEqual(self.utils.normalizeString(\" a string    \"),\n                 'a-string')\nself.assertEqual(self.utils.normalizeString(\">here's another!\"),\n                 'here-s-another')\nself.assertEqual(self.utils.normalizeString(\"one with !@#$!@#$ stuff in the middle\"),\n                 'one-with-stuff-in-the-middle')", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# See if we can create one at all\n", "func_signal": "def testCreateNavTree(self):\n", "code": "tree = self.utils.createNavTree(self.portal)\nself.failUnless(tree, tree)\nself.failUnless(tree.has_key('children'))", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# European accented chars will be transliterated to rough ASCII equivalents\n", "func_signal": "def testNormalizeStringAccents(self):\n", "code": "input = u\"Eksempel \\xe6\\xf8\\xe5 norsk \\xc6\\xd8\\xc5\"\nself.assertEqual(self.utils.normalizeString(input),\n                 'eksempel-eoa-norsk-eoa')", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Test Navtree workflow state filtering\n", "func_signal": "def testStateFiltering(self):\n", "code": "workflow = self.portal.portal_workflow\nntp=self.portal.portal_properties.navtree_properties\nntp.manage_changeProperties(wf_states_to_show=['published'])\nntp.manage_changeProperties(enable_wf_state_filtering=True)\ntree = self.utils.createNavTree(self.portal.folder2)\nself.failUnless(tree, tree)\nself.failUnless(tree.has_key('children'))\n#Should only contain current object\nself.assertEqual(len(tree['children']), 1)\n#change workflow for folder1\nworkflow.doActionFor(self.portal.folder1, 'publish')\nself.portal.folder1.reindexObject()\ntree = self.utils.createNavTree(self.portal.folder2)\n#Should only contain current object and published folder\nself.assertEqual(len(tree['children']), 2)", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# This was raising an IndexError exception for\n# Zope < 2.7.3 (DateTime.py < 1.85.12.11) and a\n# SyntaxError for Zope >= 2.7.3 (DateTime.py >= 1.85.12.11)\n", "func_signal": "def testValidPortalTypeNameButNotAutoGeneratedId(self):\n", "code": "r = self.utils.isIDAutoGenerated('document.tar.gz')\nself.assertEqual(r, False)\nr = self.utils.isIDAutoGenerated('document.tar.12/32/2004')\nself.assertEqual(r, False)\nr = self.utils.isIDAutoGenerated('document.tar.12/31/2004 12:62')\nself.assertEqual(r, False)", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Punctuation and spacing is removed and replaced by '-'\n", "func_signal": "def testNormalizeStringPunctuation(self):\n", "code": "self.assertEqual(self.utils.normalizeString(\"a string with spaces\"),\n                 'a-string-with-spaces')\nself.assertEqual(self.utils.normalizeString(\"p.u,n;c(t)u!a@t#i$o%n\"),\n                 'p-u-n-c-t-u-a-t-i-o-n')", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# With the context set to folder2 it should return a dict with\n# currentItem set to True\n", "func_signal": "def testCreateNavTreeCurrentItem(self):\n", "code": "tree = self.utils.createNavTree(self.portal.folder2)\nself.failUnless(tree, tree)\nself.assertEqual(tree['children'][-1]['currentItem'], True)", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def testBBImport(self):\n", "code": "self.setRoles(['Manager'])\nitt = self.portal.portal_IMSTransportTool\nself.portal.invokeFactory('Folder', 'testbbfolder')\ntestbbfolder = getattr(self.portal, 'testbbfolder')\nresults = itt.importZipfile(testbbfolder, \n                            'Products/IMSTransport/tests/BBTest.zip', \n                            'Blackboard_import_xform',\n                            'LOMv1.0')\nassert results[0], results[2]\n\nassert getattr(testbbfolder, 'res00001.html')\ntestdocument = getattr(testbbfolder, 'res00001.html')\nassert 'COURSE_DEFAULT.CourseInformation.CONTENT_LINK.label' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert '<a href=\"res00007.html\">Test Image</a>' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00005.html')\ntestdocument = getattr(testbbfolder, 'res00005.html')\nassert 'Test Document' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert 'Hello World<p></p>\\n' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00006.html')\ntestdocument = getattr(testbbfolder, 'res00006.html')\nassert 'Test File' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert 'samplefile.txt' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00006')\ntestfolder = getattr(testbbfolder, 'res00006')\nassert 'Folder' == testfolder.Type()\nassert getattr(testfolder, 'samplefile.txt')\ntestfile = getattr(testfolder, 'samplefile.txt')\nassert 'Test File' == testfile.Title()\nassert 'test_user_1_' in testfile.Creators()\nassert 'text/plain' == testfile.Format()\nassert 'File' == testfile.Type()\nassert testfile.size() > 0\n\nassert getattr(testbbfolder, 'res00007.html')\ntestdocument = getattr(testbbfolder, 'res00007.html')\nassert 'Test Image' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert 'sampleimage.gif' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00007')\ntestfolder = getattr(testbbfolder, 'res00007')\nassert 'Folder' == testfolder.Type()\nassert getattr(testfolder, 'sampleimage.gif')\ntestimage = getattr(testfolder, 'sampleimage.gif')\nassert 'Test Image' == testimage.Title()\nassert 'test_user_1_' in testimage.Creators()\nassert 'image/gif' == testimage.Format()\nassert 'Image' == testimage.Type()\nassert testimage.size() > 0\n\nassert getattr(testbbfolder, 'res00009.html')\ntestdocument = getattr(testbbfolder, 'res00009.html')\nassert 'Test Document 2' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert 'Another test<p></p>\\n' == testdocument.getText()\n\nassert getattr(testbbfolder, 'res00002.html')\ntestdocument = getattr(testbbfolder, 'res00002.html')\nassert 'COURSE_DEFAULT.CourseDocuments.CONTENT_LINK.label' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert '<a href=\"res00010.html\">Test Document</a>' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00010.html')\ntestdocument = getattr(testbbfolder, 'res00010.html')\nassert 'Test Document' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert '<a href=\"res00010//course.html\" title=\"course.html\">' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00010')\ntestfolder = getattr(testbbfolder, 'res00010')\nassert 'Folder' == testfolder.Type()\nassert getattr(testfolder, 'course.html')\ntestdocument = getattr(testfolder, 'course.html')\nassert 'Test Document' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert '<h3>COURSE TITLE</h3>' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00003.html')\ntestdocument = getattr(testbbfolder, 'res00003.html')\nassert 'COURSE_DEFAULT.Assignments.CONTENT_LINK.label' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert '<a href=\"res00011.html\">Test Document</a>' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00011.html')\ntestdocument = getattr(testbbfolder, 'res00011.html')\nassert 'Test Document' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert 'Hello World<p></p>\\n' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00002.html')\ntestdocument = getattr(testbbfolder, 'res00002.html')\nassert 'COURSE_DEFAULT.CourseDocuments.CONTENT_LINK.label' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert '<a href=\"res00010.html\">Test Document</a>' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00004.html')\ntestdocument = getattr(testbbfolder, 'res00004.html')\nassert 'COURSE_DEFAULT.ExternalLinks.CONTENT_LINK.label' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert '<a href=\"res00012.html\">Slashdot</a>' in testdocument.getText()\n\nassert getattr(testbbfolder, 'res00012.html')\ntestdocument = getattr(testbbfolder, 'res00012.html')\nassert 'Slashdot' == testdocument.Title()\nassert 'test_user_1_' in testdocument.Creators()\nassert 'text/html' == testdocument.Format()\nassert 'Page' == testdocument.Type()\nassert '<a href=\"http://slashdot.org\">Slashdot</a><p></p>\\n' in testdocument.getText()", "path": "IMSTransport\\tests\\testIMSImport.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "\"\"\"\n\"\"\"\n", "func_signal": "def testWebCTImport(self):\n", "code": "self.setRoles(['Manager'])\nitt = self.portal.portal_IMSTransportTool\nself.portal.invokeFactory('Folder', 'testwctfolder')\ntestwctfolder = getattr(self.portal, 'testwctfolder')\nresults = itt.importZipfile(testwctfolder, \n                            'Products/IMSTransport/tests/WebCTTest.zip', \n                            'WebCT_import_xform',\n                            'LOMv1.0')\nassert results[0], results[2]\n\nassert getattr(testwctfolder, 'URL_8805229_R.html')\ntestpage = getattr(testwctfolder, 'URL_8805229_R.html')\nassert 'assignments-external web' == testpage.Title()\nassert 'test_user_1_' in testpage.Creators()\nassert 'text/html' == testpage.Format()\nassert 'Page' == testpage.Type()\nassert '<a href=\"http://cnn.com\">assignments-external web</a>' in testpage.getText()\n\nassert getattr(testwctfolder, 'URL_8805232_R.html')\ntestpage = getattr(testwctfolder, 'URL_8805232_R.html')\nassert 'syllabus-external web' == testpage.Title()\nassert 'test_user_1_' in testpage.Creators()\nassert 'text/html' == testpage.Format()\nassert 'Page' == testpage.Type()\nassert '<a href=\"http://slashdot.org\">syllabus-external web</a>' in testpage.getText()\n\nassert getattr(testwctfolder, 'URL_8805235_R.html')\ntestpage = getattr(testwctfolder, 'URL_8805235_R.html')\nassert 'exams-external web' == testpage.Title()\nassert 'test_user_1_' in testpage.Creators()\nassert 'text/html' == testpage.Format()\nassert 'Page' == testpage.Type()\nassert '<a href=\"http://slashdot.org\">exams-external web</a>' in testpage.getText()\n\nassert getattr(testwctfolder, 'URL_8805241_R.html')\ntestpage = getattr(testwctfolder, 'URL_8805241_R.html')\nassert 'External Website' == testpage.Title()\nassert 'test_user_1_' in testpage.Creators()\nassert 'text/html' == testpage.Format()\nassert 'Page' == testpage.Type()\nassert '<a href=\"http://digg.com\">External Website</a>' in testpage.getText()\n\nassert getattr(testwctfolder, 'CMD_8805178_R.html')\ntestpage = getattr(testwctfolder, 'CMD_8805178_R.html')\nassert 'assignment worksheets' == testpage.Title()\nassert 'test_user_1_' in testpage.Creators()\nassert 'text/html' == testpage.Format()\nassert 'Page' == testpage.Type()\nassert '<td><a href=\"COURSE_8805174_M/my_files/untitled.html\">Put your title here</a></td>' in testpage.getText()\n\nassert getattr(testwctfolder, 'COURSE_8805174_M')\ntestfolder = getattr(testwctfolder, 'COURSE_8805174_M')\nassert getattr(testfolder, 'my_files')\nmy_files = getattr(testfolder, 'my_files')\nassert getattr(my_files, 'untitled.html')\nassert getattr(my_files, 'samplefile.txt')\nassert getattr(my_files, 'sampleimage.gif')\nassert getattr(my_files, 'untitled2.html')\n\nassert getattr(testwctfolder, 'CMD_8805179_R.html')\ntestpage = getattr(testwctfolder, 'CMD_8805179_R.html')\nassert 'readings' == testpage.Title()\nassert 'test_user_1_' in testpage.Creators()\nassert 'text/html' == testpage.Format()\nassert 'Page' == testpage.Type()\nassert '<td><a href=\"COURSE_8805174_M/my_files/untitled2.html\">Put your title here</a></td>' in testpage.getText()\n\nassert getattr(testwctfolder, 'URL_8805290_R.html')\ntestpage = getattr(testwctfolder, 'URL_8805290_R.html')\nassert 'Google' == testpage.Title()\nassert 'test_user_1_' in testpage.Creators()\nassert 'text/html' == testpage.Format()\nassert 'Page' == testpage.Type()\nassert '<a href=\"http://www.google.com\">Google</a>' in testpage.getText()", "path": "IMSTransport\\tests\\testIMSImport.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Make sure that items whose ids are in the idsNotToList navTree\n# property get no_display set to True\n", "func_signal": "def testNavTreeMarksParentMetaTypesNotToQuery(self):\n", "code": "tree = self.utils.createNavTree(self.portal.folder2.file21)\nitems = tree['children']\nself.failUnless(items[-1]['show_children'] == True, items)\nntp=self.portal.portal_properties.navtree_properties\nntp.manage_changeProperties(parentMetaTypesNotToQuery=['Folder'])\ntree = self.utils.createNavTree(self.portal.folder2.file21)\nitems = tree['children']\nself.failUnless(items[-1]['show_children'] == False, items)", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Greek letters (not supported by UnicodeData)\n", "func_signal": "def testNormalizeGreek(self):\n", "code": "input = u'\\u039d\\u03af\\u03ba\\u03bf\\u03c2 \\u03a4\\u03b6\\u03ac\\u03bd\\u03bf\\u03c2'\nself.assertEqual(self.utils.normalizeString(input), 'nikos-tzanos')", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Greek letters (not supported by UnicodeData)\n", "func_signal": "def testNormalizeGreekUTF8(self):\n", "code": "input = u'\\u039d\\u03af\\u03ba\\u03bf\\u03c2 \\u03a4\\u03b6\\u03ac\\u03bd\\u03bf\\u03c2'.encode('utf-8')\nself.assertEqual(self.utils.normalizeString(input), 'nikos-tzanos')", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# If there is something that looks like a file extensions\n# it will be preserved.\n", "func_signal": "def testNormalizeStringFileExtensions(self):\n", "code": "self.assertEqual(self.utils.normalizeString(\"this is a file.gif\"),\n                 'this-is-a-file.gif')\nself.assertEqual(self.utils.normalizeString(\"this is. also. a file.html\"),\n                 'this-is-also-a-file.html')", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Objects should be converted to strings using repr()\n", "func_signal": "def testNormalizeStringObject(self):\n", "code": "self.assertEqual(self.utils.normalizeString(None), 'none')\nself.assertEqual(self.utils.normalizeString(True), 'true')\nself.assertEqual(self.utils.normalizeString(False), 'false')", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Everything that can't be transliterated will be hex'd\n", "func_signal": "def testNormalizeStringHex(self):\n", "code": "self.assertEqual(self.utils.normalizeString(u\"\\u9ad8\\u8054\\u5408 Chinese\"),\n                 '9ad880545408-chinese')\nself.assertEqual(self.utils.normalizeString(u\"\\uc774\\ubbf8\\uc9f1 Korean\"),\n                 'c774bbf8c9f1-korean')", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# With a File or Image as current action it should return a\n# currentItem which has '/view' appended to the url\n", "func_signal": "def testCreateNavTreeRespectsTypesWithViewAction(self):\n", "code": "tree = self.utils.createNavTree(self.portal.folder2.file21)\nself.failUnless(tree, tree)\n# Fail if 'view' is used for parent folder; it should only be on the File\nself.failIf(tree['children'][-1]['absolute_url'][-5:]=='/view')\n# Verify we have the correct object and it is the current item\nself.assertEqual(tree['children'][-1]['children'][-1]['currentItem'],True)\nself.assertEqual(tree['children'][-1]['children'][-1]['path'].split('/')[-1],'file21')\n# Verify that we have '/view'\nself.assertEqual(tree['children'][-1]['children'][-1]['absolute_url'][-5:],'/view')", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Make sure that items whose ids are in the idsNotToList navTree\n# property get no_display set to True\n", "func_signal": "def testNavTreeExcludesItemsInIdsNotToList(self):\n", "code": "ntp=self.portal.portal_properties.navtree_properties\nntp.manage_changeProperties(idsNotToList=['folder2'])\ntree = self.utils.createNavTree(self.portal.folder2.file21)\nself.failUnless(tree, tree)\nself.assertEqual(tree['children'][-1]['no_display'],True)\n# Shouldn't exlude anything else\nself.assertEqual(tree['children'][0]['no_display'],False)", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Should just return 'portal_catalog'\n", "func_signal": "def Title(self):\n", "code": "tool = getToolByName(self, 'portal_catalog')\n# Use implicit acquisition even, the horror\ntool = self.portal_catalog\nreturn tool.getId()", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "# Make sure typesToList() returns the expected types\n", "func_signal": "def testTypesToList(self):\n", "code": "wl = self.utils.typesToList()\nself.failUnless('Folder' in wl)\nself.failUnless('Large Plone Folder' in wl)\nself.failUnless('Topic' in wl)\nself.failIf('ATReferenceCriterion' in wl)", "path": "LinguaPlone\\tests\\testCompat20.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "\"\"\" Test that IMS import works for a default package. \"\"\"\n", "func_signal": "def testImport(self):\n", "code": "self.setRoles(['Manager'])\nitt = self.portal.portal_IMSTransportTool\nself.portal.invokeFactory('Folder', 'testfolder')\ntestfolder = getattr(self.portal, 'testfolder')\nresults = itt.importZipfile(testfolder, \n                            'Products/IMSTransport/tests/IMS_Sample_Course.zip', \n                            'Default',\n                            'LOMv1.0')\nassert results[0], results[2]\n\n# Test sample folder\nassert getattr(testfolder, 'course')\nassert getattr(testfolder, 'course.html')\n\n# Test the sample page\nassert getattr(testfolder.course, 'sample-page')\nsamplepage = getattr(testfolder.course, 'sample-page')\n\nassert 'Sample Page' == samplepage.Title()\nassert 'This is a sample page for IMS Packaging' == samplepage.Description()\nassert 'Ray, David' in samplepage.Creators()\nassert 'text/html' == samplepage.Format()\nassert 'Page' == samplepage.Type()\nassert '<h2>Sample Text</h2>' in samplepage.getText()\n\n# Test example subfolder\nassert getattr(testfolder.course, 'sample-folder')\nsamplefolder = getattr(testfolder.course, 'sample-folder')\nassert 'sample-page-in-folder' in samplefolder.objectIds()\nsfolderpage = getattr(samplefolder, 'sample-page-in-folder')\nassert 'Page' == sfolderpage.Type()\nassert '<h2>My parent folder is Sample Folder</h2>' in sfolderpage.getText()\n\n# Test Sample file\nassert getattr(testfolder.course, 'samplefile.txt')\nsamplefile = getattr(testfolder.course, 'samplefile.txt')\nassert 'Sample File' == samplefile.Title()\nassert 'This is a sample file for IMS Packaging.' == samplefile.Description()\nassert 'Ray, David' in samplefile.Creators()\nassert 'text/plain' == samplefile.Format()\nassert 'File' == samplefile.Type()\nassert samplefile.size() > 0\n\n# Test Sample image\nassert getattr(testfolder.course, 'sampleimage.gif')\nsampleimage = getattr(testfolder.course, 'sampleimage.gif')\nassert 'Sample Image' == sampleimage.Title()\nassert 'This is a sample image for IMS Packaging.' == sampleimage.Description()\nassert 'Ray, David' in sampleimage.Creators()\nassert 'image/gif' == sampleimage.Format()\nassert 'Image' == sampleimage.Type()\nassert sampleimage.size() > 0", "path": "IMSTransport\\tests\\testIMSImport.py", "repo_name": "dtgit/ecec", "stars": 4, "license": "None", "language": "python", "size": 1800}
{"docstring": "\"\"\"paramnames : paramnames ',' paramname\n              | paramname\"\"\"\n", "func_signal": "def p_paramnames(p):\n", "code": "if len(p) == 4:\n    p[0] = extendorappend( p[1], p[3] )\nelse:\n    p[0] = [p[1]]", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"paramname : SYMBOL '=' STRING\n             | SYMBOL\"\"\"\n", "func_signal": "def p_paramname(p):\n", "code": "if len(p) == 4:\n    p[0] = ParamNode( p[1], StringNode(p[3]) )\nelse:\n    p[0] = ParamNode( p[1], None )", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"paramexpr : exprs\n             | group\"\"\"\n", "func_signal": "def p_paramexpr(p):\n", "code": "if hasattr(p[1],'__iter__'):\n    if len(p[1]) > 1:\n        p[0] = BlockNode(p[1])\n    else:\n        #it might be a single expression that evaluates to an array, which will be automatically expanded on evaluation.\n        p[0] = p[1][0]\nelse:\n    p[0] = p[1]", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"Extends the list of the first argument using the second argument, which\ncan be another list or a single element.\"\"\"\n", "func_signal": "def extendorappend( the_list, list_or_single):\n", "code": "if isinstance(list_or_single, list):\n    the_list.extend(list_or_single)\nelse:\n    the_list.append(list_or_single)\nreturn the_list", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"Evaluates the given block of code for the given param. It makes the\nfollowing implicit values available in the context:\n\n$index: current position in the iteration, starting at 0.\n$first: True if this is the first iteration, '' otherwise.\n$last: True if this is the last iteration, '' otherwise.\n$notlast: True if this is not the last iteration, '' otherwise.\n$key: name of the parameter if given.\n$value: actual value of the parameter.\n\"\"\"\n", "func_signal": "def execparam(self, param, code, context ):\n", "code": "mycontext = context.copy()\nmycontext['$index'] = param.pos\nmycontext['$first'] = param.pos == 0 and 'True' or ''\nmycontext['$last'] = param.islast and 'True' or ''\nmycontext['$notlast'] = (not param.islast) and 'True' or ''\nif param.name:\n    mycontext['$key'] = param.name\nmycontext['$value'] = param.value\nreturn code.eval(mycontext)", "path": "magro\\ast.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"Obtains the name of the class of the object and of the classes it is a\nsubclass of.\"\"\"\n", "func_signal": "def gettypenames( obj ):\n", "code": "thetype = type( obj )\nif thetype.__name__ == 'instance':\n    thetype = obj.__class__\n\nresult = [thetype.__name__]\nresult.extend( gettypebases(thetype) )\n\nreturn result", "path": "magro\\ast.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"paramvalue : SYMBOL '=' paramexpr\n              | paramexpr\"\"\"\n", "func_signal": "def p_paramvalue(p):\n", "code": "if len(p) == 4:\n    p[0] = ParamNode( p[1], p[3] )\nelse:\n    p[0] = ParamNode( None, p[1] )", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"\nEvaluates its single BlockNode child.\n\"\"\"\n", "func_signal": "def eval( self, context=None ):\n", "code": "if context: \n    mycontext = context.copy()\nelse:\n    mycontext = {}\nfor key, val in self.defs.items():\n    if key not in mycontext:\n        mycontext[key] = val\n\nreturn self.code.eval( mycontext )", "path": "magro\\ast.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"elseblock : ':' exprs EOL\n             | ':' EOL block\n             | ':' cyclecall\n             | empty\"\"\"\n", "func_signal": "def p_elseblock(p):\n", "code": "if len(p) > 2:\n    if hasattr(p[2],'__iter__'):\n        p[0] = BlockNode(p[2])\n    elif len(p) > 3:\n        p[0] = p[3]\n    else:\n        p[0] = BlockNode([p[2]])\nelse:\n    p[0] = None", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"Evaluates each contained node and concatenates the result.\n\nThe following implicit variables will be available in the context\nduring evaluation:\n\n$level: current level of indentation of this block\n$nextlevel: $level+1\n$previouslevel: $level-1\n\"\"\"\n", "func_signal": "def eval(self, context, uselevel=False):\n", "code": "result_buf = StringIO()\nif uselevel:\n    context = context.copy()\n    context['$level'] = self.level\n    context['$nextlevel'] = self.level+1\n    if self.level:\n        context['$previouslevel'] = self.level-1\nfor node in self.code:\n    text = unicode(node.eval( context ))\n    if text:\n        result_buf.write(text)\n\nresult = result_buf.getvalue()\nresult_buf.close()\nreturn result", "path": "magro\\ast.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"Returns the result of evaluating its value\nor '' if no value was given.\"\"\"\n", "func_signal": "def eval(self, context):\n", "code": "if self.value:\n    return self.value.eval( context )\nelse:\n    return ''", "path": "magro\\ast.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"Evaluates all parameters and invoques the block of code that\ncorresponds to its value.\"\"\"\n", "func_signal": "def eval( self, context ):\n", "code": "result = ''\nfor param in self.paramsgenerator( context ):\n    if param.name or param.value:\n        result += self.execparam( param, self.code, context )\n    elif self.elsecode:\n        result += self.execparam( param, self.elsecode, context )\nreturn result", "path": "magro\\ast.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"Evaluates all call parameters and applies default values when one\nis not defined.\n\nAll defined values that are not declared in the macro definition are\nstored in the context as the $undeclared implicit variable.\n\"\"\"\n", "func_signal": "def processparams(self, params, context ):\n", "code": "exp_params = self.expandparams( params, context )\ncontext['$all'] = exp_params\n\n#evaluate params and see if they were declared.\nundeclared = []\ni = 1\niunnamed = 0\nfor param in exp_params:\n    value = param.eval(context)\n    \n    context[ '$%d' % (i,) ] = value\n    if param.name:\n        context[ param.name ] = value\n        if param.name not in self.paramnames:\n            ev_param = ParamNode( param.name, StringNode(value) )\n            undeclared.append( ev_param )\n    else:\n        iunnamed += 1\n        if iunnamed > len(self.params):\n            ev_param = ParamNode( None, StringNode(value) )\n            undeclared.append( ev_param)\n    i += 1\ncontext['$undeclared'] = undeclared\n\n#search default values for undefined parameters\ni = 1\nfor param in self.params:\n    if not context.has_key( param.name ):\n        if i <= len(exp_params) and not exp_params[i-1].name:\n            ivar = '$%d' % (i,)\n            context[param.name] = context[ivar]\n        else:\n            context[param.name] = param.eval(context)\n    i += 1", "path": "magro\\ast.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"Used by eval to iterate over each of the parameters. \nFlattens the parameters one level if a parameter evaluates to an\niterable and is not given a name.\"\"\"\n", "func_signal": "def paramsgenerator( self, context ):\n", "code": "def namevalue(param):\n    \"Obtains a tuple with the name and value of the parameter.\"\n    if isinstance( param, Node):\n        return (param.name, param.eval(context))\n    else:\n        return (None, param)\n\ni = 0\ni_k = 0\nnparams = len(self.params)\nfor param in self.params:\n    (name, val) = namevalue(param)\n    \n    if hasattr(val,'__iter__') and not name:\n        k = 0\n        for param_ in val:\n            (name_, val_) = namevalue(param_)\n            islast = ( i == nparams-1 ) and hasattr(k,'__len__') and (k==len(val)-1)\n            yield EvalParam( i_k+k, name_, val_, islast )\n            k += 1\n        i_k += k\n    else:\n        islast = i == nparams-1\n        yield EvalParam( i_k, name, val, islast )\n        i_k += 1\n    i += 1", "path": "magro\\ast.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"paramdefs : '(' paramnames ')'\n             | '(' ')'\"\"\"\n", "func_signal": "def p_paramdefs(p):\n", "code": "if len(p) == 4:\n    p[0] = p[2]\nelse:\n    p[0] = []", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"calls : calls call\n         | empty\"\"\"\n", "func_signal": "def p_calls(p):\n", "code": "if len(p) == 3:\n    p[0] = extendorappend( p[1], p[2] )\nelse:\n    p[0] = []", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"callparams : '(' paramvalues ')'\n              | empty\"\"\"\n", "func_signal": "def p_callparams(p):\n", "code": "if len(p) == 4:\n    p[0] = p[2]\nelse:\n    p[0] = []", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"paramvalues : paramvalues ',' paramvalue\n               | paramvalue\"\"\"\n", "func_signal": "def p_paramvalues(p):\n", "code": "if len(p) == 4:\n    p[0] = extendorappend( p[1], p[3] )\nelse:\n    p[0] = [p[1]]", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"callsorblock : EOL block\n                | exprs EOL\"\"\"\n", "func_signal": "def p_callsorblock(p):\n", "code": "if hasattr(p[1],'__iter__'):\n    p[0] = BlockNode(p[1])\nelse:\n    p[0] = p[2]", "path": "magro\\parser.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"Gets the parents classes of a type recursively. Returns a list of list\nwhere each position stores all the classes in that level of the tree.\"\"\"\n", "func_signal": "def processtypebases(thetype, levels, currlevel):\n", "code": "while (currlevel >= len(levels)):\n    levels.append([])\nfor base in thetype.__bases__:\n    levels[currlevel].append(base.__name__)\nfor base in thetype.__bases__:\n    processtypebases(base, levels, currlevel+1)", "path": "magro\\ast.py", "repo_name": "isaacrivas/magro", "stars": 4, "license": "other", "language": "python", "size": 336}
{"docstring": "\"\"\"\nThis is a compatibility function that takes a C{float} and converts it to an\nC{int} if the values are equal.\n\"\"\"\n", "func_signal": "def _check_for_int(x):\n", "code": "try:\n    y = int(x)\nexcept (OverflowError, ValueError):\n    pass\nelse:\n    # There is no way in AMF0 to distinguish between integers and floats\n    if x == x and y == x:\n        return y\n\nreturn x", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "# Write a null string, this is an optimisation so that we don't\n# have to waste precious cycles by encoding the string etc.\n", "func_signal": "def _writeEndObject(self):\n", "code": "self.stream.write('\\x00\\x00')\nself.writeType(ASTypes.OBJECTTERM)", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nRead XML.\n\"\"\"\n", "func_signal": "def readXML(self):\n", "code": "data = self.readLongString()\nxml = util.ET.fromstring(data)\nself.context.addObject(xml)\n\nreturn xml", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nA helper function to encode an element into the AMF0 format.\n\n@type element: C{mixed}\n@keyword element: The element to encode\n@type context: L{Context<pyamf.amf0.Context>}\n@keyword context: AMF0 C{Context} to use for the encoding. This holds\n    previously referenced objects etc.\n@rtype: C{StringIO}\n@return: The encoded stream.\n\"\"\"\n", "func_signal": "def encode(*args, **kwargs):\n", "code": "context = kwargs.get('context', None)\nbuf = util.BufferedByteStream()\nencoder = Encoder(buf, context)\n\nfor element in args:\n    encoder.writeElement(element)\n\nreturn buf", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nWrite mixed array to the data stream.\n\n@type o: L{BufferedByteStream<pyamf.util.BufferedByteStream>}\n@param o: The mixed array data to be encoded to the AMF0\n    data stream.\n\"\"\"\n", "func_signal": "def writeMixedArray(self, o):\n", "code": "try:\n    self.writeReference(o)\n    return\nexcept pyamf.ReferenceError:\n    self.context.addObject(o)\n\nself.writeType(ASTypes.MIXEDARRAY)\n\n# TODO: optimise this\n# work out the highest integer index\ntry:\n    # list comprehensions to save the day\n    max_index = max([y[0] for y in o.items()\n        if isinstance(y[0], (int, long))])\n\n    if max_index < 0:\n        max_index = 0\nexcept ValueError:\n    max_index = 0\n\nself.stream.write_ulong(max_index)\n\nself._writeDict(o)\nself._writeEndObject()", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nRead UTF8 string.\n\"\"\"\n", "func_signal": "def readLongString(self):\n", "code": "len = self.stream.read_ulong()\n\nreturn self.stream.read_utf8_string(len)", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nWrite reference to the data stream.\n\n@type o: L{BufferedByteStream<pyamf.util.BufferedByteStream>}\n@param o: The reference data to be encoded to the AMF0 data\n    stream.\n\"\"\"\n", "func_signal": "def writeReference(self, o):\n", "code": "idx = self.context.getObjectReference(o)\n\nself.writeType(ASTypes.REFERENCE)\nself.stream.write_ushort(idx)", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nReads an object from the data stream.\n\n@rtype: L{ASObject<pyamf.ASObject>}\n\"\"\"\n", "func_signal": "def readObject(self):\n", "code": "obj = pyamf.ASObject()\nself.context.addObject(obj)\n\nself._readObject(obj)\n\nreturn obj", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nGets a function based on the type of data.\n\n@see: L{pyamf.BaseEncoder._writeElementFunc}\n\"\"\"\n# There is a very specific use case that we must check for.\n# In the context there is an array of amf3_objs that contain\n# references to objects that are to be encoded in amf3.\n", "func_signal": "def _writeElementFunc(self, data):\n", "code": "try:\n    self.context.getAMF3ObjectReference(data)\n    return self.writeAMF3\nexcept pyamf.ReferenceError:\n    pass\n\nreturn pyamf.BaseEncoder._writeElementFunc(self, data)", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nA helper function to decode an AMF0 datastream.\n\n@type   stream: L{BufferedByteStream<pyamf.util.BufferedByteStream>}\n@param  stream: AMF0 datastream.\n@type   context: L{Context<pyamf.amf0.Context>}\n@param  context: AMF0 Context.\n\"\"\"\n", "func_signal": "def decode(stream, context=None, strict=False):\n", "code": "decoder = Decoder(stream, context, strict=strict)\n\nwhile 1:\n    try:\n        yield decoder.readElement()\n    except pyamf.EOStream:\n        break", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nRead a C{list} from the data stream.\n\n@rtype: C{list}\n@return: C{list}\n\"\"\"\n", "func_signal": "def readList(self):\n", "code": "obj = []\nself.context.addObject(obj)\nlen = self.stream.read_ulong()\n\nfor i in xrange(len):\n    obj.append(self.readElement())\n\nreturn obj", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nWrite number to the data stream.\n\n@type   n: L{BufferedByteStream<pyamf.util.BufferedByteStream>}\n@param  n: The number data to be encoded to the AMF0 data stream.\n\"\"\"\n", "func_signal": "def writeNumber(self, n):\n", "code": "self.writeType(ASTypes.NUMBER)\nself.stream.write_double(float(n))", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nWrite XML to the data stream.\n\n@type e: L{BufferedByteStream<pyamf.util.BufferedByteStream>}\n@param e: The XML data to be encoded to the AMF0 data stream.\n\"\"\"\n", "func_signal": "def writeXML(self, e):\n", "code": "if self.use_amf3 is True:\n    self.writeAMF3(e)\n\n    return\n\nself.writeType(ASTypes.XML)\n\ndata = util.ET.tostring(e, 'utf-8')\nself.stream.write_ulong(len(data))\nself.stream.write(data)", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nWrite boolean to the data stream.\n\n@type b: L{BufferedByteStream<pyamf.util.BufferedByteStream>}\n@param b: The boolean data to be encoded to the AMF0 data stream.\n\"\"\"\n", "func_signal": "def writeBoolean(self, b):\n", "code": "self.writeType(ASTypes.BOOL)\n\nif b:\n    self.stream.write_uchar(1)\nelse:\n    self.stream.write_uchar(0)", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nResets the context.\n\n@see: L{pyamf.BaseContext.reset}\n\"\"\"\n", "func_signal": "def reset(self):\n", "code": "pyamf.BaseContext.reset(self)\n\nif hasattr(self, 'amf3_context'):\n    self.amf3_context.reset()", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nReads a string from the data stream.\n\n@rtype: C{str}\n@return: string\n\"\"\"\n", "func_signal": "def readString(self):\n", "code": "len = self.stream.read_ushort()\nreturn self.stream.read_utf8_string(len)", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\n@raise pyamf.EncodeError: Unable to determine object attributes.\n\"\"\"\n", "func_signal": "def _getObjectAttrs(self, o, alias):\n", "code": "obj_attrs = None\n\nif alias is not None:\n    obj_attrs = {}\n\n    for attrs in alias.getAttributes(o, codec=self):\n        obj_attrs.update(attrs)\n\nif obj_attrs is None:\n    obj_attrs = util.get_attrs(o)\n\nif obj_attrs is None:\n    raise pyamf.EncodeError('Unable to determine object attributes')\n\nreturn obj_attrs", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nGets a reference for an object.\n\n@raise ReferenceError: Object reference could not be found.\n\"\"\"\n", "func_signal": "def getAMF3ObjectReference(self, obj):\n", "code": "try:\n    return self.amf3_objs.getReferenceTo(obj)\nexcept KeyError:\n    raise ReferenceError", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nRead AMF3 elements from the data stream.\n\n@rtype: C{mixed}\n@return: The AMF3 element read from the stream\n\"\"\"\n", "func_signal": "def readAMF3(self):\n", "code": "if not hasattr(self.context, 'amf3_context'):\n    from pyamf import amf3\n\n    self.context.amf3_context = amf3.Context()\n\ndecoder = pyamf._get_decoder_class(pyamf.AMF3)(self.stream, self.context.amf3_context, strict=self.strict)\n\nelement = decoder.readElement()\nself.context.addAMF3Object(element)\n\nreturn element", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "\"\"\"\nWrites the type to the stream.\n\n@type   type: C{int}\n@param  type: ActionScript type.\n\n@raise pyamf.EncodeError: AMF0 type is not recognized.\n\"\"\"\n", "func_signal": "def writeType(self, type):\n", "code": "if type not in ACTIONSCRIPT_TYPES:\n    raise pyamf.EncodeError(\"Unknown AMF0 type 0x%02x at %d\" % (\n        type, self.stream.tell() - 1))\n\nself.stream.write_uchar(type)", "path": "ourstories_django\\lib\\pyamf\\amf0.py", "repo_name": "natea/Haitian-Stories", "stars": 5, "license": "None", "language": "python", "size": 4032}
{"docstring": "'''Parse a string according to the OnBlog 8-bit date format'''\n", "func_signal": "def _parse_date_onblog(dateString):\n", "code": "m = _korean_onblog_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('OnBlog date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "'''Parse a string according to the MS SQL date format'''\n", "func_signal": "def _parse_date_mssql(dateString):\n", "code": "m = _mssql_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('MS SQL date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# Several optimizations were made that skip over calls to\n# the whitespace regex, so this test is designed to try and\n# exercise the uncommon cases. The array cases are already covered.\n", "func_signal": "def test_decoder_optimizations(self):\n", "code": "rval = S.loads('{   \"key\"    :    \"value\"    ,  \"k\":\"v\"    }')\nself.assertEquals(rval, {\"key\":\"value\", \"k\":\"v\"})", "path": "extlib\\simplejson\\tests\\test_decode.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "'''Changes an XML data stream on the fly to specify a new encoding\n\ndata is a raw sequence of bytes (not Unicode) that is presumed to be in %encoding already\nencoding is a string recognized by encodings.aliases\n'''\n", "func_signal": "def _toUTF8(data, encoding):\n", "code": "if _debug: sys.stderr.write('entering _toUTF8, trying encoding %s\\n' % encoding)\n# strip Byte Order Mark (if present)\nif (len(data) >= 4) and (data[:2] == '\\xfe\\xff') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16be':\n            sys.stderr.write('trying utf-16be instead\\n')\n    encoding = 'utf-16be'\n    data = data[2:]\nelif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') and (data[2:4] != '\\x00\\x00'):\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-16le':\n            sys.stderr.write('trying utf-16le instead\\n')\n    encoding = 'utf-16le'\n    data = data[2:]\nelif data[:3] == '\\xef\\xbb\\xbf':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-8':\n            sys.stderr.write('trying utf-8 instead\\n')\n    encoding = 'utf-8'\n    data = data[3:]\nelif data[:4] == '\\x00\\x00\\xfe\\xff':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32be':\n            sys.stderr.write('trying utf-32be instead\\n')\n    encoding = 'utf-32be'\n    data = data[4:]\nelif data[:4] == '\\xff\\xfe\\x00\\x00':\n    if _debug:\n        sys.stderr.write('stripping BOM\\n')\n        if encoding != 'utf-32le':\n            sys.stderr.write('trying utf-32le instead\\n')\n    encoding = 'utf-32le'\n    data = data[4:]\nnewdata = unicode(data, encoding)\nif _debug: sys.stderr.write('successfully converted %s data to unicode\\n' % encoding)\ndeclmatch = re.compile('^<\\?xml[^>]*?>')\nnewdecl = '''<?xml version='1.0' encoding='utf-8'?>'''\nif declmatch.search(newdata):\n    newdata = declmatch.sub(newdecl, newdata)\nelse:\n    newdata = newdecl + u'\\n' + newdata\nreturn newdata.encode('utf-8')", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "\"\"\"URL, filename, or string --> stream\n\nThis function lets you define parsers that take any input source\n(URL, pathname to local or network file, or actual data as a string)\nand deal with it in a uniform manner.  Returned object is guaranteed\nto have all the basic stdio read methods (read, readline, readlines).\nJust .close() the object when you're done with it.\n\nIf the etag argument is supplied, it will be used as the value of an\nIf-None-Match request header.\n\nIf the modified argument is supplied, it must be a tuple of 9 integers\nas returned by gmtime() in the standard Python time module. This MUST\nbe in GMT (Greenwich Mean Time). The formatted date/time will be used\nas the value of an If-Modified-Since request header.\n\nIf the agent argument is supplied, it will be used as the value of a\nUser-Agent request header.\n\nIf the referrer argument is supplied, it will be used as the value of a\nReferer[sic] request header.\n\nIf handlers is supplied, it is a list of handlers used to build a\nurllib2 opener.\n\"\"\"\n\n", "func_signal": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers):\n", "code": "if hasattr(url_file_stream_or_string, 'read'):\n    return url_file_stream_or_string\n\nif url_file_stream_or_string == '-':\n    return sys.stdin\n\nif urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):\n    if not agent:\n        agent = USER_AGENT\n    # test for inline user:password for basic auth\n    auth = None\n    if base64:\n        urltype, rest = urllib.splittype(url_file_stream_or_string)\n        realhost, rest = urllib.splithost(rest)\n        if realhost:\n            user_passwd, realhost = urllib.splituser(realhost)\n            if user_passwd:\n                url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)\n                auth = base64.encodestring(user_passwd).strip()\n    # try to open with urllib2 (to use optional headers)\n    request = urllib2.Request(url_file_stream_or_string)\n    request.add_header('User-Agent', agent)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        # format into an RFC 1123-compliant timestamp. We can't use\n        # time.strftime() since the %a and %b directives can be affected\n        # by the current locale, but RFC 2616 states that dates must be\n        # in English.\n        short_weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n        request.add_header('If-Modified-Since', '%s, %02d %s %04d %02d:%02d:%02d GMT' % (short_weekdays[modified[6]], modified[2], months[modified[1] - 1], modified[0], modified[3], modified[4], modified[5]))\n    if referrer:\n        request.add_header('Referer', referrer)\n    if gzip and zlib:\n        request.add_header('Accept-encoding', 'gzip, deflate')\n    elif gzip:\n        request.add_header('Accept-encoding', 'gzip')\n    elif zlib:\n        request.add_header('Accept-encoding', 'deflate')\n    else:\n        request.add_header('Accept-encoding', '')\n    if auth:\n        request.add_header('Authorization', 'Basic %s' % auth)\n    if ACCEPT_HEADER:\n        request.add_header('Accept', ACCEPT_HEADER)\n    request.add_header('A-IM', 'feed') # RFC 3229 support\n    opener = apply(urllib2.build_opener, tuple([_FeedURLHandler()] + handlers))\n    opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent\n    try:\n        return opener.open(request)\n    finally:\n        opener.close() # JohnD\n\n# try to open with native open function (if url_file_stream_or_string is a filename)\ntry:\n    return open(url_file_stream_or_string)\nexcept:\n    pass\n\n# treat url_file_stream_or_string as string\nreturn _StringIO(str(url_file_stream_or_string))", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = S.loads(JSON)\nout = S.dumps(res)\nself.assertEquals(res, S.loads(out))", "path": "extlib\\simplejson\\tests\\test_pass3.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "'''Return the Time Zone Designator as an offset in seconds from UTC.'''\n", "func_signal": "def __extract_tzd(m):\n", "code": "if not m:\n    return 0\ntzd = m.group('tzd')\nif not tzd:\n    return 0\nif tzd == 'Z':\n    return 0\nhours = int(m.group('tzdhours'))\nminutes = m.group('tzdminutes')\nif minutes:\n    minutes = int(minutes)\nelse:\n    minutes = 0\noffset = (hours*60 + minutes) * 60\nif tzd[0] == '+':\n    return -offset\nreturn offset", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "\"\"\"Encode the given object and yield each string\nrepresentation as available.\n\nFor example::\n\n    for chunk in JSONEncoder().iterencode(bigobject):\n        mysocket.write(chunk)\n\n\"\"\"\n", "func_signal": "def iterencode(self, o, _one_shot=False):\n", "code": "if self.check_circular:\n    markers = {}\nelse:\n    markers = None\nif self.ensure_ascii:\n    _encoder = encode_basestring_ascii\nelse:\n    _encoder = encode_basestring\nif self.encoding != 'utf-8':\n    def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):\n        if isinstance(o, str):\n            o = o.decode(_encoding)\n        return _orig_encoder(o)\n\ndef floatstr(o, allow_nan=self.allow_nan, _repr=FLOAT_REPR, _inf=INFINITY, _neginf=-INFINITY):\n    # Check for specials.  Note that this type of test is processor- and/or\n    # platform-specific, so do tests which don't depend on the internals.\n\n    if o != o:\n        text = 'NaN'\n    elif o == _inf:\n        text = 'Infinity'\n    elif o == _neginf:\n        text = '-Infinity'\n    else:\n        return _repr(o)\n\n    if not allow_nan:\n        raise ValueError(\"Out of range float values are not JSON compliant: %r\"\n            % (o,))\n\n    return text\n\n\nif _one_shot and c_make_encoder is not None and not self.indent and not self.sort_keys:\n    _iterencode = c_make_encoder(\n        markers, self.default, _encoder, self.indent,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, self.allow_nan)\nelse:\n    _iterencode = _make_iterencode(\n        markers, self.default, _encoder, self.indent, floatstr,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, _one_shot)\nreturn _iterencode(o, 0)", "path": "extlib\\simplejson\\encoder.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if not self.elementstack: return\nref = ref.lower()\nif ref in ('34', '38', '39', '60', '62', 'x22', 'x26', 'x27', 'x3c', 'x3e'):\n    text = '&#%s;' % ref\nelse:\n    if ref[0] == 'x':\n        c = int(ref[1:], 16)\n    else:\n        c = int(ref)\n    text = unichr(c).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# called for each start tag\n# attrs is a list of (attr, value) tuples\n# e.g. for <pre class='screen'>, tag='pre', attrs=[('class', 'screen')]\n", "func_signal": "def unknown_starttag(self, tag, attrs):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, unknown_starttag, tag=%s\\n' % tag)\nuattrs = []\n# thanks to Kevin Marks for this breathtaking hack to deal with (valid) high-bit attribute values in UTF-8 feeds\nfor key, value in attrs:\n    if type(value) != type(u''):\n        value = unicode(value, self.encoding)\n    uattrs.append((unicode(key, self.encoding), value))\nstrattrs = u''.join([u' %s=\"%s\"' % (key, value) for key, value in uattrs]).encode(self.encoding)\nif tag in self.elements_no_end_tag:\n    self.pieces.append('<%(tag)s%(strattrs)s />' % locals())\nelse:\n    self.pieces.append('<%(tag)s%(strattrs)s>' % locals())", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n# Store the original text verbatim.\n", "func_signal": "def handle_data(self, text):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, handle_text, text=%s\\n' % text)\nself.pieces.append(text)", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "'''Get the character encoding of the XML document\n\nhttp_headers is a dictionary\nxml_data is a raw string (not Unicode)\n\nThis is so much trickier than it sounds, it's not even funny.\nAccording to RFC 3023 ('XML Media Types'), if the HTTP Content-Type\nis application/xml, application/*+xml,\napplication/xml-external-parsed-entity, or application/xml-dtd,\nthe encoding given in the charset parameter of the HTTP Content-Type\ntakes precedence over the encoding given in the XML prefix within the\ndocument, and defaults to 'utf-8' if neither are specified.  But, if\nthe HTTP Content-Type is text/xml, text/*+xml, or\ntext/xml-external-parsed-entity, the encoding given in the XML prefix\nwithin the document is ALWAYS IGNORED and only the encoding given in\nthe charset parameter of the HTTP Content-Type header should be\nrespected, and it defaults to 'us-ascii' if not specified.\n\nFurthermore, discussion on the atom-syntax mailing list with the\nauthor of RFC 3023 leads me to the conclusion that any document\nserved with a Content-Type of text/* and no charset parameter\nmust be treated as us-ascii.  (We now do this.)  And also that it\nmust always be flagged as non-well-formed.  (We now do this too.)\n\nIf Content-Type is unspecified (input was local file or non-HTTP source)\nor unrecognized (server just got it totally wrong), then go by the\nencoding given in the XML prefix of the document and default to\n'iso-8859-1' as per the HTTP specification (RFC 2616).\n\nThen, assuming we didn't find a character encoding in the HTTP headers\n(and the HTTP Content-type allowed us to look in the body), we need\nto sniff the first few bytes of the XML data and try to determine\nwhether the encoding is ASCII-compatible.  Section F of the XML\nspecification shows the way here:\nhttp://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\n\nIf the sniffed encoding is not ASCII-compatible, we need to make it\nASCII compatible so that we can sniff further into the XML declaration\nto find the encoding attribute, which will tell us the true encoding.\n\nOf course, none of this guarantees that we will be able to parse the\nfeed in the declared character encoding (assuming it was declared\ncorrectly, which many are not).  CJKCodecs and iconv_codec help a lot;\nyou should definitely install them if you can.\nhttp://cjkpython.i18n.org/\n'''\n\n", "func_signal": "def _getCharacterEncoding(http_headers, xml_data):\n", "code": "def _parseHTTPContentType(content_type):\n    '''takes HTTP Content-Type header and returns (content type, charset)\n\n    If no charset is specified, returns (content type, '')\n    If no content type is specified, returns ('', '')\n    Both return parameters are guaranteed to be lowercase strings\n    '''\n    content_type = content_type or ''\n    content_type, params = cgi.parse_header(content_type)\n    return content_type, params.get('charset', '').replace(\"'\", '')\n\nsniffed_xml_encoding = ''\nxml_encoding = ''\ntrue_encoding = ''\nhttp_content_type, http_encoding = _parseHTTPContentType(http_headers.get('content-type'))\n# Must sniff for non-ASCII-compatible character encodings before\n# searching for XML declaration.  This heuristic is defined in\n# section F of the XML specification:\n# http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = _ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        # ASCII-compatible\n        pass\n    xml_encoding_match = re.compile('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode', 'iso-10646-ucs-4', 'ucs-4', 'csucs4', 'utf-16', 'utf-32', 'utf_16', 'utf_32', 'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nacceptable_content_type = 0\napplication_content_types = ('application/xml', 'application/xml-dtd', 'application/xml-external-parsed-entity')\ntext_content_types = ('text/xml', 'text/xml-external-parsed-entity')\nif (http_content_type in application_content_types) or \\\n   (http_content_type.startswith('application/') and http_content_type.endswith('+xml')):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or xml_encoding or 'utf-8'\nelif (http_content_type in text_content_types) or \\\n     (http_content_type.startswith('text/')) and http_content_type.endswith('+xml'):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or 'us-ascii'\nelif http_content_type.startswith('text/'):\n    true_encoding = http_encoding or 'us-ascii'\nelif http_headers and (not http_headers.has_key('content-type')):\n    true_encoding = xml_encoding or 'iso-8859-1'\nelse:\n    true_encoding = xml_encoding or 'utf-8'\nreturn true_encoding, http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# called for each end tag, e.g. for </pre>, tag will be 'pre'\n# Reconstruct the original end tag.\n", "func_signal": "def unknown_endtag(self, tag):\n", "code": "if tag not in self.elements_no_end_tag:\n    self.pieces.append(\"</%(tag)s>\" % locals())", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# override internal declaration handler to handle CDATA blocks\n", "func_signal": "def parse_declaration(self, i):\n", "code": "if _debug: sys.stderr.write('entering parse_declaration\\n')\nif self.rawdata[i:i+9] == '<![CDATA[':\n    k = self.rawdata.find(']]>', i)\n    if k == -1: k = len(self.rawdata)\n    self.handle_data(_xmlescape(self.rawdata[i+9:k]), 0)\n    return k+3\nelse:\n    k = self.rawdata.find('>', i)\n    return k+1", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "'''Strips DOCTYPE from XML document, returns (rss_version, stripped_data)\n\nrss_version may be 'rss091n' or None\nstripped_data is the same XML document, minus the DOCTYPE\n'''\n", "func_signal": "def _stripDoctype(data):\n", "code": "entity_pattern = re.compile(r'<!ENTITY([^>]*?)>', re.MULTILINE)\ndata = entity_pattern.sub('', data)\ndoctype_pattern = re.compile(r'<!DOCTYPE([^>]*?)>', re.MULTILINE)\ndoctype_results = doctype_pattern.findall(data)\ndoctype = doctype_results and doctype_results[0] or ''\nif doctype.lower().count('netscape'):\n    version = 'rss091n'\nelse:\n    version = None\ndata = doctype_pattern.sub('', data)\nreturn version, data", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "'''Parse a string according to a Hungarian 8-bit date format.'''\n", "func_signal": "def _parse_date_hungarian(dateString):\n", "code": "m = _hungarian_date_format_re.match(dateString)\nif not m: return\ntry:\n    month = _hungarian_months[m.group(2)]\n    day = m.group(3)\n    if len(day) == 1:\n        day = '0' + day\n    hour = m.group(4)\n    if len(hour) == 1:\n        hour = '0' + hour\nexcept:\n    return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': month, 'day': day,\\\n             'hour': hour, 'minute': m.group(5),\\\n             'zonediff': m.group(6)}\nif _debug: sys.stderr.write('Hungarian date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# utility method to be called by descendants\n", "func_signal": "def normalize_attrs(self, attrs):\n", "code": "attrs = [(k.lower(), v) for k, v in attrs]\nattrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]\nreturn attrs", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "\"\"\"Return a JSON string representation of a Python data structure.\n\n>>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n'{\"foo\": [\"bar\", \"baz\"]}'\n\n\"\"\"\n# This is for extremely simple cases and benchmarks.\n", "func_signal": "def encode(self, o):\n", "code": "if isinstance(o, basestring):\n    if isinstance(o, str):\n        _encoding = self.encoding\n        if (_encoding is not None\n                and not (_encoding == 'utf-8')):\n            o = o.decode(_encoding)\n    if self.ensure_ascii:\n        return encode_basestring_ascii(o)\n    else:\n        return encode_basestring(o)\n# This doesn't pass the iterator directly to ''.join() because the\n# exceptions aren't as detailed.  The list call should be roughly\n# equivalent to the PySequence_Fast that ''.join() would do.\nchunks = self.iterencode(o, _one_shot=True)\nif not isinstance(chunks, (list, tuple)):\n    chunks = list(chunks)\nreturn ''.join(chunks)", "path": "extlib\\simplejson\\encoder.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if not self.elementstack: return\nif _debug: sys.stderr.write('entering handle_entityref with %s\\n' % ref)\nif ref in ('lt', 'gt', 'quot', 'amp', 'apos'):\n    text = '&%s;' % ref\nelse:\n    # entity resolution graciously donated by Aaron Swartz\n    def name2cp(k):\n        import htmlentitydefs\n        if hasattr(htmlentitydefs, 'name2codepoint'): # requires Python 2.3\n            return htmlentitydefs.name2codepoint[k]\n        k = htmlentitydefs.entitydefs[k]\n        if k.startswith('&#') and k.endswith(';'):\n            return int(k[2:-1]) # not in latin-1\n        return ord(k)\n    try: name2cp(ref)\n    except KeyError: text = '&%s;' % ref\n    else: text = unichr(name2cp(ref)).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n", "func_signal": "def handle_data(self, text, escape=1):\n", "code": "if not self.elementstack: return\nif escape and self.contentparams.get('type') == 'application/xhtml+xml':\n    text = _xmlescape(text)\nself.elementstack[-1][2].append(text)", "path": "extlib\\feedparser.py", "repo_name": "lmorchard/feedmagick-gae", "stars": 4, "license": "None", "language": "python", "size": 161}
{"docstring": "# utility method to be called by descendants\n", "func_signal": "def normalize_attrs(self, attrs):\n", "code": "attrs = [(k.lower(), v) for k, v in attrs]\nattrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]\nreturn attrs", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "'''Get the character encoding of the XML document\n\nhttp_headers is a dictionary\nxml_data is a raw string (not Unicode)\n\nThis is so much trickier than it sounds, it's not even funny.\nAccording to RFC 3023 ('XML Media Types'), if the HTTP Content-Type\nis application/xml, application/*+xml,\napplication/xml-external-parsed-entity, or application/xml-dtd,\nthe encoding given in the charset parameter of the HTTP Content-Type\ntakes precedence over the encoding given in the XML prefix within the\ndocument, and defaults to 'utf-8' if neither are specified.  But, if\nthe HTTP Content-Type is text/xml, text/*+xml, or\ntext/xml-external-parsed-entity, the encoding given in the XML prefix\nwithin the document is ALWAYS IGNORED and only the encoding given in\nthe charset parameter of the HTTP Content-Type header should be\nrespected, and it defaults to 'us-ascii' if not specified.\n\nFurthermore, discussion on the atom-syntax mailing list with the\nauthor of RFC 3023 leads me to the conclusion that any document\nserved with a Content-Type of text/* and no charset parameter\nmust be treated as us-ascii.  (We now do this.)  And also that it\nmust always be flagged as non-well-formed.  (We now do this too.)\n\nIf Content-Type is unspecified (input was local file or non-HTTP source)\nor unrecognized (server just got it totally wrong), then go by the\nencoding given in the XML prefix of the document and default to\n'iso-8859-1' as per the HTTP specification (RFC 2616).\n\nThen, assuming we didn't find a character encoding in the HTTP headers\n(and the HTTP Content-type allowed us to look in the body), we need\nto sniff the first few bytes of the XML data and try to determine\nwhether the encoding is ASCII-compatible.  Section F of the XML\nspecification shows the way here:\nhttp://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\n\nIf the sniffed encoding is not ASCII-compatible, we need to make it\nASCII compatible so that we can sniff further into the XML declaration\nto find the encoding attribute, which will tell us the true encoding.\n\nOf course, none of this guarantees that we will be able to parse the\nfeed in the declared character encoding (assuming it was declared\ncorrectly, which many are not).  CJKCodecs and iconv_codec help a lot;\nyou should definitely install them if you can.\nhttp://cjkpython.i18n.org/\n'''\n\n", "func_signal": "def _getCharacterEncoding(http_headers, xml_data):\n", "code": "def _parseHTTPContentType(content_type):\n    '''takes HTTP Content-Type header and returns (content type, charset)\n\n    If no charset is specified, returns (content type, '')\n    If no content type is specified, returns ('', '')\n    Both return parameters are guaranteed to be lowercase strings\n    '''\n    content_type = content_type or ''\n    content_type, params = cgi.parse_header(content_type)\n    return content_type, params.get('charset', '').replace(\"'\", '')\n\nsniffed_xml_encoding = ''\nxml_encoding = ''\ntrue_encoding = ''\nhttp_content_type, http_encoding = _parseHTTPContentType(http_headers.get('content-type'))\n# Must sniff for non-ASCII-compatible character encodings before\n# searching for XML declaration.  This heuristic is defined in\n# section F of the XML specification:\n# http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info\ntry:\n    if xml_data[:4] == '\\x4c\\x6f\\xa7\\x94':\n        # EBCDIC\n        xml_data = _ebcdic_to_ascii(xml_data)\n    elif xml_data[:4] == '\\x00\\x3c\\x00\\x3f':\n        # UTF-16BE\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xfe\\xff') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16BE with BOM\n        sniffed_xml_encoding = 'utf-16be'\n        xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x3f\\x00':\n        # UTF-16LE\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')\n    elif (len(xml_data) >= 4) and (xml_data[:2] == '\\xff\\xfe') and (xml_data[2:4] != '\\x00\\x00'):\n        # UTF-16LE with BOM\n        sniffed_xml_encoding = 'utf-16le'\n        xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\x00\\x3c':\n        # UTF-32BE\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\x3c\\x00\\x00\\x00':\n        # UTF-32LE\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')\n    elif xml_data[:4] == '\\x00\\x00\\xfe\\xff':\n        # UTF-32BE with BOM\n        sniffed_xml_encoding = 'utf-32be'\n        xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')\n    elif xml_data[:4] == '\\xff\\xfe\\x00\\x00':\n        # UTF-32LE with BOM\n        sniffed_xml_encoding = 'utf-32le'\n        xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')\n    elif xml_data[:3] == '\\xef\\xbb\\xbf':\n        # UTF-8 with BOM\n        sniffed_xml_encoding = 'utf-8'\n        xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')\n    else:\n        # ASCII-compatible\n        pass\n    xml_encoding_match = re.compile('^<\\?.*encoding=[\\'\"](.*?)[\\'\"].*\\?>').match(xml_data)\nexcept:\n    xml_encoding_match = None\nif xml_encoding_match:\n    xml_encoding = xml_encoding_match.groups()[0].lower()\n    if sniffed_xml_encoding and (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode', 'iso-10646-ucs-4', 'ucs-4', 'csucs4', 'utf-16', 'utf-32', 'utf_16', 'utf_32', 'utf16', 'u16')):\n        xml_encoding = sniffed_xml_encoding\nacceptable_content_type = 0\napplication_content_types = ('application/xml', 'application/xml-dtd', 'application/xml-external-parsed-entity')\ntext_content_types = ('text/xml', 'text/xml-external-parsed-entity')\nif (http_content_type in application_content_types) or \\\n   (http_content_type.startswith('application/') and http_content_type.endswith('+xml')):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or xml_encoding or 'utf-8'\nelif (http_content_type in text_content_types) or \\\n     (http_content_type.startswith('text/')) and http_content_type.endswith('+xml'):\n    acceptable_content_type = 1\n    true_encoding = http_encoding or 'us-ascii'\nelif http_content_type.startswith('text/'):\n    true_encoding = http_encoding or 'us-ascii'\nelif http_headers and (not http_headers.has_key('content-type')):\n    true_encoding = xml_encoding or 'iso-8859-1'\nelse:\n    true_encoding = xml_encoding or 'utf-8'\nreturn true_encoding, http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "\"\"\"\nView Stream\n\nGrabs last NUM_ENTRIES entries for all sources.\n\nTemplate:  ```vz_stream/stream_view.html```\nContext:\n    entries\n        Last NUM_ENTRIES Entry queryset\n\"\"\"\n", "func_signal": "def view_stream(request, num_entries=None, template='vz_stream/stream_view.html', mimetype='text/html'):\n", "code": "if num_entries is None:\n    num_entries = STREAM_NUM_ENTRIES\nentries = Entry.objects.filter(source__enabled=True).values_list(\n    'source__name',\n    'text',\n    'url',\n    'created_on'\n)[0:num_entries]\nif mimetype == 'application/json':\n    import simplejson\n    entries_list = []\n    for source, text, url, created_on in entries:\n        entries_list.append({\n            'source': source,\n            'text': text,\n            'url': url,\n            'created_on': created_on.isoformat()\n        })\n    return HttpResponse(simplejson.dumps(entries_list), mimetype=mimetype)\n\nreturn render_to_response(\n    template,\n    { 'entries': entries },\n    context_instance=RequestContext(request)\n)", "path": "vz_stream\\views.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "\"\"\"\nStream Stats view\n\nTemplate: ``vz_stream/stream_stats.html``\nContext:\n    sources\n        Queryset of all Source objects.\n    date\n        Date: year and month for stats.\n    ebs_total\n        List of Source objects with Entry count for all time.\n    month_data\n        List of Source objects with Entry count for Year and Month\n    day_data\n        List of Sources with Entry count per day for Year Month\n    day_range\n        Range of days in month from 1 - number of days in month\n\"\"\"\n", "func_signal": "def stream_stats(request, year=None, month=None, template='vz_stream/stream_stats.html'):\n", "code": "sources = get_list_or_404(Source)\n\ntoday = datetime.date.today()\nif int(year) > today.year:\n    year = None\n\nif year is None:\n    year = today.year\nif month is None:\n    month = today.month\n\ndate = datetime.date(month=int(month), year=int(year), day=1)\n\nmonth_data = Entry.objects.filter(\n    created_on__year=date.year,\n    created_on__month=date.month\n).values('source__name').annotate(count=Count('source')).order_by().select_related()\n\nday_data = dict()\nday_range = range(1, calendar.monthrange(date.year, date.month)[1]+1)\nfor day in day_range:\n    if day > 0:\n        for source in sources:\n            if source.name not in day_data:\n                day_data[source.name] = dict()\n            day_data[source.name][day] = Entry.objects.filter(\n                    source=source,\n                    created_on__year=date.year,\n                    created_on__month=date.month,\n                    created_on__day=day\n                ).count()\n        if 'all sources' not in day_data:\n            day_data['all sources'] = dict()\n        day_data['all sources'][day] = Entry.objects.filter(\n            created_on__year=date.year,\n            created_on__month=date.month,\n            created_on__day=day\n        ).count()            \n\nreturn render_to_response(\n    template,\n    {\n        'sources': sources,\n        'date': date,\n        'month_data': month_data,\n        'day_data': day_data,\n        'day_range': day_range\n    },\n    context_instance=RequestContext(request)\n)", "path": "vz_stream\\views.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "'''Parse a string according to the Nate 8-bit date format'''\n", "func_signal": "def _parse_date_nate(dateString):\n", "code": "m = _korean_nate_date_re.match(dateString)\nif not m: return\nhour = int(m.group(5))\nampm = m.group(4)\nif (ampm == _korean_pm):\n    hour += 12\nhour = str(hour)\nif len(hour) == 1:\n    hour = '0' + hour\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': hour, 'minute': m.group(6), 'second': m.group(7),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('Nate date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "'''Parse a string according to the MS SQL date format'''\n", "func_signal": "def _parse_date_mssql(dateString):\n", "code": "m = _mssql_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('MS SQL date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "# called for each end tag, e.g. for </pre>, tag will be 'pre'\n# Reconstruct the original end tag.\n", "func_signal": "def unknown_endtag(self, tag):\n", "code": "if tag not in self.elements_no_end_tag:\n    self.pieces.append(\"</%(tag)s>\" % locals())", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n# Store the original text verbatim.\n", "func_signal": "def handle_data(self, text):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, handle_text, text=%s\\n' % text)\nself.pieces.append(text)", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "'''Parse a variety of ISO-8601-compatible formats like 20040105'''\n", "func_signal": "def _parse_date_iso8601(dateString):\n", "code": "m = None\nfor _iso8601_match in _iso8601_matches:\n    m = _iso8601_match(dateString)\n    if m: break\nif not m: return\nif m.span() == (0, 0): return\nparams = m.groupdict()\nordinal = params.get('ordinal', 0)\nif ordinal:\n    ordinal = int(ordinal)\nelse:\n    ordinal = 0\nyear = params.get('year', '--')\nif not year or year == '--':\n    year = time.gmtime()[0]\nelif len(year) == 2:\n    # ISO 8601 assumes current century, i.e. 93 -> 2093, NOT 1993\n    year = 100 * int(time.gmtime()[0] / 100) + int(year)\nelse:\n    year = int(year)\nmonth = params.get('month', '-')\nif not month or month == '-':\n    # ordinals are NOT normalized by mktime, we simulate them\n    # by setting month=1, day=ordinal\n    if ordinal:\n        month = 1\n    else:\n        month = time.gmtime()[1]\nmonth = int(month)\nday = params.get('day', 0)\nif not day:\n    # see above\n    if ordinal:\n        day = ordinal\n    elif params.get('century', 0) or \\\n             params.get('year', 0) or params.get('month', 0):\n        day = 1\n    else:\n        day = time.gmtime()[2]\nelse:\n    day = int(day)\n# special case of the century - is the first year of the 21st century\n# 2000 or 2001 ? The debate goes on...\nif 'century' in params.keys():\n    year = (int(params['century']) - 1) * 100 + 1\n# in ISO 8601 most fields are optional\nfor field in ['hour', 'minute', 'second', 'tzhour', 'tzmin']:\n    if not params.get(field, None):\n        params[field] = 0\nhour = int(params.get('hour', 0))\nminute = int(params.get('minute', 0))\nsecond = int(params.get('second', 0))\n# weekday is normalized by mktime(), we can ignore it\nweekday = 0\n# daylight savings is complex, but not needed for feedparser's purposes\n# as time zones, if specified, include mention of whether it is active\n# (e.g. PST vs. PDT, CET). Using -1 is implementation-dependent and\n# and most implementations have DST bugs\ndaylight_savings_flag = 0\ntm = [year, month, day, hour, minute, second, weekday,\n      ordinal, daylight_savings_flag]\n# ISO 8601 time zone adjustments\ntz = params.get('tz')\nif tz and tz != 'Z':\n    if tz[0] == '-':\n        tm[3] += int(params.get('tzhour', 0))\n        tm[4] += int(params.get('tzmin', 0))\n    elif tz[0] == '+':\n        tm[3] -= int(params.get('tzhour', 0))\n        tm[4] -= int(params.get('tzmin', 0))\n    else:\n        return None\n# Python's time.mktime() is a wrapper around the ANSI C mktime(3c)\n# which is guaranteed to normalize d/m/y/h/m/s.\n# Many implementations have bugs, but we'll pretend they don't.\nreturn time.localtime(time.mktime(tm))", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "\"\"\"URL, filename, or string --> stream\n\nThis function lets you define parsers that take any input source\n(URL, pathname to local or network file, or actual data as a string)\nand deal with it in a uniform manner.  Returned object is guaranteed\nto have all the basic stdio read methods (read, readline, readlines).\nJust .close() the object when you're done with it.\n\nIf the etag argument is supplied, it will be used as the value of an\nIf-None-Match request header.\n\nIf the modified argument is supplied, it must be a tuple of 9 integers\nas returned by gmtime() in the standard Python time module. This MUST\nbe in GMT (Greenwich Mean Time). The formatted date/time will be used\nas the value of an If-Modified-Since request header.\n\nIf the agent argument is supplied, it will be used as the value of a\nUser-Agent request header.\n\nIf the referrer argument is supplied, it will be used as the value of a\nReferer[sic] request header.\n\nIf handlers is supplied, it is a list of handlers used to build a\nurllib2 opener.\n\"\"\"\n\n", "func_signal": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers):\n", "code": "if hasattr(url_file_stream_or_string, 'read'):\n    return url_file_stream_or_string\n\nif url_file_stream_or_string == '-':\n    return sys.stdin\n\nif urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):\n    if not agent:\n        agent = USER_AGENT\n    # test for inline user:password for basic auth\n    auth = None\n    if base64:\n        urltype, rest = urllib.splittype(url_file_stream_or_string)\n        realhost, rest = urllib.splithost(rest)\n        if realhost:\n            user_passwd, realhost = urllib.splituser(realhost)\n            if user_passwd:\n                url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)\n                auth = base64.encodestring(user_passwd).strip()\n    # try to open with urllib2 (to use optional headers)\n    request = urllib2.Request(url_file_stream_or_string)\n    request.add_header('User-Agent', agent)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        # format into an RFC 1123-compliant timestamp. We can't use\n        # time.strftime() since the %a and %b directives can be affected\n        # by the current locale, but RFC 2616 states that dates must be\n        # in English.\n        short_weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n        request.add_header('If-Modified-Since', '%s, %02d %s %04d %02d:%02d:%02d GMT' % (short_weekdays[modified[6]], modified[2], months[modified[1] - 1], modified[0], modified[3], modified[4], modified[5]))\n    if referrer:\n        request.add_header('Referer', referrer)\n    if gzip and zlib:\n        request.add_header('Accept-encoding', 'gzip, deflate')\n    elif gzip:\n        request.add_header('Accept-encoding', 'gzip')\n    elif zlib:\n        request.add_header('Accept-encoding', 'deflate')\n    else:\n        request.add_header('Accept-encoding', '')\n    if auth:\n        request.add_header('Authorization', 'Basic %s' % auth)\n    if ACCEPT_HEADER:\n        request.add_header('Accept', ACCEPT_HEADER)\n    request.add_header('A-IM', 'feed') # RFC 3229 support\n    opener = apply(urllib2.build_opener, tuple([_FeedURLHandler()] + handlers))\n    opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent\n    try:\n        return opener.open(request)\n    finally:\n        opener.close() # JohnD\n\n# try to open with native open function (if url_file_stream_or_string is a filename)\ntry:\n    return open(url_file_stream_or_string)\nexcept:\n    pass\n\n# treat url_file_stream_or_string as string\nreturn _StringIO(str(url_file_stream_or_string))", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "# called for each character reference, e.g. for '&#160;', ref will be '160'\n", "func_signal": "def handle_charref(self, ref):\n", "code": "if not self.elementstack: return\nref = ref.lower()\nif ref in ('34', '38', '39', '60', '62', 'x22', 'x26', 'x27', 'x3c', 'x3e'):\n    text = '&#%s;' % ref\nelse:\n    if ref[0] == 'x':\n        c = int(ref[1:], 16)\n    else:\n        c = int(ref)\n    text = unichr(c).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "'''Parse a string according to the OnBlog 8-bit date format'''\n", "func_signal": "def _parse_date_onblog(dateString):\n", "code": "m = _korean_onblog_date_re.match(dateString)\nif not m: return\nw3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \\\n            {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\\\n             'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\\\n             'zonediff': '+09:00'}\nif _debug: sys.stderr.write('OnBlog date parsed as: %s\\n' % w3dtfdate)\nreturn _parse_date_w3dtf(w3dtfdate)", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "'''Parse a string according to a Greek 8-bit date format.'''\n", "func_signal": "def _parse_date_greek(dateString):\n", "code": "m = _greek_date_format_re.match(dateString)\nif not m: return\ntry:\n    wday = _greek_wdays[m.group(1)]\n    month = _greek_months[m.group(3)]\nexcept:\n    return\nrfc822date = '%(wday)s, %(day)s %(month)s %(year)s %(hour)s:%(minute)s:%(second)s %(zonediff)s' % \\\n             {'wday': wday, 'day': m.group(2), 'month': month, 'year': m.group(4),\\\n              'hour': m.group(5), 'minute': m.group(6), 'second': m.group(7),\\\n              'zonediff': m.group(8)}\nif _debug: sys.stderr.write('Greek date parsed as: %s\\n' % rfc822date)\nreturn _parse_date_rfc822(rfc822date)", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "'''Strips DOCTYPE from XML document, returns (rss_version, stripped_data)\n\nrss_version may be 'rss091n' or None\nstripped_data is the same XML document, minus the DOCTYPE\n'''\n", "func_signal": "def _stripDoctype(data):\n", "code": "entity_pattern = re.compile(r'<!ENTITY([^>]*?)>', re.MULTILINE)\ndata = entity_pattern.sub('', data)\ndoctype_pattern = re.compile(r'<!DOCTYPE([^>]*?)>', re.MULTILINE)\ndoctype_results = doctype_pattern.findall(data)\ndoctype = doctype_results and doctype_results[0] or ''\nif doctype.lower().count('netscape'):\n    version = 'rss091n'\nelse:\n    version = None\ndata = doctype_pattern.sub('', data)\nreturn version, data", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "# override internal declaration handler to handle CDATA blocks\n", "func_signal": "def parse_declaration(self, i):\n", "code": "if _debug: sys.stderr.write('entering parse_declaration\\n')\nif self.rawdata[i:i+9] == '<![CDATA[':\n    k = self.rawdata.find(']]>', i)\n    if k == -1: k = len(self.rawdata)\n    self.handle_data(_xmlescape(self.rawdata[i+9:k]), 0)\n    return k+3\nelse:\n    k = self.rawdata.find('>', i)\n    return k+1", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "'''Return the Time Zone Designator as an offset in seconds from UTC.'''\n", "func_signal": "def __extract_tzd(m):\n", "code": "if not m:\n    return 0\ntzd = m.group('tzd')\nif not tzd:\n    return 0\nif tzd == 'Z':\n    return 0\nhours = int(m.group('tzdhours'))\nminutes = m.group('tzdminutes')\nif minutes:\n    minutes = int(minutes)\nelse:\n    minutes = 0\noffset = (hours*60 + minutes) * 60\nif tzd[0] == '+':\n    return -offset\nreturn offset", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "'''Parse an RFC822, RFC1123, RFC2822, or asctime-style date'''\n", "func_signal": "def _parse_date_rfc822(dateString):\n", "code": "data = dateString.split()\nif data[0][-1] in (',', '.') or data[0].lower() in rfc822._daynames:\n    del data[0]\nif len(data) == 4:\n    s = data[3]\n    i = s.find('+')\n    if i > 0:\n        data[3:] = [s[:i], s[i+1:]]\n    else:\n        data.append('')\n    dateString = \" \".join(data)\nif len(data) < 5:\n    dateString += ' 00:00:00 GMT'\ntm = rfc822.parsedate_tz(dateString)\nif tm:\n    return time.gmtime(rfc822.mktime_tz(tm))", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "# called for each block of plain text, i.e. outside of any tag and\n# not containing any character or entity references\n", "func_signal": "def handle_data(self, text, escape=1):\n", "code": "if not self.elementstack: return\nif escape and self.contentparams.get('type') == 'application/xhtml+xml':\n    text = _xmlescape(text)\nself.elementstack[-1][2].append(text)", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "# called for each start tag\n# attrs is a list of (attr, value) tuples\n# e.g. for <pre class='screen'>, tag='pre', attrs=[('class', 'screen')]\n", "func_signal": "def unknown_starttag(self, tag, attrs):\n", "code": "if _debug: sys.stderr.write('_BaseHTMLProcessor, unknown_starttag, tag=%s\\n' % tag)\nuattrs = []\n# thanks to Kevin Marks for this breathtaking hack to deal with (valid) high-bit attribute values in UTF-8 feeds\nfor key, value in attrs:\n    if type(value) != type(u''):\n        value = unicode(value, self.encoding)\n    uattrs.append((unicode(key, self.encoding), value))\nstrattrs = u''.join([u' %s=\"%s\"' % (key, value) for key, value in uattrs]).encode(self.encoding)\nif tag in self.elements_no_end_tag:\n    self.pieces.append('<%(tag)s%(strattrs)s />' % locals())\nelse:\n    self.pieces.append('<%(tag)s%(strattrs)s>' % locals())", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "# called for each entity reference, e.g. for '&copy;', ref will be 'copy'\n", "func_signal": "def handle_entityref(self, ref):\n", "code": "if not self.elementstack: return\nif _debug: sys.stderr.write('entering handle_entityref with %s\\n' % ref)\nif ref in ('lt', 'gt', 'quot', 'amp', 'apos'):\n    text = '&%s;' % ref\nelse:\n    # entity resolution graciously donated by Aaron Swartz\n    def name2cp(k):\n        import htmlentitydefs\n        if hasattr(htmlentitydefs, 'name2codepoint'): # requires Python 2.3\n            return htmlentitydefs.name2codepoint[k]\n        k = htmlentitydefs.entitydefs[k]\n        if k.startswith('&#') and k.endswith(';'):\n            return int(k[2:-1]) # not in latin-1\n        return ord(k)\n    try: name2cp(ref)\n    except KeyError: text = '&%s;' % ref\n    else: text = unichr(name2cp(ref)).encode('utf-8')\nself.elementstack[-1][2].append(text)", "path": "vz_stream\\utils\\feedparser.py", "repo_name": "jobscry/vz-stream", "stars": 5, "license": "mit", "language": "python", "size": 148}
{"docstring": "\"\"\"Verifies an api call and checks all the parameters.\"\"\"\n# -> consumer and token\n", "func_signal": "def verify_request(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\n# Get the access token.\ntoken = self._get_token(oauth_request, 'access')\nself._check_signature(oauth_request, consumer, token)\nparameters = oauth_request.get_nonoauth_parameters()\nreturn consumer, token, parameters", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Processes an access_token request and returns the\naccess token on success.\n\"\"\"\n", "func_signal": "def fetch_access_token(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\ntry:\n    verifier = self._get_verifier(oauth_request)\nexcept OAuthError:\n    verifier = None\n# Get the request token.\ntoken = self._get_token(oauth_request, 'request')\nself._check_signature(oauth_request, consumer, token)\nnew_token = self.data_store.fetch_access_token(consumer, token, verifier)\nreturn new_token", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Turn Authorization: header into parameters.\"\"\"\n", "func_signal": "def _split_header(header):\n", "code": "params = {}\nparts = header.split(',')\nfor param in parts:\n    # Ignore realm parameter.\n    if param.find('realm') > -1:\n        continue\n    # Remove whitespace.\n    param = param.strip()\n    # Split key-value.\n    param_parts = param.split('=', 1)\n    # Remove quotes and unescape the value.\n    params[param_parts[0]] = urllib.unquote(param_parts[1].strip('\\\"'))\nreturn params", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Concatenates the consumer key and secret.\"\"\"\n", "func_signal": "def build_signature_base_string(self, oauth_request, consumer, token):\n", "code": "sig = '%s&' % escape(consumer.secret)\nif token:\n    sig = sig + escape(token.secret)\nreturn sig, sig", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Convert unicode to utf-8.\"\"\"\n", "func_signal": "def _utf8_str(s):\n", "code": "if isinstance(s, unicode):\n    return s.encode(\"utf-8\")\nelse:\n    return str(s)", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Serialize as a header for an HTTPAuth request.\"\"\"\n", "func_signal": "def to_header(self, realm=''):\n", "code": "auth_header = 'OAuth realm=\"%s\"' % realm\n# Add the oauth parameters.\nif self.parameters:\n    for k, v in self.parameters.iteritems():\n        if k[:6] == 'oauth_':\n            auth_header += ', %s=\"%s\"' % (k, escape(str(v)))\nreturn {'Authorization': auth_header}", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Get any non-OAuth parameters.\"\"\"\n", "func_signal": "def get_nonoauth_parameters(self):\n", "code": "parameters = {}\nfor k, v in self.parameters.iteritems():\n    # Ignore oauth parameters.\n    if k.find('oauth_') < 0:\n        parameters[k] = v\nreturn parameters", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Set the signature parameter to the result of build_signature.\"\"\"\n# Set the signature method.\n", "func_signal": "def sign_request(self, signature_method, consumer, token):\n", "code": "self.set_parameter('oauth_signature_method',\n    signature_method.get_name())\n# Set the signature.\nself.set_parameter('oauth_signature',\n    self.build_signature(signature_method, consumer, token))", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Processes a request_token request and returns the\nrequest token on success.\n\"\"\"\n", "func_signal": "def fetch_request_token(self, oauth_request):\n", "code": "try:\n    # Get the request token for authorization.\n    token = self._get_token(oauth_request, 'request')\nexcept OAuthError:\n    # No token required for the initial token request.\n    version = self._get_version(oauth_request)\n    consumer = self._get_consumer(oauth_request)\n    try:\n        callback = self.get_callback(oauth_request)\n    except OAuthError:\n        callback = None # 1.0, no callback specified.\n    self._check_signature(oauth_request, consumer, None)\n    # Fetch a new token.\n    token = self.data_store.fetch_request_token(consumer, callback)\nreturn token", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\" Returns a token from something like:\noauth_token_secret=xxx&oauth_token=xxx\n\"\"\"\n", "func_signal": "def from_string(s):\n", "code": "params = cgi.parse_qs(s, keep_blank_values=False)\nkey = params['oauth_token'][0]\nsecret = params['oauth_token_secret'][0]\ntoken = OAuthToken(key, secret)\ntry:\n    token.callback_confirmed = params['oauth_callback_confirmed'][0]\nexcept KeyError:\n    pass # 1.0, no callback confirmed.\nreturn token", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Verify that timestamp is recentish.\"\"\"\n", "func_signal": "def _check_timestamp(self, timestamp):\n", "code": "timestamp = int(timestamp)\nnow = int(time.time())\nlapsed = abs(now - timestamp)\nif lapsed > self.timestamp_threshold:\n    raise OAuthError('Expired timestamp: given %d and now %s has a '\n        'greater difference than threshold %d' %\n        (timestamp, now, self.timestamp_threshold))", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Verify that the nonce is uniqueish.\"\"\"\n", "func_signal": "def _check_nonce(self, consumer, token, nonce):\n", "code": "nonce = self.data_store.lookup_nonce(consumer, token, nonce)\nif nonce:\n    raise OAuthError('Nonce already used: %s' % str(nonce))", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Try to find the token for the provided request token key.\"\"\"\n", "func_signal": "def _get_token(self, oauth_request, token_type='access'):\n", "code": "token_field = oauth_request.get_parameter('oauth_token')\ntoken = self.data_store.lookup_token(token_type, token_field)\nif not token:\n    raise OAuthError('Invalid %s token: %s' % (token_type, token_field))\nreturn token", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Parses the URL and rebuilds it to be scheme://host/path.\"\"\"\n", "func_signal": "def get_normalized_http_url(self):\n", "code": "parts = urlparse.urlparse(self.http_url)\nscheme, netloc, path = parts[:3]\n# Exclude default port numbers.\nif scheme == 'http' and netloc[-3:] == ':80':\n    netloc = netloc[:-3]\nelif scheme == 'https' and netloc[-4:] == ':443':\n    netloc = netloc[:-4]\nreturn '%s://%s%s' % (scheme, netloc, path)", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Verify the correct version request for this server.\"\"\"\n", "func_signal": "def _get_version(self, oauth_request):\n", "code": "try:\n    version = oauth_request.get_parameter('oauth_version')\nexcept:\n    version = VERSION\nif version and version != self.version:\n    raise OAuthError('OAuth version %s not supported.' % str(version))\nreturn version", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Turn URL string into parameters.\"\"\"\n", "func_signal": "def _split_url_string(param_str):\n", "code": "parameters = cgi.parse_qs(param_str, keep_blank_values=False)\nfor k, v in parameters.iteritems():\n    parameters[k] = urllib.unquote(v[0])\nreturn parameters", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "# send a 401 error\n", "func_signal": "def send_oauth_error(self, err=None):\n", "code": "self.send_error(401, str(err.message))\n# return the authenticate header\nheader = oauth.build_authenticate_header(realm=REALM)\nfor k, v in header.iteritems():\n    self.send_header(k, v)", "path": "example\\server.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Builds the base signature string.\"\"\"\n", "func_signal": "def build_signature(self, oauth_request, consumer, token):\n", "code": "key, raw = self.build_signature_base_string(oauth_request, consumer,\n    token)\n\n# HMAC object.\ntry:\n    import hashlib # 2.5\n    hashed = hmac.new(key, raw, hashlib.sha1)\nexcept:\n    import sha # Deprecated\n    hashed = hmac.new(key, raw, sha)\n\n# Calculate the digest base 64.\nreturn binascii.b2a_base64(hashed.digest())[:-1]", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Figure out the signature with some defaults.\"\"\"\n", "func_signal": "def _get_signature_method(self, oauth_request):\n", "code": "try:\n    signature_method = oauth_request.get_parameter(\n        'oauth_signature_method')\nexcept:\n    signature_method = SIGNATURE_METHOD\ntry:\n    # Get the signature method object.\n    signature_method = self.signature_methods[signature_method]\nexcept:\n    signature_method_names = ', '.join(self.signature_methods.keys())\n    raise OAuthError('Signature method %s not supported try one of the '\n        'following: %s' % (signature_method, signature_method_names))\n\nreturn signature_method", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Serialize as post data for a POST request.\"\"\"\n", "func_signal": "def to_postdata(self):\n", "code": "return '&'.join(['%s=%s' % (escape(str(k)), escape(str(v))) \\\n    for k, v in self.parameters.iteritems()])", "path": "oauth.py", "repo_name": "clones/python-oauth", "stars": 4, "license": "None", "language": "python", "size": 130}
{"docstring": "\"\"\"Handles HTTP GET request.\"\"\"\n", "func_signal": "def get(self):\n", "code": "self.response.headers['Content-Type'] = 'application/json'\nself.response.out.write(self._robot.GetProfileJson())", "path": "waveapi\\robot.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Parse a JSON string and return a context and an event list.\"\"\"\n", "func_signal": "def ParseJSONBody(json_body):\n", "code": "json = simplejson.loads(json_body)\n# TODO(davidbyttow): Remove this once no longer needed.\ndata = util.CollapseJavaCollections(json)\ncontext = ops.CreateContext(data)\nevent_list = [model.Event(event_data) for event_data in data['events']]\nreturn context, event_list", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Collapses the unnecessary extra data structures in the wire format.\n\nCurrently the wire format is built from marshalling of Java objects. This\nintroduces overhead of extra key/value pairs with respect to collections and\nsuperfluous fields. As such, this method attempts to collapse those structures\nout of the data format by collapsing the collection objects and removing\nthe java class fields.\n\nThis preserves the data that is passed in and only removes the collection\ntypes.\n\nArgs:\n  data: Some arbitrary dict, list or primitive type.\n\nReturns:\n  The same data structure with the collapsed and unnecessary objects\n  removed.\n\"\"\"\n", "func_signal": "def CollapseJavaCollections(data):\n", "code": "if IsDict(data):\n  java_class = data.get('javaClass')\n  if java_class:\n    del data['javaClass']\n  if java_class == 'java.util.HashMap':\n    return CollapseJavaCollections(data['map'])\n  elif java_class == 'java.util.ArrayList':\n    return CollapseJavaCollections(data['list'])\n  for key, val in data.iteritems():\n    data[key] = CollapseJavaCollections(val)\nelif IsListOrDict(data):\n  for index in range(len(data)):\n    data[index] = CollapseJavaCollections(data[index])\n  return data\nreturn data", "path": "waveapi\\util.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"\nDeserialize ``s`` (a ``str`` or ``unicode`` instance containing a JSON\ndocument) to a Python object.\n\nIf ``s`` is a ``str`` instance and is encoded with an ASCII based encoding\nother than utf-8 (e.g. latin-1) then an appropriate ``encoding`` name\nmust be specified.  Encodings that are not ASCII based (such as UCS-2)\nare not allowed and should be decoded to ``unicode`` first.\n\n``object_hook`` is an optional function that will be called with the\nresult of any object literal decode (a ``dict``).  The return value of\n``object_hook`` will be used instead of the ``dict``.  This feature\ncan be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\nTo use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\nkwarg.\n\"\"\"\n", "func_signal": "def loads(s, encoding=None, cls=None, object_hook=None, **kw):\n", "code": "if cls is None:\n    cls = JSONDecoder\nif object_hook is not None:\n    kw['object_hook'] = object_hook\nreturn cls(encoding=encoding, **kw).decode(s)", "path": "waveapi\\simplejson\\__init__.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Registers all event handlers exported by the given object.\n\nArgs:\n  listener: an object with methods corresponding to wave events.\n    Methods should be named either in camel case, e.g. 'OnBlipSubmitted',\n    or in lowercase, e.g. 'on_blip_submitted', with names corresponding\n    to the event names in the events module.\n\"\"\"\n", "func_signal": "def RegisterListener(self, listener):\n", "code": "for event in dir(events):\n  if event.startswith('_'):\n    continue\n  lowercase_method_name = 'on_' + event.lower()\n  camelcase_method_name = 'On' + util.ToUpperCamelCase(event)\n  if hasattr(listener, lowercase_method_name):\n    handler = getattr(listener, lowercase_method_name)\n  elif hasattr(listener, camelcase_method_name):\n    handler = getattr(listener, camelcase_method_name)\n  else:\n    continue\n  if callable(handler):\n    self.RegisterHandler(event, handler)", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Initializes self with robot information.\"\"\"\n", "func_signal": "def __init__(self, name, version, image_url='', profile_url=''):\n", "code": "self._handlers = {}\nself.name = name\nself.version = version\nself.image_url = image_url\nself.profile_url = profile_url\nself.cron_jobs = []", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"\njson-py API compatibility hook.  Use loads(s) instead.\n\"\"\"\n", "func_signal": "def read(s):\n", "code": "import warnings\nwarnings.warn(\"simplejson.loads(s) should be used instead of read(s)\",\n    DeprecationWarning)\nreturn loads(s)", "path": "waveapi\\simplejson\\__init__.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Returns JSON body for any profile handler.\n\nReturns:\n  String of JSON to be sent as a response.\n\"\"\"\n", "func_signal": "def GetProfileJson(self):\n", "code": "data = {}\ndata['name'] = self.name\ndata['imageUrl'] = self.image_url\ndata['profileUrl'] = self.profile_url\n# TODO(davidbyttow): Remove this java nonsense.\ndata['javaClass'] = 'com.google.wave.api.ParticipantProfile'\nreturn simplejson.dumps(data)", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Handles the get event for debugging. Ops usually too long.\"\"\"\n", "func_signal": "def get(self):\n", "code": "ops = self.request.get('ops')\nlogging.info('get: ' + ops)\nif ops:\n  self.request.body = ops\n  self.post()\n  self.response.headers['Content-Type'] = 'text/html'", "path": "waveapi\\robot.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Invokes serialize on all of its key/value pairs.\n\nArgs:\n  d: The dict instance to serialize.\n  key_writer: Optional key writer function.\n\nReturns:\n  The serialized dict.\n\"\"\"\n", "func_signal": "def _SerializeDict(d, key_writer=DefaultKeyWriter):\n", "code": "data = {}\nfor k, v in d.iteritems():\n  data[key_writer(k)] = Serialize(v)\nreturn {\n    'javaClass': 'java.util.HashMap',\n    'map': data\n}", "path": "waveapi\\util.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Return this robot's capabilities as an XML string.\"\"\"\n", "func_signal": "def GetCapabilitiesXml(self):\n", "code": "lines = ['<w:version>%s</w:version>' % self.version]\n\nlines.append('<w:capabilities>')\nfor capability in self._handlers:\n  lines.append('  <w:capability name=\"%s\"/>' % capability)\nlines.append('</w:capabilities>')\n\nif self.cron_jobs:\n  lines.append('<w:crons>')\n  for job in self.cron_jobs:\n    lines.append('  <w:cron path=\"%s\" timerinseconds=\"%s\"/>' % job)\n  lines.append('</w:crons>')\n\nrobot_attrs = ' name=\"%s\"' % self.name\nif self.image_url:\n  robot_attrs += ' imageurl=\"%s\"' % self.image_url\nif self.profile_url:\n  robot_attrs += ' profileurl=\"%s\"' % self.profile_url\nlines.append('<w:profile%s/>' % robot_attrs)\nreturn ('<?xml version=\"1.0\"?>\\n'\n        '<w:robot xmlns:w=\"http://wave.google.com/extensions/robots/1.0\">\\n'\n        '%s\\n</w:robot>\\n') % ('\\n'.join(lines))", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Handles HTTP GET request.\"\"\"\n", "func_signal": "def get(self):\n", "code": "xml = self._robot.GetCapabilitiesXml()\nself.response.headers['Content-Type'] = 'text/xml'\nself.response.out.write(xml)", "path": "waveapi\\robot.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"\njson-py API compatibility hook.  Use dumps(s) instead.\n\"\"\"\n", "func_signal": "def write(obj):\n", "code": "import warnings\nwarnings.warn(\"simplejson.dumps(s) should be used instead of write(s)\",\n    DeprecationWarning)\nreturn dumps(obj)", "path": "waveapi\\simplejson\\__init__.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Handles HTTP POST requests.\"\"\"\n", "func_signal": "def post(self):\n", "code": "json_body = self.request.body\nif not json_body:\n  # TODO(davidbyttow): Log error?\n  return\nlogging.info('Incoming: ' + json_body)\n\ncontext, events = robot_abstract.ParseJSONBody(json_body)\nfor event in events:\n  try:\n    self._robot.HandleEvent(event, context)\n  except:\n    logging.error(traceback.format_exc())\n\njson_response = robot_abstract.SerializeContext(context,\n                                                self._robot.version)\n# Build the response.\nlogging.info('Outgoing: ' + json_response)\nself.response.headers['Content-Type'] = 'application/json'\nself.response.out.write(json_response)", "path": "waveapi\\robot.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Calls all of the handlers associated with an event.\"\"\"\n", "func_signal": "def HandleEvent(self, event, context):\n", "code": "for handler in self._handlers.get(event.type, []):\n  # TODO(jacobly): pass the event in to the handlers directly\n  # instead of passing the properties dictionary.\n  handler(event.properties, context)", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Serializes any instance.\n\nIf this is a user-defined instance\ntype, it will first check for a custom Serialize() function and use that\nif it exists. Otherwise, it will invoke serialize all of its public\nattributes. Lists and dicts are serialized trivially.\n\nArgs:\n  obj: The instance to serialize.\n  key_writer: Optional key writer function.\n\nReturns:\n  The serialized object.\n\"\"\"\n", "func_signal": "def Serialize(obj, key_writer=DefaultKeyWriter):\n", "code": "if IsInstance(obj):\n  if obj and hasattr(obj, CUSTOM_SERIALIZE_METHOD_NAME):\n    method = getattr(obj, CUSTOM_SERIALIZE_METHOD_NAME)\n    if callable(method):\n      return method()\n  return _SerializeAttributes(obj, key_writer)\nelif IsDict(obj):\n  return _SerializeDict(obj, key_writer)\nelif IsListOrDict(obj):\n  return _SerializeList(obj)\nreturn obj", "path": "waveapi\\util.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Invokes Serialize on all of its elements.\n\nArgs:\n  l: The list object to serialize.\n\nReturns:\n  The serialized list.\n\"\"\"\n", "func_signal": "def _SerializeList(l):\n", "code": "data = [Serialize(v) for v in l]\nreturn {\n    'javaClass': 'java.util.ArrayList',\n    'list': data\n}", "path": "waveapi\\util.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Sets up the webapp handlers for this robot and starts listening.\n\nArgs:\n  debug: Optional variable that defaults to False and is passed through\n      to the webapp application to determine if it should show debug info.\n\"\"\"\n# App Engine expects to construct a class with no arguments, so we\n# pass a lambda that constructs the appropriate handler with\n# arguments from the enclosing scope.\n", "func_signal": "def Run(self, debug=False):\n", "code": "app = webapp.WSGIApplication([\n    ('/_wave/capabilities.xml', lambda: RobotCapabilitiesHandler(self)),\n    ('/_wave/robot/profile', lambda: RobotProfileHandler(self)),\n    ('/_wave/robot/jsonrpc', lambda: RobotEventHandler(self)),\n], debug=debug)\nrun_wsgi_app(app)", "path": "waveapi\\robot.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Create a new wave with the initial participants on it.\"\"\"\n# we shouldn't need a wave/wavelet id here, but we do\n", "func_signal": "def NewWave(context, participants=None):\n", "code": "wavelet = context.GetRootWavelet()\nreturn context.builder.WaveletCreate(wavelet.GetWaveId(), wavelet.GetId(), participants)", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"Return a JSON string representing the given context.\"\"\"\n", "func_signal": "def SerializeContext(context, version):\n", "code": "context_dict = util.Serialize(context)\ncontext_dict['version'] = str(version)\nreturn simplejson.dumps(context_dict)", "path": "waveapi\\robot_abstract.py", "repo_name": "scottferg/Profanity-Modifier", "stars": 4, "license": "other", "language": "python", "size": 148}
{"docstring": "\"\"\"\nApply `pattern` on the current position and return\nthe match object. (Doesn't touch pos). Use this for\nlookahead.\n\"\"\"\n", "func_signal": "def check(self, pattern):\n", "code": "if self.eos:\n    raise EndOfText()\nif pattern not in self._re_cache:\n    self._re_cache[pattern] = re.compile(pattern, self.flags)\nreturn self._re_cache[pattern].match(self.data, self.pos)", "path": "app\\lib\\console\\app\\pygments\\scanner.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nReturn a static text analysation function that\nreturns float values.\n\"\"\"\n", "func_signal": "def make_analysator(f):\n", "code": "def text_analyse(text):\n    rv = f(text)\n    if not rv:\n        return 0.0\n    return min(1.0, max(0.0, float(rv)))\ntext_analyse.__doc__ = f.__doc__\nreturn staticmethod(text_analyse)", "path": "app\\lib\\console\\app\\pygments\\util.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nFormat ``text`` with a color and/or some attributes::\n\n    color       normal color\n    *color*     bold color\n    _color_     underlined color\n    +color+     blinking color\n\"\"\"\n", "func_signal": "def ansiformat(attr, text):\n", "code": "result = []\nif attr[:1] == attr[-1:] == '+':\n    result.append(codes['blink'])\n    attr = attr[1:-1]\nif attr[:1] == attr[-1:] == '*':\n    result.append(codes['bold'])\n    attr = attr[1:-1]\nif attr[:1] == attr[-1:] == '_':\n    result.append(codes['underline'])\n    attr = attr[1:-1]\nresult.append(codes[attr])\nresult.append(text)\nresult.append(codes['reset'])\nreturn ''.join(result)", "path": "app\\lib\\console\\app\\pygments\\console.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\n:param text:    The text which should be scanned\n:param flags:   default regular expression flags\n\"\"\"\n", "func_signal": "def __init__(self, text, flags=0):\n", "code": "self.data = text\nself.data_length = len(text)\nself.start_pos = 0\nself.pos = 0\nself.flags = flags\nself.last = None\nself.match = None\nself._re_cache = {}", "path": "app\\lib\\console\\app\\pygments\\scanner.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nGet a lexer by an alias.\n\"\"\"\n# lookup builtin lexers\n", "func_signal": "def get_lexer_by_name(_alias, **options):\n", "code": "for module_name, name, aliases, _, _ in LEXERS.itervalues():\n    if _alias in aliases:\n        if name not in _lexer_cache:\n            _load_lexers(module_name)\n        return _lexer_cache[name](**options)\n# continue with lexers from setuptools entrypoints\nfor cls in find_plugin_lexers():\n    if _alias in cls.aliases:\n        return cls(**options)\nraise ClassNotFound('no lexer for alias %r found' % _alias)", "path": "app\\lib\\console\\app\\pygments\\lexers\\__init__.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nCheck if the doctype matches a regular expression (if present).\nNote that this method only checks the first part of a DOCTYPE.\neg: 'html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"'\n\"\"\"\n", "func_signal": "def doctype_matches(text, regex):\n", "code": "m = doctype_lookup_re.match(text)\nif m is None:\n    return False\ndoctype = m.group(2)\nreturn re.compile(regex).match(doctype.strip()) is not None", "path": "app\\lib\\console\\app\\pygments\\util.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\n    Returns a typed_values value for a given string of text.\n\n    \"true\"  -> True\n    \"yes\"   -> True\n    \"no     -> False\n    \"faLSe\" -> False      <= Case does not matter in all above cases.\n\n    \"0\"          -> 0\n    \"2938423984\" -> 2938423984\n    \"23.45\"      -> Decimal(\"23.45\")\n\"\"\"\n\n", "func_signal": "def get_ebs_typed_value(v):\n", "code": "try:\n    value = parse_ebs_datetime(v)\nexcept AttributeError:\n    try:\n        value = int(v, 10)\n    except ValueError:\n        try:\n            value = Decimal(v)\n        except InvalidOperation:\n            s_v = v.lower()\n            if s_v in ['true', 'yes']:\n                value = True\n            elif s_v in ['false', 'no']:\n                value = False\n            else:\n                value = v\nreturn value", "path": "app\\lib\\ebs\\merchant\\api.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nLoad a lexer (and all others in the module too).\n\"\"\"\n", "func_signal": "def _load_lexers(module_name):\n", "code": "mod = __import__(module_name, None, None, ['__all__'])\nfor lexer_name in mod.__all__:\n    cls = getattr(mod, lexer_name)\n    _lexer_cache[cls.name] = cls", "path": "app\\lib\\console\\app\\pygments\\lexers\\__init__.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nLookup all lexers that handle those filenames primary (``filenames``)\nor secondary (``alias_filenames``). Then run a text analysis for those\nlexers and choose the best result.\n\nusage::\n\n    >>> from pygments.lexers import guess_lexer_for_filename\n    >>> guess_lexer_for_filename('hello.html', '<%= @foo %>')\n    <pygments.lexers.templates.RhtmlLexer object at 0xb7d2f32c>\n    >>> guess_lexer_for_filename('hello.html', '<h1>{{ title|e }}</h1>')\n    <pygments.lexers.templates.HtmlDjangoLexer object at 0xb7d2f2ac>\n    >>> guess_lexer_for_filename('style.css', 'a { color: <?= $link ?> }')\n    <pygments.lexers.templates.CssPhpLexer object at 0xb7ba518c>\n\"\"\"\n", "func_signal": "def guess_lexer_for_filename(_fn, _text, **options):\n", "code": "fn = basename(_fn)\nprimary = None\nmatching_lexers = set()\nfor lexer in _iter_lexerclasses():\n    for filename in lexer.filenames:\n        if fnmatch.fnmatch(fn, filename):\n            matching_lexers.add(lexer)\n            primary = lexer\n    for filename in lexer.alias_filenames:\n        if fnmatch.fnmatch(fn, filename):\n            matching_lexers.add(lexer)\nif not matching_lexers:\n    raise ClassNotFound('no lexer for filename %r found' % fn)\nif len(matching_lexers) == 1:\n    return matching_lexers.pop()(**options)\nresult = []\nfor lexer in matching_lexers:\n    rv = lexer.analyse_text(_text)\n    if rv == 1.0:\n        return lexer(**options)\n    result.append((rv, lexer))\nresult.sort()\nif not result[-1][0] and primary is not None:\n    return primary(**options)\nreturn result[-1][1](**options)", "path": "app\\lib\\console\\app\\pygments\\lexers\\__init__.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nCheck if the given regular expression matches the last part of the\nshebang if one exists.\n\n    >>> from pygments.util import shebang_matches\n    >>> shebang_matches('#!/usr/bin/env python', r'python(2\\.\\d)?')\n    True\n    >>> shebang_matches('#!/usr/bin/python2.4', r'python(2\\.\\d)?')\n    True\n    >>> shebang_matches('#!/usr/bin/python-ruby', r'python(2\\.\\d)?')\n    False\n    >>> shebang_matches('#!/usr/bin/python/ruby', r'python(2\\.\\d)?')\n    False\n    >>> shebang_matches('#!/usr/bin/startsomethingwith python',\n    ...                 r'python(2\\.\\d)?')\n    True\n\nIt also checks for common windows executable file extensions::\n\n    >>> shebang_matches('#!C:\\\\Python2.4\\\\Python.exe', r'python(2\\.\\d)?')\n    True\n\nParameters (``'-f'`` or ``'--foo'`` are ignored so ``'perl'`` does\nthe same as ``'perl -e'``)\n\nNote that this method automatically searches the whole string (eg:\nthe regular expression is wrapped in ``'^$'``)\n\"\"\"\n", "func_signal": "def shebang_matches(text, regex):\n", "code": "index = text.find('\\n')\nif index >= 0:\n    first_line = text[:index].lower()\nelse:\n    first_line = text.lower()\nif first_line.startswith('#!'):\n    try:\n        found = [x for x in split_path_re.split(first_line[2:].strip())\n                 if x and not x.startswith('-')][-1]\n    except IndexError:\n        return False\n    regex = re.compile('^%s(\\.(exe|cmd|bat|bin))?$' % regex, re.IGNORECASE)\n    if regex.search(found) is not None:\n        return True\nreturn False", "path": "app\\lib\\console\\app\\pygments\\util.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nGuess a lexer by strong distinctions in the text (eg, shebang).\n\"\"\"\n", "func_signal": "def guess_lexer(_text, **options):\n", "code": "best_lexer = [0.0, None]\nfor lexer in _iter_lexerclasses():\n    rv = lexer.analyse_text(_text)\n    if rv == 1.0:\n        return lexer(**options)\n    if rv > best_lexer[0]:\n        best_lexer[:] = (rv, lexer)\nif not best_lexer[0] or best_lexer[1] is None:\n    raise ClassNotFound('no lexer matching the text found')\nreturn best_lexer[1](**options)", "path": "app\\lib\\console\\app\\pygments\\lexers\\__init__.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nScan the text for the given pattern and update pos/match\nand related fields. The return value is a boolen that\nindicates if the pattern matched. The matched value is\nstored on the instance as ``match``, the last value is\nstored as ``last``. ``start_pos`` is the position of the\npointer before the pattern was matched, ``pos`` is the\nend position.\n\"\"\"\n", "func_signal": "def scan(self, pattern):\n", "code": "if self.eos:\n    raise EndOfText()\nif pattern not in self._re_cache:\n    self._re_cache[pattern] = re.compile(pattern, self.flags)\nself.last = self.match\nm = self._re_cache[pattern].match(self.data, self.pos)\nif m is None:\n    return False\nself.start_pos = m.start()\nself.pos = m.end()\nself.match = m.group()\nreturn True", "path": "app\\lib\\console\\app\\pygments\\scanner.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nReturn a generator of tuples in the form ``(name, aliases,\nfilenames, mimetypes)`` of all know lexers.\n\"\"\"\n", "func_signal": "def get_all_lexers():\n", "code": "for item in LEXERS.itervalues():\n    yield item[1:]\nfor lexer in find_plugin_lexers():\n    yield lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes", "path": "app\\lib\\console\\app\\pygments\\lexers\\__init__.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\n    Encrypts a body of text using Alleged-RC4 (Ron's Code 4) secret-key encryption.\n\n    key\n        secret key\n\n    data\n        data to encrypt\n\"\"\"\n", "func_signal": "def arc4_encrypt(key, data):\n", "code": "encryptor = ARC4.new(key)\nreturn encryptor.encrypt(data)", "path": "app\\lib\\ebs\\merchant\\api.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nReturn an iterator over all lexer classes.\n\"\"\"\n", "func_signal": "def _iter_lexerclasses():\n", "code": "for module_name, name, _, _, _ in LEXERS.itervalues():\n    if name not in _lexer_cache:\n        _load_lexers(module_name)\n    yield _lexer_cache[name]\nfor lexer in find_plugin_lexers():\n    yield lexer", "path": "app\\lib\\console\\app\\pygments\\lexers\\__init__.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nGet a lexer for a mimetype.\n\"\"\"\n", "func_signal": "def get_lexer_for_mimetype(_mime, **options):\n", "code": "for modname, name, _, _, mimetypes in LEXERS.itervalues():\n    if _mime in mimetypes:\n        if name not in _lexer_cache:\n            _load_lexers(modname)\n        return _lexer_cache[name](**options)\nfor cls in find_plugin_lexers():\n    if _mime in cls.mimetypes:\n        return cls(**options)\nraise ClassNotFound('no lexer for mimetype %r found' % _mime)", "path": "app\\lib\\console\\app\\pygments\\lexers\\__init__.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nSince ERB doesn't allow \"<%\" and other tags inside of ruby\nblocks we have to use a split approach here that fails for\nthat too.\n\"\"\"\n", "func_signal": "def get_tokens_unprocessed(self, text):\n", "code": "tokens = self._block_re.split(text)\ntokens.reverse()\nstate = idx = 0\ntry:\n    while True:\n        # text\n        if state == 0:\n            val = tokens.pop()\n            yield idx, Other, val\n            idx += len(val)\n            state = 1\n        # block starts\n        elif state == 1:\n            tag = tokens.pop()\n            # literals\n            if tag in ('<%%', '%%>'):\n                yield idx, Other, tag\n                idx += 3\n                state = 0\n            # comment\n            elif tag == '<%#':\n                yield idx, Comment.Preproc, tag\n                val = tokens.pop()\n                yield idx + 3, Comment, val\n                idx += 3 + len(val)\n                state = 2\n            # blocks or output\n            elif tag in ('<%', '<%=', '<%-'):\n                yield idx, Comment.Preproc, tag\n                idx += len(tag)\n                data = tokens.pop()\n                r_idx = 0\n                for r_idx, r_token, r_value in \\\n                    self.ruby_lexer.get_tokens_unprocessed(data):\n                    yield r_idx + idx, r_token, r_value\n                idx += len(data)\n                state = 2\n            elif tag in ('%>', '-%>'):\n                yield idx, Error, tag\n                idx += len(tag)\n                state = 0\n            # % raw ruby statements\n            else:\n                yield idx, Comment.Preproc, tag[0]\n                r_idx = 0\n                for r_idx, r_token, r_value in \\\n                    self.ruby_lexer.get_tokens_unprocessed(tag[1:]):\n                    yield idx + 1 + r_idx, r_token, r_value\n                idx += len(tag)\n                state = 0\n        # block ends\n        elif state == 2:\n            tag = tokens.pop()\n            if tag not in ('%>', '-%>'):\n                yield idx, Other, tag\n            else:\n                yield idx, Comment.Preproc, tag\n            idx += len(tag)\n            state = 0\nexcept IndexError:\n    return", "path": "app\\lib\\console\\app\\pygments\\lexers\\templates.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\n    Parses an EBS-provided datetime string into a Python datetime object.\n\n    Warning:\n1.  Uses regular expressions.\n2.  EBS does not send TZ info so we do not assume any timezone.\n    It is your responsibility to pick the correct one.\n\"\"\"\n", "func_signal": "def parse_ebs_datetime(datestring, compiled_re=EBS_DATESTRING_RE):\n", "code": "m = compiled_re.match(datestring)\nreturn datetime(*[int(i, 10) for i in m.groups()])", "path": "app\\lib\\ebs\\merchant\\api.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\n    Decrypts a body of text from Alleged-RC4 (Ron's Code 4) encrypted data.\n\n    key\n        secret key\n\n    data\n        data to decrypt\n\"\"\"\n", "func_signal": "def arc4_decrypt(key, data):\n", "code": "decryptor = ARC4.new(key)\nreturn decryptor.decrypt(data)", "path": "app\\lib\\ebs\\merchant\\api.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\"\nLookup a lexer class by name. Return None if not found.\n\"\"\"\n", "func_signal": "def find_lexer_class(name):\n", "code": "if name in _lexer_cache:\n    return _lexer_cache[name]\n# lookup builtin lexers\nfor module_name, lname, aliases, _, _ in LEXERS.itervalues():\n    if name == lname:\n        _load_lexers(module_name)\n        return _lexer_cache[name]\n# continue with lexers from setuptools entrypoints\nfor cls in find_plugin_lexers():\n    if cls.name == name:\n        return cls", "path": "app\\lib\\console\\app\\pygments\\lexers\\__init__.py", "repo_name": "yesudeep/clo-summit", "stars": 5, "license": "None", "language": "python", "size": 7990}
{"docstring": "\"\"\" Aborts execution and causes a 307 redirect \"\"\"\n", "func_signal": "def redirect(url, code=307):\n", "code": "response.status = code\nresponse.header['Location'] = url\nraise BreakTheBottle(\"\")", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\" Returns a mix of GET and POST data. POST overwrites GET \"\"\"\n", "func_signal": "def params(self):\n", "code": "if self._GETPOST is None:\n    self._GETPOST = dict(self.GET)\n    self._GETPOST.update(dict(self.POST))\nreturn self._GETPOST", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "''' Returns a wsgi conform list of header/value pairs '''\n", "func_signal": "def wsgiheaders(self):\n", "code": "for c in self.COOKIES.itervalues():\n    self.header.add_header('Set-Cookie', c.OutputString())\nreturn [(h.title(), str(v)) for h, v in self.header.items()]", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nReturns the current default app or sets a new one.\nDefaults to an instance of Bottle\n\"\"\"\n", "func_signal": "def default_app(newapp = None):\n", "code": "global _default_app\nif newapp:\n    _default_app = newapp\nif not _default_app:\n    _default_app = Bottle()\nreturn _default_app", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\" Clears old data and creates a brand new Response object \"\"\"\n", "func_signal": "def bind(self):\n", "code": "self._COOKIES = None\nself.status = 200\nself.header_list = []\nself.header = HeaderWrapper(self.header_list)\nself.content_type = 'text/html'\nself.error = None\nself.charset = 'utf8'", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "''' Decorator: Rendes a template for a handler.\n    Return a dict of template vars to fill out the template.\n'''\n", "func_signal": "def view(tpl_name, **defaults):\n", "code": "def decorator(func):\n    def wrapper(**kargs):\n        out = func(**kargs)\n        defaults.update(out)\n        return template(tpl_name, **defaults)\n    return wrapper\nreturn decorator", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nCreate a new template.\nIf a name is provided, but no filename and no template string, the\nfilename is guessed using the lookup path list.\nSubclasses can assume that either self.template or self.filename is set.\nIf both are present, self.template should be used.\n\"\"\"\n", "func_signal": "def __init__(self, template='', name=None, filename=None, lookup=[]):\n", "code": "self.name = name\nself.filename = filename\nself.template = template\nself.lookup = lookup\nif self.name and not self.filename:\n    for path in self.lookup:\n        fpath = os.path.join(path, self.name+'.tpl')\n        if os.path.isfile(fpath):\n            self.filename = fpath\nif not self.template and not self.filename:\n    raise TemplateError('Template (%s) not found.' % self.name)\nself.prepare()", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nParses date strings usually found in HTTP header and returns UTC epoch.\nUnderstands rfc1123, rfc850 and asctime.\n\"\"\"\n", "func_signal": "def parse_date(ims):\n", "code": "try:\n    ts = email.utils.parsedate_tz(ims)\n    if ts is not None:\n        if ts[9] is None:\n            return time.mktime(ts[:8] + (0,)) - time.timezone\n        else:\n            return time.mktime(ts[:8] + (0,)) - ts[9] - time.timezone\nexcept (ValueError, IndexError):\n    return None", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nReturns the first matching handler and a parameter dict or (None, None)\n\"\"\"\n", "func_signal": "def match_url(self, url, method='GET'):\n", "code": "url = url.strip().lstrip(\"/ \")\n# Search for static routes first\nroute = self.simple_routes.get(method,{}).get(url,None)\nif route:\n    return (route, {})\n\nroutes = self.regexp_routes.get(method,[])\nfor i in range(len(routes)):\n    match = routes[i][0].match(url)\n    if match:\n        handler = routes[i][1]\n        if i > 0 and self.optimize and random.random() <= 0.001:\n            routes[i-1], routes[i] = routes[i], routes[i-1]\n        return (handler, match.groupdict())\nif self.default_route:\n    return (self.default_route, {})\nif method == 'HEAD': # Fall back to GET\n    return self.match_url(url)\nelse:\n    return (None, None)", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\" Get a dict with GET parameters. \"\"\"\n", "func_signal": "def GET(self):\n", "code": "if self._GET is None:\n    data = parse_qs(self.query_string, keep_blank_values=True)\n    self._GET = {}\n    for key, value in data.iteritems():\n        if len(value) == 1:\n            self._GET[key] = value[0]\n        else:\n            self._GET[key] = value\nreturn self._GET", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\" Adds a controller class or object \"\"\"\n", "func_signal": "def add_controller(self, route, controller, **kargs):\n", "code": "if '{action}' not in route and 'action' not in kargs:\n    raise BottleException(\"Routes to controller classes or object MUST\"\n        \" contain an {action} placeholder or use the action-parameter\")\nfor action in (m for m in dir(controller) if not m.startswith('_')):\n    handler = getattr(controller, action)\n    if callable(handler) and action == kargs.get('action', action):\n        self.add_route(route.replace('{action}', action), handler, **kargs)", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nSets a Cookie. Optional settings:\nexpires, path, comment, domain, max-age, secure, version, httponly\n\"\"\"\n", "func_signal": "def set_cookie(self, key, value, **kargs):\n", "code": "self.COOKIES[key] = value\nfor k, v in kargs.iteritems():\n    self.COOKIES[key][k] = v", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "'''\nGet a rendered template as a string iterator.\nYou can use a name, a filename or a template string as first parameter.\n'''\n", "func_signal": "def template(tpl, template_adapter=SimpleTemplate, **args):\n", "code": "lookup = args.get('template_lookup', TEMPLATE_PATH)\nif tpl not in TEMPLATES or DEBUG:\n    if \"\\n\" in tpl or \"{\" in tpl or \"%\" in tpl or '$' in tpl:\n        TEMPLATES[tpl] = template_adapter(template=tpl, lookup=lookup)\n    elif '.' in tpl:\n        TEMPLATES[tpl] = template_adapter(filename=tpl, lookup=lookup)\n    else:\n        TEMPLATES[tpl] = template_adapter(name=tpl, lookup=lookup)\nif not TEMPLATES[tpl]:\n    abort(500, 'Template (%s) not found' % tpl)\nargs['abort'] = abort\nargs['request'] = request\nargs['response'] = response\nreturn TEMPLATES[tpl].render(**args)", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\" Returns a dict with COOKIES. \"\"\"\n", "func_signal": "def COOKIES(self):\n", "code": "if self._COOKIES is None:\n    raw_dict = SimpleCookie(self._environ.get('HTTP_COOKIE',''))\n    self._COOKIES = {}\n    for cookie in raw_dict.itervalues():\n        self._COOKIES[cookie.key] = cookie.value\nreturn self._COOKIES", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nBinds the enviroment of the current request to this request handler\n\"\"\"\n", "func_signal": "def bind(self, environ):\n", "code": "self._environ = environ\nself.environ = self._environ\nself._GET = None\nself._POST = None\nself._GETPOST = None\nself._COOKIES = None\nself.path = self._environ.get('PATH_INFO', '/').strip()\nif not self.path.startswith('/'):\n    self.path = '/' + self.path", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nDecorator for error handler.\nSame as set_error_handler(code, handler).\n\"\"\"\n", "func_signal": "def error(self, code=500):\n", "code": "def wrapper(handler):\n    self.set_error_handler(code, handler)\n    return handler\nreturn wrapper", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\" Adds a new route to the route mappings. \"\"\"\n", "func_signal": "def add_route(self, route, handler, method='GET', simple=False, **kargs):\n", "code": "if isinstance(handler, type) and issubclass(handler, BaseController):\n    handler = handler()\nif isinstance(handler, BaseController):\n    self.add_controller(route, handler, method=method, simple=simple, **kargs)\n    return\nmethod = method.strip().upper()\nroute = route.strip().lstrip('$^/ ').rstrip('$^ ')\nif re.match(r'^(\\w+/)*\\w*$', route) or simple:\n    self.simple_routes.setdefault(method, {})[route] = handler\nelse:\n    route = re.sub(r':([a-zA-Z_]+)(?P<uniq>[^\\w/])(?P<re>.+?)(?P=uniq)',\n                   r'(?P<\\1>\\g<re>)',route)\n    route = re.sub(r':([a-zA-Z_]+)', r'(?P<\\1>[^/]+)', route)\n    route = re.compile('^%s$' % route)\n    self.regexp_routes.setdefault(method, []).append([route, handler])", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\" Render the template using keyword arguments as local variables. \"\"\"\n", "func_signal": "def render(self, **args):\n", "code": "stdout = []\nself.execute(stdout, **args)\nreturn stdout", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\" Aborts execution and sends a static files as response. \"\"\"\n", "func_signal": "def send_file(filename, root, guessmime = True, mimetype = None):\n", "code": "root = os.path.abspath(root) + os.sep\nfilename = os.path.abspath(os.path.join(root, filename.strip('/\\\\')))\n\nif not filename.startswith(root):\n    abort(401, \"Access denied.\")\nif not os.path.exists(filename) or not os.path.isfile(filename):\n    abort(404, \"File does not exist.\")\nif not os.access(filename, os.R_OK):\n    abort(401, \"You do not have permission to access this file.\")\n\nif guessmime and not mimetype:\n    mimetype = mimetypes.guess_type(filename)[0]\nif not mimetype: mimetype = 'text/plain'\nresponse.content_type = mimetype\n\nstats = os.stat(filename)\nif 'Last-Modified' not in response.header:\n    lm = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime(stats.st_mtime))\n    response.header['Last-Modified'] = lm\nif 'HTTP_IF_MODIFIED_SINCE' in request.environ:\n    ims = request.environ['HTTP_IF_MODIFIED_SINCE']\n    # IE sends \"<date>; length=146\"\n    ims = ims.split(\";\")[0].strip()\n    ims = parse_date(ims)\n    if ims is not None and ims >= stats.st_mtime:\n       abort(304, \"Not modified\")\nif 'Content-Length' not in response.header:\n    response.header['Content-Length'] = str(stats.st_size)\nraise BreakTheBottle(open(filename, 'rb'))", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\" Get a dict with parsed POST or PUT data. \"\"\"\n", "func_signal": "def POST(self):\n", "code": "if self._POST is None:\n    data = cgi.FieldStorage(fp=self._environ['wsgi.input'],\n        environ=self._environ, keep_blank_values=True)\n    self._POST  = {}\n    for item in data.list:\n        name = item.name\n        if not item.filename:\n            item = item.value\n        self._POST.setdefault(name, []).append(item)\n    for key in self._POST:\n        if len(self._POST[key]) == 1:\n            self._POST[key] = self._POST[key][0]\nreturn self._POST", "path": "bottle.py", "repo_name": "tlossen/app_engine_bottle", "stars": 6, "license": "None", "language": "python", "size": 88}
{"docstring": "\"\"\"\nsleeps until SPACEBAR is pressed\n\"\"\"\n", "func_signal": "def pause_for_space(self):\n", "code": "while True:\n    for event in pygame.event.get(): \n        if event.type == pygame.KEYDOWN:\n            if event.key == pygame.K_SPACE:\n                return True\n    time.sleep(.2)", "path": "virtual machine\\legacy\\virtualmachine9.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "\"\"\"\nInitializes window and drawing parameters, including displaying a pygame window.\n\"\"\"\n", "func_signal": "def __init__(self, pen_zoom_pairs=None):\n", "code": "self.max_x = 840\nself.max_y = 680\n\nself.zooms = {}\nfor pen, zoom in pen_zoom_pairs:\n    self.zooms[pen] = zoom\n    \n# line color when pen up unless more sophisticated coloring is occuring\nself.up = (255, 255, 255)\n# line color when pen down unless more sophisticated coloring is occuring\nself.down = (255, 0, 0)\n# True if pen is up, False if pen is down\nself.is_ups = {}\n# current pen location along x axis\nself.xs = {}\n# current pen location along y axis\nself.ys = {}\n\npygame.init() \n#create the screen\nself.window = pygame.display.set_mode((self.max_x, self.max_y))\n#set background\n#self.window.fill( (30, 30, 255) )\nself.window.fill( (0,0,0) )", "path": "virtual machine\\legacy\\virtualmachine9.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#this function defines how the virtual machine moves based on its configuration and perhaps additional sensor inputs.\n#the goal is to create a model of the the machine which can be used in a virtual feedback loop by the controller.\n", "func_signal": "def move(self,commandmove):\n", "code": "returnmove = numpy.zeros(self.numberofaxes)\nfor i in range(self.numberofaxes):\n    returnmove=returnmove + self.guides[i].move(commandmove[i])\nreturn returnmove", "path": "virtual machine\\legacy\\virtualmachine2.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#this function defines how the virtual machine moves based on its configuration and perhaps additional sensor inputs.\n#the goal is to create a model of the the machine which can be used in a virtual feedback loop by the controller.\n", "func_signal": "def move(self,commandmove):\n", "code": "returnmove = numpy.zeros(self.numberofaxes)\nfor i in range(self.numberofaxes):\n    returnmove=returnmove + self.guides[i].move(commandmove[i])\nreturn returnmove", "path": "virtual machine\\legacy\\virtualmachine9.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#this function defines how the virtual machine moves based on its configuration and perhaps additional sensor inputs.\n#the goal is to create a model of the the machine which can be used in a virtual feedback loop by the controller.\n", "func_signal": "def move(self,commandmove):\n", "code": "returnmove = numpy.zeros(self.numberofaxes)\nfor i in range(self.numberofaxes):\n    returnmove=returnmove + self.guides[i].move(commandmove[i])\nreturn returnmove", "path": "virtual machine\\legacy\\virtualmachine3.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#this function defines how the virtual machine moves based on its configuration and perhaps additional sensor inputs.\n#the goal is to create a model of the the machine which can be used in a virtual feedback loop by the controller.\n", "func_signal": "def move(self,commandmove):\n", "code": "returnmove = numpy.zeros(self.numberofaxes)\nfor i in range(self.numberofaxes):\n    returnmove=returnmove + self.guides[i].move(commandmove[i])\nreturn returnmove", "path": "virtual machine\\legacy\\virtualmachine7.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "\"\"\"\nDraws a line (x, y) from the current pen position. x and y are assumed to be \nrelative to current position.\n\n@param x: int\n@param y: int\n@param rate: \n@param movetime: \n\"\"\"\n", "func_signal": "def goto(self, x, y, pen, relative=True, zoom=None, rate=None, movetime=None):\n", "code": "if not pen in self.xs:\n    self.init_pen(pen, zoom)\n    \nif rate and movetime:\n    diffx = abs(self.xs[pen] - x)\n    diffy = abs(self.ys[pen] - y)\n    h = math.sqrt(diffx*diffx + diffy*diffy)\n\n    nrate = round(rate)\n   #print \"ABC\", self.is_ups[pen], nrate, \"   \", h, \"      \", rate\n    hr = rate / 9.0 * 255\n    #hr = h/rate\n    #hr = (h*rate / 2500) * 255\n    #hr = movetime/rate * 200\n    \n    if hr > 255:\n        hr = 255\n    if hr < 0:\n        hr = 0\n    hg = hb = self.is_ups[pen] and hr or 0\n    color = (hr, hg, hb)\n    ret = hr\nelse:\n    color = self.is_ups[pen] and self.up or self.down\n    ret = 0\n#pygame.draw.line(self.window, (255,255,255), (self.x, self.y), (self.x+self.zoom*x, self.y+self.zoom*y), 4)\nif relative:\n    pygame.draw.line(self.window, color, (self.xs[pen], self.ys[pen]), (self.xs[pen]+self.zooms[pen]*x, self.ys[pen]+self.zooms[pen]*y), 1)\n    self.xs[pen] += self.zooms[pen]*x\n    self.ys[pen] += self.zooms[pen]*y\nelse:\n    pygame.draw.line(self.window, color, (self.xs[pen], self.ys[pen]), (self.zooms[pen]*x, self.zooms[pen]*y), 1)\n    \n# update window\npygame.display.flip()\n\nreturn ret / 1000.0", "path": "virtual machine\\legacy\\virtualmachine9.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#packet format:\n#byte0 - Start Byte\n#byte1 - hwcounter\n#byte2 - AXIS 1 Rate 0\n#byte3 - AXIS 1 Rate 1\n#byte4 - AXIS 2 Rate 0\n#byte5 - AXIS 2 Rate 1\n#byte6 - AXIS 3 Rate 0\n#byte7 - AXIS 3 Rate 1\n#byte8 - move duration 0\n#byte9 - move duration 1\n#byte10 - move duration 2\n#byte11 - move duration 3\n#byte12 - sync / motor direction (00ZrZfYrYfXrXf) where r is reverse and f is forward\n", "func_signal": "def xmit(self):\n", "code": "packetlength = 13\nxmitteraxes = virtualmachine.numberofaxes\noutgoing = numpy.zeros(packetlength)\noutgoing[0] = 255\noutgoing[1] = virtualmachine.motorcontrollers[0].hardwarecounter\nfor i in range(xmitteraxes):\n    outgoing[i*2+2] = int(virtualmachine.motorcontrollers[i].softwarecounter % 256)\n    outgoing[i*2+3] = int(virtualmachine.motorcontrollers[i].softwarecounter / 256)\n    outgoing[12] = outgoing[12] + virtualmachine.motorcontrollers[i].direction*(4**i)\n    \nduration = virtualmachine.motorcontrollers[0].duration\n\noutgoingindex = 11\nremainder = duration\nfor i in range(4):\n    outgoing[11-i] = int(remainder / 256**(3-i))\n    remainder = remainder % 256**(3-i)\n\n\n#print outgoing\n#print duration\n#print outgoing[8]+outgoing[9]*256+outgoing[10]*256**2+outgoing[11]*256**3\n\n#-------CONFIGURE AND OPEN SERIAL PORT-------------------------------\nportnumber = 3\nbaudrate = 19200\nsertimeout = None #in seconds\n\nserport = serial.Serial(portnumber,baudrate, timeout=sertimeout)\n\n#-------SEND COMMAND-------------------------------------------------\n\nfor i in range(len(outgoing)):\n    serport.write(chr(outgoing[i]))\n    a = serport.read()\n    #print ord(a)\n\n\nserport.read()\nreturn outgoing", "path": "virtual machine\\legacy\\virtualmachine7.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "\"\"\"\npygame window loop. check if ESCAPE key is pressed to close window and exit program\n\"\"\"\n", "func_signal": "def check_keyboard(self):\n", "code": "for event in pygame.event.get(): \n    if event.type == pygame.KEYDOWN:\n        if event.key == pygame.K_ESCAPE: \n            sys.exit(0)", "path": "virtual machine\\legacy\\virtualmachine9.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "\"\"\"\nDraws a line (x, y) from the current pen position. x and y are assumed to be \nrelative to current position.\n\n@param x: int\n@param y: int\n@param rate: \n@param movetime: \n\"\"\"\n", "func_signal": "def goto(self, x, y, pen, relative=True, zoom=None, rate=None, movetime=None):\n", "code": "if not pen in self.xs:\n    self.init_pen(pen, zoom)\n    \nif rate and movetime:\n    diffx = abs(self.xs[pen] - x)\n    diffy = abs(self.ys[pen] - y)\n    h = math.sqrt(diffx*diffx + diffy*diffy)\n\n    nrate = round(rate)\n   #print \"ABC\", self.is_ups[pen], nrate, \"   \", h, \"      \", rate\n    hr = rate / 9.0 * 255\n    #hr = h/rate\n    #hr = (h*rate / 2500) * 255\n    #hr = movetime/rate * 200\n    \n    if hr > 255:\n        hr = 255\n    if hr < 0:\n        hr = 0\n    hg = hb = self.is_ups[pen] and hr or 0\n    color = (hr, hg, hb)\n    ret = hr\nelse:\n    color = self.is_ups[pen] and self.up or self.down\n    ret = 0\n#pygame.draw.line(self.window, (255,255,255), (self.x, self.y), (self.x+self.zoom*x, self.y+self.zoom*y), 4)\nif relative:\n    pygame.draw.line(self.window, color, (self.xs[pen], self.ys[pen]), (self.xs[pen]+self.zooms[pen]*x, self.ys[pen]+self.zooms[pen]*y), 1)\n    self.xs[pen] += self.zooms[pen]*x\n    self.ys[pen] += self.zooms[pen]*y\nelse:\n    pygame.draw.line(self.window, color, (self.xs[pen], self.ys[pen]), (self.zooms[pen]*x, self.zooms[pen]*y), 1)\n    \n# update window\npygame.display.flip()\n\nreturn ret / 1000.0", "path": "virtual machine\\legacy\\virtualmachine8.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "\"\"\"\nsleeps until SPACEBAR is pressed\n\"\"\"\n", "func_signal": "def pause_for_space(self):\n", "code": "while True:\n    for event in pygame.event.get(): \n        if event.type == pygame.KEYDOWN:\n            if event.key == pygame.K_SPACE:\n                return True\n    time.sleep(.2)", "path": "virtual machine\\legacy\\virtualmachine8.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#\n# send I0 packet\n#\n", "func_signal": "def put_i0_packet():\n", "code": "ip_header_size = 20\nudp_header_size = 8\noutgoing_data_size = 10\n# ip_header_size_before_checksum = 10\n# ip_header_size_after_checksum = 8\n# outgoing_packet_size = ip_header_size + udp_header_size + outgoing_data_size\n# outgoing_start = RAMSTART\n# outgoing_source_address = outgoing_start + 12\n# outgoing_destination_address = outgoing_start + 16\n# outgoing_data = outgoing_start + ip_header_size + udp_header_size\n#\n# IP\n#\npacket = []\npacket.append(0x45) # version = 4 (4 bits), header length = 5 32-bit words (4 bits)\npacket.append(0) # type of service\npacket.append(high(ip_header_size + udp_header_size + outgoing_data_size)) # packet length\npacket.append(low(ip_header_size + udp_header_size + outgoing_data_size)) # packet length\npacket.append(0) # identification (high byte)\npacket.append(0) # identification (low byte)\npacket.append(0) # flag (3 bits), fragment offset (13 bits) (high byte)\npacket.append(0) # flag (3 bits), fragment offset (13 bits) (low byte)\npacket.append(255) # time to live\npacket.append(17) # protocol = 17 for UDP\npacket.append(0) # header checksum (to be calculated)\npacket.append(0) # header checksum (to be calculated)\npacket.append(int(source1.get())) # source address byte 1\npacket.append(int(source2.get())) # source address byte 2\npacket.append(int(source3.get())) # source address byte 3\npacket.append(int(source4.get())) # source address byte 4\npacket.append(int(dest1.get())) # destination address byte 1\npacket.append(int(dest2.get())) # destination address byte 2\npacket.append(int(dest3.get())) # destination address byte 3\npacket.append(int(dest4.get())) # destination address byte 4\n#\n# UDP\n#\npacket.append(high(int(sourceport.get()))) # source port\npacket.append(low(int(sourceport.get()))) # source port\npacket.append(high(int(destport.get()))) # destination port\npacket.append(low(int(destport.get()))) # destination port\npacket.append(high(udp_header_size + outgoing_data_size)) # payload length\npacket.append(low(udp_header_size + outgoing_data_size)) # payload length\npacket.append(0) # payload checksum (not used)\npacket.append(0) # payload checksum (not used)\n#\n# data\n#\npacket.append(int(databyte0.get())) #HW Counter\npacket.append(int(databyte1.get())) #Command_Travel0\npacket.append(int(databyte2.get())) #Command_Travel1\npacket.append(int(databyte3.get())) #Duration0\npacket.append(int(databyte4.get())) #Duration1\npacket.append(int(databyte5.get())) #Configuration\n\n#\n# send the packet with SLIP mapping and framing\n#\noutput(END)\nfor byte in range(len(packet)):\n   if (packet[byte] == END):\n      output(ESC)\n      output(ESC_END)\n   elif (packet[byte] == ESC):\n      output(ESC)\n      output(ESC_ESC)\n   else:\n      output(packet[byte])\noutput(END)\n#\n# pause for bridge after sending packet\n#\ntime.sleep(.05)", "path": "virtual machine\\legacy\\i0vm.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#\n# idle routine\n#\n", "func_signal": "def idle(parent):\n", "code": "if (ser.inWaiting() != 0):\n   #\n   # I0 serial data waiting, read packet\n   #\n   # set timout alarm\n   #\n   signal.signal(signal.SIGALRM, handler)\n   signal.alarm(TIMEOUT)\n   #\n   # try to read packet, otherwise set error message\n   #\n   try:\n      get_i0_packet()\n   except:\n      sdata.set(\"I0 timeout\")\n   #\n   # turn off alarm\n   #\n   signal.alarm(0)\n#\n# sleep to reduce OS load\n#\ntime.sleep(0.001)\nparent.after_idle(idle,parent)", "path": "virtual machine\\legacy\\i0vm.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#this function defines how the virtual machine moves based on its configuration and perhaps additional sensor inputs.\n#the goal is to create a model of the the machine which can be used in a virtual feedback loop by the controller.\n", "func_signal": "def move(self,commandmove):\n", "code": "returnmove = numpy.zeros(self.numberofaxes)\nfor i in range(self.numberofaxes):\n    returnmove=returnmove + self.guides[i].move(commandmove[i])\nreturn returnmove", "path": "virtual machine\\legacy\\virtualmachine4.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "\"\"\"\npygame window loop. check if ESCAPE key is pressed to close window and exit program\n\"\"\"\n", "func_signal": "def check_keyboard(self):\n", "code": "for event in pygame.event.get(): \n    if event.type == pygame.KEYDOWN:\n        if event.key == pygame.K_ESCAPE: \n            sys.exit(0)", "path": "virtual machine\\legacy\\virtualmachine8.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#packet format:\n#byte0 - Start Byte\n#byte1 - hwcounter\n#byte2 - AXIS 1 Rate 0\n#byte3 - AXIS 1 Rate 1\n#byte4 - AXIS 2 Rate 0\n#byte5 - AXIS 2 Rate 1\n#byte6 - AXIS 3 Rate 0\n#byte7 - AXIS 3 Rate 1\n#byte8 - move duration 0\n#byte9 - move duration 1\n#byte10 - move duration 2\n#byte11 - move duration 3\n#byte12 - sync / motor direction (00ZrZfYrYfXrXf) where r is reverse and f is forward\n", "func_signal": "def xmit(self):\n", "code": "packetlength = 13\nxmitteraxes = virtualmachine.numberofaxes\noutgoing = numpy.zeros(packetlength)\noutgoing[0] = 255\noutgoing[1] = virtualmachine.motorcontrollers[0].hardwarecounter\nfor i in range(xmitteraxes):\n    outgoing[i*2+2] = int(virtualmachine.motorcontrollers[i].softwarecounter % 256)\n    outgoing[i*2+3] = int(virtualmachine.motorcontrollers[i].softwarecounter / 256)\n    outgoing[12] = outgoing[12] + virtualmachine.motorcontrollers[i].direction*(4**i)\n    \nduration = virtualmachine.motorcontrollers[0].duration\n\noutgoingindex = 11\nremainder = duration\nfor i in range(4):\n    outgoing[11-i] = int(remainder / 256**(3-i))\n    remainder = remainder % 256**(3-i)\n\n\n#print outgoing\n#print duration\n#print outgoing[8]+outgoing[9]*256+outgoing[10]*256**2+outgoing[11]*256**3\n\n#-------CONFIGURE AND OPEN SERIAL PORT-------------------------------\nportnumber = 3\nbaudrate = 19200\nsertimeout = None #in seconds\n\nserport = serial.Serial(portnumber,baudrate, timeout=sertimeout)\n\n#-------SEND COMMAND-------------------------------------------------\n\nfor i in range(len(outgoing)):\n    serport.write(chr(outgoing[i]))\n    a = serport.read()\n    #print ord(a)\n\n\nserport.read()\nreturn outgoing", "path": "virtual machine\\legacy\\vm8\\virtualmachine8.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#this function defines how the virtual machine moves based on its configuration and perhaps additional sensor inputs.\n#the goal is to create a model of the the machine which can be used in a virtual feedback loop by the controller.\n", "func_signal": "def move(self,commandmove):\n", "code": "returnmove = numpy.zeros(self.numberofaxes)\nfor i in range(self.numberofaxes):\n    returnmove=returnmove + self.guides[i].move(commandmove[i])\nreturn returnmove", "path": "virtual machine\\legacy\\virtualmachine5.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "\"\"\"\nInitializes window and drawing parameters, including displaying a pygame window.\n\"\"\"\n", "func_signal": "def __init__(self, pen_zoom_pairs=None):\n", "code": "self.max_x = 840\nself.max_y = 680\n\nself.zooms = {}\nfor pen, zoom in pen_zoom_pairs:\n    self.zooms[pen] = zoom\n    \n# line color when pen up unless more sophisticated coloring is occuring\nself.up = (255, 255, 255)\n# line color when pen down unless more sophisticated coloring is occuring\nself.down = (255, 0, 0)\n# True if pen is up, False if pen is down\nself.is_ups = {}\n# current pen location along x axis\nself.xs = {}\n# current pen location along y axis\nself.ys = {}\n\npygame.init() \n#create the screen\nself.window = pygame.display.set_mode((self.max_x, self.max_y))\n#set background\n#self.window.fill( (30, 30, 255) )\nself.window.fill( (0,0,0) )", "path": "virtual machine\\legacy\\virtualmachine8.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#this function defines how the virtual machine moves based on its configuration and perhaps additional sensor inputs.\n#the goal is to create a model of the the machine which can be used in a virtual feedback loop by the controller.\n", "func_signal": "def move(self,commandmove):\n", "code": "returnmove = numpy.zeros(self.numberofaxes)\nfor i in range(self.numberofaxes):\n    returnmove=returnmove + self.guides[i].move(commandmove[i])\nreturn returnmove", "path": "virtual machine\\legacy\\vm8\\virtualmachine8.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "#this function defines how the virtual machine moves based on its configuration and perhaps additional sensor inputs.\n#the goal is to create a model of the the machine which can be used in a virtual feedback loop by the controller.\n", "func_signal": "def move(self,commandmove):\n", "code": "returnmove = numpy.zeros(self.numberofaxes)\nfor i in range(self.numberofaxes):\n    returnmove=returnmove + self.guides[i].move(commandmove[i])\nreturn returnmove", "path": "virtual machine\\legacy\\virtualmachine6.py", "repo_name": "imoyer/MtMM", "stars": 6, "license": "None", "language": "python", "size": 35600}
{"docstring": "\"\"\"init StaticText\"\"\"\n", "func_signal": "def __init__(self, parent, text, **attributes):\n", "code": "self.text = text\nbase.Control.__init__(self, parent, width=\"50\", height=\"90\", style=\"default_sizer\", expand=\"True\", **attributes)", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"create the rectangle of the TextCtrl\n\nthis processes the 'numlines' and 'charwidth' attributes, as well as the normal\n'width' and 'height' attributes. Note that width and height override the TextCtrl specific attributes.\nAlso note that the charwidth attributes is only accurate for fixed-width fonts. It approximates otherwise\nby using the width of a lone \"a\" character.\n\"\"\"\n", "func_signal": "def create(self):\n", "code": "base.Control.create(self)\nself.font = self.style.font()\nif self.rect.height == 0:\n    self.rect.height = int(self.attributes.get(\"numlines\", 1)) * self.font.get_linesize()\nif self.rect.width == 0:\n    self.rect.width = int(self.attributes.get(\"charwidth\", 20)) * self.font.size(\"a\")[0]\nself.rect.width += self.style['padding'] * 2\nself.rect.height += self.style['padding'] * 2\nself.render_text()", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"wrap text\n\ngiven a string with a specific pygame font object and a maximum width, return a list of strings that\nare as long as possible without exceeding the maximum length. Breaks up at whitespace if possible,\notherwise starts breaking up words themselves.\n\"\"\"\n", "func_signal": "def wrap_text(text, font, maxwidth):\n", "code": "text = text.lstrip()\nif font.size(text)[0] <= maxwidth:\n    return [text]\nelse:\n    orig_text = text\n    while font.size(text)[0] > maxwidth:\n        split = text.rsplit(None, 1)[0]\n        #start cutting words if splitting at whitespace doesn't cut it (pardon pun)\n        text = split[:-1] if split == text else split\n    return [text] + wrap_text(orig_text[len(text):], font, maxwidth)", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"expand the element to cover all available space\n\nargument can be either True or False. Of course, the false case is default,\nso rather pointless, but it's there for the sake of consistency\n\"\"\"\n", "func_signal": "def expand(element, area, arg):\n", "code": "if arg == \"True\":\n    element.pos = area.topleft\n    element.rect.size = area.size\n    #we've changed our size! better rearrange children\n    element.arrange_children()\nelif arg != \"False\":\n    raise RuntimeError(\"Invalid value of attribute expand: %s\" % arg)", "path": "style.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"align the element vertically in the given area\n\nsame as align, but vertical. The argument can be one of: 'top', 'bottom', 'center'\n\"\"\"\n", "func_signal": "def valign(element, area, arg):\n", "code": "if arg == \"top\":\n    element.pos = element.pos[0], area.top\nelif arg == \"bottom\":\n    element.pos = element.pos[0], area.bottom - element.rect.height\nelif arg == \"center\":\n    element.pos = element.pos[0], area.centery - (element.rect.height / 2)\nelse:\n    raise RuntimeError(\"Invalid value of attribute valign: %s\" % arg)", "path": "style.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"render TextCtrl\"\"\"\n", "func_signal": "def render(self, event):\n", "code": "surface = event.display.screen\ngoo.draw.rounded_rect(surface, self.rect, (self.style[\"background_color\"], 0, self.style[\"border_radius\"], self.style[\"border_rounding\"]))\ngoo.draw.rounded_rect(surface, self.rect, self.style)\nif self.cursor_pos is not None and self.cursor_blink:\n    x = self.rect.left + self.style[\"padding\"] + self.font.size(self.text[:self.cursor_pos])[0]\n    y = self.rect.centery\n    pygame.draw.line(surface, (0,0,0), (x, y + self.font.get_linesize() // 2), (x, y - self.font.get_linesize() // 2))\nbase.Control.render(self, event)", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"binary search for the position of the cursor\"\"\"\n", "func_signal": "def find_cursorpos(cursor_x, text, font):\n", "code": "if cursor_x >= font.size(text)[0]:\n    return len(text) - 1\nif cursor_x <= 0:\n    return -1\n\nhalf = len(text) // 2\nn = int(round(half / 2.))\nwhile 1:\n    char_start = font.size(text[:half])[0]\n    char_end = font.size(text[:half + 1])[0]\n    if char_start < cursor_x <= char_end or len(text) == 1:\n        return half\n    elif cursor_x > char_end:\n        half += n\n        n = int(round(n / 2.))\n    elif cursor_x <= char_start:\n        half -= n\n        n = int(round(n / 2.))", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"find the first non-text node in the node list\"\"\"\n", "func_signal": "def find_firstnode(parent):\n", "code": "for node in parent.childnodes:\n    if node.nodeType != 3:\n        return node", "path": "composite\\base.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"align the element in the given area\n\nthe argument can be either 'left', 'right', or 'center'\n\"\"\"\n", "func_signal": "def align(element, area, arg):\n", "code": "if arg == \"left\":\n    element.pos = area.left, element.pos[1]\nelif arg == \"right\":\n    element.pos = area.right - element.rect.width, element.pos[1]\nelif arg == \"center\":\n    element.pos = area.centerx - (element.rect.width / 2), element.pos[1]\nelse:\n    raise RuntimeError(\"Invalid value of attribute align: %s\" % arg)", "path": "style.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"moves cursor one space right\"\"\"\n", "func_signal": "def on_movekey(self, event):\n", "code": "if self.cursor_pos is not None:\n    if event.key == pygame.K_RIGHT and self.cursor_pos < len(self.text):\n        self.cursor_pos += 1\n    elif event.key == pygame.K_LEFT and self.cursor_pos > 0:\n        self.cursor_pos -= 1", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"render text inside the TextCtrl\"\"\"\n", "func_signal": "def render_text(self):\n", "code": "lines = wrap_text(self.text, self.font, self.rect.width)\nself.img = goo.draw.alpha_surface(self.rect.size)\n\nfor n, line in enumerate(lines):\n    if n * self.font.get_linesize() > self.rect.height:\n        break\n    s = self.font.render(line, True, self.style['font_color'])\n    self.img.blit(s, (self.style['padding'], self.style['padding'] + n * (self.font.get_linesize())))", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"set a style option.\n\nThe method checks for invalid option names, and verifies the type of\nthe object. A KeyError is raised if you try to set a non-existant\noption. A value of the wrong type causes a TypeError.\n\"\"\"\n", "func_signal": "def __setitem__(self, key, value):\n", "code": "if key not in DEFAULT_OPTIONS:\n    raise KeyError(\"Attempt to set invalid GUI style option: '%s'\" % key)\nelif type(value) != type(DEFAULT_OPTIONS[key]):\n    raise TypeError(\"new value of '%s' is of wrong type ('%s' instead of '%s')\" % (key, type(value), type(DEFAULT_OPTIONS[key])))\nelse:\n    dict.__setitem__(self, key, value)", "path": "style.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"update TextCtrl\"\"\"\n", "func_signal": "def update(self, event):\n", "code": "base.Control.update(self, event)\nself.blink_count += 1\nif self.blink_count > 14:\n    self.cursor_blink = not self.cursor_blink\n    self.blink_count = 0", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"initialize this composite widget\"\"\"\n", "func_signal": "def __init__(self, parent, children, source, **attributes):\n", "code": "self.childnodes = children\nchildren = goo.parser.get_root(goo.xml_loader[source]).childNodes\nfor node in children:\n    if node.nodeType not in (3, 8):\n        new_attributes = goo.parser.get_attributes(node)\n        new_attributes.update(attributes)\n        break\ngoo.containers.Sizer.__init__(self, parent, children, **new_attributes)", "path": "composite\\base.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"this does the job the create method would normally be doing.\n\nThis is required since the content changes with the size of the element.\n\"\"\"\n", "func_signal": "def arrange(self, area):\n", "code": "self.rect = base.Control.arrange(self, area)\nself.font = self.style.font()\nlines = wrap_multiline(self.text, self.font, self.rect.width)\nself.img = goo.draw.alpha_surface(self.rect.size)\n\nfor n, line in enumerate(lines):\n    s = self.font.render(line, True, self.style['font_color'])\n    self.img.blit(s, (self.style['padding'], self.style['padding'] + n * (self.font.get_linesize())))\nreturn self.rect", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"init TextCtrl\"\"\"\n", "func_signal": "def __init__(self, parent, text, **attributes):\n", "code": "self.text = text\nself.cursor_pos = None\nself.blink_count = 0\nself.cursor_blink = False\nbase.Control.__init__(self, parent, style=\"default_text\", **attributes)", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"find the composite method in the hierarchy\"\"\"\n", "func_signal": "def find_composite(parent):\n", "code": "while not isinstance(parent, Composite):\n    parent = parent.parent\n    if isinstance(parent, goo.NullParent):\n        raise RuntimeError(\"Content tag used in non-composite file\")\nreturn parent", "path": "composite\\base.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"called in case a non-existant key is requested\n\nIt might be the case that the item is simply not set in this instance.\nTherefore, we first check if there is a DEFAULT_OPTIONS value for this setting\navailable. If that is not the case either, the key is invalid, and we\nmust raise a KeyError.\n\"\"\"\n", "func_signal": "def __missing__(self, key):\n", "code": "if key in DEFAULT_OPTIONS:\n    return DEFAULT_OPTIONS[key]\nelse:\n    raise KeyError(\"Attempt to retrieve invalid GUI style option: '%s'\" % key)", "path": "style.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"Create a new Style object.\n\nAny options that you don't specify will be taken from the DEFAULT_OPTIONS\noptions. The options are passed into the constructor as keyword\narguments. If you have options in a dictionary, use extended call\nsyntax: Style(**options)\n\"\"\"\n", "func_signal": "def __init__(self, name, **kwargs):\n", "code": "dict.__init__(self)\nfor key, value in kwargs.items():\n    self[key] = value\nself.name = name", "path": "style.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"event handler for mouse click\"\"\"\n", "func_signal": "def on_mousedown(self, event):\n", "code": "if self.rect.collidepoint(event.pos):\n    self.cursor_pos = 1 + find_cursorpos(event.pos[0] - (self.rect.left + self.style['padding']), self.text, self.font)\nelse:\n    self.cursor_pos = None", "path": "controls\\text.py", "repo_name": "HugoArts/goo", "stars": 5, "license": "None", "language": "python", "size": 288}
{"docstring": "\"\"\"The main loop of the server, run in a seperate tasklet by start().\"\"\"\n", "func_signal": "def _accept_loop(self):\n", "code": "while self.running:\n    # This line will suspend the server tasklet until there is a connection\n    s, addr = self.sock_server.accept()\n    \n    # See if we have already been asked to stop\n    if not self.running:\n        return\n    \n    # Initialize the WSGI environment\n    environ = self.environ.copy()\n    environ[\"SERVER_SOFTWARE\"] = \"%s WSGI Server\" % self.version\n    environ[\"ACTUAL_SERVER_PROTOCOL\"] = self.protocol\n    environ[\"SERVER_NAME\"] = self.server_name\n    environ[\"SERVER_PORT\"] = str(self.bind_addr[1])\n    environ[\"REMOTE_ADDR\"] = addr[0]\n    environ[\"REMOTE_PORT\"] = str(addr[1])\n    \n    # self.connection_class is a reference to a class that will\n    # take care of reading and parsing requests out of the connection\n    conn = self.connection_class(s, self.wsgi_app, environ)\n    \n    # We create a new tasklet for each connection. This is similar\n    # to how threaded web servers work, except they usually keep a thread\n    # pool with an upper limit on number of threads. We just create new tasklets\n    # blindly without regard for how many requests the server is serving\n    # already. This is possible because of the light-weight nature of\n    # tasklets compared to threads.\n    def comm(connection):\n        try:\n            connection.communicate()\n        finally:\n            connection.close()\n    self.tasklet_class(comm)(conn)", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Create, configure and return the routes Mapper\"\"\"\n", "func_signal": "def make_map():\n", "code": "map = Mapper(directory=config['pylons.paths']['controllers'],\n             always_scan=config['debug'])\n\n# The ErrorController route (handles 404/500 error pages); it should\n# likely stay at the top, ensuring it can always be resolved\nmap.connect('error/:action/:id', controller='error')\n\n# CUSTOM ROUTES HERE\n\nmap.connect(':controller/:action/:id')\nmap.connect('*url', controller='template', action='view')\n\nreturn map", "path": "testpaste\\testpaste\\config\\routing.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Call the appropriate WSGI app and write its iterable output.\"\"\"\n", "func_signal": "def respond(self):\n", "code": "response = self.wsgi_app(self.environ, self.start_response)\ntry:\n    for chunk in response:\n        # \"The start_response callable must not actually transmit\n        # the response headers. Instead, it must store them for the\n        # server or gateway to transmit only after the first\n        # iteration of the application return value that yields\n        # a NON-EMPTY string, or upon the application's first\n        # invocation of the write() callable.\" (PEP 333)\n        if chunk:\n            self.write(chunk)\n        stackless.schedule()\nfinally:\n    if hasattr(response, \"close\"):\n        response.close()\nif (self.ready and not self.sent_headers):\n    self.sent_headers = True\n    self.send_headers()\nif self.chunked_write:\n    self.sendall(\"0\\r\\n\\r\\n\")", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Close the socket underlying this connection.\"\"\"\n", "func_signal": "def close(self):\n", "code": "self.rfile.close()\nself.sock_chan.close()", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "# A call to this method will suspend the current tasklet until there is\n# data available to be received. See handle_read on how that happens.\n", "func_signal": "def recv(self, byte_count):\n", "code": "if byte_count > 0 and len(self.recv_buffer) == 0 or self.recv_channel.balance > 0:\n    self.recv_buffer += self.recv_channel.receive()\n\n# Give the caller only as much as he asked for\nret = self.recv_buffer[:byte_count]\n\n# and stash the rest in self.recv_buffer\nself.recv_buffer = self.recv_buffer[byte_count:]\n\nreturn ret", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"WSGI callable to write unbuffered data to the client.\n\nThis method is also used internally by start_response (to write\ndata from the iterable returned by the WSGI application).\n\"\"\"\n", "func_signal": "def write(self, chunk):\n", "code": "if not self.started_response:\n    raise AssertionError(\"WSGI write called before start_response.\")\n\nif not self.sent_headers:\n    self.sent_headers = True\n    self.send_headers()\n\nif self.chunked_write and chunk:\n    buf = [hex(len(chunk))[2:], \"\\r\\n\", chunk, \"\\r\\n\"]\n    self.sendall(\"\".join(buf))\nelse:\n    self.sendall(chunk)", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Create a Pylons WSGI application and return it\n\n``global_conf``\n    The inherited configuration for this application. Normally from\n    the [DEFAULT] section of the Paste ini file.\n\n``full_stack``\n    Whether or not this application provides a full WSGI stack (by\n    default, meaning it handles its own exceptions and errors).\n    Disable full_stack when this application is \"managed\" by\n    another WSGI middleware.\n\n``app_conf``\n    The application's local configuration. Normally specified in the\n    [app:<name>] section of the Paste ini file (where <name>\n    defaults to main).\n\"\"\"\n# Configure the Pylons environment\n", "func_signal": "def make_app(global_conf, full_stack=True, **app_conf):\n", "code": "load_environment(global_conf, app_conf)\n\n# The Pylons WSGI app\napp = PylonsApp()\n\n# CUSTOM MIDDLEWARE HERE (filtered by error handling middlewares)\n\nif asbool(full_stack):\n    # Handle Python exceptions\n    app = ErrorHandler(app, global_conf, error_template=error_template,\n                       **config['pylons.errorware'])\n\n    # Display error documents for 401, 403, 404 status codes (and\n    # 500 when debug is disabled)\n    app = ErrorDocuments(app, global_conf, mapper=error_mapper, **app_conf)\n\n# Establish the Registry for this application\napp = RegistryManager(app)\n\n# Static files\njavascripts_app = StaticJavascripts()\nstatic_app = StaticURLParser(config['pylons.paths']['static_files'])\napp = Cascade([static_app, javascripts_app, app])\nreturn app", "path": "testpaste\\testpaste\\config\\middleware.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Render the error document\"\"\"\n", "func_signal": "def document(self):\n", "code": "page = error_document_template % \\\n    dict(prefix=request.environ.get('SCRIPT_NAME', ''),\n         code=request.params.get('code', ''),\n         message=request.params.get('message', ''))\nreturn page", "path": "testpaste\\testpaste\\controllers\\error.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Bind to addr and start listening\"\"\"\n", "func_signal": "def __init__(self, addr):\n", "code": "asyncore.dispatcher.__init__(self)\nself.accept_channel = None\nself.addr = addr\nself.create_socket(socket.AF_INET, socket.SOCK_STREAM)\nself.bind(addr)\nself.listen(5)\n\n# Start the asyncore polling loop if it's not already running\nif not asyncore_loop.running:\n    stackless.tasklet(asyncore_loop)()", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"This function loops indefinately, and on every 10 second boundary\nfrom the epoch, it dispatches an event to a list of listeners. The listeners\nare called with the current timestamp as the sole argument.\"\"\"\n", "func_signal": "def fake_event_source():\n", "code": "while 1:\n    if len(fake_event_listeners):\n        now = int(time.time())\n        if now % 10 == 0:\n            for f in fake_event_listeners:\n                f(now)\n    sleep(1)", "path": "sandbox\\app_comet.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Configure the Pylons environment via the ``pylons.config``\nobject\n\"\"\"\n# Pylons paths\n", "func_signal": "def load_environment(global_conf, app_conf):\n", "code": "root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\npaths = dict(root=root,\n             controllers=os.path.join(root, 'controllers'),\n             static_files=os.path.join(root, 'public'),\n             templates=[os.path.join(root, 'templates')])\n\n# Initialize config with the basic options\nconfig.init_app(global_conf, app_conf, package='testpaste',\n                template_engine='mako', paths=paths)\n\nconfig['routes.map'] = make_map()\nconfig['pylons.g'] = app_globals.Globals()\nconfig['pylons.h'] = testpaste.lib.helpers\n\n# Customize templating options via this variable\ntmpl_options = config['buffet.template_options']\n\n# CONFIGURATION OPTIONS HERE (note: all config options will override\n# any Pylons config options)", "path": "testpaste\\testpaste\\config\\environment.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Initialize and start handling the connection on sock. Usually called\nby sock_server\"\"\"\n", "func_signal": "def __init__(self, sock):\n", "code": "if sock.type == socket.SOCK_DGRAM:\n    raise NotImplementedError(\"sock_channel can only handle TCP sockets\")\nasyncore.dispatcher.__init__(self, sock)\nself.send_buffer = \"\"\nself.recv_buffer = \"\"\nself.sendall_channel = None\nself.recv_channel = stackless.channel()", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Read header lines from the incoming stream.\"\"\"\n", "func_signal": "def read_headers(self):\n", "code": "environ = self.environ\n\nwhile True:\n    line = self.rfile.readline()\n    if not line:\n        # No more data--illegal end of headers\n        raise ValueError(\"Illegal end of headers.\")\n    \n    if line == '\\r\\n':\n        # Normal end of headers\n        break\n    \n    if line[0] in ' \\t':\n        # It's a continuation line.\n        v = line.strip()\n    else:\n        k, v = line.split(\":\", 1)\n        k, v = k.strip().upper(), v.strip()\n        envname = \"HTTP_\" + k.replace(\"-\", \"_\")\n    \n    if k in comma_separated_headers:\n        existing = environ.get(envname)\n        if existing:\n            v = \", \".join((existing, v))\n    environ[envname] = v\n\nct = environ.pop(\"HTTP_CONTENT_TYPE\", None)\nif ct:\n    environ[\"CONTENT_TYPE\"] = ct\ncl = environ.pop(\"HTTP_CONTENT_LENGTH\", None)\nif cl:\n    environ[\"CONTENT_LENGTH\"] = cl", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Invoke the controller.\"\"\"\n# The return value of a controller should be written to the\n# http client as a final response.\n", "func_signal": "def __call__(self):\n", "code": "try:\n    final_data = self._controller(self)\n    self._channel.send(final_data)\nexcept TimeoutException:\n    # The controller didn't handle the timeout itself, so we just ignore it\n    pass", "path": "sandbox\\app_sessionless.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "# Make sure only one invocation is active at any time\n", "func_signal": "def asyncore_loop():\n", "code": "assert asyncore_loop.running == False\nasyncore_loop.running = True\ntry:\n    while len(asyncore.socket_map):\n        asyncore.poll(0.05)\n        stackless.schedule()\nfinally:\n    asyncore_loop.running = False", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Instantiate a WSGI server.\n- bind_addr is a (hostname,port) tuple\n- wsgi_app is a callable application as per the WSGI spec\n- server_name is the server name, defaulting to the local hostname\n\"\"\"\n", "func_signal": "def __init__(self, bind_addr, wsgi_app, server_name=None):\n", "code": "self.bind_addr = bind_addr\nself.wsgi_app = wsgi_app\nif not server_name:\n    server_name = socket.gethostname()\nself.server_name = server_name\n\nself.connection_class = HTTPConnection\nself.tasklet_class = stackless.tasklet\n\nself.running = False", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "# Generate some pseudo-cryptic unique code\n", "func_signal": "def _get_continuation_id():\n", "code": "global _continuation_id\nc = _continuation_id\n_continuation_id += 1\nreturn md5.new(\"%d%d\" % (c, random.randint(0, 1023))).hexdigest()", "path": "sandbox\\app_sessionless.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Write a simple response back to the client.\"\"\"\n", "func_signal": "def simple_response(self, status, msg=\"\"):\n", "code": "status = str(status)\nbuf = [\"%s %s\\r\\n\" % (self.environ['ACTUAL_SERVER_PROTOCOL'], status),\n       \"Content-Length: %s\\r\\n\" % len(msg)]\n\nif status[:3] == \"413\" and self.response_protocol == 'HTTP/1.1':\n    # Request Entity Too Large\n    self.close_connection = True\n    buf.append(\"Connection: close\\r\\n\")\n\nbuf.append(\"\\r\\n\")\nif msg:\n    buf.append(msg)\nself.sendall(\"\".join(buf))", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Start serving HTTP requests on bound port. If start_stackless\nis True (default) this will call stackless.run() and block. Set it\nto False if you intend to start the stackless processing loop yourself or\ncall stackless.schedule by some other means. Otherwise the server will\nnot function since all work is deferred to a tasklet.\n\"\"\"\n", "func_signal": "def start(self, start_stackless=True):\n", "code": "self.sock_server = sock_server(self.bind_addr)\nself.running = True\n\nself.tasklet_class(self._accept_loop)()\n\nif start_stackless:\n    stackless.run()", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Decode the 'chunked' transfer coding.\"\"\"\n", "func_signal": "def decode_chunked(self):\n", "code": "cl = 0\ndata = StringIO.StringIO()\nwhile True:\n    line = self.rfile.readline().strip().split(\";\", 1)\n    chunk_size = int(line.pop(0), 16)\n    if chunk_size <= 0:\n        break\n    cl += chunk_size\n    data.write(self.rfile.read(chunk_size))\n    crlf = self.rfile.read(2)\n    if crlf != \"\\r\\n\":\n        self.simple_response(\"400 Bad Request\",\n                             \"Bad chunked transfer coding \"\n                             \"(expected '\\\\r\\\\n', got %r)\" % crlf)\n        return\n\n# Grab any trailer headers\nself.read_headers()\n\ndata.seek(0)\nself.environ[\"wsgi.input\"] = data\nself.environ[\"CONTENT_LENGTH\"] = str(cl) or \"\"\nreturn True", "path": "stacklesswsgi\\stacklesswsgi.py", "repo_name": "JustinTulloss/yaps", "stars": 5, "license": "None", "language": "python", "size": 2320}
{"docstring": "\"\"\"Returns the entries the given user has commented on or \"liked\".\"\"\"\n", "func_signal": "def fetch_user_discussion_feed(self, nickname, **kwargs):\n", "code": "return self._fetch_feed(\n    \"/api/feed/user/\" + urllib.quote_plus(nickname) + \"/discussion\",\n    **kwargs)", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"\nTODO: This shouldn't be in views\n\"\"\"\n", "func_signal": "def add_vote(request, entry_index, rating):\n", "code": "favs = request.session['favs']\npostData = favs[entry_index]\nuserData = postData['from']\nrater,created = Rater.objects.get_or_create(name=request.session['username'])\nif created:\n   rater.save()\nposter,created = Poster.objects.get_or_create(name=userData['id'],text=simplejson.dumps(userData))\nif created:\n    poster.save()\npost = Entry(poster=poster,post_id=postData['id'],text=simplejson.dumps(postData))\npost.save()\nrating = Rating(entry=post,rater=rater,score=rating_score[rating])\nrating.save()", "path": "feed\\views.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Makes all strings in the given JSON-like structure unicode.\"\"\"\n", "func_signal": "def _unicodify(json):\n", "code": "if isinstance(json, str):\n    return json.decode(\"utf-8\")\nelif isinstance(json, dict):\n    for name in json:\n        json[name] = _unicodify(json[name])\nelif isinstance(json, list):\n    for part in json:\n        _unicodify(part)\nreturn json", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Returns the entries shared by the user with the given nickname.\n\nAuthentication is required if the user's feed is not public.\n\"\"\"\n", "func_signal": "def fetch_user_feed(self, nickname, **kwargs):\n", "code": "return self._fetch_feed(\n    \"/api/feed/user/\" + urllib.quote_plus(nickname), **kwargs)", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "# TODO: Better way of seeing if user is authenticated\n", "func_signal": "def start(request):\n", "code": "if 'username' in request.session:\n    try:\n        friendfeed_initialize_trials(request)\n    except HTTPError:            \n        return logout(request, msg=\"There was a problem authenticating you. Please try again.\")\n    return render_to_response(\"start.html\", {\"username\": request.session[\"username\"]}, context_instance=RequestContext(request))\nelse:\n    return HttpResponseRedirect(\"/login\")", "path": "views.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Deletes the 'Like' for the entry with the given ID (if any).\"\"\"\n", "func_signal": "def delete_like(self, entry_id):\n", "code": "self._fetch(\"/api/like/delete\", {\n    \"entry\": entry_id,\n})", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Updates the comment with the given ID.\"\"\"\n", "func_signal": "def edit_comment(self, entry_id, comment_id, body):\n", "code": "self._fetch(\"/api/comment\", {\n    \"entry\": entry_id,\n    \"comment\": comment_id,\n    \"body\": body\n})", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Adds the given comment to the entry with the given ID.\n\nWe return the ID of the new comment, which can be used to edit or\ndelete the comment.\n\"\"\"\n", "func_signal": "def add_comment(self, entry_id, body, via=None):\n", "code": "args = {\n    \"entry\": entry_id,\n    \"body\": body\n}\nif via: args[\"via\"] = via\nresult = self._fetch(\"/api/comment\", args)\nreturn result[\"id\"]", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Returns a merged feed with all of the given users' entries.\n\nAuthentication is required if any one of the users' feeds is not\npublic.\n\"\"\"\n", "func_signal": "def fetch_multi_user_feed(self, nicknames, **kwargs):\n", "code": "return self._fetch_feed(\"/api/feed/user\", nickname=\",\".join(nicknames),\n                        **kwargs)", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Returns the entries the given user has \"liked\".\"\"\"\n", "func_signal": "def fetch_user_likes_feed(self, nickname, **kwargs):\n", "code": "return self._fetch_feed(\n    \"/api/feed/user/\" + urllib.quote_plus(nickname) + \"/likes\",\n    **kwargs)", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Returns the profile of the given user.\"\"\"\n", "func_signal": "def fetch_user_profile(self, nickname, **kwargs):\n", "code": "return self._fetch_feed(\n    \"/api/user/\" + urllib.quote_plus(nickname) + \"/profile\",\n    **kwargs)", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"\nInitialize the friendfeed session, given the current authentication information.\nWe will start a new set of trials, even if one currently exists\n(e.g. the user stops halfway through and reloads the start page.)\nTODO: Move this out of views.py\n\"\"\"\n", "func_signal": "def friendfeed_initialize_trials(request):\n", "code": "uname = request.session['username']\nrkey = request.session['remote_key']\nif 'ffsession' not in request.session:\n    request.session['ffsession'] = friendfeed.FriendFeed(uname, rkey)\nffsession = request.session['ffsession']\nif 'favs' in request.session:\n    del request.session['favs']\nif 'blind_entries' in request.session:\n    del request.session['blind_entries']\nif 'entry_index' in request.session:\n    del request.session['entry_index']\nif 'entry_index' not in request.session:\n    request.session['entry_index'] = 0\nif 'favs' not in request.session:\n    favs = ffsession.fetch_favorites()['entries']\n    seed()\n    shuffle(favs) \n    request.session['favs'] = []\n    users = {}\n    # Find 5 posts by unique people\n    for f in favs:\n        # TODO: Don't hardcode 5\n        if len(request.session['favs']) >= ENTRY_SEQUENCE_LENGTH: break\n        u = f[\"from\"][\"id\"]\n        # Unique posts by author\n        if u in users: continue\n        request.session['favs'].append(f)\n        users[u] = True\nif 'blind_entries' not in request.session:\n    favs = request.session['favs']\n    request.session['blind_entries'] = []\n    for e in favs:\n        btxt = e[u\"body\"]\n        if \"thumbnails\" in e:\n            btxt += \"<br>\"\n            for t in e[\"thumbnails\"]:\n                # TODO: check that url and link don't contain \"'\"\n                btxt += \"<a href='%s'><img src='%s' width=%d height=%d></a>\" % (t[\"link\"], t[\"url\"], t[\"width\"], t[\"height\"])\n#        if \"comments\" in e:\n#            btxt += \"<table border=1><tr><td>Comments:</td></tr>\"\n#            for c in e[\"comments\"]:\n#                btxt += \"<tr><td>%s</td></tr>\" % c[\"body\"]\n#            btxt += \"</table>\"\n#        import re\n#        btxt = re.sub(\"lt\", \"gt\", btxt)\n        request.session['blind_entries'].append(btxt)", "path": "feed\\views.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"\nTODO: Want to get the user's timeline, not the public timeline.\n\"\"\"\n", "func_signal": "def twitter_feed(request, user):\n", "code": "import twitter\ntwitterapi = twitter.Api()      # NOTE: This is an unauthenticated twitter\nimport re\nusernamere = re.compile(\"@\\S+\")\n\n#timeline = twitterapi.GetFriendsTimeline(user, count=200)\ntimeline = twitterapi.GetFriendsTimeline(user)\n#timeline = twitterapi.GetPublicTimeline()\nfor status in timeline:\n    status.blind_text = status.GetText()\n    status.blind_text = usernamere.sub(\"@anonymous\", status.blind_text)\nreturn render_to_response(\"feed.html\", {\"timeline\": timeline}, context_instance=RequestContext(request))", "path": "feed\\views.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Returns the entries the given user has commented on.\"\"\"\n", "func_signal": "def fetch_user_comments_feed(self, nickname, **kwargs):\n", "code": "return self._fetch_feed(\n    \"/api/feed/user/\" + urllib.quote_plus(nickname) + \"/comments\",\n    **kwargs)", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Creates a new FriendFeed session for the given user.\n\nThe credentials are optional for some operations, but required for\nprivate feeds and all operations that write data, like publish_link.\n\"\"\"\n", "func_signal": "def __init__(self, auth_nickname=None, auth_key=None):\n", "code": "self.auth_nickname = auth_nickname\nself.auth_key = auth_key", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"'Likes' the entry with the given ID.\"\"\"\n", "func_signal": "def add_like(self, entry_id):\n", "code": "self._fetch(\"/api/like\", {\n    \"entry\": entry_id,\n})", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Deletes the comment with the given ID.\"\"\"\n", "func_signal": "def delete_comment(self, entry_id, comment_id):\n", "code": "self._fetch(\"/api/comment/delete\", {\n    \"entry\": entry_id,\n    \"comment\": comment_id,\n})", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Un-deletes the comment with the given ID.\"\"\"\n", "func_signal": "def undelete_comment(self, entry_id, comment_id):\n", "code": "self._fetch(\"/api/comment/delete\", {\n    \"entry\": entry_id,\n    \"comment\": comment_id,\n    \"undelete\": 1,\n})", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"\nReturn percentage string of a and b, e.g.:\n    \"1 of 10 (10%)\"\n\"\"\"\n", "func_signal": "def percent(a, b):\n", "code": "assert a <= b\nassert a >= 0\nassert b > 0\nreturn \"%s of %s (%.2f%%)\" % (a, b, 100.*a/b)", "path": "feed\\views.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Searches over entries in FriendFeed.\n\nIf the request is authenticated, the default scope is over all of the\nentries in the authenticated user's Friends Feed. If the request is\nnot authenticated, the default scope is over all public entries.\n\nThe query syntax is the same syntax as\nhttp://friendfeed.com/advancedsearch\n\"\"\"\n", "func_signal": "def search(self, q, **kwargs):\n", "code": "kwargs[\"q\"] = q\nreturn self._fetch_feed(\"/api/feed/search\", **kwargs)", "path": "friendfeed.py", "repo_name": "turian/doubleblind", "stars": 4, "license": "None", "language": "python", "size": 321}
{"docstring": "\"\"\"Constructor takes the following parameters:\n- C{configuration} - a reference to the application\n  L{configuration<PyWebMvcConfiguration>}.\n- C{globalForwardKey} - string. the id of the global forward this local\n  forward will go to.\n- C{redirect} - boolean. Whether to do a local forward or a browser\n  redirect to get to the next action.\n\"\"\"\n", "func_signal": "def __init__(self,configuration, globalForwardKey, redirect=False):\n", "code": "self.configuration = configuration\nself.globalForwardKey = globalForwardKey\nsuper(LocalForward,self).__init__( lambda req,mapping: configuration.globalForwards[globalForwardKey], redirect=redirect)", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Subclasses may implement this method for custom hidden field rendering.  Otherwise, rendering is redirected to the generic hidden renderer.\"\"\"\n", "func_signal": "def renderHidden(self, request, mapping, name, value, cssClass, **kwargs):\n", "code": "fmd = mapping.getFormMetadata(request).getFieldMetadata(name)\nif fmd.list:\n  type = Renderer.TYPE_HIDDEN_LIST\nelse:\n  type = Renderer.TYPE_HIDDEN\nredirect = self.getRenderer(request, mapping, type, name)\nreturn redirect.render(request, mapping, name, value, cssClass, **kwargs)", "path": "src\\code\\pywebmvc\\framework\\render\\widget.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"puts the whole page together in the correct order. This is not usually\noverridden\"\"\"\n", "func_signal": "def render(self,req, mapping):\n", "code": "page = \"%s%s%s%s%s%s%s\" % (\n                            self.getPreamble(req,mapping),\n                            self.getPageStart(req,mapping),\n                            self.getHeaderStart(req,mapping),\n                            self.getHeaderEnd(req,mapping),\n                            self.getBodyStart(req,mapping),\n                            self.getBodyEnd(req,mapping),\n                            self.getPageEnd(req,mapping)\n                          )\nreturn page.encode(self.getCharset(req,mapping))", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Construct the full, server-relative URL for this action mapping.\n\"\"\"\n", "func_signal": "def getUrl(self):\n", "code": "url = self.path\nif self.config.prefix:\n  url = self.config.prefix + url\nif self.config.extension:\n  url += \".\"+self.config.extension\nreturn url", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Adds a L{forward<ActionForward>} with the specified globalId.\nGlobal forwards are created automatically for each L{page<PageMapping>} and\nL{action<ActionMapping>} mapping.\"\"\"\n", "func_signal": "def addGlobalForward(self,globalId,forward):\n", "code": "if self.globalForwards.has_key(globalId):\n  raise PyWebMvcInvalidConfigurationException(\"global forward id '%s' already exists\" % (globalId))\nelse:\n  self.globalForwards[globalId] = forward", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"boolean check function to see if a forward would be found by\nL{findForward}.\"\"\"\n", "func_signal": "def hasForward(self, id):\n", "code": "try:\n  self.findForward(id)\n  return True\nexcept KeyError:\n  return False", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Accepts a C{mod_python} request object and uses the following options to create the application L{configuration<pywebmvc.framework.core.PyWebMvcConfiguration>}:\n  - C{pyWebMvcConfig} - The full path to the configuration xml file. E.g.\n    /home/foo/myapp/config.xml\n  - C{pyWebMvcPrefix} - The application context for this application.\n    Defaults to \"/\", If overridden, should begin with a \"/\" but not end with\n    one.\n  - C{pyWebMvcExtension} - If provided, add C{\".\"+extension} to each action\n    path. If omitted, no extension will be used.\n\"\"\"\n", "func_signal": "def readConfig(req):\n", "code": "options = req.get_options()\nfile = options[\"pyWebMvcConfig\"]\nprefix = None\nextension = None\nif options.has_key(\"pyWebMvcPrefix\"):\n  prefix = options[\"pyWebMvcPrefix\"]\nif options.has_key(\"pyWebMvcExtension\"):\n  extension = options[\"pyWebMvcExtension\"]\nreturn readConfigFromFile(file,prefix,extension)", "path": "src\\code\\pywebmvc\\framework\\parser.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"returns the url to be used as the base for the page according to the\naction mapping context. This will ensure that pages work correctly when\nserver-side forwards are used. not usually overridden.\"\"\"\n", "func_signal": "def getBase(self, req, mapping):\n", "code": "from formutils import href\nfrom mod_python import apache as mod_apache\ntry:\n  host = req.hostname\n  if req.parsed_uri and req.parsed_uri[mod_apache.URI_PORT]:\n    host += \":\"+req.parsed_uri[mod_apache.URI_PORT]\n  return \"%s://%s%s\" % (\n  (\"http\",\"https\")[self.isHttps(req)],\n  host,\n  href(req, mapping.id)\n  )\nexcept:\n  return None", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Constructor parameters:\n- id - string.\n- pythonClass - The imported class object referred to by the python-class\n  attribute. Should be an instance of L{Page} but can be anything that\n  implements C{__call__(request, mapping)}.\n\"\"\"\n", "func_signal": "def __init__(self, id, pythonClass):\n", "code": "self.id = id\nself.pythonClass = pythonClass", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Adds a L{PageMapping} to the application.\"\"\"\n", "func_signal": "def addPageMapping(self,pageMapping):\n", "code": "self.pageMappings[pageMapping.id] = pageMapping\nself.addGlobalForward(pageMapping.id,ActionForward(pageMapping.createInstance()))", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Constructor takes the following parameters:\n  - C{req} - the request object.\n  - C{forward} - the original forward instance.\n  - C{urlQueryString} - the query string. everything after the question\n    mark.\n\"\"\"\n", "func_signal": "def __init__(self, req, forward, urlQueryString):\n", "code": "self.origForward = forward\nself.queryString = urlQueryString\nsuper(AppendQueryForward,self).__init__( req, lambda req,mapping: None, True)", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"add a field named C{name} with a tab index of C{index}. If index is not\nspecified, it will be assigned the index in the order of the unspecified\nfields in the form. TODO: make all the unspecified fields come after the\nspecified fields (as it is in html) -- currently they come first.\"\"\"\n", "func_signal": "def add(self, name, index = None):\n", "code": "if index is None:\n  while self.indexDict.has_key(self.count):\n    self.count += 1\n  index = self.count\nself.fieldDict[name] = index\nself.indexDict[index] = name", "path": "src\\code\\pywebmvc\\framework\\util.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"returns the exclusive end index\"\"\"\n", "func_signal": "def getEndIndex(self,req):\n", "code": "endIndex = self.getStartIndex(req)+self.__getCurrentPageSize(req)\nreturn min(self.__getListSize(req),endIndex)", "path": "src\\code\\pywebmvc\\tools\\table.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"If the C{html} is longer than C{size}, truncate it and have the truncated\ntext become a hyperlink that, when clicked, displays the full text.\"\"\"\n", "func_signal": "def truncateText(html, size):\n", "code": "if len(html) > size:\n  return '<a href=\"\" onclick=\"alert(this.firstChild.firstChild.nodeValue); return false;\"><span style=\"display: none;\">%s</span>%s...</a>' % (escapeHtml(html),escapeHtml(html[0:size]))\nelse:\n  return html", "path": "src\\code\\pywebmvc\\framework\\htmlutil.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Subclasses may implement this method for custom read-only rendering.  Otherwise, rendering is redirected to the generic read-only renderer.\"\"\"\n", "func_signal": "def renderReadOnly(self, request, mapping, name, value, cssClass, **kwargs):\n", "code": "fmd = mapping.getFormMetadata(request).getFieldMetadata(name)\nif fmd.list:\n  type = Renderer.TYPE_READ_ONLY_LIST\nelse:\n  type = Renderer.TYPE_READ_ONLY\nredirect = self.getRenderer(request, mapping, type, name)\nreturn redirect.render(request, mapping, name, value, cssClass, **kwargs)", "path": "src\\code\\pywebmvc\\framework\\render\\widget.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Constructor takes the following parameters:\n- C{id} - string\n- C{path} - string\n- C{pythonClass} - the class object referred to by C{python.module::class}.\n- C{config} - a backreference to the\n  L{configuration<PyWebMvcConfiguration>}.  Stored as a weak reference.\n- C{formMetadata} - the L{form metadata<metadata.FormMetadata>} object\n  referred to by id in the xml.\n- C{rendererFactory} - the L{renderer factory<pywebmvc.framework.render.factory.RendererFactory>}\n  object referred to by id in the xml.\n\"\"\"\n", "func_signal": "def __init__(self, id, path, pythonClass, config, formMetadata = None, rendererFactory=None):\n", "code": "self.config = weakref.proxy(config)\nself.id = id\nself.path = path\nself.pythonClass = pythonClass\nself.formMetadata = formMetadata\nself.rendererFactory = rendererFactory\nself.forwards = {}\nself.attributes = {}", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"Gets the L{ActionMapping} having the specified C{path} (prefix\nand extension removed).\"\"\"\n", "func_signal": "def getActionMappingByPath(self,path):\n", "code": "for actionMapping in self.actionMappings.values():\n  if actionMapping.path == path:\n    return actionMapping\nraise ActionNotFoundException(path)", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"C{prefix} and C{extension} are read from the apache configuration::\n\nPythonOption pyWebMvcPrefix /myapp\nPythonOption pyWebMvcExtension pwm\n\"\"\"\n", "func_signal": "def __init__(self,prefix,extension):\n", "code": "from render.factory import pyWebMvcDefaultRendererFactory\nfrom metadata import FieldMetadata, FormMetadata, MetadataGroup\nself.prefix = prefix\nself.extension = extension\nself.typeMetadata = {}\nself.rendererFactories = {}\nself.defaultRendererFactory = pyWebMvcDefaultRendererFactory\nself.formMetadata = {}\nself.pageMappings = {}\nself.actionMappings = {}\nself.globalForwards = {}\nself.errorHandlers = []\nself.metadataClasses = {\n  \"field\" : FieldMetadata,\n  \"form\" : FormMetadata,\n  \"fieldgroup\" : MetadataGroup,\n}", "path": "src\\code\\pywebmvc\\framework\\core.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"**kwargs is a mapping of additional html attibute/values\n   to be rendered.\"\"\"\n", "func_signal": "def render(self, request, mapping, name, value, cssClass, **kwargs):\n", "code": "display = metadata.DISPLAY_NORMAL\nif kwargs.has_key(\"display\"):\n  display = kwargs[\"display\"]    \nif display == metadata.DISPLAY_HIDDEN:\n  return self.renderHidden(request, mapping, name, value, cssClass, **kwargs)\nelif display == metadata.DISPLAY_READONLY:\n  return self.renderReadOnly(request, mapping, name, value, cssClass, **kwargs)\nelse:\n  return self.renderNormal(request, mapping, name, value, cssClass, **kwargs)", "path": "src\\code\\pywebmvc\\framework\\render\\widget.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"returns the inclusive start index\"\"\"\n", "func_signal": "def getStartIndex(self,req):\n", "code": "startIndex = self.getCurrentPage(req)*self.__getCurrentPageSize(req)\nreturn min(self.__getListSize(req),startIndex)", "path": "src\\code\\pywebmvc\\tools\\table.py", "repo_name": "chriseppstein/pywebmvc", "stars": 4, "license": "None", "language": "python", "size": 515}
{"docstring": "\"\"\"\nAssociates the given object with a tag.\n\"\"\"\n", "func_signal": "def add_tag(self, obj, tag_name):\n", "code": "tag_names = parse_tag_input(tag_name)\nif not len(tag_names):\n    raise AttributeError(_('No tags were given: \"%s\".') % tag_name)\nif len(tag_names) > 1:\n    raise AttributeError(_('Multiple tags were given: \"%s\".') % tag_name)\ntag_name = tag_names[0]\nif settings.FORCE_LOWERCASE_TAGS:\n    tag_name = tag_name.lower()\ntag, created = self.get_or_create(name=tag_name)\nctype = ContentType.objects.get_for_model(obj)\nTaggedItem._default_manager.get_or_create(\n    tag=tag, content_type=ctype, object_id=obj.pk)", "path": "tagging\\models.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nTag getter. Returns an instance's tags if accessed on an instance, and\nall of a model's tags if called on a class. That is, this model::\n\n   class Link(models.Model):\n       ...\n       tags = TagField()\n\nLets you do both of these::\n\n   >>> l = Link.objects.get(...)\n   >>> l.tags\n   'tag1 tag2 tag3'\n\n   >>> Link.tags\n   'tag1 tag2 tag3 tag4'\n\n\"\"\"\n# Handle access on the model (i.e. Link.tags)\n", "func_signal": "def __get__(self, instance, owner=None):\n", "code": "if instance is None:\n    return edit_string_for_tags(Tag.objects.usage_for_model(owner))\n\ntags = self._get_instance_tag_cache(instance)\nif tags is None:\n    if instance.pk is None:\n        self._set_instance_tag_cache(instance, '')\n    else:\n        self._set_instance_tag_cache(\n            instance, edit_string_for_tags(Tag.objects.get_for_object(instance)))\nreturn self._get_instance_tag_cache(instance)", "path": "tagging\\fields.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nUtility function for accepting tag input in a flexible manner.\n\nIf a ``Tag`` object is given, it will be returned in a list as\nits single occupant.\n\nIf given, the tag names in the following will be used to create a\n``Tag`` ``QuerySet``:\n\n   * A string, which may contain multiple tag names.\n   * A list or tuple of strings corresponding to tag names.\n   * A list or tuple of integers corresponding to tag ids.\n\nIf given, the following will be returned as-is:\n\n   * A list or tuple of ``Tag`` objects.\n   * A ``Tag`` ``QuerySet``.\n\n\"\"\"\n", "func_signal": "def get_tag_list(tags):\n", "code": "from tagging.models import Tag\nif isinstance(tags, Tag):\n    return [tags]\nelif isinstance(tags, QuerySet) and tags.model is Tag:\n    return tags\nelif isinstance(tags, types.StringTypes):\n    return Tag.objects.filter(name__in=parse_tag_input(tags))\nelif isinstance(tags, (types.ListType, types.TupleType)):\n    if len(tags) == 0:\n        return tags\n    contents = set()\n    for item in tags:\n        if isinstance(item, types.StringTypes):\n            contents.add('string')\n        elif isinstance(item, Tag):\n            contents.add('tag')\n        elif isinstance(item, (types.IntType, types.LongType)):\n            contents.add('int')\n    if len(contents) == 1:\n        if 'string' in contents:\n            return Tag.objects.filter(name__in=[force_unicode(tag) \\\n                                                for tag in tags])\n        elif 'tag' in contents:\n            return tags\n        elif 'int' in contents:\n            return Tag.objects.filter(id__in=tags)\n    else:\n        raise ValueError(_('If a list or tuple of tags is provided, they must all be tag names, Tag objects or Tag ids.'))\nelse:\n    raise ValueError(_('The tag input given was invalid.'))", "path": "tagging\\utils.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nGiven list of ``Tag`` instances, creates a string representation of\nthe list suitable for editing by the user, such that submitting the\ngiven string representation back without changing it will give the\nsame list of tags.\n\nTag names which contain commas will be double quoted.\n\nIf any tag name which isn't being quoted contains whitespace, the\nresulting string of tag names will be comma-delimited, otherwise\nit will be space-delimited.\n\"\"\"\n", "func_signal": "def edit_string_for_tags(tags):\n", "code": "names = []\nuse_commas = False\nfor tag in tags:\n    name = tag.name\n    if u',' in name:\n        names.append('\"%s\"' % name)\n        continue\n    elif u' ' in name:\n        if not use_commas:\n            use_commas = True\n    names.append(name)\nif use_commas:\n    glue = u', '\nelse:\n    glue = u' '\nreturn glue.join(names)", "path": "tagging\\utils.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nObtain a list of tags associated with instances of the given\nModel class.\n\nIf ``counts`` is True, a ``count`` attribute will be added to\neach tag, indicating how many times it has been used against\nthe Model class in question.\n\nIf ``min_count`` is given, only tags which have a ``count``\ngreater than or equal to ``min_count`` will be returned.\nPassing a value for ``min_count`` implies ``counts=True``.\n\nTo limit the tags (and counts, if specified) returned to those\nused by a subset of the Model's instances, pass a dictionary\nof field lookups to be applied to the given Model as the\n``filters`` argument.\n\"\"\"\n", "func_signal": "def usage_for_model(self, model, counts=False, min_count=None, filters=None):\n", "code": "if filters is None: filters = {}\n\nqueryset = model._default_manager.filter()\nfor f in filters.items():\n    queryset.query.add_filter(f)\nusage = self.usage_for_queryset(queryset, counts, min_count)\n\nreturn usage", "path": "tagging\\models.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nGiven a ``QuerySet`` or a ``Model``, returns a two-tuple of\n(queryset, model).\n\nIf a ``Model`` is given, the ``QuerySet`` returned will be created\nusing its default manager.\n\"\"\"\n", "func_signal": "def get_queryset_and_model(queryset_or_model):\n", "code": "try:\n    return queryset_or_model, queryset_or_model.model\nexcept AttributeError:\n    return queryset_or_model._default_manager.all(), queryset_or_model", "path": "tagging\\utils.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nSplit a pathname into components (the opposite of os.path.join) in a\nplatform-neutral way.\n\"\"\"\n", "func_signal": "def fullsplit(path, result=None):\n", "code": "if result is None:\n    result = []\nhead, tail = os.path.split(path)\nif head == '':\n    return [tail] + result\nif head == path:\n    return result\nreturn fullsplit(head, [tail] + result)", "path": "setup.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nCreate a ``QuerySet`` containing instances of the specified\nmodel associated with *all* of the given list of tags.\n\"\"\"\n", "func_signal": "def get_intersection_by_model(self, queryset_or_model, tags):\n", "code": "tags = get_tag_list(tags)\ntag_count = len(tags)\nqueryset, model = get_queryset_and_model(queryset_or_model)\n\nif not tag_count:\n    return model._default_manager.none()\n\nmodel_table = qn(model._meta.db_table)\n# This query selects the ids of all objects which have all the\n# given tags.\nquery = \"\"\"\nSELECT %(model_pk)s\nFROM %(model)s, %(tagged_item)s\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tagged_item)s.tag_id IN (%(tag_id_placeholders)s)\n  AND %(model_pk)s = %(tagged_item)s.object_id\nGROUP BY %(model_pk)s\nHAVING COUNT(%(model_pk)s) = %(tag_count)s\"\"\" % {\n    'model_pk': '%s.%s' % (model_table, qn(model._meta.pk.column)),\n    'model': model_table,\n    'tagged_item': qn(self.model._meta.db_table),\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n    'tag_id_placeholders': ','.join(['%s'] * tag_count),\n    'tag_count': tag_count,\n}\n\ncursor = connection.cursor()\ncursor.execute(query, [tag.pk for tag in tags])\nobject_ids = [row[0] for row in cursor.fetchall()]\nif len(object_ids) > 0:\n    return queryset.filter(pk__in=object_ids)\nelse:\n    return model._default_manager.none()", "path": "tagging\\models.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nSet an object's tags.\n\"\"\"\n", "func_signal": "def __set__(self, instance, value):\n", "code": "if instance is None:\n    raise AttributeError(_('%s can only be set on instances.') % self.name)\nif settings.FORCE_LOWERCASE_TAGS and value is not None:\n    value = value.lower()\nself._set_instance_tag_cache(instance, value)", "path": "tagging\\fields.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nCreate a ``QuerySet`` containing instances of the specified\nmodel associated with a given tag or list of tags.\n\"\"\"\n", "func_signal": "def get_by_model(self, queryset_or_model, tags):\n", "code": "tags = get_tag_list(tags)\ntag_count = len(tags)\nif tag_count == 0:\n    # No existing tags were given\n    queryset, model = get_queryset_and_model(queryset_or_model)\n    return model._default_manager.none()\nelif tag_count == 1:\n    # Optimisation for single tag - fall through to the simpler\n    # query below.\n    tag = tags[0]\nelse:\n    return self.get_intersection_by_model(queryset_or_model, tags)\n\nqueryset, model = get_queryset_and_model(queryset_or_model)\ncontent_type = ContentType.objects.get_for_model(model)\nopts = self.model._meta\ntagged_item_table = qn(opts.db_table)\nreturn queryset.extra(\n    tables=[opts.db_table],\n    where=[\n        '%s.content_type_id = %%s' % tagged_item_table,\n        '%s.tag_id = %%s' % tagged_item_table,\n        '%s.%s = %s.object_id' % (qn(model._meta.db_table),\n                                  qn(model._meta.pk.column),\n                                  tagged_item_table)\n    ],\n    params=[content_type.pk, tag.pk],\n)", "path": "tagging\\models.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nPerform the custom SQL query for ``usage_for_model`` and\n``usage_for_queryset``.\n\"\"\"\n", "func_signal": "def _get_usage(self, model, counts=False, min_count=None, extra_joins=None, extra_criteria=None, params=None):\n", "code": "if min_count is not None: counts = True\n\nmodel_table = qn(model._meta.db_table)\nmodel_pk = '%s.%s' % (model_table, qn(model._meta.pk.column))\nquery = \"\"\"\nSELECT DISTINCT %(tag)s.id, %(tag)s.name%(count_sql)s\nFROM\n    %(tag)s\n    INNER JOIN %(tagged_item)s\n        ON %(tag)s.id = %(tagged_item)s.tag_id\n    INNER JOIN %(model)s\n        ON %(tagged_item)s.object_id = %(model_pk)s\n    %%s\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n    %%s\nGROUP BY %(tag)s.id, %(tag)s.name\n%%s\nORDER BY %(tag)s.name ASC\"\"\" % {\n    'tag': qn(self.model._meta.db_table),\n    'count_sql': counts and (', COUNT(%s)' % model_pk) or '',\n    'tagged_item': qn(TaggedItem._meta.db_table),\n    'model': model_table,\n    'model_pk': model_pk,\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n}\n\nmin_count_sql = ''\nif min_count is not None:\n    min_count_sql = 'HAVING COUNT(%s) >= %%s' % model_pk\n    params.append(min_count)\n\ncursor = connection.cursor()\ncursor.execute(query % (extra_joins, extra_criteria, min_count_sql), params)\ntags = []\nfor row in cursor.fetchall():\n    t = self.model(*row[:2])\n    if counts:\n        t.count = row[2]\n    tags.append(t)\nreturn tags", "path": "tagging\\models.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nAdd a ``font_size`` attribute to each tag according to the\nfrequency of its use, as indicated by its ``count``\nattribute.\n\n``steps`` defines the range of font sizes - ``font_size`` will\nbe an integer between 1 and ``steps`` (inclusive).\n\n``distribution`` defines the type of font size distribution\nalgorithm which will be used - logarithmic or linear. It must be\none of ``tagging.utils.LOGARITHMIC`` or ``tagging.utils.LINEAR``.\n\"\"\"\n", "func_signal": "def calculate_cloud(tags, steps=4, distribution=LOGARITHMIC):\n", "code": "if len(tags) > 0:\n    counts = [tag.count for tag in tags]\n    min_weight = float(min(counts))\n    max_weight = float(max(counts))\n    thresholds = _calculate_thresholds(min_weight, max_weight, steps)\n    for tag in tags:\n        font_set = False\n        tag_weight = _calculate_tag_weight(tag.count, max_weight, distribution)\n        for i in range(steps):\n            if not font_set and tag_weight <= thresholds[i]:\n                tag.font_size = i + 1\n                font_set = True\nreturn tags", "path": "tagging\\utils.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nUtility function for accepting single tag input in a flexible\nmanner.\n\nIf a ``Tag`` object is given it will be returned as-is; if a\nstring or integer are given, they will be used to lookup the\nappropriate ``Tag``.\n\nIf no matching tag can be found, ``None`` will be returned.\n\"\"\"\n", "func_signal": "def get_tag(tag):\n", "code": "from tagging.models import Tag\nif isinstance(tag, Tag):\n    return tag\n\ntry:\n    if isinstance(tag, types.StringTypes):\n        return Tag.objects.get(name=tag)\n    elif isinstance(tag, (types.IntType, types.LongType)):\n        return Tag.objects.get(id=tag)\nexcept Tag.DoesNotExist:\n    pass\n\nreturn None", "path": "tagging\\utils.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nObtain a list of tags related to a given list of tags - that\nis, other tags used by items which have all the given tags.\n\nIf ``counts`` is True, a ``count`` attribute will be added to\neach tag, indicating the number of items which have it in\naddition to the given list of tags.\n\nIf ``min_count`` is given, only tags which have a ``count``\ngreater than or equal to ``min_count`` will be returned.\nPassing a value for ``min_count`` implies ``counts=True``.\n\"\"\"\n", "func_signal": "def related_for_model(self, tags, model, counts=False, min_count=None):\n", "code": "if min_count is not None: counts = True\ntags = get_tag_list(tags)\ntag_count = len(tags)\ntagged_item_table = qn(TaggedItem._meta.db_table)\nquery = \"\"\"\nSELECT %(tag)s.id, %(tag)s.name%(count_sql)s\nFROM %(tagged_item)s INNER JOIN %(tag)s ON %(tagged_item)s.tag_id = %(tag)s.id\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tagged_item)s.object_id IN\n  (\n      SELECT %(tagged_item)s.object_id\n      FROM %(tagged_item)s, %(tag)s\n      WHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n        AND %(tag)s.id = %(tagged_item)s.tag_id\n        AND %(tag)s.id IN (%(tag_id_placeholders)s)\n      GROUP BY %(tagged_item)s.object_id\n      HAVING COUNT(%(tagged_item)s.object_id) = %(tag_count)s\n  )\n  AND %(tag)s.id NOT IN (%(tag_id_placeholders)s)\nGROUP BY %(tag)s.id, %(tag)s.name\n%(min_count_sql)s\nORDER BY %(tag)s.name ASC\"\"\" % {\n    'tag': qn(self.model._meta.db_table),\n    'count_sql': counts and ', COUNT(%s.object_id)' % tagged_item_table or '',\n    'tagged_item': tagged_item_table,\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n    'tag_id_placeholders': ','.join(['%s'] * tag_count),\n    'tag_count': tag_count,\n    'min_count_sql': min_count is not None and ('HAVING COUNT(%s.object_id) >= %%s' % tagged_item_table) or '',\n}\n\nparams = [tag.pk for tag in tags] * 2\nif min_count is not None:\n    params.append(min_count)\n\ncursor = connection.cursor()\ncursor.execute(query, params)\nrelated = []\nfor row in cursor.fetchall():\n    tag = self.model(*row[:2])\n    if counts is True:\n        tag.count = row[2]\n    related.append(tag)\nreturn related", "path": "tagging\\models.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nRetrieve a list of instances of the specified model which share\ntags with the model instance ``obj``, ordered by the number of\nshared tags in descending order.\n\nIf ``num`` is given, a maximum of ``num`` instances will be\nreturned.\n\"\"\"\n", "func_signal": "def get_related(self, obj, queryset_or_model, num=None):\n", "code": "queryset, model = get_queryset_and_model(queryset_or_model)\nmodel_table = qn(model._meta.db_table)\ncontent_type = ContentType.objects.get_for_model(obj)\nrelated_content_type = ContentType.objects.get_for_model(model)\nquery = \"\"\"\nSELECT %(model_pk)s, COUNT(related_tagged_item.object_id) AS %(count)s\nFROM %(model)s, %(tagged_item)s, %(tag)s, %(tagged_item)s related_tagged_item\nWHERE %(tagged_item)s.object_id = %%s\n  AND %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tag)s.id = %(tagged_item)s.tag_id\n  AND related_tagged_item.content_type_id = %(related_content_type_id)s\n  AND related_tagged_item.tag_id = %(tagged_item)s.tag_id\n  AND %(model_pk)s = related_tagged_item.object_id\"\"\"\nif content_type.pk == related_content_type.pk:\n    # Exclude the given instance itself if determining related\n    # instances for the same model.\n    query += \"\"\"\n  AND related_tagged_item.object_id != %(tagged_item)s.object_id\"\"\"\nquery += \"\"\"\nGROUP BY %(model_pk)s\nORDER BY %(count)s DESC\n%(limit_offset)s\"\"\"\nquery = query % {\n    'model_pk': '%s.%s' % (model_table, qn(model._meta.pk.column)),\n    'count': qn('count'),\n    'model': model_table,\n    'tagged_item': qn(self.model._meta.db_table),\n    'tag': qn(self.model._meta.get_field('tag').rel.to._meta.db_table),\n    'content_type_id': content_type.pk,\n    'related_content_type_id': related_content_type.pk,\n    # Hardcoding this for now just to get tests working again - this\n    # should now be handled by the query object.\n    'limit_offset': num is not None and 'LIMIT %s' or '',\n}\n\ncursor = connection.cursor()\nparams = [obj.pk]\nif num is not None:\n    params.append(num)\ncursor.execute(query, params)\nobject_ids = [row[0] for row in cursor.fetchall()]\nif len(object_ids) > 0:\n    # Use in_bulk here instead of an id__in lookup, because id__in would\n    # clobber the ordering.\n    object_dict = queryset.in_bulk(object_ids)\n    return [object_dict[object_id] for object_id in object_ids \\\n            if object_id in object_dict]\nelse:\n    return []", "path": "tagging\\models.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nSplits ``input`` on ``delimiter``, stripping each resulting string\nand returning a list of non-empty strings.\n\"\"\"\n", "func_signal": "def split_strip(input, delimiter=u','):\n", "code": "if not input:\n    return []\n\nwords = [w.strip() for w in input.split(delimiter)]\nreturn [w for w in words if w]", "path": "tagging\\utils.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nParses tag input, with multiple word input being activated and\ndelineated by commas and double quotes. Quotes take precedence, so\nthey may contain commas.\n\nReturns a sorted list of unique tag names.\n\"\"\"\n", "func_signal": "def parse_tag_input(input):\n", "code": "if not input:\n    return []\n\ninput = force_unicode(input)\n\n# Special case - if there are no commas or double quotes in the\n# input, we don't *do* a recall... I mean, we know we only need to\n# split on spaces.\nif u',' not in input and u'\"' not in input:\n    words = list(set(split_strip(input, u' ')))\n    words.sort()\n    return words\n\nwords = []\nbuffer = []\n# Defer splitting of non-quoted sections until we know if there are\n# any unquoted commas.\nto_be_split = []\nsaw_loose_comma = False\nopen_quote = False\ni = iter(input)\ntry:\n    while 1:\n        c = i.next()\n        if c == u'\"':\n            if buffer:\n                to_be_split.append(u''.join(buffer))\n                buffer = []\n            # Find the matching quote\n            open_quote = True\n            c = i.next()\n            while c != u'\"':\n                buffer.append(c)\n                c = i.next()\n            if buffer:\n                word = u''.join(buffer).strip()\n                if word:\n                    words.append(word)\n                buffer = []\n            open_quote = False\n        else:\n            if not saw_loose_comma and c == u',':\n                saw_loose_comma = True\n            buffer.append(c)\nexcept StopIteration:\n    # If we were parsing an open quote which was never closed treat\n    # the buffer as unquoted.\n    if buffer:\n        if open_quote and u',' in buffer:\n            saw_loose_comma = True\n        to_be_split.append(u''.join(buffer))\nif to_be_split:\n    if saw_loose_comma:\n        delimiter = u','\n    else:\n        delimiter = u' '\n    for chunk in to_be_split:\n        words.extend(split_strip(chunk, delimiter))\nwords = list(set(words))\nwords.sort()\nreturn words", "path": "tagging\\utils.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nLogarithmic tag weight calculation is based on code from the\n`Tag Cloud`_ plugin for Mephisto, by Sven Fuchs.\n\n.. _`Tag Cloud`: http://www.artweb-design.de/projects/mephisto-plugin-tag-cloud\n\"\"\"\n", "func_signal": "def _calculate_tag_weight(weight, max_weight, distribution):\n", "code": "if distribution == LINEAR or max_weight == 1:\n    return weight\nelif distribution == LOGARITHMIC:\n    return math.log(weight) * max_weight / math.log(max_weight)\nraise ValueError(_('Invalid distribution algorithm specified: %s.') % distribution)", "path": "tagging\\utils.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nCreate a queryset matching all tags associated with the given\nobject.\n\"\"\"\n", "func_signal": "def get_for_object(self, obj):\n", "code": "ctype = ContentType.objects.get_for_model(obj)\nreturn self.filter(items__content_type__pk=ctype.pk,\n                   items__object_id=obj.pk)", "path": "tagging\\models.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"\nCreate a ``QuerySet`` containing instances of the specified\nmodel associated with *any* of the given list of tags.\n\"\"\"\n", "func_signal": "def get_union_by_model(self, queryset_or_model, tags):\n", "code": "tags = get_tag_list(tags)\ntag_count = len(tags)\nqueryset, model = get_queryset_and_model(queryset_or_model)\n\nif not tag_count:\n    return model._default_manager.none()\n\nmodel_table = qn(model._meta.db_table)\n# This query selects the ids of all objects which have any of\n# the given tags.\nquery = \"\"\"\nSELECT %(model_pk)s\nFROM %(model)s, %(tagged_item)s\nWHERE %(tagged_item)s.content_type_id = %(content_type_id)s\n  AND %(tagged_item)s.tag_id IN (%(tag_id_placeholders)s)\n  AND %(model_pk)s = %(tagged_item)s.object_id\nGROUP BY %(model_pk)s\"\"\" % {\n    'model_pk': '%s.%s' % (model_table, qn(model._meta.pk.column)),\n    'model': model_table,\n    'tagged_item': qn(self.model._meta.db_table),\n    'content_type_id': ContentType.objects.get_for_model(model).pk,\n    'tag_id_placeholders': ','.join(['%s'] * tag_count),\n}\n\ncursor = connection.cursor()\ncursor.execute(query, [tag.pk for tag in tags])\nobject_ids = [row[0] for row in cursor.fetchall()]\nif len(object_ids) > 0:\n    return queryset.filter(pk__in=object_ids)\nelse:\n    return model._default_manager.none()", "path": "tagging\\models.py", "repo_name": "ryszard/django-tagging", "stars": 6, "license": "other", "language": "python", "size": 216}
{"docstring": "\"\"\"Create the byte sequences for a ``PLTE`` and if necessary a\n``tRNS`` chunk.  Returned as a pair (*p*, *t*).  *t* will be\n``None`` if no ``tRNS`` chunk is necessary.\n\"\"\"\n\n", "func_signal": "def make_palette(self):\n", "code": "p = array('B')\nt = array('B')\n\nfor x in self.palette:\n    p.extend(x[0:3])\n    if len(x) > 3:\n        t.append(x[3])\np = tostring(p)\nt = tostring(t)\nif t:\n    return p,t\nreturn p,None", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"\nGenerator for interlaced scanlines from an array.  `pixels` is\nthe full source image in flat row flat pixel format.  The\ngenerator yields each scanline of the reduced passes in turn, in\nboxed row flat pixel format.\n\"\"\"\n\n# http://www.w3.org/TR/PNG/#8InterlaceMethods\n# Array type.\n", "func_signal": "def array_scanlines_interlace(self, pixels):\n", "code": "fmt = 'BH'[self.bitdepth > 8]\n# Value per row\nvpr = self.width * self.planes\nfor xstart, ystart, xstep, ystep in _adam7:\n    if xstart >= self.width:\n        continue\n    # Pixels per row (of reduced image)\n    ppr = int(math.ceil((self.width-xstart)/float(xstep)))\n    # number of values in reduced image row.\n    row_len = ppr*self.planes\n    for y in range(ystart, self.height, ystep):\n        if xstep == 1:\n            offset = y * vpr\n            yield pixels[offset:offset+vpr]\n        else:\n            row = array(fmt)\n            # There's no easier way to set the length of an array\n            row.extend(pixels[0:row_len])\n            offset = y * vpr + xstart * self.planes\n            end_offset = (y+1) * vpr\n            skip = self.planes * xstep\n            for i in range(self.planes):\n                row[i::self.planes] = \\\n                    pixels[offset+i:end_offset:skip]\n            yield row", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"\nWrite a PNG chunk to the output file, including length and\nchecksum.\n\"\"\"\n\n# http://www.w3.org/TR/PNG/#5Chunk-layout\n", "func_signal": "def write_chunk(outfile, tag, data=''):\n", "code": "outfile.write(struct.pack(\"!I\", len(data)))\noutfile.write(tag)\noutfile.write(data)\nchecksum = zlib.crc32(tag)\nchecksum = zlib.crc32(data, checksum)\noutfile.write(struct.pack(\"!i\", checksum))", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"\nCreate a PNG test image and write the file to stdout.\n\"\"\"\n\n# Below is a big stack of test image generators.\n# They're all really tiny, so PEP 8 rules are suspended.\n\n", "func_signal": "def test_suite(options, args):\n", "code": "def test_gradient_horizontal_lr(x, y): return x\ndef test_gradient_horizontal_rl(x, y): return 1-x\ndef test_gradient_vertical_tb(x, y): return y\ndef test_gradient_vertical_bt(x, y): return 1-y\ndef test_radial_tl(x, y): return max(1-math.sqrt(x*x+y*y), 0.0)\ndef test_radial_center(x, y): return test_radial_tl(x-0.5, y-0.5)\ndef test_radial_tr(x, y): return test_radial_tl(1-x, y)\ndef test_radial_bl(x, y): return test_radial_tl(x, 1-y)\ndef test_radial_br(x, y): return test_radial_tl(1-x, 1-y)\ndef test_stripe(x, n): return float(int(x*n) & 1)\ndef test_stripe_h_2(x, y): return test_stripe(x, 2)\ndef test_stripe_h_4(x, y): return test_stripe(x, 4)\ndef test_stripe_h_10(x, y): return test_stripe(x, 10)\ndef test_stripe_v_2(x, y): return test_stripe(y, 2)\ndef test_stripe_v_4(x, y): return test_stripe(y, 4)\ndef test_stripe_v_10(x, y): return test_stripe(y, 10)\ndef test_stripe_lr_10(x, y): return test_stripe(x+y, 10)\ndef test_stripe_rl_10(x, y): return test_stripe(1+x-y, 10)\ndef test_checker(x, y, n): return float((int(x*n) & 1) ^ (int(y*n) & 1))\ndef test_checker_8(x, y): return test_checker(x, y, 8)\ndef test_checker_15(x, y): return test_checker(x, y, 15)\ndef test_zero(x, y): return 0\ndef test_one(x, y): return 1\n\ntest_patterns = {\n    'GLR': test_gradient_horizontal_lr,\n    'GRL': test_gradient_horizontal_rl,\n    'GTB': test_gradient_vertical_tb,\n    'GBT': test_gradient_vertical_bt,\n    'RTL': test_radial_tl,\n    'RTR': test_radial_tr,\n    'RBL': test_radial_bl,\n    'RBR': test_radial_br,\n    'RCTR': test_radial_center,\n    'HS2': test_stripe_h_2,\n    'HS4': test_stripe_h_4,\n    'HS10': test_stripe_h_10,\n    'VS2': test_stripe_v_2,\n    'VS4': test_stripe_v_4,\n    'VS10': test_stripe_v_10,\n    'LRS': test_stripe_lr_10,\n    'RLS': test_stripe_rl_10,\n    'CK8': test_checker_8,\n    'CK15': test_checker_15,\n    'ZERO': test_zero,\n    'ONE': test_one,\n    }\n\ndef test_pattern(width, height, bitdepth, pattern):\n    \"\"\"Create a single plane (monochrome) test pattern.  Returns a\n    flat row flat pixel array.\n    \"\"\"\n\n    maxval = 2**bitdepth-1\n    if maxval > 255:\n        a = array('H')\n    else:\n        a = array('B')\n    fw = float(width)\n    fh = float(height)\n    pfun = test_patterns[pattern]\n    for y in range(height):\n        fy = float(y)/fh\n        for x in range(width):\n            a.append(int(round(pfun(float(x)/fw, fy) * maxval)))\n    return a\n\ndef test_rgba(size=256, bitdepth=8,\n                red=\"GTB\", green=\"GLR\", blue=\"RTL\", alpha=None):\n    \"\"\"\n    Create a test image.  Each channel is generated from the\n    specified pattern; any channel apart from red can be set to\n    None, which will cause it not to be in the image.  It\n    is possible to create all PNG channel types (L, RGB, LA, RGBA),\n    as well as non PNG channel types (RGA, and so on).\n    \"\"\"\n\n    i = test_pattern(size, size, bitdepth, red)\n    psize = 1\n    for channel in (green, blue, alpha):\n        if channel:\n            c = test_pattern(size, size, bitdepth, channel)\n            i = interleave_planes(i, c, psize, 1)\n            psize += 1\n    return i\n\ndef pngsuite_image(name):\n    \"\"\"\n    Create a test image by reading an internal copy of the files\n    from the PngSuite.  Returned in flat row flat pixel format.\n    \"\"\"\n\n    if name not in _pngsuite:\n        raise NotImplementedError(\"cannot find PngSuite file %s (use -L for a list)\" % name)\n    r = Reader(bytes=_pngsuite[name])\n    w,h,pixels,meta = r.asDirect()\n    assert w == h\n    # LAn for n < 8 is a special case for which we need to rescale\n    # the data.\n    if meta['greyscale'] and meta['alpha'] and meta['bitdepth'] < 8:\n        factor = 255 // (2**meta['bitdepth']-1)\n        def rescale(data):\n            for row in data:\n                yield map(factor.__mul__, row)\n        pixels = rescale(pixels)\n        meta['bitdepth'] = 8\n    arraycode = 'BH'[meta['bitdepth']>8]\n    return w, array(arraycode, itertools.chain(*pixels)), meta\n\n# The body of test_suite()\nsize = 256\nif options.test_size:\n    size = options.test_size\noptions.bitdepth = 8\nif options.test_deep:\n    options.bitdepth = 16\noptions.greyscale=bool(options.test_black)\n\nkwargs = {}\nif options.test_red:\n    kwargs[\"red\"] = options.test_red\nif options.test_green:\n    kwargs[\"green\"] = options.test_green\nif options.test_blue:\n    kwargs[\"blue\"] = options.test_blue\nif options.test_alpha:\n    kwargs[\"alpha\"] = options.test_alpha\nif options.greyscale:\n    if options.test_red or options.test_green or options.test_blue:\n        raise ValueError(\"cannot specify colours (R, G, B) when greyscale image (black channel, K) is specified\")\n    kwargs[\"red\"] = options.test_black\n    kwargs[\"green\"] = None\n    kwargs[\"blue\"] = None\noptions.alpha = bool(options.test_alpha)\nif not args:\n    pixels = test_rgba(size, options.bitdepth, **kwargs)\nelse:\n    size,pixels,meta = pngsuite_image(args[0])\n    for k in ['bitdepth', 'alpha', 'greyscale']:\n        setattr(options, k, meta[k])\n\nwriter = Writer(size, size,\n                bitdepth=options.bitdepth,\n                transparent=options.transparent,\n                background=options.background,\n                gamma=options.gamma,\n                greyscale=options.greyscale,\n                alpha=options.alpha,\n                compression=options.compression,\n                interlace=options.interlace)\nwriter.write_array(sys.stdout, pixels)", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Adam7 interlace writing.\nFor each test image in the PngSuite, write an interlaced\nand a straightlaced version.  Decode both, and compare results.\n\"\"\"\n# Not such a great test, because the only way we can check what\n# we have written is to read it back again.\n\n", "func_signal": "def testAdam7write(self):\n", "code": "for name,bytes in _pngsuite.items():\n    # Only certain colour types supported for this test.\n    if name[3:5] not in ['n0', 'n2', 'n4', 'n6']:\n        continue\n    it = Reader(bytes=bytes)\n    x,y,pixels,meta = it.read()\n    pngi = topngbytes('adam7wn'+name+'.png', pixels,\n      x=x, y=y, bitdepth=it.bitdepth,\n      greyscale=it.greyscale, alpha=it.alpha,\n      transparent=it.transparent,\n      interlace=False)\n    x,y,ps,meta = Reader(bytes=pngi).read()\n    it = Reader(bytes=bytes)\n    x,y,pixels,meta = it.read()\n    pngs = topngbytes('adam7wi'+name+'.png', pixels,\n      x=x, y=y, bitdepth=it.bitdepth,\n      greyscale=it.greyscale, alpha=it.alpha,\n      transparent=it.transparent,\n      interlace=True)\n    x,y,pi,meta = Reader(bytes=pngs).read()\n    self.assertEqual(map(list, ps), map(list, pi))", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Test file that contains too many pixels.\"\"\"\n\n", "func_signal": "def testExtraPixels(self):\n", "code": "def eachchunk(chunk):\n    if chunk[0] != 'IDAT':\n        return chunk\n    data = chunk[1].decode('zip')\n    data += '\\x00garbage'\n    data = data.encode('zip')\n    chunk = (chunk[0], data)\n    return chunk\nself.assertRaises(FormatError, self.helperFormat, eachchunk)", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"\nWrite an array in flat row flat pixel format as a PNG file on\nthe output file.  See also :meth:`write` method.\n\"\"\"\n\n", "func_signal": "def write_array(self, outfile, pixels):\n", "code": "if self.interlace:\n    self.write_passes(outfile, self.array_scanlines_interlace(pixels))\nelse:\n    self.write_passes(outfile, self.array_scanlines(pixels))", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Convert serial format (byte stream) pixel data to flat row\nflat pixel.\n\"\"\"\n\n", "func_signal": "def serialtoflat(self, bytes, width=None):\n", "code": "if self.bitdepth == 8:\n    return bytes\nif self.bitdepth == 16:\n    bytes = tostring(bytes)\n    return array('H',\n      struct.unpack('!%dH' % (len(bytes)//2), bytes))\nassert self.bitdepth < 8\nif width is None:\n    width = self.width\n# Samples per byte\nspb = 8//self.bitdepth\nout = array('B')\nmask = 2**self.bitdepth - 1\nshifts = map(self.bitdepth.__mul__, reversed(range(spb)))\nl = width\nfor o in bytes:\n    out.extend(map(lambda i: mask&(o>>i), shifts)[:l])\n    l -= spb\n    if l <= 0:\n        l = width\nreturn out", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Undo the filter for a scanline.  `scanline` is a sequence of\nbytes that does not include the initial filter type byte.\n`previous` is decoded previous scanline (for straightlaced\nimages this is the previous pixel row, but for interlaced\nimages, it is the previous scanline in the reduced image, which\nin general is not the previous pixel row in the final image).\nWhen there is no previous scanline (the first row of a\nstraightlaced image, or the first row in one of the passes in an\ninterlaced image), then this argument should be ``None``.\n\nThe scanline will have the effects of filtering removed, and the\nresult will be returned as a fresh sequence of bytes.\n\"\"\"\n\n# :todo: Would it be better to update scanline in place?\n\n# Create the result byte array.  It seems that the best way to\n# create the array to be the right size is to copy from an\n# existing sequence.  *sigh*\n# If we fill the result with scanline, then this allows a\n# micro-optimisation in the \"null\" and \"sub\" cases.\n", "func_signal": "def undo_filter(self, filter_type, scanline, previous):\n", "code": "result = array('B', scanline)\n\nif filter_type == 0:\n    # And here, we _rely_ on filling the result with scanline,\n    # above.\n    return result\n\nif filter_type not in (1,2,3,4):\n    raise FormatError('Invalid PNG Filter Type.'\n      '  See http://www.w3.org/TR/2003/REC-PNG-20031110/#9Filters .')\n\n# Filter unit.  The stride from one pixel to the corresponding\n# byte from the previous previous.  Normally this is the pixel\n# size in bytes, but when this is smaller than 1, the previous\n# byte is used instead.\nfu = max(1, self.psize)\n\n# For the first line of a pass, synthesize a dummy previous\n# line.  An alternative approach would be to observe that on the\n# first line 'up' is the same as 'null', 'paeth' is the same\n# as 'sub', with only 'average' requiring any special case.\nif not previous:\n    previous = array('B', [0]*len(scanline))\n\ndef sub():\n    \"\"\"Undo sub filter.\"\"\"\n\n    ai = 0\n    # Loops starts at index fu.  Observe that the initial part\n    # of the result is already filled in correctly with\n    # scanline.\n    for i in range(fu, len(result)):\n        x = scanline[i]\n        a = result[ai]\n        result[i] = (x + a) & 0xff\n        ai += 1\n\ndef up():\n    \"\"\"Undo up filter.\"\"\"\n\n    for i in range(len(result)):\n        x = scanline[i]\n        b = previous[i]\n        result[i] = (x + b) & 0xff\n\ndef average():\n    \"\"\"Undo average filter.\"\"\"\n\n    ai = -fu\n    for i in range(len(result)):\n        x = scanline[i]\n        if ai < 0:\n            a = 0\n        else:\n            a = result[ai]\n        b = previous[i]\n        result[i] = (x + ((a + b) >> 1)) & 0xff\n        ai += 1\n\ndef paeth():\n    \"\"\"Undo Paeth filter.\"\"\"\n\n    # Also used for ci.\n    ai = -fu\n    for i in range(len(result)):\n        x = scanline[i]\n        if ai < 0:\n            a = c = 0\n        else:\n            a = result[ai]\n            c = previous[ai]\n        b = previous[i]\n        p = a + b - c\n        pa = abs(p - a)\n        pb = abs(p - b)\n        pc = abs(p - c)\n        if pa <= pb and pa <= pc:\n            pr = a\n        elif pb <= pc:\n            pr = b\n        else:\n            pr = c\n        result[i] = (x + pr) & 0xff\n        ai += 1\n\n# Call appropriate filter algorithm.  Note that 0 has already\n# been dealt with.\n(None, sub, up, average, paeth)[filter_type]()\nreturn result", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"\nWrite PNG file to `outfile`.  The pixel data comes from `rows`\nwhich should be in boxed row packed format.  Each row should be\na sequence of packed bytes.\n\nTechnically, this method does work for interlaced images but it\nis best avoided.  For interlaced images, the rows should be\npresented in the order that they appear in the file.\n\nThis method should not be used when the source image bit depth\nis not one naturally supported by PNG; the bit depth should be\n1, 2, 4, 8, or 16.\n\"\"\"\n\n", "func_signal": "def write_packed(self, outfile, rows):\n", "code": "if self.rescale:\n    raise Error(\"write_packed method not suitable for bit depth %d\" %\n      self.rescale[0])\nreturn self.write_passes(outfile, rows, packed=True)", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Write a PNG image to the output file.  `rows` should be\nan iterable that yields each row in boxed row flat pixel format.\nThe rows should be the rows of the original image, so there\nshould be ``self.height`` rows of ``self.width * self.planes`` values.\nIf `interlace` is specified (when creating the instance), then\nan interlaced PNG file will be written.  Supply the rows in the\nnormal image order; the interlacing is carried out internally.\n\n.. note ::\n\n  Interlacing will require the entire image to be in working memory.\n\"\"\"\n\n", "func_signal": "def write(self, outfile, rows):\n", "code": "if self.interlace:\n    fmt = 'BH'[self.bitdepth > 8]\n    a = array(fmt, itertools.chain(*rows))\n    return self.write_array(outfile, a)\nelse:\n    nrows = self.write_passes(outfile, rows)\n    if nrows != self.height:\n        raise ValueError(\n          \"rows supplied (%d) does not match height (%d)\" %\n          (nrows, self.height))", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"\nConvert a command line colour value to a RGB triple of integers.\nFIXME: Somewhere we need support for greyscale backgrounds etc.\n\"\"\"\n", "func_signal": "def color_triple(color):\n", "code": "if color.startswith('#') and len(color) == 4:\n    return (int(color[1], 16),\n            int(color[2], 16),\n            int(color[3], 16))\nif color.startswith('#') and len(color) == 7:\n    return (int(color[1:3], 16),\n            int(color[3:5], 16),\n            int(color[5:7], 16))\nelif color.startswith('#') and len(color) == 13:\n    return (int(color[1:5], 16),\n            int(color[5:9], 16),\n            int(color[9:13], 16))", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"numpy bool.\"\"\"\n\n", "func_signal": "def testNumpybool(self):\n", "code": "try:\n    import numpy\nexcept ImportError:\n    print >>sys.stderr, \"skipping numpy test\"\n    return\n\nrows = [map(numpy.bool, [0,1])]\nb = topngbytes('numpybool.png', rows, 2, 1,\n    greyscale=True, alpha=False, bitdepth=1)", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Reads just enough of the input to determine the next\nchunk's length and type, returned as a (*length*, *type*) pair\nwhere *type* is a string.  If there are no more chunks, ``None``\nis returned.\n\"\"\"\n\n", "func_signal": "def chunklentype(self):\n", "code": "x = self.file.read(8)\nif not x:\n    return None\nif len(x) != 8:\n    raise FormatError(\n      'End of file whilst reading chunk length and type.')\nlength,type = struct.unpack('!I4s', x)\nif length > 2**31-1:\n    raise FormatError('Chunk %s is too large: %d.' % (type,length))\nreturn length,type", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Test that the command line tool can read PGM files.\"\"\"\n", "func_signal": "def testPGMin(self):\n", "code": "def do():\n    return _main(['testPGMin'])\ns = StringIO()\ns.write('P5 2 2 3\\n')\ns.write('\\x00\\x01\\x02\\x03')\ns.flush()\ns.seek(0)\no = StringIO()\ntestWithIO(s, o, do)\nr = Reader(bytes=o.getvalue())\nx,y,pixels,meta = r.read()\nself.assert_(r.greyscale)\nself.assertEqual(r.bitdepth, 2)", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Helper used by :meth:`asRGB8` and :meth:`asRGBA8`.\"\"\"\n\n", "func_signal": "def _as_rescale(self, get, targetbitdepth):\n", "code": "width,height,pixels,meta = get()\nmaxval = 2**meta['bitdepth'] - 1\ntargetmaxval = 2**targetbitdepth - 1\nfactor = float(targetmaxval) / float(maxval)\nmeta['bitdepth'] = targetbitdepth\ndef iterscale():\n    for row in pixels:\n        yield map(lambda x: int(round(x*factor)), row)\nreturn width, height, iterscale(), meta", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Return image as RGB pixels.  Greyscales are expanded into RGB\ntriplets.  An alpha channel in the source image will raise an\nexception.  The return values are as for the :meth:`read` method\nexcept that the *metadata* reflect the returned pixels, not the\nsource image.  In particular, for this method\n``metadata['greyscale']`` will be ``False``.\n\"\"\"\n\n", "func_signal": "def asRGB(self):\n", "code": "width,height,pixels,meta = self.asDirect()\nif meta['alpha']:\n    raise Error(\"will not convert image with alpha channel to RGB\")\nif not meta['greyscale']:\n    return width,height,pixels,meta\nmeta['greyscale'] = False\ntypecode = 'BH'[meta['bitdepth'] > 8]\ndef iterrgb():\n    for row in pixels:\n        a = array(typecode, [0]) * 3 * width\n        for i in range(3):\n            a[i::3] = row\n        yield a\nreturn width,height,iterrgb(),meta", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"\nExtract the image metadata by reading the initial part of the PNG\nfile up to the start of the ``IDAT`` chunk.  All the chunks that\nprecede the ``IDAT`` chunk are read and either processed for\nmetadata or discarded.\n\"\"\"\n\n", "func_signal": "def preamble(self):\n", "code": "self.validate_signature()\n\nwhile True:\n    if not self.atchunk:\n        self.atchunk = self.chunklentype()\n        if self.atchunk is None:\n            raise FormatError(\n              'This PNG file has no IDAT chunks.')\n    if self.atchunk[1] == 'IDAT':\n        return\n    self.process_chunk()", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"Return image pixels as per :meth:`asDirect` method, but scale\nall pixel values to be floating point values between 0.0 and\n*maxval*.\n\"\"\"\n\n", "func_signal": "def asFloat(self, maxval=1.0):\n", "code": "x,y,pixels,info = self.asDirect()\nsourcemaxval = 2**info['bitdepth']-1\ndel info['bitdepth']\ninfo['maxval'] = float(maxval)\nfactor = float(maxval)/float(sourcemaxval)\ndef iterfloat():\n    for row in pixels:\n        yield map(factor.__mul__, row)\nreturn x,y,iterfloat(),info", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "\"\"\"\nRead (the rest of a) PAM header.  `infile` should be positioned\nimmediately after the initial 'P7' line (at the beginning of the\nsecond line).  Returns are as for `read_pnm_header`.\n\"\"\"\n\n# Unlike PBM, PGM, and PPM, we can read the header a line at a time.\n", "func_signal": "def read_pam_header(infile):\n", "code": "header = dict()\nwhile True:\n    l = infile.readline().strip()\n    if l == 'ENDHDR':\n        break\n    if l == '':\n        raise EOFError('PAM ended prematurely')\n    if l[0] == '#':\n        continue\n    l = l.split(None, 1)\n    if l[0] not in header:\n        header[l[0]] = l[1]\n    else:\n        header[l[0]] += ' ' + l[1]\n\nif ('WIDTH' not in header or\n    'HEIGHT' not in header or\n    'DEPTH' not in header or\n    'MAXVAL' not in header):\n    raise Error('PAM file must specify WIDTH, HEIGHT, DEPTH, and MAXVAL')\nwidth = int(header['WIDTH'])\nheight = int(header['HEIGHT'])\ndepth = int(header['DEPTH'])\nmaxval = int(header['MAXVAL'])\nif (width <= 0 or\n    height <= 0 or\n    depth <= 0 or\n    maxval <= 0):\n    raise Error(\n      'WIDTH, HEIGHT, DEPTH, MAXVAL must all be positive integers')\nreturn 'P7', width, height, depth, maxval", "path": "png.py", "repo_name": "quag/py.hid.im", "stars": 4, "license": "None", "language": "python", "size": 298}
{"docstring": "#print \"opening \" + name\n", "func_signal": "def openFile(self, name):\n", "code": "f = FileStream(self.fpath(name), 'rb')\nf._name = name\nreturn f", "path": "external\\lupy\\newstore.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"set license key\"\"\"\n", "func_signal": "def setLicense(license_key):\n", "code": "global LICENSE_KEY\nLICENSE_KEY = license_key", "path": "server\\parsers\\ourAmazon.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "#print \"Push\", tag.name\n", "func_signal": "def pushTag(self, tag):\n", "code": "if self.currentTag:\n    self.currentTag.append(tag)\nself.tagStack.append(tag)\nself.currentTag = self.tagStack[-1]", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"Returns a string or Unicode representation of this tag and\nits contents.\n\nNOTE: since Python's HTML parser consumes whitespace, this\nmethod is not certain to reproduce the whitespace present in\nthe original string.\"\"\"\n\n", "func_signal": "def __str__(self, needUnicode=None, showStructureIndent=None):\n", "code": "attrs = []\nif self.attrs:\n    for key, val in self.attrs:\n        attrs.append('%s=\"%s\"' % (key, val))\nclose = ''\ncloseTag = ''\nif self.isSelfClosing():\n    close = ' /'\nelse:\n    closeTag = '</%s>' % self.name\nindentIncrement = None        \nif showStructureIndent != None:\n    indentIncrement = showStructureIndent\n    if not self.hidden:\n        indentIncrement += 1\ncontents = self.renderContents(indentIncrement, needUnicode=needUnicode)        \nif showStructureIndent:\n    space = '\\n%s' % (' ' * showStructureIndent)\nif self.hidden:\n    s = contents\nelse:\n    s = []\n    attributeString = ''\n    if attrs:\n        attributeString = ' ' + ' '.join(attrs)            \n    if showStructureIndent:\n        s.append(space)\n    s.append('<%s%s%s>' % (self.name, attributeString, close))\n    s.append(contents)\n    if closeTag and showStructureIndent != None:\n        s.append(space)\n    s.append(closeTag)\n    s = ''.join(s)\nisUnicode = type(s) == types.UnicodeType\nif needUnicode and not isUnicode:\n    s = unicode(s)\nelif isUnicode and needUnicode==False:\n    s = str(s)\nreturn s", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"Return only the first child of this\nTag matching the given criteria.\"\"\"\n", "func_signal": "def first(self, name=None, attrs={}, recursive=True, text=None):\n", "code": "r = Null\nl = self.fetch(name, attrs, recursive, text, 1)\nif l:\n    r = l[0]\nreturn r", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "## from www.dexonline.com\n## no zip accepted:\n", "func_signal": "def _retrieve_dex_business(name,cityOrZip,state,surrounding,categoryOrName):\n", "code": "if cityOrZip.isdigit() and len(cityOrZip)==5:\n    log(SEV_EXC, \"_retrieve_dex_business doesn't support cityOrZip='%s'\" % cityOrZip)\n    return RETRIEVE_FAILED, None    \nurl = \"\"\nsur = \"false\"\nif surrounding == \"Yes\":\n    sur = \"true\"\n\nif categoryOrName == \"Name\":\n    url = dexServerUrlBusinessSearch % (urllib.quote(cityOrZip),urllib.quote(state), sur, urllib.quote(name))\nelif categoryOrName == \"Category\":\n    url = dexServerUrlBusinessSearchCategory % (sur, urllib.quote(name), urllib.quote(cityOrZip),urllib.quote(state))\n\nhtmlText = getHttp(url)\nif htmlText is None:\n    return (RETRIEVE_FAILED, None)\nres, data = m411_by411.businessSearchDex(htmlText)\nif res == UNKNOWN_FORMAT:\n    logParsingFailure(\"411-Business-Search\", name+\",\"+cityOrZip+\",\"+state+\",\"+surrounding+\",\"+categoryOrName, htmlText, url)\nreturn res, data", "path": "server\\yp_retrieve.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "#print 'looking for %s in %s' % (howToMatch, chunk)\n#\n# If given a list of items, return true if the list contains a\n# text element that matches.\n", "func_signal": "def _matches(self, chunk, howToMatch):\n", "code": "if isList(chunk) and not isinstance(chunk, Tag):\n    for tag in chunk:\n        if isinstance(tag, NavigableText) and self._matches(tag, howToMatch):\n            return True\n    return False\nif callable(howToMatch):\n    return howToMatch(chunk)\nif isinstance(chunk, Tag):\n    #Custom match methods take the tag as an argument, but all other\n    #ways of matching match the tag name as a string\n    chunk = chunk.name\n#Now we know that chunk is a string\nif not isinstance(chunk, basestring):\n    chunk = str(chunk)\nif hasattr(howToMatch, 'match'):\n    # It's a regexp object.\n    return howToMatch.search(chunk)\nif isList(howToMatch):\n    return chunk in howToMatch\nif hasattr(howToMatch, 'items'):\n    return howToMatch.has_key(chunk)\n#It's just a string\nreturn str(howToMatch) == chunk", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"Sets up the initial relations between this element and\nother elements.\"\"\"\n", "func_signal": "def setup(self, parent=Null, previous=Null):\n", "code": "self.parent = parent\nself.previous = previous\nself.next = Null\nself.previousSibling = Null\nself.nextSibling = Null\nif self.parent and self.parent.contents:\n    self.previousSibling = self.parent.contents[-1]\n    self.previousSibling.nextSibling = self", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "# print \"_getHttpHelper(%s)\" % url\n", "func_signal": "def _getHttpHelper(url, postData, handleRedirect, dbgLevel, referer, cookieJar):\n", "code": "timer = Timer(fStart=True)\nreq = urllib2.Request(url)\nreq.add_header(user_agent_hdr, user_agent_val)\nreq.add_header(accept_hdr, accept_val)\nreq.add_header(accept_lang_hdr, accept_lang_val)\nreq.add_header(accept_charset_hdr, accept_charset_val)\nreq.add_header(keep_alive_hdr, keep_alive_val)\nreq.add_header(connection_hdr, connection_val)\nif None != referer:\n    req.add_header(referer_hdr, referer)\n\nif None != postData:\n    req.add_data(urllib.urlencode(postData))\n\nhttpHandler = urllib2.HTTPHandler(debuglevel=dbgLevel)\nhttpsHandler = urllib2.HTTPSHandler(debuglevel=dbgLevel)\nif cookieJar is None:\n    cookieJar = cookielib.CookieJar()\n\ncookieHandler = urllib2.HTTPCookieProcessor(cookieJar)\n\nif handleRedirect:\n    opener = urllib2.build_opener(cookieHandler, httpHandler, httpsHandler)\nelse:\n    noRedirectHandler = HTTPRedirectHandlerNoRedirect()\n    opener = urllib2.build_opener(cookieHandler, httpHandler, httpsHandler, noRedirectHandler)\n\nurl_handle = opener.open(req)\nif not url_handle:\n    # print \"_getHttpHelper() - no url_handle\"\n    return None\n\nheaders = url_handle.info()\n\n# TODO: is it always present? What happens if it's not?\nencoding = url_handle.headers.get(content_encoding_hdr)\n\nif \"gzip\" == encoding:\n    htmlCompressed = url_handle.read()\n    compressedStream = StringIO.StringIO(htmlCompressed)\n    gzipper = gzip.GzipFile(fileobj=compressedStream)\n    htmlTxt = gzipper.read()\nelse:\n    htmlTxt = url_handle.read()\n\nurl_handle.close()\ntimer.stop()\nduration = timer.getDuration()\n# TODO: log somewhere how long retrieving this url took\n# print \"_getHttpHelper() htmlTxt size=%d\" % len(htmlTxt)\nreturn htmlTxt", "path": "server\\Retrieve.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"Renders the contents of this tag as a (possibly Unicode) \nstring.\"\"\"\n", "func_signal": "def renderContents(self, showStructureIndent=None, needUnicode=None):\n", "code": "s=[]\nfor c in self:\n    text = None\n    if isinstance(c, NavigableUnicodeString) or type(c) == types.UnicodeType:\n        text = unicode(c)\n    elif isinstance(c, Tag):\n        s.append(c.__str__(needUnicode, showStructureIndent))\n    elif needUnicode:\n        text = unicode(c)\n    else:\n        text = str(c)\n    if text:\n        if showStructureIndent != None:\n            if text[-1] == '\\n':\n                text = text[:-1]\n        s.append(text)\nreturn ''.join(s)", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "#print \"creating \" + name\n", "func_signal": "def createFile(self, name):\n", "code": "f = FileStream(self.fpath(name), 'wb')\nf._name = name\nreturn f", "path": "external\\lupy\\newstore.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"Returns true iff this tag has the same name, the same attributes,\nand the same contents (recursively) as the given tag.\n\nNOTE: right now this will return false if two tags have the\nsame attributes in a different order. Should this be fixed?\"\"\"\n", "func_signal": "def __eq__(self, other):\n", "code": "if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):\n    return False\nfor i in range(0, len(self.contents)):\n    if self.contents[i] != other.contents[i]:\n        return False\nreturn True", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"Called when you're done parsing, so that the unclosed tags can be\ncorrectly processed.\"\"\"\n", "func_signal": "def done(self):\n", "code": "self.endData() #NEW\nwhile self.currentTag.name != self.ROOT_TAG_NAME:\n    self.popTag()", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"Pops the tag stack up to and including the most recent\ninstance of the given tag. If inclusivePop is false, pops the tag\nstack up to but *not* including the most recent instqance of\nthe given tag.\"\"\"\n", "func_signal": "def _popToTag(self, name, inclusivePop=True):\n", "code": "if name == self.ROOT_TAG_NAME:\n    return            \n\nnumPops = 0\nmostRecentTag = None\nfor i in range(len(self.tagStack)-1, 0, -1):\n    if name == self.tagStack[i].name:\n        numPops = len(self.tagStack)-i\n        break\nif not inclusivePop:\n    numPops = numPops - 1\n\nfor i in range(0, numPops):\n    mostRecentTag = self.popTag()\nreturn mostRecentTag", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"Turns a list of maps, lists, or scalars into a single map.\nUsed to build the SELF_CLOSING_TAGS and NESTABLE_TAGS maps out\nof lists and partial maps.\"\"\"\n", "func_signal": "def buildTagMap(default, *args):\n", "code": "built = {}\nfor portion in args:\n    if hasattr(portion, 'items'):\n        #It's a map. Merge it.\n        for k,v in portion.items():\n            built[k] = v\n    elif isList(portion):\n        #It's a list. Map each item to the default.\n        for k in portion:\n            built[k] = default\n    else:\n        #It's a scalar. Map it to the default.\n        built[portion] = default\nreturn built", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"Convenience method that works with all 2.x versions of Python\nto determine whether or not something is listlike.\"\"\"\n", "func_signal": "def isList(l):\n", "code": "return hasattr(l, '__iter__') \\\n       or (type(l) in (types.ListType, types.TupleType))", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "# (flyNo, airlines, airportFrom, airportTo, dateUp, timeUp)\n# (webCode; url)\n", "func_signal": "def retrieveFlights(fieldValue):\n", "code": "flyNo = None\nwebCode = None\ntry:\n    flyNo, airlines, airportFrom, airportTo, dateUp, timeUp = fieldValue.split(\";\")\nexcept:\n    try:\n        webCode, url = fieldValue.split(\";\")\n    except:\n        pass\nif flyNo != None:\n    return retrieveFullSearch(flyNo, airlines, airportFrom, airportTo, dateUp, timeUp)\nif webCode != None:\n    if webCode == \"toc\":\n        return retrieveUrlToc(url)\nreturn INVALID_REQUEST, None", "path": "server\\parsers\\flights.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "# TODO: should I check if the file exists? Or maybe just append?\n", "func_signal": "def saveNewCodesToSQL(newCodes):\n", "code": "fo = open(getSQLFileName(), \"wb\")\nfor code in newCodes.keys():\n    purpose = newCodes[code][0]\n    fo.write(\"INSERT INTO VALUES('%s','%s')\\n\" % (code, purpose))\nfo.close()", "path": "scripts\\infoman_gen_reg_codes.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"set locale\"\"\"\n", "func_signal": "def setLocale(locale):\n", "code": "global LOCALE\n_checkLocaleSupported(locale)\nLOCALE = locale", "path": "server\\parsers\\ourAmazon.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"This method routes method call requests to either the SGMLParser\nsuperclass or the Tag superclass, depending on the method name.\"\"\"\n", "func_signal": "def __getattr__(self, methodName):\n", "code": "if methodName.find('start_') == 0 or methodName.find('end_') == 0 \\\n       or methodName.find('do_') == 0:\n    return SGMLParser.__getattr__(self, methodName)\nelif methodName.find('__') != 0:\n    return Tag.__getattr__(self, methodName)\nelse:\n    raise AttributeError", "path": "external\\BeautifulSoup21.py", "repo_name": "kjk/moriarty-palm", "stars": 4, "license": "None", "language": "python", "size": 2544}
{"docstring": "\"\"\"\nReturns (url, revision), where both are strings\n\"\"\"\n", "func_signal": "def get_info(self, location):\n", "code": "assert not location.rstrip('/').endswith(self.dirname), 'Bad directory: %s' % location\nreturn self.get_url(location), self.get_revision(location)", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Yields all links in the page\"\"\"\n", "func_signal": "def links(self):\n", "code": "for match in self._href_re.finditer(self.content):\n    url = match.group(1) or match.group(2) or match.group(3)\n    url = self.clean_link(urlparse.urljoin(self.base_url, url))\n    yield Link(url, self)", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Like os.path.splitext, but take off .tar too\"\"\"\n", "func_signal": "def splitext(path):\n", "code": "base, ext = posixpath.splitext(path)\nif base.lower().endswith('.tar'):\n    ext = base[-4:] + ext\n    base = base[:-4]\nreturn base, ext", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Set the proxy handler given the option passed on the command\nline.  If an empty string is passed it looks at the HTTP_PROXY\nenvironment variable.  \"\"\"\n", "func_signal": "def setup_proxy_handler(proxystr=''):\n", "code": "proxy = get_proxy(proxystr)\nif proxy:\n    proxy_support = urllib2.ProxyHandler({\"http\": proxy, \"ftp\": proxy})\n    opener = urllib2.build_opener(proxy_support, urllib2.CacheFTPHandler)\n    urllib2.install_opener(opener)", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Export the Bazaar repository at the url to the destination location\"\"\"\n", "func_signal": "def export(self, location):\n", "code": "temp_dir = tempfile.mkdtemp('-export', 'pip-')\nself.unpack(temp_dir)\nif os.path.exists(location):\n    # Remove the location to make sure Bazaar can export it correctly\n    shutil.rmtree(location, onerror=rmtree_errorhandler)\ntry:\n    call_subprocess([self.cmd, 'export', location], cwd=temp_dir,\n                    filter_stdout=self._filter, show_stdout=False)\nfinally:\n    shutil.rmtree(temp_dir)", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Remove paths in ``self.paths`` with confirmation (unless\n``auto_confirm`` is True).\"\"\"\n", "func_signal": "def remove(self, auto_confirm=False):\n", "code": "logger.notify('Uninstalling %s:' % self.dist.project_name)\nlogger.indent += 2\npaths = sorted(self.compact(self.paths))\ntry:\n    if auto_confirm:\n        response = 'y'\n    else:\n        for prefix, path in paths:\n            logger.notify(os.path.join(prefix, path))\n        response = ask('Proceed (y/n)? ', ('y', 'n'))\n    if self._refuse:\n        logger.notify('Not removing or modifying (outside of prefix):')\n        for prefix, path in self.compact(self._refuse):\n            logger.notify(os.path.join(prefix, path))\n    if response == 'y':\n        self.save_dir = tempfile.mkdtemp('-uninstall', 'pip-')\n        for prefix, path in paths:\n            full_path = os.path.join(prefix, path)\n            new_path = os.path.join(self.save_dir, path)\n            new_dir = os.path.dirname(new_path)\n            logger.info('Removing file or directory %s' % full_path)\n            self._moved_paths.append((prefix, path))\n            os.renames(full_path, new_path)\n        for pth in self.pth.values():\n            pth.remove()\n        logger.notify('Successfully uninstalled %s' % self.dist.project_name)\n\nfinally:\n    logger.indent -= 2", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Returns true if the page appears to be the index page of an svn repository\"\"\"\n", "func_signal": "def is_svn_page(html):\n", "code": "return (re.search(r'<title>[^<]*Revision \\d+:', html)\n        and re.search(r'Powered by (?:<a[^>]*?>)?Subversion', html, re.I))", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Get the proxy given the option passed on the command line.  If an\nempty string is passed it looks at the HTTP_PROXY environment\nvariable.\"\"\"\n", "func_signal": "def get_proxy(proxystr=''):\n", "code": "if not proxystr:\n    proxystr = os.environ.get('HTTP_PROXY', '')\nif proxystr:\n    if '@' in proxystr:\n        user_password, server_port = proxystr.split('@', 1)\n        if ':' in user_password:\n            user, password = user_password.split(':', 1)\n        else:\n            user = user_password\n            import getpass\n            prompt = 'Password for %s@%s: ' % (user, server_port)\n            password = urllib.quote(getpass.getpass(prompt))\n        return '%s:%s@%s' % (user, password, server_port)\n    else:\n        return proxystr\nelse:\n    return None", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Translate a name like Foo-1.2 to Foo==1.3\"\"\"\n", "func_signal": "def package_to_requirement(package_name):\n", "code": "match = re.search(r'^(.*?)(-dev|-\\d.*)', package_name)\nif match:\n    name = match.group(1)\n    version = match.group(2)\nelse:\n    name = package_name\n    version = ''\nif version:\n    return '%s==%s' % (name, version)\nelse:\n    return name", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Returns (url, revision), where both are strings\"\"\"\n", "func_signal": "def get_info(self, location):\n", "code": "assert not location.rstrip('/').endswith(self.dirname), 'Bad directory: %s' % location\noutput = call_subprocess(\n    ['svn', 'info', location], show_stdout=False, extra_environ={'LANG': 'C'})\nmatch = _svn_url_re.search(output)\nif not match:\n    logger.warn('Cannot determine URL of svn checkout %s' % display_path(location))\n    logger.info('Output that cannot be parsed: \\n%s' % output)\n    return None, None\nurl = match.group(1).strip()\nmatch = _svn_revision_re.search(output)\nif not match:\n    logger.warn('Cannot determine revision of svn checkout %s' % display_path(location))\n    logger.info('Output that cannot be parsed: \\n%s' % output)\n    return url, None\nreturn url, match.group(1)", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Returns the level that stdout runs at\"\"\"\n", "func_signal": "def _stdout_level(self):\n", "code": "for level, consumer in self.consumers:\n    if consumer is sys.stdout:\n        return level\nreturn self.FATAL", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Figure out the name of a directory to back up the given dir to\n(adding .bak, .bak2, etc)\"\"\"\n", "func_signal": "def backup_dir(dir, ext='.bak'):\n", "code": "n = 1\nextension = ext\nwhile os.path.exists(dir + extension):\n    n += 1\n    extension = ext + str(n)\nreturn dir + extension", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"\nConvert a path to a file: URL.  The path will be made absolute and have\nquoted path parts.\n\"\"\"\n", "func_signal": "def filename_to_url2(filename):\n", "code": "filename = os.path.normcase(os.path.abspath(filename))\ndrive, filename = os.path.splitdrive(filename)\nfilepath = filename.split(os.path.sep)\nurl = '/'.join([urllib.quote(part) for part in filepath])\nif not drive:\n    url = url.lstrip('/')\nreturn 'file:///' + drive + url", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Get the Content-Type of the given url, using a HEAD request\"\"\"\n", "func_signal": "def _get_content_type(url):\n", "code": "scheme, netloc, path, query, fragment = urlparse.urlsplit(url)\nif scheme == 'http':\n    ConnClass = httplib.HTTPConnection\nelif scheme == 'https':\n    ConnClass = httplib.HTTPSConnection\nelse:\n    ## FIXME: some warning or something?\n    ## assertion error?\n    return ''\nif query:\n    path += '?' + query\nconn = ConnClass(netloc)\ntry:\n    conn.request('HEAD', path, headers={'Host': netloc})\n    resp = conn.getresponse()\n    if resp.status != 200:\n        ## FIXME: doesn't handle redirects\n        return ''\n    return resp.getheader('Content-Type') or ''\nfinally:\n    conn.close()", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Yields all links with the given relations\"\"\"\n", "func_signal": "def explicit_rel_links(self, rels=('homepage', 'download')):\n", "code": "for match in self._rel_re.finditer(self.content):\n    found_rels = match.group(1).lower().split()\n    for rel in rels:\n        if rel in found_rels:\n            break\n    else:\n        continue\n    match = self._href_re.search(match.group(0))\n    if not match:\n        continue\n    url = match.group(1) or match.group(2) or match.group(3)\n    url = self.clean_link(urlparse.urljoin(self.base_url, url))\n    yield Link(url, self)", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"All the entries of sys.path, possibly restricted by --path\"\"\"\n", "func_signal": "def paths(self):\n", "code": "if not self.select_paths:\n    return sys.path\nresult = []\nmatch_any = set()\nfor path in sys.path:\n    path = os.path.normcase(os.path.abspath(path))\n    for match in self.select_paths:\n        match = os.path.normcase(os.path.abspath(match))\n        if '*' in match:\n            if re.search(fnmatch.translate(match+'*'), path):\n                result.append(path)\n                match_any.add(match)\n                break\n        else:\n            if path.startswith(match):\n                result.append(path)\n                match_any.add(match)\n                break\n    else:\n        logger.debug(\"Skipping path %s because it doesn't match %s\"\n                     % (path, ', '.join(self.select_paths)))\nfor match in self.select_paths:\n    if match not in match_any and '*' not in match:\n        result.append(match)\n        logger.debug(\"Adding path %s because it doesn't match anything already on sys.path\"\n                     % match)\nreturn result", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"\nReturn the maximum revision for all files under a given location\n\"\"\"\n# Note: taken from setuptools.command.egg_info\n", "func_signal": "def get_revision(self, location):\n", "code": "revision = 0\n\nfor base, dirs, files in os.walk(location):\n    if self.dirname not in dirs:\n        dirs[:] = []\n        continue    # no sense walking uncontrolled subdirs\n    dirs.remove(self.dirname)\n    entries_fn = os.path.join(base, self.dirname, 'entries')\n    if not os.path.exists(entries_fn):\n        ## FIXME: should we warn?\n        continue\n    f = open(entries_fn)\n    data = f.read()\n    f.close()\n\n    if data.startswith('8') or data.startswith('9') or data.startswith('10'):\n        data = map(str.splitlines,data.split('\\n\\x0c\\n'))\n        del data[0][0]  # get rid of the '8'\n        dirurl = data[0][3]\n        revs = [int(d[9]) for d in data if len(d)>9 and d[9]]+[0]\n        if revs:\n            localrev = max(revs)\n        else:\n            localrev = 0\n    elif data.startswith('<?xml'):\n        dirurl = _svn_xml_url_re.search(data).group(1)    # get repository URL\n        revs = [int(m.group(1)) for m in _svn_rev_re.finditer(data)]+[0]\n        if revs:\n            localrev = max(revs)\n        else:\n            localrev = 0\n    else:\n        logger.warn(\"Unrecognized .svn/entries format; skipping %s\", base)\n        dirs[:] = []\n        continue\n    if base == location:\n        base_url = dirurl+'/'   # save the root url\n    elif not dirurl.startswith(base_url):\n        dirs[:] = []\n        continue    # not part of the same svn tree, skip it\n    revision = max(revision, localrev)\nreturn revision", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "# Make sure we have all global options carried over\n", "func_signal": "def merge_options(self, initial_options, options):\n", "code": "for attr in ['log', 'venv', 'proxy', 'venv_base', 'require_venv',\n             'respect_venv', 'log_explicit_levels', 'log_file',\n             'timeout', 'default_vcs', 'skip_requirements_regex']:\n    setattr(options, attr, getattr(initial_options, attr) or getattr(options, attr))\noptions.quiet += initial_options.quiet\noptions.verbose += initial_options.verbose", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Compact a path set to contain the minimal number of paths\nnecessary to contain all paths in the set. If /a/path/ and\n/a/path/to/a/file.txt are both in the set, leave only the\nshorter path.\"\"\"\n", "func_signal": "def compact(self, paths):\n", "code": "short_paths = set()\ndef sort_set(x, y):\n    prefix_x, path_x = x\n    prefix_y, path_y = y\n    return cmp(len(path_x), len(path_y))\nfor prefix, path in sorted(paths, sort_set):\n    if not any([(path.startswith(shortpath) and\n                 path[len(shortpath.rstrip(os.path.sep))] == os.path.sep)\n                for shortprefix, shortpath in short_paths]):\n        short_paths.add((prefix, path))\nreturn short_paths", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"\nReturns the correct repository URL and revision by parsing the given\nrepository URL\n\"\"\"\n", "func_signal": "def get_url_rev(self):\n", "code": "url = self.url.split('+', 1)[1]\nscheme, netloc, path, query, frag = urlparse.urlsplit(url)\nrev = None\nif '@' in path:\n    path, rev = path.rsplit('@', 1)\nurl = urlparse.urlunsplit((scheme, netloc, path, query, ''))\nreturn url, rev", "path": "pip.py", "repo_name": "ericholscher/pip", "stars": 4, "license": "None", "language": "python", "size": 807}
{"docstring": "\"\"\"Turns a sequence file into a single SeqRecord.\n\nhandle   - handle to the file.\nformat   - string describing the file format.\nalphabet - optional Alphabet object, useful when the sequence type cannot\n           be automatically inferred from the file itself (e.g. fasta)\n\nThis function is for use parsing sequence files containing\nexactly one record.  For example, reading a GenBank file:\n\n>>> from Bio import SeqIO\n>>> record = SeqIO.read(open(\"GenBank/arab1.gb\", \"rU\"), \"genbank\")\n>>> print \"ID\", record.id\nID AC007323.5\n>>> print \"Sequence length\", len(record)\nSequence length 86436\n>>> print \"Sequence alphabet\", record.seq.alphabet\nSequence alphabet IUPACAmbiguousDNA()\n\nIf the handle contains no records, or more than one record,\nan exception is raised.  For example:\n\n>>> from Bio import SeqIO\n>>> record = SeqIO.read(open(\"GenBank/cor6_6.gb\", \"rU\"), \"genbank\")\nTraceback (most recent call last):\n    ...\nValueError: More than one record found in handle\n\nIf however you want the first record from a file containing\nmultiple records this function would raise an exception (as\nshown in the example above).  Instead use:\n\n>>> from Bio import SeqIO\n>>> record = SeqIO.parse(open(\"GenBank/cor6_6.gb\", \"rU\"), \"genbank\").next()\n>>> print \"First record's ID\", record.id\nFirst record's ID X55053.1\n\nUse the Bio.SeqIO.parse(handle, format) function if you want\nto read multiple records from the handle.\n\"\"\"\n", "func_signal": "def read(handle, format, alphabet=None) :\n", "code": "iterator = parse(handle, format, alphabet)\ntry :\n    first = iterator.next()\nexcept StopIteration :\n    first = None\nif first is None :\n    raise ValueError(\"No records found in handle\")\ntry :\n    second = iterator.next()\nexcept StopIteration :\n    second = None\nif second is not None :\n    raise ValueError(\"More than one record found in handle\")\nreturn first", "path": "Bio\\SeqIO\\__init__.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Write complete set of sequences to a file.\n\nsequences - A list (or iterator) of SeqRecord objects.\nhandle    - File handle object to write to.\nformat    - lower case string describing the file format to write.\n\nYou should close the handle after calling this function.\n\nReturns the number of records written (as an integer).\n\"\"\"\n", "func_signal": "def write(sequences, handle, format) :\n", "code": "from Bio import AlignIO\n\n#Try and give helpful error messages:\nif isinstance(handle, basestring) :\n    raise TypeError(\"Need a file handle, not a string (i.e. not a filename)\")\nif not isinstance(format, basestring) :\n    raise TypeError(\"Need a string for the file format (lower case)\")\nif not format :\n    raise ValueError(\"Format required (lower case string)\")\nif format != format.lower() :\n    raise ValueError(\"Format string '%s' should be lower case\" % format)\nif isinstance(sequences,SeqRecord):\n    raise ValueError(\"Use a SeqRecord list/iterator, not just a single SeqRecord\")\n\n#Map the file format to a writer class\nif format in _FormatToWriter :\n    writer_class = _FormatToWriter[format]\n    count = writer_class(handle).write_file(sequences)\nelif format in AlignIO._FormatToWriter :\n    #Try and turn all the records into a single alignment,\n    #and write that using Bio.AlignIO\n    alignment = to_alignment(sequences)\n    alignment_count = AlignIO.write([alignment], handle, format)\n    assert alignment_count == 1, \"Internal error - the underlying writer \" \\\n       + \" should have returned 1, not %s\" % repr(alignment_count)\n    count = len(alignment.get_all_seqs())\n    del alignment_count, alignment\nelif format in _FormatToIterator or format in AlignIO._FormatToIterator :\n    raise ValueError(\"Reading format '%s' is supported, but not writing\" \\\n                     % format)\nelse :\n    raise ValueError(\"Unknown format '%s'\" % format)\n\nassert isinstance(count, int), \"Internal error - the underlying writer \" \\\n       + \" should have returned the record count, not %s\" % repr(count)\nreturn count", "path": "Bio\\SeqIO\\__init__.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"\nTitle     __initialize__\nUsage\nFunction  initialize a new object.\nExample\nReturns\nArgument\n\"\"\"\n\n\n", "func_signal": "def __init__( self ):\n", "code": "self.failures = []\nself.errors = []\nself.warnings = []", "path": "Tests\\UnitTests\\UnitTestResults.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"\nTitle     print_warnings()\nUsage     print_warnings\n          print_warnings( file_handle )\nFunction  Prints warning messages\n\nReturns\nArgument  file handle\n\"\"\"\n\n\n", "func_signal": "def print_warnings( self, outfile = sys.stdout ):\n", "code": "for warning in self.get_warnings():\n    outfile.write(  '%s\\n' % ( warning ) )", "path": "Tests\\UnitTests\\UnitTestResults.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "# Enforce string storage\n", "func_signal": "def __init__(self, data):\n", "code": "if type(data) != type(\"\"):\n    raise CrystalError('Hetero data must be an alphameric string')\nif data.isalnum() == 0:\n    raise CrystalError('Hetero data must be an alphameric string')\nif len(data) > 3:\n    raise CrystalError('Hetero data may contain up to 3 characters')\nif len(data) < 1:\n    raise CrystalError('Hetero data must not be empty')\n\nself.data = data[:].lower()", "path": "Bio\\Crystal\\__init__.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"parse(self, handle)\"\"\"\n", "func_signal": "def parse(self, handle):\n", "code": "self._scanner.feed(handle, self._consumer)\nreturn self._consumer.data", "path": "Bio\\AlignAce\\Parser.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Simple string to display a floating point matrix\n\nThis should give the same output on multiple systems.  This is\nneeded because a simple \"print matrix\" uses scientific notation\nwhich varies between platforms.\n\nOnly 4 decimal places are used to avoid false test failures due\nto slight differences in the calculation (e.g. due to different\nversions of the underlying libraries or the compilation options\nthey used).\n\"\"\"\n\n#This uses a fancy double nested list expression.\n#If and when Biopython requires Python 2.4 or later,\n#it would be slightly nicer to use generator expressions.\n", "func_signal": "def simple_matrix_print(matrix) :\n", "code": "return \"[\" \\\n+ \"\\n \".join([\"[\" \\\n             + \" \".join([\"% 1.4f\" % val for val in row]) \\\n             + \"]\" for row in matrix]) \\\n+ \"]\"", "path": "Tests\\test_SVDSuperimposer.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"\nTitle     print_errors()\nUsage     print_errors\n          print_errors( file_handle )\nFunction  Prints error messages\n\nReturns\nArgument  file handle\n\"\"\"\n\n\n", "func_signal": "def print_errors( self, outfile = sys.stdout ):\n", "code": "for error in self.get_errors():\n    error.print_message( outfile )\n    error.print_trace( outfile )", "path": "Tests\\UnitTests\\UnitTestResults.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Iterate over all records in several alignments (PRIVATE).\"\"\"\n", "func_signal": "def _iterate_via_AlignIO(handle, format, alphabet) :\n", "code": "from Bio import AlignIO\nfor align in AlignIO.parse(handle, format, alphabet=alphabet) :\n    for record in align :\n        yield record", "path": "Bio\\SeqIO\\__init__.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"__init__(self)\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self._scanner = AlignAceScanner()\nself._consumer = AlignAceConsumer()", "path": "Bio\\AlignAce\\Parser.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Returns a multiple sequence alignment (OBSOLETE).\n\nsequences -An iterator that returns SeqRecord objects,\n           or simply a list of SeqRecord objects.\n           All the record sequences must be the same length.\nalphabet - Optional alphabet.  Stongly recommended.\nstrict   - Optional, defaults to True.  Should error checking\n           be done?\n\nUsing this function is now discouraged.  Rather doing this:\n\nfrom Bio import SeqIO\nalignment = SeqIO.to_alignment(SeqIO.parse(handle, format))\n\nYou are now encouraged to use Bio.AlignIO instead, e.g.\n\nfrom Bio import AlignIO\nalignment = AlignIO.read(handle, format)\n\"\"\"\n#TODO - Move this functionality into the Alignment class instead?\n", "func_signal": "def to_alignment(sequences, alphabet=None, strict=True) :\n", "code": "from Bio.Alphabet import generic_alphabet\nfrom Bio.Alphabet import _consensus_alphabet\nif alphabet is None :\n    sequences = list(sequences)\n    alphabet = _consensus_alphabet([rec.seq.alphabet for rec in sequences \\\n                                    if rec.seq is not None])\n\nif not (isinstance(alphabet, Alphabet) or isinstance(alphabet, AlphabetEncoder)) :\n    raise ValueError(\"Invalid alphabet\")\n\nalignment_length = None\nalignment = Alignment(alphabet)\nfor record in sequences :\n    if strict :\n        if alignment_length is None :\n            alignment_length = len(record.seq)\n        elif alignment_length != len(record.seq) :\n            raise ValueError(\"Sequences must all be the same length\")\n\n        assert isinstance(record.seq.alphabet, Alphabet) \\\n        or isinstance(record.seq.alphabet, AlphabetEncoder), \\\n            \"Sequence does not have a valid alphabet\"\n\n        #TODO - Move this alphabet comparison code into the Alphabet module/class?\n        #TODO - Is a normal alphabet \"ungapped\" by default, or does it just mean\n        #undecided?\n        if isinstance(record.seq.alphabet, Alphabet) \\\n        and isinstance(alphabet, Alphabet) :\n            #Comparing two non-gapped alphabets            \n            if not isinstance(record.seq.alphabet, alphabet.__class__) :\n                raise ValueError(\"Incompatible sequence alphabet \" \\\n                                 + \"%s for %s alignment\" \\\n                                 % (record.seq.alphabet, alphabet))\n        elif isinstance(record.seq.alphabet, AlphabetEncoder) \\\n        and isinstance(alphabet, Alphabet) :\n            raise ValueError(\"Sequence has a gapped alphabet, alignment does not\")\n        elif isinstance(record.seq.alphabet, Alphabet) \\\n        and isinstance(alphabet, Gapped) :\n            #Sequence isn't gapped, alignment is.\n            if not isinstance(record.seq.alphabet, alphabet.alphabet.__class__) :\n                raise ValueError(\"Incompatible sequence alphabet \" \\\n                                 + \"%s for %s alignment\" \\\n                                 % (record.seq.alphabet, alphabet))\n        else :\n            #Comparing two gapped alphabets\n            if not isinstance(record.seq.alphabet, alphabet.__class__) :\n                raise ValueError(\"Incompatible sequence alphabet \" \\\n                                 + \"%s for %s alignment\" \\\n                                 % (record.seq.alphabet, alphabet))\n            if record.seq.alphabet.gap_char != alphabet.gap_char :\n                raise ValueError(\"Sequence gap characters != alignment gap char\")\n        #ToDo, additional checks on the specified alignment...\n        #Should we look at the alphabet.contains() method?\n    if record.seq is None :\n        raise TypeError(\"SeqRecord (id=%s) has None for its sequence.\" % record.id)\n        \n    #This is abusing the \"private\" records list,\n    #we should really have a method like add_sequence\n    #but which takes SeqRecord objects.  See also Bug 1944\n    alignment._records.append(record)\nreturn alignment", "path": "Bio\\SeqIO\\__init__.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Can we parse a CLA file?\"\"\"\n", "func_signal": "def testParse(self):\n", "code": "f=open(self.filename)\ntry: \n    count = 0\n    records = Cla.parse(f)\n    for record in records:\n        count +=1\n    assert count == 14, \"Wrong number of records?!\"\nfinally:\n    f.close()", "path": "Tests\\test_SCOP_Cla.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "# Enforcestorage\n", "func_signal": "def __init__(self, data = {}):\n", "code": "if type(data) != type({}):\n    raise CrystalError('Crystal must be a dictionary')\nself.data = data\nself.fix()", "path": "Bio\\Crystal\\__init__.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Turns a sequence file into an iterator returning SeqRecords.\n\nhandle   - handle to the file.\nformat   - lower case string describing the file format.\nalphabet - optional Alphabet object, useful when the sequence type cannot\n           be automatically inferred from the file itself (e.g. fasta)\n\nTypical usage, opening a file to read in, and looping over the record(s):\n\n>>> from Bio import SeqIO\n>>> filename = \"Nucleic/sweetpea.nu\"\n>>> for record in SeqIO.parse(open(filename,\"rU\"), \"fasta\") :\n...    print \"ID\", record.id\n...    print \"Sequence length\", len(record)\n...    print \"Sequence alphabet\", record.seq.alphabet\nID gi|3176602|gb|U78617.1|LOU78617\nSequence length 309\nSequence alphabet SingleLetterAlphabet()\n\nFor file formats like FASTA where the alphabet cannot be determined, it\nmay be useful to specify the alphabet explicitly:\n\n>>> from Bio import SeqIO\n>>> from Bio.Alphabet import generic_dna\n>>> filename = \"Nucleic/sweetpea.nu\"\n>>> for record in SeqIO.parse(open(filename,\"rU\"), \"fasta\", generic_dna) :\n...    print \"ID\", record.id\n...    print \"Sequence length\", len(record)\n...    print \"Sequence alphabet\", record.seq.alphabet\nID gi|3176602|gb|U78617.1|LOU78617\nSequence length 309\nSequence alphabet DNAAlphabet()\n\nIf you have a string 'data' containing the file contents, you must\nfirst turn this into a handle in order to parse it:\n\nfrom Bio import SeqIO\nfrom StringIO import StringIO\nmy_iterator = SeqIO.parse(StringIO(data), format)\n\nUse the Bio.SeqIO.read(handle, format) function when you expect a single\nrecord only.\n\"\"\"\n", "func_signal": "def parse(handle, format, alphabet=None) :\n", "code": "from Bio import AlignIO\n\n#Try and give helpful error messages:\nif isinstance(handle, basestring) :\n    raise TypeError(\"Need a file handle, not a string (i.e. not a filename)\")\nif not isinstance(format, basestring) :\n    raise TypeError(\"Need a string for the file format (lower case)\")\nif not format :\n    raise ValueError(\"Format required (lower case string)\")\nif format != format.lower() :\n    raise ValueError(\"Format string '%s' should be lower case\" % format)\nif alphabet is not None and not (isinstance(alphabet, Alphabet) or \\\n                                 isinstance(alphabet, AlphabetEncoder)) :\n    raise ValueError(\"Invalid alphabet, %s\" % repr(alphabet))\n\n#Map the file format to a sequence iterator:    \nif format in _FormatToIterator :\n    iterator_generator = _FormatToIterator[format]\n    if alphabet is None :\n        return iterator_generator(handle)\n    try :\n        return iterator_generator(handle, alphabet=alphabet)\n    except :\n        return _force_alphabet(iterator_generator(handle), alphabet)\nelif format in AlignIO._FormatToIterator :\n    #Use Bio.AlignIO to read in the alignments\n    #TODO - Once we drop support for Python 2.3, this helper function can be\n    #replaced with a generator expression.\n    return _iterate_via_AlignIO(handle, format, alphabet)\nelse :\n    raise ValueError(\"Unknown format '%s'\" % format)", "path": "Bio\\SeqIO\\__init__.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Iterate over records, over-riding the alphabet (PRIVATE).\"\"\"\n#Assume the alphabet argument has been pre-validated\n", "func_signal": "def _force_alphabet(record_iterator, alphabet) :\n", "code": "given_base_class = _get_base_alphabet(alphabet).__class__\nfor record in record_iterator :\n    if isinstance(_get_base_alphabet(record.seq.alphabet),\n                  given_base_class) :\n        record.seq.alphabet = alphabet\n        yield record\n    else :\n        raise ValueError(\"Specified alphabet %s clashes with \"\\\n                         \"that determined from the file, %s\" \\\n                         % (repr(alphabet), repr(record.seq.alphabet)))", "path": "Bio\\SeqIO\\__init__.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"\nTitle     print_failures()\nUsage     print_failures\n          print_failures( file_handle );\nFunction  Prints assertion failure messages\n\nReturns\nArgument  file handle\n\"\"\"\n\n\n", "func_signal": "def print_failures( self, outfile = sys.stdout ):\n", "code": "for failure in self.get_failures():\n    failure.print_message( outfile )\n    failure.print_trace( outfile )", "path": "Tests\\UnitTests\\UnitTestResults.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"parse(self, handle)\"\"\"\n", "func_signal": "def parse(self, handle):\n", "code": "self._scanner.feed(handle, self._consumer)\nreturn self._consumer.motifs", "path": "Bio\\AlignAce\\Parser.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Predict outputs from the neural network with the given inputs.\n\nThis uses the current neural network to predict outputs, no\ntraining of the neural network is done here.\n\"\"\"\n# update the predicted values for these inputs\n", "func_signal": "def predict(self, inputs):\n", "code": "self._input.update(inputs)\n\noutput_keys = self._output.values.keys()\noutput_keys.sort()\n\noutputs = []\nfor output_key in output_keys:\n    outputs.append(self._output.values[output_key])\nreturn outputs", "path": "Bio\\NeuralNetwork\\BackPropagation\\Network.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"__init__(self)\"\"\"\n", "func_signal": "def __init__(self):\n", "code": "self._scanner = CompareAceScanner()\nself._consumer = CompareAceConsumer()", "path": "Bio\\AlignAce\\Parser.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Initialize the network with the three layers.\n\"\"\"\n", "func_signal": "def __init__(self, input_layer, hidden_layer, output_layer):\n", "code": "self._input = input_layer\nself._hidden = hidden_layer\nself._output = output_layer", "path": "Bio\\NeuralNetwork\\BackPropagation\\Network.py", "repo_name": "nuin/biopython", "stars": 4, "license": "other", "language": "python", "size": 11920}
{"docstring": "\"\"\"Initialize serial communication object.\n\nArgs:\n  filename:  String, name of serial device, e.g. '/dev/ttyS0'.\n\"\"\"\n", "func_signal": "def __init__(self, filename):\n", "code": "self.__receiver_running = False\n\n# Open device and set serial communications parameters.\n# (115k baud, 8N1, no handshake, no buffering in kernel)\nself._fd = os.open(filename, os.O_RDWR | os.O_NOCTTY | os.O_NONBLOCK)\nattr = termios.tcgetattr(self._fd)\nattr[0] = 0   # iflag\nattr[1] = 0   # oflag\nattr[2] = termios.CS8 | termios.CREAD | termios.CLOCAL  # cflag\nattr[3] = 0   # lflag\nattr[4] = termios.B115200  # ispeed\nattr[5] = termios.B115200  # ospeed\nattr[6][termios.VMIN] = 1\nattr[6][termios.VTIME] = 0\ntermios.tcsetattr(self._fd, termios.TCSAFLUSH, attr)\n\n# Clean kernel buffers of stale data.\ntermios.tcflush(self._fd, termios.TCIOFLUSH)\n\n# Set up communication buffers and start receiver thread.\nself.__buffer = Queue.Queue(0)\nself.__buffer2 = None\n\nself.__receiver = threading.Thread(target=self.__ReceiverThread)\nself.__receiver_running = True\nself.__receiver.start()", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "# There is a bug in the firmware when handling PROY003:  If the logger has\n# not made a lock on satellites yet and therefore has no idea of the time\n# it answer with LOG002 instead of LOG003!\n", "func_signal": "def GetTimestamp(self):\n", "code": "data = self.SendRecv('PROY003', lines=1)\nif not data[0].startswith('LOG003,'):\n  return None\nself.ClearProgress()\ndata = data[0].split(',')\n# LOG003,20071226,101221\nreturn ParseDateTime(data[1], data[2])", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "# Instances of this class should be short-lived and many features require\n# this global info.  Speed it up by caching it.\n", "func_signal": "def GetInfo(self):\n", "code": "if self._cached_info:\n  return self._cached_info\nmsg = self.SendRecv('PROY108', result='LOG108,')\ndata = msg[0].split(',')[1:]\n# data type, ?, ?, memory full, ?, interval, ?, #tracks, #waypoints in last track\nresult = map(int, data)\nself._cached_info = result\nreturn result", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Convert radians to degree values.\n\nArgs:\n  value: Float, radians value of latitude/longitude.\n\nReturns:\n  (is_positive, degree, minutes) where is_positive is a boolean, degree is\n  an integer and minutes is a float.\n\"\"\"\n", "func_signal": "def _Rad2Deg(cls, value):\n", "code": "value *= cls.RAD2DEG\nis_positive = value >= 0.0\nvalue = abs(value)\ndegree = int(value)\nminutes = (value-degree) * 60.0\nreturn is_positive, degree, minutes", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Retrieve a specific amount of data or time out.\n\nThis method returns if either 'length' data was received or 'timeout'\nseconds have passed.  The returned string can have less than 'length'\nbytes.  If 'timeout' is None this method will block until 'length' bytes\ncan be returned.\n\nArgs:\n  length: Integer, amount of bytes to retrieve.\n\nReturns:\n  String of received data, can be empty.\n\"\"\"\n# Time out one second after we start.\n", "func_signal": "def read(self, length=1):\n", "code": "endtime = time.time() + 1.0\n\n# self.__buffer is the Queue feeding us data from the receiver thread.\n# self.__buffer2 is data we already got from the Queue but didn't use yet.\ndata = ''\nwhile length:\n  # Have we timed out?\n  if time.time() > endtime:\n    break\n\n  # Fill up buffer2 by getting data from the queue.\n  if not self.__buffer2:\n    try:\n      self.__buffer2 = self.__buffer.get_nowait()\n    except Queue.Empty:\n      # Don't eat all CPU!\n      time.sleep(0.001)\n      continue\n\n  # Extract data from buffer2.\n  if len(self.__buffer2) <= length:\n    # We need all of the data in buffer2.\n    data += self.__buffer2\n    length -= len(self.__buffer2)\n    self.__buffer2 = None\n  else:\n    # We only need some data in buffer2, split it out and leave the rest\n    # for next time.\n    data += self.__buffer2[:length]\n    self.__buffer2 = self.__buffer2[length:]\n    break\n\n  # If the receiver thread isn't running anymore there will be no more\n  # data.  So just bail out if we haven't finished yet.\n  if length and not self.__receiver_running:\n    raise SerialConnectionLost\n\nreturn data", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Initialize serial communication object.\n\nArgs:\n  filename:  String, name of serial device, e.g. '/dev/ttyS0'.\n\"\"\"\n", "func_signal": "def __init__(self, filename):\n", "code": "self.__receiver_running = False\n\n# Open device and set serial communications parameters.\n# (115k baud, 8N1, no handshake, no buffering in kernel)\nself._fd = os.open(filename, os.O_RDWR | os.O_NOCTTY | os.O_NONBLOCK)\nattr = termios.tcgetattr(self._fd)\nattr[0] = 0   # iflag\nattr[1] = 0   # oflag\nattr[2] = termios.CS8 | termios.CREAD | termios.CLOCAL  # cflag\nattr[3] = 0   # lflag\nattr[4] = termios.B115200  # ispeed\nattr[5] = termios.B115200  # ospeed\nattr[6][termios.VMIN] = 1\nattr[6][termios.VTIME] = 0\ntermios.tcsetattr(self._fd, termios.TCSAFLUSH, attr)\n\n# Clean kernel buffers of stale data.\ntermios.tcflush(self._fd, termios.TCIOFLUSH)\n\n# Set up communication buffers and start receiver thread.\nself.__buffer = Queue.Queue(0)\nself.__buffer2 = None\n\nself.__receiver = threading.Thread(target=self.__ReceiverThread)\nself.__receiver_running = True\nself.__receiver.start()", "path": "rgm3800app\\rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Send a request and receive the response.\n\nIf no valid result is received then the request is repeated.\n\nArgs:\n  request: String, request to send.\n  result: String, prefix of lines to return.\n  lines: Number of lines to return.\n\nReturns:\n  List of strings, received response.\n\"\"\"\n", "func_signal": "def SendRecv(self, request, result=None, lines=1):\n", "code": "result_lines = []\nfor i in range(5):\n  # Send the command.\n  self.SendMessage(request)\n\n  # Only accept a certain amount of noise.  If there is too much noise it's\n  # likely communication is broken anyway.\n  lines_acceptable = 20 + 5*lines\n\n  while lines_acceptable:\n    # Get a line.\n    msg = self.RecvMessage()\n    if not msg:\n      # Got no complete line until timeout.  Finished receiving.\n      break\n    lines_acceptable -= 1\n\n    if not result or msg.startswith(result):\n      result_lines.append(msg)\n\n    if lines and lines == len(result_lines):\n      break\n\n  if not result_lines:\n    # Failed receiving.  Retry.\n    self.ShowInfo('Timeout talking to device.  Retrying.')\n    continue\n\n  if lines and lines != len(result_lines):\n    # Not enough lines received.\n    self.ShowInfo('Incomplete results.  Retrying.')\n    continue\n\n  # Got a result, return it to the caller.\n  return result_lines\n\n# If we reach this point we failed repeatedly to talk to the device.\nraise SerialCommunicationError('Can not talk to device.')", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Convert radians to degree values.\n\nArgs:\n  value: Float, radians value of latitude/longitude.\n\nReturns:\n  (is_positive, degree, minutes) where is_positive is a boolean, degree is\n  an integer and minutes is a float.\n\"\"\"\n", "func_signal": "def _Rad2Deg(cls, value):\n", "code": "value *= cls.RAD2DEG\nis_positive = value >= 0.0\nvalue = abs(value)\ndegree = int(value)\nminutes = (value-degree) * 60.0\nreturn is_positive, degree, minutes", "path": "rgm3800app\\rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Parse a date (and optional time) string and return as date[time] objects.\n\nArgs:\n  datestr: String, date spec, e.g. \"20080605\" for June 5th, 2008.\n  timestr: String, time spec, e.g. \"143500\" for 14:35:00 (2:35pm).\n\nReturns:\n  datetime.datetime instance if timestr given, else datetime.date instance.\n\"\"\"\n", "func_signal": "def ParseDateTime(datestr, timestr=None):\n", "code": "assert len(datestr) == 8\nyear  = int(datestr[0:4])\nmonth = int(datestr[4:6])\nday   = int(datestr[6:8])\nif timestr:\n  assert len(timestr) == 6\n  hour  = int(timestr[0:2])\n  min   = int(timestr[2:4])\n  sec   = int(timestr[4:6])\n  return datetime.datetime(year, month, day, hour, min, sec)\nelse:\n  return datetime.date(year, month, day)", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Shut down serial communication.\"\"\"\n# Notify receiver thread to stop, then wait for it.\n", "func_signal": "def close(self):\n", "code": "if self.__receiver_running:\n  self.__receiver_running = False\n  self.__receiver.join()\n\nself._Close()", "path": "rgm3800app\\rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Parse a range description and assert a range.\n\nSupported formats:\n  \"\":    min_ .. max_\n  \"x\":   x .. x\n  \"x-\":  x .. max_\n  \"x-y\": x .. y\n  \"-z:   max_-z+1 .. max_  (\"The last z entries.\")\n\nArgs:\n  arg: String, input argument.\n  min_: Integer, minimum value for start and end of range.\n  max_: Integer, maximum value for start and end of range.\n\nReturns:\n  None if unable to parse arg or iterator for the requested range,\n  yielding integers.\n\"\"\"\n", "func_signal": "def ParseRange(arg, min_, max_):\n", "code": "start = min_\nend = max_\n\nif arg:\n  arg_match = re.match(r'^(?:(\\d+)|(\\d+)-|-(\\d+)|(\\d+)-(\\d+))$', arg)\n  if not arg_match:\n    return None\n  arg = arg_match.groups()\n  if arg[0] != None:\n    # \"x\"\n    start = end = int(arg[0])\n  elif arg[1] != None:\n    # \"x-\"\n    start = int(arg[1])\n  elif arg[2] != None:\n    # \"-z\"\n    start = max_ - int(arg[2]) + 1\n  else:\n    # \"x-y\"\n    start = int(arg[3])\n    end = int(arg[4])\n\nif min_ <= start <= end <= max_:\n  return xrange(start, end + 1)\nelse:\n  return None", "path": "rgm3800app\\rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Send some data to the device.\n\nArgs:\n  data: String, data to send.\n\"\"\"\n", "func_signal": "def write(self, data):\n", "code": "if not self.__receiver_running:\n  raise SerialConnectionLost\nos.write(self._fd, data)", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Shut down serial communication.\"\"\"\n# Notify receiver thread to stop, then wait for it.\n", "func_signal": "def close(self):\n", "code": "if self.__receiver_running:\n  self.__receiver_running = False\n  self.__receiver.join()\n\nself._Close()", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Parse a date (and optional time) string and return as date[time] objects.\n\nArgs:\n  datestr: String, date spec, e.g. \"20080605\" for June 5th, 2008.\n  timestr: String, time spec, e.g. \"143500\" for 14:35:00 (2:35pm).\n\nReturns:\n  datetime.datetime instance if timestr given, else datetime.date instance.\n\"\"\"\n", "func_signal": "def ParseDateTime(datestr, timestr=None):\n", "code": "assert len(datestr) == 8\nyear  = int(datestr[0:4])\nmonth = int(datestr[4:6])\nday   = int(datestr[6:8])\nif timestr:\n  assert len(timestr) == 6\n  hour  = int(timestr[0:2])\n  min   = int(timestr[2:4])\n  sec   = int(timestr[4:6])\n  return datetime.datetime(year, month, day, hour, min, sec)\nelse:\n  return datetime.date(year, month, day)", "path": "rgm3800app\\rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Decorator to print function with arguments and return value.\"\"\"\n", "func_signal": "def PrintCallInfo(func):\n", "code": "def _Runner(*args, **kwargs):\n  all_args = ['%r' % val for val in args]\n  for key, val in kwargs.iteritems():\n    all_args.append('%s=%r' % (key, val))\n  str_args = ', '.join(all_args)\n  print >>sys.stderr, '%s(%s) <-' % (func.__name__, str_args)\n  retval = func(*args, **kwargs)\n  print >>sys.stderr, '%s(%s) -> %r' % (func.__name__, str_args, retval)\n  return retval\nreturn _Runner", "path": "rgm3800app\\rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Flush data and close serial port file handle.\"\"\"\n", "func_signal": "def _Close(self):\n", "code": "if self._fd:\n  try:\n    termios.tcflush(self._fd, termios.TCIOFLUSH)\n  except termios.error:\n    pass\n  os.close(self._fd)\n  self._fd = None", "path": "rgm3800app\\rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Decorator to print function with arguments and return value.\"\"\"\n", "func_signal": "def PrintCallInfo(func):\n", "code": "def _Runner(*args, **kwargs):\n  all_args = ['%r' % val for val in args]\n  for key, val in kwargs.iteritems():\n    all_args.append('%s=%r' % (key, val))\n  str_args = ', '.join(all_args)\n  print >>sys.stderr, '%s(%s) <-' % (func.__name__, str_args)\n  retval = func(*args, **kwargs)\n  print >>sys.stderr, '%s(%s) -> %r' % (func.__name__, str_args, retval)\n  return retval\nreturn _Runner", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "# There is a bug in the firmware when handling PROY003:  If the logger has\n# not made a lock on satellites yet and therefore has no idea of the time\n# it answer with LOG002 instead of LOG003!\n", "func_signal": "def GetTimestamp(self):\n", "code": "data = self.SendRecv('PROY003', lines=1)\nif not data[0].startswith('LOG003,'):\n  return None\nself.ClearProgress()\ndata = data[0].split(',')\n# LOG003,20071226,101221\nreturn ParseDateTime(data[1], data[2])", "path": "rgm3800app\\rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Parse a range description and assert a range.\n\nSupported formats:\n  \"\":    min_ .. max_\n  \"x\":   x .. x\n  \"x-\":  x .. max_\n  \"x-y\": x .. y\n  \"-z:   max_-z+1 .. max_  (\"The last z entries.\")\n\nArgs:\n  arg: String, input argument.\n  min_: Integer, minimum value for start and end of range.\n  max_: Integer, maximum value for start and end of range.\n\nReturns:\n  None if unable to parse arg or iterator for the requested range,\n  yielding integers.\n\"\"\"\n", "func_signal": "def ParseRange(arg, min_, max_):\n", "code": "start = min_\nend = max_\n\nif arg:\n  arg_match = re.match(r'^(?:(\\d+)|(\\d+)-|-(\\d+)|(\\d+)-(\\d+))$', arg)\n  if not arg_match:\n    return None\n  arg = arg_match.groups()\n  if arg[0] != None:\n    # \"x\"\n    start = end = int(arg[0])\n  elif arg[1] != None:\n    # \"x-\"\n    start = int(arg[1])\n  elif arg[2] != None:\n    # \"-z\"\n    start = max_ - int(arg[2]) + 1\n  else:\n    # \"x-y\"\n    start = int(arg[3])\n    end = int(arg[4])\n\nif min_ <= start <= end <= max_:\n  return xrange(start, end + 1)\nelse:\n  return None", "path": "rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Send a request and receive the response.\n\nIf no valid result is received then the request is repeated.\n\nArgs:\n  request: String, request to send.\n  result: String, prefix of lines to return.\n  lines: Number of lines to return.\n\nReturns:\n  List of strings, received response.\n\"\"\"\n", "func_signal": "def SendRecv(self, request, result=None, lines=1):\n", "code": "result_lines = []\nfor i in range(5):\n  # Send the command.\n  self.SendMessage(request)\n\n  # Only accept a certain amount of noise.  If there is too much noise it's\n  # likely communication is broken anyway.\n  lines_acceptable = 20 + 5*lines\n\n  while lines_acceptable:\n    # Get a line.\n    msg = self.RecvMessage()\n    if not msg:\n      # Got no complete line until timeout.  Finished receiving.\n      break\n    lines_acceptable -= 1\n\n    if not result or msg.startswith(result):\n      result_lines.append(msg)\n\n    if lines and lines == len(result_lines):\n      break\n\n  if not result_lines:\n    # Failed receiving.  Retry.\n    self.ShowInfo('Timeout talking to device.  Retrying.')\n    continue\n\n  if lines and lines != len(result_lines):\n    # Not enough lines received.\n    self.ShowInfo('Incomplete results.  Retrying.')\n    continue\n\n  # Got a result, return it to the caller.\n  return result_lines\n\n# If we reach this point we failed repeatedly to talk to the device.\nraise SerialCommunicationError('Can not talk to device.')", "path": "rgm3800app\\rgm3800.py", "repo_name": "snaewe/rgm3800py", "stars": 4, "license": "None", "language": "python", "size": 193}
{"docstring": "\"\"\"Reset this token stream by setting its token source.\"\"\"\n\n", "func_signal": "def setTokenSource(self, tokenSource):\n", "code": "self.tokenSource = tokenSource\nself.tokens = []\nself.p = -1\nself.channel = DEFAULT_CHANNEL", "path": "3rdparty\\pylibs\\antlr3\\streams.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"\nGiven a starting index, return the index of the first on-channel\ntoken.\n\"\"\"\n\n", "func_signal": "def skipOffTokenChannels(self, i):\n", "code": "try:\n    while self.tokens[i].channel != self.channel:\n        i += 1\nexcept IndexError:\n    # hit the end of token stream\n    pass\n\nreturn i", "path": "3rdparty\\pylibs\\antlr3\\streams.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "# we must also copy additional attributes!\n\n", "func_signal": "def copy(self, copyChildren):\n", "code": "if copyChildren:\n\treturn copy.deepcopy(self)\nelse:\n\t# we must also use deepcopy even when no children should be copied - there may be unknown attributes added to this instance which should not be copied in a shallow way\n\tsaveChildren = self.children\n\tself.children = []\n\tt = copy.deepcopy(self)\n\tself.children = saveChildren\n\treturn t", "path": "src\\compiler\\tree.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "# use a very simple aproach:\n# copy source variables into temporary variables\n# copy data from temporary variables to destination variables\n# this avoids difficult cases like: a,b = b,a or a,b,c = b,b,b\n# but a,b = c,d is a bit slower - but the optimizer should transform that to an efficient version\n\n# copy source -> temp\n", "func_signal": "def _onListAssign(self, ast, variableNames, expressions):\n", "code": "temps = []\nn = len(variableNames)\nassert(n == len(expressions))\nfor i in range(n):\n\tself._dispatch(expressions[i])\n\n\tref = self._currentBuilder.alloca(expressions[i].esType.toLLVMType(), u'listassign_tmp')\n\tself._currentBuilder.store(expressions[i].llvmValue, ref)\n\n\tesVar = ESVariable(u'listassign_tmp', '__local', '__local', expressions[i].esType) # TODO insert real pkg / module names\n\tesVar.llvmRef = ref\n\ttemps.append(esVar)\n\n# copy temp -> destination\n# this is a simple assignment\nfor i in range(n):\n\tif variableNames[i].type == TreeType.VARIABLE:\n\t\tvar = self._findSymbol(fromTree=variableNames[i].children[0], type_=ESVariable)\n\t\tvalue = self._currentBuilder.load(temps[i].llvmRef)\n\t\tself._simpleAssignment(var, value)\n\telse:\n\t\tassert(0 and 'TODO')", "path": "src\\compiler\\ast2llvm.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "# FIXME\n", "func_signal": "def _createAllocaForVar(self, name, llvmType, value=None):\n", "code": "if llvmType.kind == TYPE_INTEGER:\n\tdefaultValue = Constant.int(llvmType, 0)\nelif llvmType.kind in [TYPE_FLOAT, TYPE_DOUBLE]:\n\tdefaultValue = Constant.real(llvmType, 0)\nelif llvmType.kind == TYPE_POINTER:\n\tdefaultValue = Constant.null(llvmType)\nelif llvmType.kind == TYPE_STRUCT:\n\tdefaultValue= Constant.null(llvmType)\nelse:\n\tassert(0 and 'unsupported variable type')\n\nif value == None:\n\tvalue = defaultValue\n\n# use the usual LLVM pattern to create mutable variables: use alloca\n# important: the mem2reg pass is limited to analyzing the entry block of functions,\n# so all variables must be defined there\nllvmFunc = self._findCurrentFunction().llvmRef\n\nentryBB = llvmFunc.get_entry_basic_block()\nentryBuilder = Builder.new(entryBB)\nentryBuilder.position_at_beginning(entryBB)\nref = entryBuilder.alloca(llvmType, name)\nentryBuilder.store(value, ref)\n\nreturn ref", "path": "src\\compiler\\ast2llvm.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"\nFrom the input stream, predict what alternative will succeed\n\tusing this DFA (representing the covering regular approximation\n\tto the underlying CFL).  Return an alternative number 1..n.  Throw\n\t an exception upon error.\n\t\"\"\"\n", "func_signal": "def predict(self, input):\n", "code": "mark = input.mark()\ns = 0 # we always start at s0\ntry:\n    for _ in xrange(50000):\n        #print \"***Current state = %d\" % s\n        \n        specialState = self.special[s]\n        if specialState >= 0:\n            #print \"is special\"\n            s = self.specialStateTransition(specialState, input)\n            if s == -1:\n                self.noViableAlt(s, input)\n                return 0\n            input.consume()\n            continue\n\n        if self.accept[s] >= 1:\n            #print \"accept state for alt %d\" % self.accept[s]\n            return self.accept[s]\n\n        # look for a normal char transition\n        c = input.LA(1)\n\n        #print \"LA = %d (%r)\" % (c, unichr(c) if c >= 0 else 'EOF')\n        #print \"range = %d..%d\" % (self.min[s], self.max[s])\n\n        if c >= self.min[s] and c <= self.max[s]:\n            # move to next state\n            snext = self.transition[s][c-self.min[s]]\n            #print \"in range, next state = %d\" % snext\n            \n            if snext < 0:\n                #print \"not a normal transition\"\n                # was in range but not a normal transition\n                # must check EOT, which is like the else clause.\n                # eot[s]>=0 indicates that an EOT edge goes to another\n                # state.\n                if self.eot[s] >= 0: # EOT Transition to accept state?\n                    #print \"EOT trans to accept state %d\" % self.eot[s]\n                    \n                    s = self.eot[s]\n                    input.consume()\n                    # TODO: I had this as return accept[eot[s]]\n                    # which assumed here that the EOT edge always\n                    # went to an accept...faster to do this, but\n                    # what about predicated edges coming from EOT\n                    # target?\n                    continue\n\n                #print \"no viable alt\"\n                self.noViableAlt(s, input)\n                return 0\n\n            s = snext\n            input.consume()\n            continue\n\n        if self.eot[s] >= 0:\n            #print \"EOT to %d\" % self.eot[s]\n            \n            s = self.eot[s]\n            input.consume()\n            continue\n\n        # EOF Transition to accept state?\n        if c == EOF and self.eof[s] >= 0:\n            #print \"EOF Transition to accept state %d\" \\\n            #  % self.accept[self.eof[s]]\n            return self.accept[self.eof[s]]\n\n        # not in range and not EOF/EOT, must be invalid symbol\n        self.noViableAlt(s, input)\n        return 0\n\n    else:\n        raise RuntimeError(\"DFA bang!\")\n    \nfinally:\n    input.rewind(mark)", "path": "3rdparty\\pylibs\\antlr3\\dfa.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"\n@param tokenSource A TokenSource instance (usually a Lexer) to pull\n    the tokens from.\n\n@param channel Skip tokens on any channel but this one; this is how we\n    skip whitespace...\n    \n\"\"\"\n\n", "func_signal": "def __init__(self, tokenSource=None, channel=DEFAULT_CHANNEL):\n", "code": "TokenStream.__init__(self)\n\nself.tokenSource = tokenSource", "path": "3rdparty\\pylibs\\antlr3\\streams.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"\nWe need to combine operations and report invalid operations (like\noverlapping replaces that are not completed nested).  Inserts to\nsame index need to be combined etc...   Here are the cases:\n\nI.i.u I.j.v                           leave alone, nonoverlapping\nI.i.u I.i.v                           combine: Iivu\n\nR.i-j.u R.x-y.v | i-j in x-y          delete first R\nR.i-j.u R.i-j.v                       delete first R\nR.i-j.u R.x-y.v | x-y in i-j          ERROR\nR.i-j.u R.x-y.v | boundaries overlap  ERROR\n\nI.i.u R.x-y.v   | i in x-y            delete I\nI.i.u R.x-y.v   | i not in x-y        leave alone, nonoverlapping\nR.x-y.v I.i.u   | i in x-y            ERROR\nR.x-y.v I.x.u                         R.x-y.uv (combine, delete I)\nR.x-y.v I.i.u   | i not in x-y        leave alone, nonoverlapping\n\nI.i.u = insert u before op @ index i\nR.x-y.u = replace x-y indexed tokens with u\n\nFirst we need to examine replaces.  For any replace op:\n\n  1. wipe out any insertions before op within that range.\n  2. Drop any replace op before that is contained completely within\n     that range.\n  3. Throw exception upon boundary overlap with any previous replace.\n\nThen we can deal with inserts:\n\n  1. for any inserts to same index, combine even if not adjacent.\n  2. for any prior replace with same left boundary, combine this\n     insert with replace and delete this replace.\n  3. throw exception if index in same range as previous replace\n\nDon't actually delete; make op null in list. Easier to walk list.\nLater we can throw as we add to index -> op map.\n\nNote that I.2 R.2-2 will wipe out I.2 even though, technically, the\ninserted stuff would be before the replace range.  But, if you\nadd tokens in front of a method body '{' and then delete the method\nbody, I think the stuff before the '{' you added should disappear too.\n\nReturn a map from token index to operation.\n\"\"\"\n\n# WALK REPLACES\n", "func_signal": "def reduceToSingleOperationPerIndex(self, rewrites):\n", "code": "for i, rop in enumerate(rewrites):\n    if rop is None:\n        continue\n\n    if not isinstance(rop, ReplaceOp):\n        continue\n\n    # Wipe prior inserts within range\n    for j, iop in self.getKindOfOps(rewrites, InsertBeforeOp, i):\n        if iop.index >= rop.index and iop.index <= rop.lastIndex:\n            rewrites[j] = None  # delete insert as it's a no-op.\n\n    # Drop any prior replaces contained within\n    for j, prevRop in self.getKindOfOps(rewrites, ReplaceOp, i):\n        if (prevRop.index >= rop.index\n            and prevRop.lastIndex <= rop.lastIndex):\n            rewrites[j] = None  # delete replace as it's a no-op.\n            continue\n\n        # throw exception unless disjoint or identical\n        disjoint = (prevRop.lastIndex < rop.index\n                    or prevRop.index > rop.lastIndex)\n        same = (prevRop.index == rop.index\n                and prevRop.lastIndex == rop.lastIndex)\n        if not disjoint and not same:\n            raise ValueError(\n                \"replace op boundaries of %s overlap with previous %s\"\n                % (rop, prevRop))\n\n# WALK INSERTS\nfor i, iop in enumerate(rewrites):\n    if iop is None:\n        continue\n\n    if not isinstance(iop, InsertBeforeOp):\n        continue\n\n    # combine current insert with prior if any at same index\n    for j, prevIop in self.getKindOfOps(rewrites, InsertBeforeOp, i):\n        if prevIop.index == iop.index: # combine objects\n            # convert to strings...we're in process of toString'ing\n            # whole token buffer so no lazy eval issue with any\n            # templates\n            iop.text = self.catOpText(iop.text, prevIop.text)\n            rewrites[j] = None  # delete redundant prior insert\n\n    # look for replaces where iop.index is in range; error\n    for j, rop in self.getKindOfOps(rewrites, ReplaceOp, i):\n        if iop.index == rop.index:\n            rop.text = self.catOpText(iop.text, rop.text)\n            rewrites[i] = None  # delete current insert\n            continue\n\n        if iop.index >= rop.index and iop.index <= rop.lastIndex:\n            raise ValueError(\n                \"insert op %s within boundaries of previous %s\"\n                % (iop, rop))\n\nm = {}\nfor i, op in enumerate(rewrites):\n    if op is None:\n        continue # ignore deleted ops\n\n    assert op.index not in m, \"should only be one op per index\"\n    m[op.index] = op\n\nreturn m", "path": "3rdparty\\pylibs\\antlr3\\streams.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"\nconsume() ahead until p==index; can't just set p=index as we must\nupdate line and charPositionInLine.\n\"\"\"\n\n", "func_signal": "def seek(self, index):\n", "code": "if index <= self.p:\n    self.p = index # just jump; don't update stream state (line, ...)\n    return\n\n# seek forward, consume until p hits index\nwhile self.p < index:\n    self.consume()", "path": "3rdparty\\pylibs\\antlr3\\streams.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"@brief Unpack the runlength encoded table data.\n\nTerence implemented packed table initializers, because Java has a\nsize restriction on .class files and the lookup tables can grow\npretty large. The generated JavaLexer.java of the Java.g example\nwould be about 15MB with uncompressed array initializers.\n\nPython does not have any size restrictions, but the compilation of\nsuch large source files seems to be pretty memory hungry. The memory\nconsumption of the python process grew to >1.5GB when importing a\n15MB lexer, eating all my swap space and I was to impacient to see,\nif it could finish at all. With packed initializers that are unpacked\nat import time of the lexer module, everything works like a charm.\n\n\"\"\"\n\n", "func_signal": "def unpack(cls, string):\n", "code": "ret = []\nfor i in range(len(string) / 2):\n    (n, v) = ord(string[i*2]), ord(string[i*2+1])\n\n    # Is there a bitwise operation to do this?\n    if v == 0xFFFF:\n        v = -1\n\n    ret += [v] * n\n\nreturn ret", "path": "3rdparty\\pylibs\\antlr3\\dfa.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"\n@param fileName The path to the file to be opened. The file will be\n   opened with mode 'rb'.\n\n@param encoding If you set the optional encoding argument, then the\n   data will be decoded on the fly.\n   \n\"\"\"\n\n", "func_signal": "def __init__(self, fileName, encoding=None):\n", "code": "self.fileName = fileName\n\nfp = codecs.open(fileName, 'rb', encoding)\ntry:\n    data = fp.read()\nfinally:\n    fp.close()\n    \nANTLRStringStream.__init__(self, data)", "path": "3rdparty\\pylibs\\antlr3\\streams.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "# first try to find a function (which resolves to it's address), then a normal variable\n", "func_signal": "def _onVariable(self, ast, variableName):\n", "code": "flist = self._findSymbol(fromTree=variableName, type_=ESFunction, mayFail=True)\nif flist:\n\tif len(flist) > 1:\n\t\tself._raiseException(RecoverableCompileError, tree=variableName, inlineText='taking the address of a overloaded function is not implemented, yet')\n\n\tf = flist[0]\n\tast.llvmValue = f.llvmRef\n\tast.llvmRef = f.llvmRef\nelse:\n\tvar = self._findSymbol(fromTree=variableName, type_=ESVariable)\n\tast.llvmValue = self._currentBuilder.load(var.llvmRef)\n\tast.llvmRef = var.llvmRef", "path": "src\\compiler\\ast2llvm.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"\nRollback the instruction stream for a program so that\nthe indicated instruction (via instructionIndex) is no\nlonger in the stream.  UNTESTED!\n\"\"\"\n\n", "func_signal": "def rollback(self, *args):\n", "code": "if len(args) == 2:\n    programName = args[0]\n    instructionIndex = args[1]\nelif len(args) == 1:\n    programName = self.DEFAULT_PROGRAM_NAME\n    instructionIndex = args[0]\nelse:\n    raise TypeError(\"Invalid arguments\")\n\np = self.programs.get(programName, None)\nif p is not None:\n    self.programs[programName] = (\n        p[self.MIN_TOKEN_INDEX:instructionIndex])", "path": "3rdparty\\pylibs\\antlr3\\streams.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"\nLoad all tokens from the token source and put in tokens.\n\tThis is done upon first LT request because you might want to\nset some token type / channel overrides before filling buffer.\n\"\"\"\n\n\n", "func_signal": "def fillBuffer(self):\n", "code": "index = 0\nt = self.tokenSource.nextToken()\nwhile t is not None and t.type != EOF:\n    discard = False\n    \n    if self.discardSet is not None and t.type in self.discardSet:\n        discard = True\n\n    elif self.discardOffChannelTokens and t.channel != self.channel:\n        discard = True\n\n    # is there a channel override for token type?\n    try:\n        overrideChannel = self.channelOverrideMap[t.type]\n        \n    except KeyError:\n        # no override for this type\n        pass\n    \n    else:\n        if overrideChannel == self.channel:\n            t.channel = overrideChannel\n        else:\n            discard = True\n    \n    if not discard:\n        t.index = index\n        self.tokens.append(t)\n        index += 1\n\n    t = self.tokenSource.nextToken()\n       \n# leave p pointing at first token on channel\nself.p = 0\nself.p = self.skipOffTokenChannels(self.p)", "path": "3rdparty\\pylibs\\antlr3\\streams.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "''' do not call directly! use construction methods '''\n", "func_signal": "def __init__(self, parents, payload):\n", "code": "assert(isinstance(parents, list))\nfor x in parents:\n\tassert(isinstance(x, ESType))\nself.parents = parents\nself.payload = payload", "path": "src\\compiler\\estype.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "# append additional NEWLINE at end of file\n", "func_signal": "def sourcecode2AST(source, type='module'):\n", "code": "source += '\\n'\n\ninputStream = antlr3.ANTLRStringStream(source)\nlexer = Lexer(inputStream, source)\ntokens = antlr3.CommonTokenStream(lexer)\n#tokens.discardOffChannelTokens = True\nparser = Parser(tokens, source)\n\nassert type in ['module']\nif type == 'module':\n\tresult = parser.start_module()\n\n# copy ast to our own tree implementation to make modifying easier\nastTree = antlrTree2Tree(result.tree)\n\n# 'desugar' it inplace\ndesugar(astTree)\n\nreturn (parser.getNumberOfSyntaxErrors(), astTree)", "path": "src\\compiler\\source2ast.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "# int puts(char *);\n", "func_signal": "def _addHelperFunctionsPreTranslation(self):\n", "code": "returnTypes = [self._findSymbol(name=u'int32', type_=ESType)]\nparamTypes = [self._findSymbol(name=u'int8', type_=ESType).derivePointer()]\nesType = ESType.createFunction(returnTypes, paramTypes)\nesFunc = ESFunction(u'puts', '', '', esType, [u's'], mangling='C', linkage='extern')\nself._addSymbol(name=u'puts', symbol=esFunc)\ntype = esType.toLLVMType()\nfunc = self._module.add_function(type, 'puts')\n\n\n# void abort();\nreturnTypes = [self._findSymbol(name=u'void', type_=ESType)]\nparamTypes = []\nesType = ESType.createFunction(returnTypes, paramTypes)\nesFunc = ESFunction(u'abort', '', '', esType, [], mangling='C', linkage='extern')\ntype = esType.toLLVMType()\nfunc = self._module.add_function(type, 'abort')", "path": "src\\compiler\\ast2llvm.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "## Track node to number mapping so we can get proper node name back\n", "func_signal": "def __init__(self):\n", "code": "self.nodeToNumberMap = {}\n\n## Track node number so we can get unique node names\nself.nodeNumber = 0", "path": "3rdparty\\pylibs\\antlr3\\dottreegen.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "# FIXME\n", "func_signal": "def _onStringConstant(self, ast, constant):\n", "code": "s = constant.text\nassert(s.startswith('ar\"'))\ns = s[3:-1]\n\n\nstringConst = Constant.stringz(s)\nstring = self._module.add_global_variable(stringConst.type, 'internalStringConstant')\nstring.initializer = stringConst\nstring.global_constant = True\nstring.linkage = LINKAGE_INTERNAL\n\nword = self._findSymbol(name=u'word', type_=ESType).toLLVMType()\nidx = [Constant.int(word, 0), Constant.int(word, 0)]\nast.llvmValue = string.gep(idx)", "path": "src\\compiler\\ast2llvm.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"Look backwards k tokens on-channel tokens\"\"\"\n\n", "func_signal": "def LB(self, k):\n", "code": "if self.p == -1:\n    self.fillBuffer()\n\nif k == 0:\n    return None\n\nif self.p - k < 0:\n    return None\n\ni = self.p\nn = 1\n# find k good tokens looking backwards\nwhile n <= k:\n    # skip off-channel tokens\n    i = self.skipOffTokenChannelsReverse(i-1) # leave p on valid token\n    n += 1\n\nif i < 0:\n    return None\n    \nreturn self.tokens[i]", "path": "3rdparty\\pylibs\\antlr3\\streams.py", "repo_name": "fnoeding/exoself", "stars": 6, "license": "other", "language": "python", "size": 2120}
{"docstring": "\"\"\"Connect using SSL.\n\nIf address is None or not specified, sock is expected to be a connected\nregular socket. If address is specified, sock must be unconnected, and\nwill be connected to the specified address. This function starts client-\nside SSL (using set_connect_state).\n\nThe calling task will be suspended until the socket is connected and the\nSSL handshake is complete.\n\"\"\"\n", "func_signal": "def connect(sock, address=None, cert=None, verify=None, timeout=None):\n", "code": "if address is not None:\n    if timeout is not None:\n        end = time.time() + timeout\n    greennet.connect(sock, address, timeout)\n    if timeout is not None:\n        timeout = end - time.time()\nsock = _setup_connection(sock, cert, verify)\nsock.set_connect_state()\n_io(lambda sock: sock.do_handshake(), sock, timeout=timeout)\nreturn sock", "path": "greennet\\ssl.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Append an item to the left side of the Queue.\n\n>>> q = Queue(2)\n>>> q.appendleft('an item')\n>>> len(q)\n1\n>>> q.appendleft('another item')\n>>> len(q)\n2\n>>> q.appendleft('a third item', 0)\nTraceback (most recent call last):\n    ...\nTimeout\n>>> len(q)\n2\n>>> q.popleft()\n'another item'\n>>> q.popleft()\n'an item'\n\"\"\"\n", "func_signal": "def appendleft(self, item, timeout=None):\n", "code": "if self.full():\n    self._wait_for_pop(timeout)\nself.queue.appendleft(item)\nself._appended()", "path": "greennet\\queue.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Accept an SSL connection.\n\nThe sock argument is an already connected socket-like object. This\nfunction starts server-side SSL (using set_accept_state).\n\nThe calling task will be suspended until the SSL handshake is complete.\n\"\"\"\n", "func_signal": "def accept(sock, cert=None, verify=None, timeout=None):\n", "code": "sock = _setup_connection(sock, cert, verify)\nsock.set_accept_state()\n_io(lambda sock: sock.do_handshake(), sock, timeout=timeout)\nreturn sock", "path": "greennet\\ssl.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Remove all items from the Queue.\n\n>>> q = Queue()\n>>> q.append('an item')\n>>> len(q)\n1\n>>> q.clear()\n>>> len(q)\n0\n\"\"\"\n", "func_signal": "def clear(self):\n", "code": "self.queue.clear()\nself._popped()", "path": "greennet\\queue.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Run all immediately available tasks.\n\nReturns when all tasks either finish or are waiting for an event.\n\"\"\"\n", "func_signal": "def _run_tasks(self):\n", "code": "while self.tasks:\n    task, args, kwargs = self.tasks.popleft()\n    task.switch(*args, **kwargs)", "path": "greennet\\hub.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Run the task after the specified number of seconds.\"\"\"\n", "func_signal": "def call_later(self, task, timeout, *args, **kwargs):\n", "code": "expires = time.time() + timeout\nsleep = Sleep(task, expires, args, kwargs)\nself._add_timeout(sleep)", "path": "greennet\\hub.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Append an item to the right side of the Queue.\n\n>>> q = Queue(2)\n>>> q.append('an item')\n>>> len(q)\n1\n>>> q.append('another item')\n>>> len(q)\n2\n>>> q.append('a third item', 0)\nTraceback (most recent call last):\n    ...\nTimeout\n>>> len(q)\n2\n>>> q.popleft()\n'an item'\n>>> q.popleft()\n'another item'\n\"\"\"\n", "func_signal": "def append(self, item, timeout=None):\n", "code": "if self.full():\n    self._wait_for_pop(timeout)\nself.queue.append(item)\nself._appended()", "path": "greennet\\queue.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Pop an item from the left side of the Queue.\n\n>>> q = Queue()\n>>> q.append('an item')\n>>> q.append('another item')\n>>> q.popleft()\n'an item'\n>>> q.popleft()\n'another item'\n>>> q.popleft(0)\nTraceback (most recent call last):\n    ...\nTimeout\n\"\"\"\n", "func_signal": "def popleft(self, timeout=None):\n", "code": "if not self.queue:\n    self._wait_for_append(timeout)\nitem = self.queue.popleft()\nself._popped()\nreturn item", "path": "greennet\\queue.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Shut down SSL.\n\nCalls the SSL shutdown method until it completes. The calling task will be\nsuspended until this completes.\n\"\"\"\n", "func_signal": "def shutdown(sock, timeout=None):\n", "code": "if timeout is not None:\n    end = time.time() + timeout\nh = greennet.get_hub()\nwhile not sock.shutdown():\n    h.poll(sock, read=sock.want_read(),\n           write=sock.want_write(), timeout=timeout)\n    if timeout is not None:\n        timeout = end - time.time()", "path": "greennet\\ssl.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Remove a Wait object from the timeout heap.\"\"\"\n", "func_signal": "def _remove_timeout(self, item):\n", "code": "self.timeouts.remove(item)\nheapq.heapify(self.timeouts)", "path": "greennet\\hub.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Called when the Queue is reduced in size.\"\"\"\n", "func_signal": "def _popped(self):\n", "code": "if self._pop_waits:\n    wait = self._pop_waits.popleft()\n    if wait.expires is not None:\n        self.hub._remove_timeout(wait)\n    self.hub.schedule(wait.task)", "path": "greennet\\queue.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Wrap a socket in SSL.Connection and peekable objects.\"\"\"\n", "func_signal": "def _setup_connection(sock, cert, verify):\n", "code": "ctx = SSL.Context(SSL.SSLv23_METHOD)\nctx.set_options(SSL.OP_NO_SSLv2)\nctx.set_options(SSL.OP_SINGLE_DH_USE)\nctx.set_options(SSL.OP_ALL)\nif cert is not None:\n    ctx.use_certificate_file(cert['certfile'])\n    ctx.use_privatekey_file(cert['keyfile'])\n    ctx.check_privatekey()\nif verify is not None:\n    ctx.load_verify_locations(verify['cafile'])\n    ctx.set_verify(verify['mode'], verify['callback'])\n    ctx.set_verify_depth(verify.get('depth', 1))\nsock = peekable(SSL.Connection(ctx, sock))\nsock.setblocking(False)\nreturn sock", "path": "greennet\\ssl.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Returns True if the Queue is full, else False.\n\n>>> q = Queue(1)\n>>> q.full()\nFalse\n>>> q.append('an item')\n>>> q.full()\nTrue\n>>> q.pop()\n'an item'\n>>> q.full()\nFalse\n\"\"\"\n", "func_signal": "def full(self):\n", "code": "if self.maxlen is None:\n    return False\nreturn len(self.queue) >= self.maxlen", "path": "greennet\\queue.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Return a pair of connected peekable SSL connections.\"\"\"\n", "func_signal": "def peekablepair(cert1, cert2):\n", "code": "s1, s2 = socket.socketpair()\ns1 = _setup_connection(s1, cert1, None)\ns2 = _setup_connection(s2, cert2, None)\ns1.set_accept_state()\ns2.set_connect_state()\nsocks = [s1, s2]\nhandshaking = [True, True]\nwhile any(handshaking):\n    for i, h in enumerate(handshaking):\n        if not h:\n            continue\n        try:\n            socks[i].do_handshake()\n            handshaking[i] = False\n        except SSL.WantReadError:\n            pass\n        except SSL.WantWriteError:\n            pass\nreturn s1, s2", "path": "greennet\\ssl.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Suspend the current task until an IO event occurs.\"\"\"\n", "func_signal": "def poll(self, fd, read=False, write=False, exc=False, timeout=None):\n", "code": "expires = None if timeout is None else time.time() + timeout\nif hasattr(fd, 'fileno'):\n    fd = fd.fileno()\nwait = FDWait(greenlet.getcurrent(), fd, read, write, exc, expires)\nself.fdwaits.add(wait)\nif timeout is not None:\n    self._add_timeout(wait)\nself.greenlet.switch()", "path": "greennet\\hub.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Generator yielding all prefixes of s.\n\n>>> list(prefixes('foobar'))\n['fooba', 'foob', 'foo', 'fo', 'f']\n\"\"\"\n", "func_signal": "def prefixes(s):\n", "code": "for i in xrange(len(s) - 1, 0, -1):\n    yield s[:i]", "path": "greennet\\util.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Perform an operation, handling SSL error conditions.\"\"\"\n", "func_signal": "def _io(op, sock, args=(), kw=None, timeout=None):\n", "code": "if kw is None:\n    kw = {}\nif timeout is None:\n    timeout = kw.pop('timeout', None)\nif timeout is not None:\n    end = time.time() + timeout\n    kw['timeout'] = timeout\nwhile True:\n    try:\n        return op(sock, *args, **kw)\n    except SSL.ZeroReturnError:\n        return ''\n    except SSL.WantReadError:\n        greennet.readable(sock, kw.get('timeout'))\n    except SSL.WantWriteError:\n        greennet.writable(sock, kw.get('timeout'))\n    if timeout is not None:\n        kw['timeout'] = end - time.time()", "path": "greennet\\ssl.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Called when the Queue increases in size.\"\"\"\n", "func_signal": "def _appended(self):\n", "code": "if self._append_waits:\n    wait = self._append_waits.popleft()\n    if wait.expires is not None:\n        self.hub._remove_timeout(wait)\n    self.hub.schedule(wait.task)", "path": "greennet\\queue.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Suspend the current task until an append happens.\n\nCall this if popping from an empty Queue.\n\"\"\"\n", "func_signal": "def _wait_for_append(self, timeout):\n", "code": "expires = None if timeout is None else time.time() + timeout\nwait = AppendWait(greenlet.getcurrent(), self, expires)\nif timeout is not None:\n    self.hub._add_timeout(wait)\nself._append_waits.append(wait)\nself.hub.run()", "path": "greennet\\queue.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Renegotiate the SSL connection (client-side).\n\nSpecify new certificates and verification options, and re-handshake. The\ncalling task will be suspended until the re-handshake is complete.\n\"\"\"\n", "func_signal": "def renegotiate_client(sock, cert=None, verify=None, timeout=None):\n", "code": "ctx = sock.get_context()\nif cert is not None:\n    ctx.use_certificate_file(cert['certfile'])\n    ctx.use_privatekey_file(cert['keyfile'])\n    ctx.check_privatekey()\nif verify is not None:\n    ctx.load_verify_locations(verify['cafile'])\n    ctx.set_verify(verify['mode'], verify['callback'])\n    ctx.set_verify_depth(verify.get('depth', 1))\nsock.renegotiate()\n_io(lambda sock: sock.do_handshake(), sock, timeout=timeout)", "path": "greennet\\ssl.py", "repo_name": "dhain/greennet", "stars": 4, "license": "mit", "language": "python", "size": 156}
{"docstring": "\"\"\"Set or create a content for a particular page and language.\"\"\"\n", "func_signal": "def set_or_create_content(self, page, language, cnttype, body):\n", "code": "if settings.PAGE_SANITIZE_USER_INPUT:\n    body = self.sanitize(body)\ntry:\n    content = self.filter(page=page, language=language,\n                          type=cnttype).latest('creation_date')\n    content.body = body\nexcept self.model.DoesNotExist:\n    content = self.model(page=page, language=language, body=body,\n                         type=cnttype)\ncontent.save()\nreturn content", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Render a nested list of all children of the given page, including\nthis page\"\"\"\n", "func_signal": "def pages_menu(context, page, url='/'):\n", "code": "lang = context['lang']\npath = context['path']\nsite_id = None\nchildren = page.get_children_for_frontend()\nif 'current_page' in context:\n    current_page = context['current_page']\nreturn locals()", "path": "pages\\templatetags\\pages_tags.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "# {% placeholder <name> [on <page>] [with <widget>] [parsed] [as\n# <varname>] %}\n", "func_signal": "def handle_token(cls, parser, token):\n", "code": "bits = token.split_contents()\ncount = len(bits)\nerror_string = '%r tag requires at least one argument' % bits[0]\nif count <= 1:\n    raise template.TemplateSyntaxError(error_string)\nname = bits[1]\nremaining = bits[2:]\nparams = {}\nwhile remaining:\n    bit = remaining[0]\n    if bit not in ('as', 'on', 'with', 'parsed'):\n        raise template.TemplateSyntaxError(\n            \"%r is not an correct option for a placeholder\" % bit)\n    if bit in ('as', 'on', 'with'):\n        if len(remaining) < 2:\n            raise template.TemplateSyntaxError(\n            \"Placeholder option '%s' need a parameter\" % bit)\n        if bit == 'as':\n            params['as_varname'] = remaining[1]\n        if bit == 'with':\n            params['widget'] = remaining[1]\n        if bit == 'on':\n            params['page'] = remaining[1]\n        remaining = remaining[2:]\n    else:\n        params['parsed'] = True\n        remaining = remaining[1:]\nreturn cls(name, **params)", "path": "pages\\templatetags\\pages_tags.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Give a list of ``Page`` ids where the user has rights or the string\n\"All\" if the user has all rights.\"\"\"\n", "func_signal": "def get_page_id_list(self, user):\n", "code": "if user.is_superuser:\n    return 'All'\nid_list = []\nfor perm in self.filter(user=user):\n    if perm.type == 0:\n        return \"All\"\n    if perm.page.id not in id_list:\n        id_list.append(perm.page.id)\n    if perm.type == 2:\n        for page in perm.page.get_descendants():\n            if page.id not in id_list:\n                id_list.append(page.id)\nreturn id_list", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Store a content type from a page into a context variable.\n\neg: {% get_content page_object \"title\" as content %}\n\nYou can also use the slug of a page\n\neg: {% get_content \"my-page-slug\" \"title\" as content %}\n\nSyntax: {% get_content page type [lang] as name %}\nArguments:\npage -- the page object\ntype -- content_type used by a placeholder\nname -- name of the context variable to store the content in\nlang -- the wanted language\n\"\"\"\n", "func_signal": "def do_get_content(parser, token):\n", "code": "bits = token.split_contents()\nif not 5 <= len(bits) <= 6:\n    raise TemplateSyntaxError('%r expects 4 or 5 arguments' % bits[0])\nif bits[-2] != 'as':\n    raise TemplateSyntaxError(\n        '%r expects \"as\" as the second last argument' % bits[0])\npage = parser.compile_filter(bits[1])\ncontent_type = parser.compile_filter(bits[2])\nvarname = bits[-1]\nlang = None\nif len(bits) == 6:\n    lang = parser.compile_filter(bits[3])\nreturn GetContentNode(page, content_type, varname, lang)", "path": "pages\\templatetags\\pages_tags.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Filter the given pages ``QuerySet`` to obtain only published\npage.\"\"\"\n", "func_signal": "def filter_published(self, queryset):\n", "code": "if settings.PAGE_USE_SITE_ID:\n    queryset = queryset.filter(sites=settings.SITE_ID)\n\nqueryset = queryset.filter(status=self.model.PUBLISHED)\n\nif settings.PAGE_SHOW_START_DATE:\n    queryset = queryset.filter(publication_date__lte=datetime.now())\n\nif settings.PAGE_SHOW_END_DATE:\n    queryset = queryset.filter(\n        Q(publication_end_date__gt=datetime.now()) |\n        Q(publication_end_date__isnull=True)\n    )\nreturn queryset", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Return a list of PlaceholderNode found in the given template\"\"\"\n", "func_signal": "def get_placeholders(template_name):\n", "code": "try:\n    temp = loader.get_template(template_name)\nexcept TemplateDoesNotExist:\n    return []\n    \nrequest = get_request_mock()\n\ntry:\n    # to avoid circular import\n    from pages.views import details\n    context = details(request, only_context=True)\nexcept Http404:\n    context = {}\ntemp.render(RequestContext(request, context))\nplist, blist = [], []\nplaceholders_recursif(temp.nodelist, plist, blist)\nreturn plist", "path": "pages\\utils.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Gets the latest ``Content`` for a particular page and language.\nFalls back to another language if wanted.\"\"\"\n", "func_signal": "def get_content(self, page, language, ctype, language_fallback=False):\n", "code": "PAGE_CONTENT_DICT_KEY = \"page_content_dict_%d_%s\"\nif not language:\n    language = settings.PAGE_DEFAULT_LANGUAGE\n\ncontent_dict = cache.get(PAGE_CONTENT_DICT_KEY % (page.id, ctype))\n#content_dict = None\n\nif not content_dict:\n    content_dict = {}\n    for lang in settings.PAGE_LANGUAGES:\n        try:\n            content = self.filter(language=lang[0], type=ctype, page=page).latest()\n            content_dict[lang[0]] = content.body\n        except self.model.DoesNotExist:\n            content_dict[lang[0]] = ''\n    cache.set(PAGE_CONTENT_DICT_KEY % (page.id, ctype), content_dict)\n\nif content_dict[language]:\n    return content_dict[language]\n\nif language_fallback:\n    for lang in settings.PAGE_LANGUAGES:\n        if content_dict[lang[0]]:\n            return content_dict[lang[0]]\nreturn ''", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Helper function used by placeholder nodes\"\"\"\n", "func_signal": "def get_content(context, page, content_type, lang, fallback=True):\n", "code": "if not page:\n    return ''\n\nif not lang and 'lang' in context:\n    lang = context['lang']\n\n# if the page is a SafeUnicode, try to use it like a slug\nif isinstance(page, SafeUnicode) or isinstance(page, unicode):\n    page = Page.objects.from_path(page, lang)\n\nif not page:\n    return ''\n\n# now that we are sure to have a page object, we can found the content's\n# language more accuratly\n\"\"\"if not absolute_lang:\n    absolute_lang = get_language_from_request(context['lang'], page)\"\"\"\n\nc = Content.objects.get_content(page, lang, content_type, fallback)\nreturn c", "path": "pages\\templatetags\\pages_tags.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Return a normalized url with trailing and without leading slash.\n \n >>> normalize_url(None)\n '/'\n >>> normalize_url('/')\n '/'\n >>> normalize_url('/foo/bar')\n '/foo/bar'\n >>> normalize_url('foo/bar')\n '/foo/bar'\n >>> normalize_url('/foo/bar/')\n '/foo/bar'\n\"\"\"\n", "func_signal": "def normalize_url(url):\n", "code": "if not url or len(url)==0:\n    return '/'\nif not url.startswith('/'):\n    url = '/' + url\nif len(url)>1 and url.endswith('/'):\n    url = url[0:len(url)-1]\nreturn url", "path": "pages\\utils.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Get a page according to the page's path.\"\"\"\n", "func_signal": "def from_path(self, path, lang, exclude_drafts=True):\n", "code": "from pages.models import Content, Page\nfrom pages.http import get_slug_and_relative_path\nslug, rpath = get_slug_and_relative_path(path)\npage_ids = Content.objects.get_page_ids_by_slug(slug)\npages_list = self.filter(id__in=page_ids)\nif exclude_drafts:\n    pages_list = pages_list.exclude(status=Page.DRAFT)\ncurrent_page = None\nif len(pages_list) == 1:\n    return pages_list[0]\n# more than one page matching the slug, let's use the full url\nif len(pages_list) > 1:\n    for page in pages_list:\n        if page.get_url(lang) == path:\n            return page\nreturn None", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Sanitize the content to avoid XSS\"\"\"\n", "func_signal": "def sanitize(self, content):\n", "code": "import html5lib\nfrom html5lib import sanitizer\np = html5lib.HTMLParser(tokenizer=sanitizer.HTMLSanitizer)\n# we need to remove <html><head/><body>...</body></html>\nreturn p.parse(content).toxml()[19:-14]", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"\nResolve a request to an alias. returns a ``PageAlias`` object or None\nif the url matches no page at all. The aliasing system supports plain\naliases (``/foo/bar``) as well as aliases containing GET parameters\n(like ``index.php?page=foo``).\n\"\"\"\n", "func_signal": "def from_path(self, request, path=None, lang=None):\n", "code": "from pages.utils import normalize_url\nfrom pages.models import Page,PageAlias\n\nurl = normalize_url(path)\n# \u00a71: try with complete query string\nif (\"QUERY_STRING\" in request.META and\n        request.META[\"QUERY_STRING\"] != \"\"):\n    url = url + '?' + request.META[\"QUERY_STRING\"]\ntry:\n    alias = PageAlias.objects.get(url=url)\n    return alias\nexcept PageAlias.DoesNotExist:\n    pass\n# \u00a72: try with path only\nurl = normalize_url(path)\ntry:\n    alias = PageAlias.objects.get(url=url)\n    return alias\nexcept PageAlias.DoesNotExist:\n    pass\n# \u00a73: not alias found, we give up\nreturn None", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Set or create a content for a particular page and language\"\"\"\n", "func_signal": "def create_content_if_changed(self, page, language, cnttype, body):\n", "code": "if settings.PAGE_SANITIZE_USER_INPUT:\n    body = self.sanitize(body)\ntry:\n    content = self.filter(page=page, language=language,\n                          type=cnttype).latest('creation_date')\n    if content.body == body:\n        return content\nexcept self.model.DoesNotExist:\n    pass\ncontent = self.create(page=page, language=language, body=body, type=cnttype)", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Display a content type from a page.\n\neg: {% show_content page_object \"title\" %}\n\nYou can also use the slug of a page\n\neg: {% show_content \"my-page-slug\" \"title\" %}\n\nKeyword arguments:\npage -- the page object\nargs -- content_type used by a placeholder\nlang -- the wanted language (default None, use the request object to know)\nfallback -- use fallback content\n\"\"\"\n", "func_signal": "def show_content(context, page, content_type, lang=None, fallback=True):\n", "code": "return {'content':get_content(context, page, content_type, lang,\n                                                            fallback)}", "path": "pages\\templatetags\\pages_tags.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Render the admin table of pages\"\"\"\n", "func_signal": "def pages_admin_menu(context, page, url='', level=None):\n", "code": "request = context['request']\n\nif \"tree_expanded\" in request.COOKIES:\n    cookie_string = urllib.unquote(request.COOKIES['tree_expanded'])\n    if cookie_string:\n        ids = [int(id) for id in \n            urllib.unquote(request.COOKIES['tree_expanded']).split(',')]\n        if page.id in ids:\n            expanded = True\n\npage_languages = settings.PAGE_LANGUAGES\nhas_permission = page.has_page_permission(request)\nPAGES_MEDIA_URL = settings.PAGES_MEDIA_URL\nlang = context.get('lang', None)\n\nreturn locals()", "path": "pages\\templatetags\\pages_tags.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Return a ``QuerySet`` of pages that are published on the site\ndefined by the ``SITE_ID`` setting.\"\"\"\n", "func_signal": "def on_site(self, site_id=None):\n", "code": "if settings.PAGE_USE_SITE_ID:\n    if not site_id:\n        site_id = settings.SITE_ID\n    return self.filter(sites=site_id)\nreturn self", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Return all page id matching the given slug.\"\"\"\n", "func_signal": "def get_page_ids_by_slug(self, slug):\n", "code": "sql = '''SELECT pages_content.page_id,\n    MAX(pages_content.creation_date)\n    FROM pages_content WHERE (pages_content.type = %s\n    AND pages_content.body =%s)\n    GROUP BY pages_content.page_id'''\n    \ncursor = connection.cursor()\ncursor.execute(sql, ('slug', slug, ))\nreturn [c[0] for c in cursor.fetchall()]", "path": "pages\\managers.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"Return true if the current user has permission to add a new page.\"\"\"\n", "func_signal": "def has_page_add_permission(request, page=None):\n", "code": "if not settings.PAGE_PERMISSION:\n    return True\nelse:\n    from pages.models import PagePermission\n    permission = PagePermission.objects.get_page_id_list(request.user)\n    if permission == \"All\":\n        return True\nreturn False", "path": "pages\\utils.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "\"\"\"\nrender a \"dynamic\" tree menu, with all nodes expanded which are either\nancestors or the current page itself. \n\"\"\"\n", "func_signal": "def pages_dynamic_tree_menu(context, page, url='/'):\n", "code": "request = context['request']\nsite_id = None\nchildren = None\nif 'current_page' in context:\n    current_page = context['current_page']\n    # if this node is expanded, we also have to render its children\n    # a node is expanded if it is the current node or one of its ancestors        \n    if (page == current_page) or (page in current_page.get_ancestors()):\n        children = page.get_children_for_frontend() \nreturn locals()", "path": "pages\\templatetags\\pages_tags.py", "repo_name": "titaneg/django-page-cms", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 524}
{"docstring": "'''check that the callObj is called with parameter specified by paramIdx (a position index or keyword)\nfulfills the condition specified by cond.\ncond is a function that takes a single argument, the value to test.\n'''\n", "func_signal": "def expectParam(paramIdx, cond):\n", "code": "def fn(mockObj, callObj, idx):\n    param = callObj.getParam(paramIdx)\n    return cond(param)\nreturn fn", "path": "lib\\pythonmock\\mock.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "\"\"\"Decode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning\nwith a JSON document) and return a 2-tuple of the Python\nrepresentation and the index in ``s`` where the document ended.\n\nThis can be used to decode a JSON document from a string that may\nhave extraneous data at the end.\n\n\"\"\"\n", "func_signal": "def raw_decode(self, s, idx=0):\n", "code": "try:\n    obj, end = self.scan_once(s, idx)\nexcept StopIteration:\n    raise ValueError(\"No JSON object could be decoded\")\nreturn obj, end", "path": "lib\\simplejson\\decoder.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "\"\"\"Scan the string s for a JSON string. End is the index of the\ncharacter in s after the quote that started the JSON string.\nUnescapes all valid JSON string escape sequences and raises ValueError\non attempt to decode an invalid string. If strict is False then literal\ncontrol characters are allowed in the string.\n\nReturns a tuple of the decoded string and the index of the character in s\nafter the end quote.\"\"\"\n", "func_signal": "def py_scanstring(s, end, encoding=None, strict=True, _b=BACKSLASH, _m=STRINGCHUNK.match):\n", "code": "if encoding is None:\n    encoding = DEFAULT_ENCODING\nchunks = []\n_append = chunks.append\nbegin = end - 1\nwhile 1:\n    chunk = _m(s, end)\n    if chunk is None:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    end = chunk.end()\n    content, terminator = chunk.groups()\n    # Content is contains zero or more unescaped string characters\n    if content:\n        if not isinstance(content, unicode):\n            content = unicode(content, encoding)\n        _append(content)\n    # Terminator is the end of string, a literal control character,\n    # or a backslash denoting that an escape sequence follows\n    if terminator == '\"':\n        break\n    elif terminator != '\\\\':\n        if strict:\n            msg = \"Invalid control character %r at\" % (terminator,)\n            #msg = \"Invalid control character {0!r} at\".format(terminator)\n            raise ValueError(errmsg(msg, s, end))\n        else:\n            _append(terminator)\n            continue\n    try:\n        esc = s[end]\n    except IndexError:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    # If not a unicode escape sequence, must be in the lookup table\n    if esc != 'u':\n        try:\n            char = _b[esc]\n        except KeyError:\n            msg = \"Invalid \\\\escape: \" + repr(esc)\n            raise ValueError(errmsg(msg, s, end))\n        end += 1\n    else:\n        # Unicode escape sequence\n        esc = s[end + 1:end + 5]\n        next_end = end + 5\n        if len(esc) != 4:\n            msg = \"Invalid \\\\uXXXX escape\"\n            raise ValueError(errmsg(msg, s, end))\n        uni = int(esc, 16)\n        # Check for surrogate pair on UCS-4 systems\n        if 0xd800 <= uni <= 0xdbff and sys.maxunicode > 65535:\n            msg = \"Invalid \\\\uXXXX\\\\uXXXX surrogate pair\"\n            if not s[end + 5:end + 7] == '\\\\u':\n                raise ValueError(errmsg(msg, s, end))\n            esc2 = s[end + 7:end + 11]\n            if len(esc2) != 4:\n                raise ValueError(errmsg(msg, s, end))\n            uni2 = int(esc2, 16)\n            uni = 0x10000 + (((uni - 0xd800) << 10) | (uni2 - 0xdc00))\n            next_end += 6\n        char = unichr(uni)\n        end = next_end\n    # Append the unescaped character\n    _append(char)\nreturn u''.join(chunks), end", "path": "lib\\simplejson\\decoder.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "# build the base signature string\n", "func_signal": "def build_signature(self, oauth_request, consumer, token):\n", "code": "key, raw = self.build_signature_base_string(oauth_request, consumer, token)\n\n# hmac object\ntry:\n    import hashlib # 2.5\n    hashed = hmac.new(key, raw, hashlib.sha1)\nexcept:\n    import sha # deprecated\n    hashed = hmac.new(key, raw, sha)\n\n# calculate the digest base 64\nreturn base64.b64encode(hashed.digest())", "path": "lib\\oauth.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "# verify that the nonce is uniqueish\n", "func_signal": "def _check_nonce(self, consumer, token, nonce):\n", "code": "nonce = self.data_store.lookup_nonce(consumer, token, nonce)\nif nonce:\n    raise OAuthError('Nonce already used: %s' % str(nonce))", "path": "lib\\oauth.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "# combine multiple parameter sources\n", "func_signal": "def from_request(http_method, http_url, headers=None, parameters=None, query_string=None):\n", "code": "if parameters is None:\n    parameters = {}\n\n# headers\nif headers and 'Authorization' in headers:\n    auth_header = headers['Authorization']\n    # check that the authorization header is OAuth\n    if auth_header.index('OAuth') > -1:\n        try:\n            # get the parameters from the header\n            header_params = OAuthRequest._split_header(auth_header)\n            parameters.update(header_params)\n        except:\n            raise OAuthError('Unable to parse OAuth parameters from Authorization header.')\n\n# GET or POST query string\nif query_string:\n    query_params = OAuthRequest._split_url_string(query_string)\n    parameters.update(query_params)\n\n# URL parameters\nparam_str = urlparse.urlparse(http_url)[4] # query\nurl_params = OAuthRequest._split_url_string(param_str)\nparameters.update(url_params)\n\nif parameters:\n    return OAuthRequest(http_method, http_url, parameters)\n\nreturn None", "path": "lib\\oauth.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "\"\"\"\nCheck that a call to a method of the given name to the original\nclass with the given parameters would not fail. If it would fail,\nraise a MockInterfaceError.\nBased on the Python 2.3.3 Reference Manual section 5.3.4: Calls.\n\"\"\"\n", "func_signal": "def _checkInterfaceCall(self, name, callParams, callKwParams):\n", "code": "if self.realClassMethods == None:\n    return\nif not self.realClassMethods.has_key(name):\n    raise MockInterfaceError(\"Calling mock method '%s' that was not found in the original class\" % name)\n\nfunc = self.realClassMethods[name]\ntry:\n    args, varargs, varkw, defaults = inspect.getargspec(func)\nexcept TypeError:\n    # func is not a Python function. It is probably a builtin,\n    # such as __repr__ or __coerce__. TODO: Checking?\n    # For now assume params are OK.\n    return\n\n# callParams doesn't include self; args does include self.\nnumPosCallParams = 1 + len(callParams)\n\nif numPosCallParams > len(args) and not varargs:\n    raise MockInterfaceError(\"Original %s() takes at most %s arguments (%s given)\" % \n        (name, len(args), numPosCallParams))\n\n# Get the number of positional arguments that appear in the call,\n# also check for duplicate parameters and unknown parameters\nnumPosSeen = _getNumPosSeenAndCheck(numPosCallParams, callKwParams, args, varkw)\n\nlenArgsNoDefaults = len(args) - len(defaults or [])\nif numPosSeen < lenArgsNoDefaults:\n    raise MockInterfaceError(\"Original %s() takes at least %s arguments (%s given)\" % (name, lenArgsNoDefaults, numPosSeen))", "path": "lib\\pythonmock\\mock.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "'''check that the function is only called after all the functions in 'methods'\n'''\n", "func_signal": "def expectAfter(*methods):\n", "code": "def fn(mockObj, callObj, idx):\n    calledMethods = [method.getName() for method in mockObj.mockGetAllCalls()]\n    #skip last entry, since that is the current call\n    calledMethods = calledMethods[:-1]\n    for method in methods:  \n        if method not in calledMethods:\n            return False\n    return True\nreturn fn", "path": "lib\\pythonmock\\mock.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "\"\"\" springmemo\uac00 \uc798 \ub9cc\ub4e4\uc5b4\uc84c\ub2e4 \"\"\"\n", "func_signal": "def test_exist_springmemo_instance(self):\n", "code": "self.assertTrue(isinstance(self.mainapp,springmemo.SpringMemo))\nself.assertEqual(type(self.mainapp.memos),type([]))", "path": "test\\springmemo_Test.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "\"\"\"\nPositional arguments can appear as call parameters either named as\na named (keyword) parameter, or just as a value to be matched by\nposition. Count the positional arguments that are given by either\nkeyword or position, and check for duplicate specifications.\nAlso check for arguments specified by keyword that do not appear\nin the method's parameter list.\n\"\"\"\n", "func_signal": "def _getNumPosSeenAndCheck(numPosCallParams, callKwParams, args, varkw):\n", "code": "posSeen = {}\nfor arg in args[:numPosCallParams]:\n    posSeen[arg] = True\nfor kwp in callKwParams:\n    if posSeen.has_key(kwp):\n        raise MockInterfaceError(\"%s appears as both a positional and named parameter.\" % kwp)\n    if kwp in args:\n        posSeen[kwp] = True\n    elif not varkw:\n        raise MockInterfaceError(\"Original method does not have a parameter '%s'\" % kwp)\nreturn len(posSeen)", "path": "lib\\pythonmock\\mock.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "''' raise an exception when the method is called\n'''\n", "func_signal": "def expectException(exception, *args, **kwargs):\n", "code": "def fn(mockObj, callObj, idx):\n    raise exception(*args, **kwargs)\nreturn fn", "path": "lib\\pythonmock\\mock.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "# set the signature method\n", "func_signal": "def sign_request(self, signature_method, consumer, token):\n", "code": "self.set_parameter('oauth_signature_method', signature_method.get_name())\n# set the signature\nself.set_parameter('oauth_signature', self.build_signature(signature_method, consumer, token))", "path": "lib\\oauth.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "\"\"\" Depth first search for a method with a given name. \"\"\"\n", "func_signal": "def _findFunc(cl, name):\n", "code": "if cl.__dict__.has_key(name):\n    return cl.__dict__[name]\nfor base in cl.__bases__:\n    func = _findFunc(base, name)\n    if func:\n        return func\nreturn None", "path": "lib\\pythonmock\\mock.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "# -> consumer and token\n", "func_signal": "def verify_request(self, oauth_request):\n", "code": "version = self._get_version(oauth_request)\nconsumer = self._get_consumer(oauth_request)\n# get the access token\ntoken = self._get_token(oauth_request, 'access')\nself._check_signature(oauth_request, consumer, token)\nparameters = oauth_request.get_nonoauth_parameters()\nreturn consumer, token, parameters", "path": "lib\\oauth.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "'''test that the index-th call had the specified name and parameters'''\n", "func_signal": "def mockCheckCall(self, index, name, *args, **kwargs):\n", "code": "call = self.mockAllCalledMethods[index]\nassert name == call.getName(), \"%r != %r\" % (name, call.getName())\ncall.checkArgs(*args, **kwargs)", "path": "lib\\pythonmock\\mock.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "'''check that the callObj is called with specified params and keywords\n'''\n", "func_signal": "def expectParams(*params, **keywords):\n", "code": "def fn(mockObj, callObj, idx):\n    return callObj.params == params and callObj.kwparams == keywords\nreturn fn", "path": "lib\\pythonmock\\mock.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "# concatenate the consumer key and secret\n", "func_signal": "def build_signature_base_string(self, oauth_request, consumer, token):\n", "code": "sig = escape(consumer.secret) + '&'\nif token:\n    sig = sig + escape(token.secret)\nreturn sig", "path": "lib\\oauth.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "# verify that timestamp is recentish\n", "func_signal": "def _check_timestamp(self, timestamp):\n", "code": "timestamp = int(timestamp)\nnow = int(time.time())\nlapsed = now - timestamp\nif lapsed > self.timestamp_threshold:\n    raise OAuthError('Expired timestamp: given %d and now %s has a greater difference than threshold %d' % (timestamp, now, self.timestamp_threshold))", "path": "lib\\oauth.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "# Note that this function is called from _speedups\n", "func_signal": "def errmsg(msg, doc, pos, end=None):\n", "code": "lineno, colno = linecol(doc, pos)\nif end is None:\n    #fmt = '{0}: line {1} column {2} (char {3})'\n    #return fmt.format(msg, lineno, colno, pos)\n    fmt = '%s: line %d column %d (char %d)'\n    return fmt % (msg, lineno, colno, pos)\nendlineno, endcolno = linecol(doc, end)\n#fmt = '{0}: line {1} column {2} - line {3} column {4} (char {5} - {6})'\n#return fmt.format(msg, lineno, colno, endlineno, endcolno, pos, end)\nfmt = '%s: line %d column %d - line %d column %d (char %d - %d)'\nreturn fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)", "path": "lib\\simplejson\\decoder.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "\"\"\"\nThe Mock class constructor takes a dictionary of method names and\nthe values they return.  Methods that are not in the returnValues\ndictionary will return None.\nYou may also supply a class whose interface is being mocked.\nAll calls will be checked to see if they appear in the original\ninterface. Any calls to methods not appearing in the real class\nwill raise a MockInterfaceError.  Any calls that would fail due to\nnon-matching parameter lists will also raise a MockInterfaceError.\nBoth of these help to prevent the Mock class getting out of sync\nwith the class it is Mocking.\n\"\"\"\n", "func_signal": "def __init__(self, returnValues=None, realClass=None):\n", "code": "self.mockCalledMethods = {}\nself.mockAllCalledMethods = []\nself.mockReturnValues = returnValues or {}\nself.mockExpectations = {}\nself.realClassMethods = None\nif realClass:\n    self.realClassMethods = dict(inspect.getmembers(realClass, inspect.isroutine))\n    for retMethod in self.mockReturnValues.keys():\n        if not self.realClassMethods.has_key(retMethod):\n            raise MockInterfaceError(\"Return value supplied for method '%s' that was not in the original class\" % retMethod)\nself._setupSubclassMethodInterceptors()", "path": "lib\\pythonmock\\mock.py", "repo_name": "jangxyz/springmemo", "stars": 5, "license": "other", "language": "python", "size": 1915}
{"docstring": "\"\"\"Get some statistics about the database\n\"\"\"\n", "func_signal": "def stat(self):\n", "code": "yield self.sock_send(self.STAT)\nres = yield self.get_unicode()\ndefer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get the size of the database\n\"\"\"\n", "func_signal": "def size(self):\n", "code": "yield self.sock_send(self.SIZE)\nres = yield self.get_long()\ndefer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Sum given integer to existing one\n\"\"\"\n", "func_signal": "def addint(self, key, num):\n", "code": "yield self.sock_send(self.ADDINT, _ulen(key), num, key)\nres = yield self.get_int()\ndefer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get given bytes from socket\"\"\"\n#print \"Try to get bytes\",bytes,repr(self.bufer)\n", "func_signal": "def recv(self, bytes):\n", "code": "if bytes < len(self.bufer):\n    res = self.bufer[:bytes]\n    self.bufer = self.bufer[bytes:]\n    defer.returnValue(res)\nelse:\n    d = defer.Deferred()\n    self.recv_fifo.append((d,bytes))\n    #print self.recv_fifo\n    res = yield d\n    defer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get up to the first maxkeys starting with prefix\n\"\"\"\n", "func_signal": "def fwmkeys(self, prefix, maxkeys):\n", "code": "yield self.sock_send(self.FWMKEYS, _ulen(prefix), maxkeys, prefix)\nnumkeys = yield self.get_int()\nres = []\nfor i in xrange(numkeys):\n    data = yield self.get_unicode()\n    res.append(data)\ndefer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get the size of a value for key\n\"\"\"\n", "func_signal": "def vsiz(self, key):\n", "code": "yield self.sock_send(self.VSIZ, _ulen(key), key)\ndata=yield self.get_int()\ndefer.returnValue(data)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get an integer for given key. Must been added by addint\"\"\"\n", "func_signal": "def getint(self, key):\n", "code": "yield self.sock_send(self.GET, _ulen(key), key)\nval = yield self.get_str()\ndefer.returnValue(struct.unpack('I', val)[0])", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Call func(key, value) with opts\n\nopts is a bitflag that can be RDBXOLCKREC for record locking\nand/or RDBXOLCKGLB for global locking\"\"\"\n", "func_signal": "def ext(self, func, opts, key, value):\n", "code": "yield self.sock_send(self.EXT, len(func), opts, _ulen(key), _ulen(value),\n                func, key, value)\nres = yield self.get_unicode()\ndefer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get a string (n bytes, which is an integer just before string).\"\"\"\n", "func_signal": "def get_str(self):\n", "code": "data_len = yield self.get_int()\nstring = yield self.recv(data_len)\ndefer.returnValue(string)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"All databases support \"putlist\", \"outlist\", and \"getlist\".\n\"putlist\" is to store records. It receives keys and values one after\nthe other, and returns an empty list.\n\"outlist\" is to remove records. It receives keys, and returns an empty\nlist.\n\"getlist\" is to retrieve records. It receives keys, and returns values\n\nTable database supports \"setindex\", \"search\", \"genuid\".\nopts is a bitflag that can be:\n    RDBMONOULOG to prevent writing to the update log\n\"\"\"\n", "func_signal": "def misc(self, func, args, opts=0):\n", "code": "try:\n    yield self.sock_send(self.MISC, len(func), opts, len(args), func, args)\nfinally:\n    numrecs = yield self.get_int()\n\nres = []\nfor i in xrange(numrecs):\n    data = yield self.get_unicode()\n    res.append(data)\ndefer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get a double for given key. Must been added by adddouble\"\"\"\n", "func_signal": "def getdouble(self, key):\n", "code": "yield self.sock_send(self.GET, _ulen(key), key)\nval = yield self.get_str()\nintpart, fracpart = struct.unpack('>QQ', val)\ndefer.returnValue(intpart + (fracpart * 1e-12))", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Pack arguments and send the buffer to the socket\"\"\"\n#print \"\u041f\u043e\u0441\u044b\u043b\u043a\u0430 -\", args, kwargs\n", "func_signal": "def sock_send(self, *args,**kwargs):\n", "code": "sync = kwargs.pop('sync', True)\n# Send message to socket, then check for errors as needed.\nself.transport.write(_pack(*args))\n\n#fail_code = yield self.get_byte()\nfail_code = yield self.recv(1)\nfail_code = ord(fail_code)\nif fail_code:\n    raise TyrantError(fail_code)\ndefer.returnValue(True)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get a long (8 bytes) from socket.\"\"\"\n", "func_signal": "def get_long(self):\n", "code": "res = yield self.recv(8)\ndefer.returnValue(struct.unpack('>Q', res)[0])", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get a unicode.\"\"\"\n", "func_signal": "def get_unicode(self):\n", "code": "string = yield self.get_str()\ndefer.returnValue(string.decode(ENCODING))", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get the number of records in the database\n\"\"\"\n", "func_signal": "def rnum(self):\n", "code": "yield self.sock_send(self.RNUM)\nres = yield self.get_long()\ndefer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Sum given double to existing one\n\"\"\"\n", "func_signal": "def adddouble(self, key, num):\n", "code": "fracpart, intpart = math.modf(num)\nfracpart, intpart = int(fracpart * 1e12), int(intpart)\nyield self.sock_send(self.ADDDOUBLE, _ulen(key), long(intpart), \n                long(fracpart), key)\nres = yield self.get_double()\ndefer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get key,value pairs from the server for the given list of keys\n\"\"\"\n", "func_signal": "def mget(self, klst):\n", "code": "yield self.sock_send(self.MGET, len(klst), klst)\nnumrecs = yield self.get_int()\nres=[]\nfor i  in xrange(numrecs):\n    data = yield self.get_strpair()\n    res.append(data)\ndefer.returnValue(res)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get an integer (4 bytes) from socket.\"\"\"\n", "func_signal": "def get_int(self):\n", "code": "res = yield self.recv(4)\ndefer.returnValue(struct.unpack('>I', res)[0])", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "# Craft string that we'll use to send data based on args type and content\n", "func_signal": "def _pack(code, *args):\n", "code": "buf = ''\nfmt = '>BB'\nlargs = []\nfor arg in args:\n    if isinstance(arg, int):\n        fmt += 'I'\n        largs.append(arg)\n\n    elif isinstance(arg, str):\n        buf += arg\n\n    elif isinstance(arg, unicode):\n        buf += arg.encode(ENCODING)\n    \n    elif isinstance(arg, long):\n        fmt += 'Q'\n        largs.append(arg)\n\n    elif isinstance(arg, (list, tuple)):\n        for v in arg:\n            v = str(v)\n            buf += \"%s%s\" % (struct.pack(\">I\", len(v)), v)\n\nreturn \"%s%s\" % (struct.pack(fmt, MAGIC_NUMBER, code, *largs), buf)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"Get string pair (n bytes, n bytes which are 2 integers just \nbefore pair)\"\"\"\n", "func_signal": "def get_strpair(self):\n", "code": "klen = yield self.get_int()\nvlen = yield self.get_int()\nkstr = yield self.recv(klen)\nvstr = yield self.recv(vlen)\ndefer.returnValue(kstr,vstr)", "path": "tx_tokyo.py", "repo_name": "Deepwalker/tx-tokyo", "stars": 5, "license": "None", "language": "python", "size": 80}
{"docstring": "\"\"\"\nReturn a new frame of the correct type for this tag\n\n@param fid: frame id\n@param frame: bytes in the frame\n\"\"\"\n", "func_signal": "def new_frame(self, fid=None, frame=None):\n", "code": "if self.version == '2.2':\n    return ID3v2_2_Frame(frame=frame, fid=fid)\nelif self.version == '2.3':\n    return ID3v2_3_Frame(frame=frame, fid=fid)\nelif self.version == '2.4':\n    return ID3v2_4_Frame(frame=frame, fid=fid)\nelse:\n    raise ID3NotImplemented(\"version %s not supported.\" % self.version)", "path": "tagger\\id3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\"\nDebugging purposes, dump the whole header of the file.\n\n@todo: dump footer and extension header as well\n\"\"\"\n", "func_signal": "def dump_header(self):\n", "code": "old_pos = self.f.tell()\noutput = ''\nif self.tag[\"size\"]:\n    self.f.seek(0)\n    output = self.f.read(ID3V2_FILE_HEADER_LENGTH + self.tag[\"size\"])\n    self.f.seek(old_pos)\n    \nreturn output", "path": "tagger\\id3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\" Parse Extension Header \"\"\"\n\n# seek to the extension header position\n", "func_signal": "def parse_ext_header(self):\n", "code": "self.f.seek(ID3V2_FILE_HEADER_LENGTH)\ndata = self.f.read(ID3V2_FILE_EXTHEADER_LENGTH)\nextsize, flagbytes = struct.unpack(\"!4sB\", data)\nextsize = unsyncsafe(extsize)\nreaddata = 0\nif flagbytes == 1:\n    flags = struct.unpack(\"!B\",self.f.read(flagbytes))[0]\n    self.tag[\"update\"] = ( flags & 0x40 ) >> 6\n    if ((flags & 0x20) >> 5):\n        self.tag[\"crc\"] = unsyncsafe(self.f.read(5))\n        readdata += 5\n    if ((flags & 0x10) >> 4):\n        self.tag[\"restrictions\"] = struct.unpack(\"!B\", self.f.read(1))[0]\n        # FIXME: store these restrictions properly\n        readdata += 1\n        \n    # work around dodgy ext headers created by libid3tag\n    if readdata < extsize - ID3V2_FILE_EXTHEADER_LENGTH - flagbytes:\n        self.f.read(extsize - ID3V2_FILE_EXTHEADER_LENGTH - flagbytes - readdata)\nelse:\n    # ignoring unrecognised extension header\n    self.f.read(extsize - ID3V2_FILE_EXTHEADER_LENGTH)\nreturn 1", "path": "tagger\\id3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "#self.seek_to_id3v1()\n", "func_signal": "def parse(self):\n", "code": "self.f.seek(-128,2)\nid3v1 = self.f.read(128)\ntag, songname, artist, album, year, comment, genre = struct.unpack(\"!3s30s30s30s4s30sb\", id3v1)\nif tag != \"TAG\":\n\treturn 0\nelse:\n\tif comment[28] == '\\x00':\n\t\ttrack = ord(comment[29])\n\t\tcomment = comment[0:27]\n\telse:\n\t\ttrack = -1\n\t\t\n\tself.tag = {}\n\tself.tag[\"songname\"] = self.unpad(songname)\n\tself.tag[\"artist\"] = self.unpad(artist)\n\tself.tag[\"album\"] = self.unpad(album)\n\tself.tag[\"year\"] = self.unpad(year)\n\tself.tag[\"comment\"] = self.unpad(comment)\n\tself.tag[\"genre\"] = genre\n\tself.tag[\"track\"] = track", "path": "pyid3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\"\nCount the number of null bytes at the specified file pointer\n\"\"\"\n", "func_signal": "def _read_null_bytes(self):\n", "code": "nullbuffer = 0\nwhile 1:\n    if self.f.read(1) == '\\x00':\n        nullbuffer += 1\n    else:\n        break\nreturn nullbuffer", "path": "tagger\\id3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\"\nConverts a String or Unicode String to a byte string of specified encoding\n\"\"\"\n", "func_signal": "def o_string(self, s, toenc, inenc='iso8859-1'):\n", "code": "try:\n\toutenc = self.encodings[toenc]\nexcept KeyError:\n\toutenc = 'iso8859-1'\n\noutstring = ''\nif type(s) == types.StringType:\n\ttry:\n\t\toutstring = s.decode(inenc).encode(outenc)\n\texcept (UnicodeEncodeError, UnicodeDecodeError):\n\t\t# FIXME: output warning\n\t\toutstring = s\nelif type(s) == types.UnicodeType:\n\ttry:\n\t\toutstring = s.encode(outenc)\n\texcept UnicodeEncodeError:\n\t\t# FIXME: output warning, unable to convert to byte string\n\t\toutstring = ''\nreturn outstring", "path": "pyid3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "# find new line and strip everything before it\n", "func_signal": "def strip_newline(data):\n", "code": "for i in range(0,len(data)):\n\tif data[i] == '\\n':\n\t\treturn data[i+1:]\n\n# oops, didn't find anything offending\nreturn data", "path": "old\\mp3check.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\"\nGiven a byte string, it will assume it is big-endian and un-SyncSafe\na number\n\"\"\"\n", "func_signal": "def unsyncsafe(data):\n", "code": "bytes = len(data)\nbs = struct.unpack(\"!%dB\" % bytes, data)\ntotal = 0\nfor i in range(0,bytes-1):\n\ttotal += bs[bytes-1-i] * pow(128,i)\nreturn total", "path": "pyid3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "# apple's id3 tags doesn't seem to follow the unsync safe format?\n", "func_signal": "def parse_frame_header(self, frame):\n", "code": "frame_header = frame[:ID3V2_FRAME_HEADER_LENGTH]\n(fid, rawsize, status, format) = struct.unpack(\"!4sIBB\", frame_header)\n\nself.fid = fid\n\nself.meta = {}\nself.meta[\"length\"] = rawsize\nself.meta[\"status\"] = status\nself.meta[\"format\"] = format\nself.meta[\"tagpreserve\"] = status & 0x40 >> 6\nself.meta[\"filepreserve\"] = status & 0x020 >> 5\nself.meta[\"readonly\"] = status & 0x10 >> 4\nself.meta[\"groupinfo\"] = format & 0x40 >> 6\nself.meta[\"compression\"] = format & 0x08 >> 3\nself.meta[\"encryption\"] = format & 0x04 >> 2\nself.meta[\"sync\"] = format & 0x02 >> 1\nself.meta[\"datalength\"] = format & 0x01\nself.meta[\"data\"] = frame[ID3V2_FRAME_HEADER_LENGTH:]", "path": "pyid3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\"\nCreate a new default ID3v2 tag data structure\n\n@param version: version of the tag to use. default is 2.4.\n@type version: float\n\"\"\"\n\n", "func_signal": "def new_header(self, version=ID3V2_DEFAULT_VERSION):\n", "code": "if version not in self.supported:\n    raise ID3ParameterException(\"version %s not supported\" % str(version))\n\nself.tag = {}\nif str(version) in self.supported:\n    self.version = str(version)\nelse:\n    raise ID3NotImplementedException(\"Version %s not supported\", \\\n                                     str(version))\n\nif self.version in ('2.4', '2.3'):\n    self.tag[\"ext\"] = 0\n    self.tag[\"exp\"] = 0\n    self.tag[\"footer\"] = 0\nelif self.version == '2.2':\n    self.tag[\"compression\"] = 0\n    \nself.tag[\"unsync\"] = 0\nself.tag[\"size\"] = 0\nself.frames = []", "path": "tagger\\id3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "# hit null bytes, read until the end of the frame\n", "func_signal": "def read_null_bytes(self):\n", "code": "nullbuffer = 0\nwhile 1:\n\tif self.f.read(1) == '\\x00':\n\t\tnullbuffer += 1\n\telse:\n\t\tbreak\nreturn nullbuffer", "path": "pyid3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\"\nReads the file object until it reaches a sync frame of an MP3 file\n(FIXME - inefficient, and possibly useless)\n\"\"\"\n", "func_signal": "def seek_to_sync(self):\n", "code": "buf = ''\nhit = -1\nread = 0\nwhile hit == -1:\n\t# keep on reading until we have 3 chars in the buffer\n\twhile len(buf) < 3:\n\t\tbuf += self.f.read(1)\n\t\tread += 1\n\t# do pattern matching for a 11 bit on pattern in the first 2 bytes\n\t# (note: that it may extend to the third byte)\n\tb0,b1,b2 = struct.unpack('!3B',buf)\n\tif (b0 & 0xff) and (b1 & 0xe0):\n\t\thit = 0\n\telif (b0 & 0x7f) and (b1 & 0xf0):\n\t\thit = 1\n\telif (b0 & 0x3f) and (b1 & 0xf8):\n\t\thit = 2\n\telif (b0 & 0x1f) and (b1 & 0xfc):\n\t\thit = 3\n\telif (b0 & 0x0f) and (b1 & 0xfe):\n\t\thit = 4\n\telif (b0 & 0x07) and (b1 & 0xff):\n\t\thit = 5\n\telif (b0 & 0x03) and (b1 & 0xff) and (b2 & 0x80):\n\t\thit = 6\n\telif (b0 & 0x01) and (b1 & 0xff) and (b2 & 0xc0):\n\t\thit = 7\n\telse:\n\t\tbuf = buf[1:]\n\t\t\nreturn read + 0.1 * hit - 3", "path": "pyid3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\" Recursively Parse Frames \"\"\"\n", "func_signal": "def parse_frames(self):\n", "code": "read = 0\nreadframes = 0\n\nwhile read < self.tag[\"size\"]:\n    framedata = self.get_next_frame(self.tag[\"size\"] - read)\n    if framedata:\n        try:\n            read += len(framedata)\n            if self.version == '2.2':\n                frame = ID3v2_2_Frame(frame=framedata)\n            elif self.version == '2.3':\n                frame = ID3v2_3_Frame(frame=framedata)\n            elif self.version == '2.4':\n                frame = ID3v2_4_Frame(frame=framedata)\n            readframes += 1\n            self.frames.append(frame)\n        except ID3Exception:\n            pass # ignore unrecognised frames\n    else:\n        self.tag[\"padding\"] = self._read_null_bytes()\n        debug(\"NULL Padding: %d\" % self.tag[\"padding\"])\n        break\n\n# do a sanity check on the size/padding\nif not self.tag.has_key(\"padding\"):\n    self.tag[\"padding\"] = 0\n    \nif self.tag[\"size\"] != read + self.tag[\"padding\"]:\n    self.tag[\"size\"] = read + self.tag[\"padding\"]\n    \nreturn len(self.frames)", "path": "tagger\\id3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\"\nConstruct an Extension Header (FIXME)\n\"\"\"\n", "func_signal": "def construct_ext_header(self):\n", "code": "self.tag['ext'] = 0\nreturn '' # FIXME!", "path": "tagger\\id3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\" Parse Extension Header \"\"\"\n", "func_signal": "def parse_ext_header(self):\n", "code": "data = self.f.read(ID3V2_FILE_EXTHEADER_LENGTH)\nextsize, flagbytes = struct.unpack(\"!4sB\", data)\nextsize = unsyncsafe(extsize)\nreaddata = 0\nif flagbytes == 1:\n\tflags = struct.unpack(\"!B\",self.f.read(flagbytes))[0]\n\tself.tag[\"update\"] = ( flags & 0x40 ) >> 6\n\tif ((flags & 0x20) >> 5):\n\t\tself.tag[\"crc\"] = unsyncsafe(self.f.read(5))\n\t\treaddata += 5\n\tif ((flags & 0x10) >> 4):\n\t\tself.tag[\"restrictions\"] = struct.unpack(\"!B\", self.f.read(1))[0]\n\t\t# FIXME: store these restrictions properly\n\t\treaddata += 1\n\t\t\n\t# work around dodgy ext headers created by libid3tag\n\tif readdata < extsize - ID3V2_FILE_EXTHEADER_LENGTH - flagbytes:\n\t\tself.f.read(extsize - ID3V2_FILE_EXTHEADER_LENGTH - flagbytes - readdata)\nelse:\n\t# ignoring unrecognised extension header\n\tself.f.read(extsize - ID3V2_FILE_EXTHEADER_LENGTH)\nreturn 1", "path": "pyid3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\"\nParse Header of the file\n\n\"\"\"\n", "func_signal": "def parse_header(self):\n", "code": "self.f.seek(0)\ndata = self.f.read(ID3V2_FILE_HEADER_LENGTH)\nif len(data) != ID3V2_FILE_HEADER_LENGTH:\n    raise ID3HeaderInvalidException(\"ID3 tag header is incomplete\")\n\nself.tag = {}\nself.frames = []\nid3, ver, flags, rawsize = struct.unpack(\"!3sHB4s\", data)\n\nif id3 != \"ID3\":\n    raise ID3HeaderInvalidException(\"ID3v2 header not found\")\n\nself.tag[\"size\"] = unsyncsafe(rawsize)\n# NOTE: size  = excluding header + footer\nversion = '2.%d' % (ver >> 8)\nif version not in self.supported:\n    raise ID3NotImplementedException(\"version %s not supported\" % \\\n                                     version)\nelse:\n    self.version = version\n    \nif self.version in ('2.4', '2.3'):\n    for flagname, bit in ID3V2_3_TAG_HEADER_FLAGS:\n        self.tag[flagname] = (flags >> bit) & 0x01\nelif self.version == '2.2':\n    for flagname, bit in ID3V2_2_TAG_HEADER_FLAGS:\n        self.tag[flagname] = (flags >> bit) & 0x01\n\nif self.tag.has_key(\"ext\") and self.tag[\"ext\"]:\n    self.parse_ext_header()\n    \ndebug(self.tag)", "path": "tagger\\id3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\" Commit Changes to MP3. This means writing to file.\nWill fail if file is not writable\n\n@param pretend: boolean\n@type pretend: Do not actually write to file, but pretend to.\n\"\"\"\n\n", "func_signal": "def commit(self, pretend=False):\n", "code": "if self.read_only:\n    return False # give up if it's readonly - don't bother!\n    \n# construct frames, footers and extensions\nframesstring = ''.join(map(lambda x: x.output(), self.frames))\nfooterstring = ''\nextstring = ''\n\nif self.tag.has_key(\"ext\") and self.tag[\"ext\"]:\n    extstring = self.construct_ext_header()\nif self.tag.has_key(\"footer\") and self.tag[\"footer\"]:\n    footerstring = self.construct_footer()\n\n\n\n# make sure there is enough space from start of file to\n# end of tag, otherwise realign tag\ntag_content_size = len(extstring) + len(framesstring)\n        \nif self.tag[\"size\"] < tag_content_size:\n    headerstring = self.construct_header(tag_content_size + \\\n                                          ID3V2_FILE_DEFAULT_PADDING)\n    \n    # backup existing mp3 data \n    self.f.seek(self.mp3_data_offset())\n    t = tempfile.TemporaryFile()\n    buf = self.f.read(1024)\n    while buf:\n        t.write(buf)\n        buf = self.f.read(1024)\n        \n    # write to a new file\n    if not pretend:\n        self.f.close()\n        self.f = open(self.filename, 'wb+')\n        self.f.write(headerstring)\n        self.f.write(extstring)\n        self.f.write(framesstring)\n        self.f.write('\\x00' * ID3V2_FILE_DEFAULT_PADDING)\n        self.f.write(footerstring)\n        \n        # write mp3 data to new file\n        t.seek(0)\n        buf = t.read(1024)\n        while buf:\n            self.f.write(buf)\n            buf = t.read(1024)\n        t.close()\n        self.f.close()\n        \n        self.f = open(self.filename, 'rb+')\n        self.tag[\"size\"] = len(headerstring) + len(extstring) + \\\n                           ID3V2_FILE_DEFAULT_PADDING\n    \nelse:\n    headerstring = self.construct_header(self.tag[\"size\"])\n    if not pretend:\n        self.f.seek(0)\n        self.f.write(headerstring)\n        self.f.write(extstring)\n        self.f.write(framesstring)\n        written = len(extstring) + len(framesstring)\n        warn(\"Written Bytes: %d\" % written)\n        # add padding\n        self.f.write('\\x00' * (self.tag[\"size\"] - written))\n        # add footerstring\n        self.f.write(footerstring)\n        self.f.flush()", "path": "tagger\\id3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "# find new line and strip everything before it\n", "func_signal": "def strip_newline(data):\n", "code": "for i in range(0,len(data)):\n\tif data[i] == '\\n':\n\t\treturn data[i+1:]\n\n# oops, didn't find anything offending\nreturn data", "path": "old\\mp3stats.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "\"\"\" Create a new default ID3v2 tag \"\"\"\n", "func_signal": "def new_header(self, version='2.4'):\n", "code": "self.tag = {}\nif version == '2.4':\n\tself.tag[\"version\"] = 0x0400\nelif version == '2.3':\n\tself.tag[\"version\"] = 0x0300\nelif version == '2.2':\n\tself.tag[\"version\"] = 0x0200\n\nif version in ['2.4', '2.3']:\n\tself.tag[\"ext\"] = 0\n\tself.tag[\"exp\"] = 0\n\tself.tag[\"footer\"] = 0\nelif version == '2.2':\n\tself.tag[\"compression\"] = 0\n\t\nself.tag[\"unsync\"] = 0\nself.tag[\"size\"] = 0\nself.frames = []", "path": "pyid3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "# FIXME: handle multiple strings seperated by \\x00\n", "func_signal": "def x_text(self):\n", "code": "data = self.meta['data']\nencoding = ord(data[0])\n\nrawtext = data[1:]\nif self.encodings[encoding] == 'iso8859-1':\n\ttext = rawtext\n\tstrings = text.split('\\x00')\nelse:\n\ttext = rawtext.decode(self.encodings[encoding])\n\tif self.encodings[encoding] == 'utf-8':\n\t\tstrings = text.split('\\x00')\n\telse:\n\t\tstrings = text.split('\\x00\\x00')\ntry:\n\ttext.encode('utf-8')\n\t_debug('Read Field: %s Len: %d Enc: %d Text: %s' %\n\t\t   (self.fid, self.meta[\"length\"], encoding, str([text])))\nexcept UnicodeDecodeError:\n\t_debug('Read Field: %s Len: %d Enc: %d Text: %s (Err)' %\n\t\t   (self.fid, self.meta[\"length\"], encoding, str([text])))\n\t\nreturn (encoding, text, strings)", "path": "pyid3v2.py", "repo_name": "Ciantic/pytagger", "stars": 6, "license": "bsd-3-clause", "language": "python", "size": 168}
{"docstring": "# Swap back in all the special characters we've hidden.\n", "func_signal": "def _unescape_special_chars(self, text):\n", "code": "for ch, hash in g_escape_table.items():\n    text = text.replace(hash, ch)\nreturn text", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "# Python markdown note: the HTML tokenization here differs from\n# that in Markdown.pl, hence the behaviour for subtle cases can\n# differ (I believe the tokenizer here does a better job because\n# it isn't susceptible to unmatched '<' and '>' in HTML tags).\n# Note, however, that '>' is not allowed in an auto-link URL\n# here.\n", "func_signal": "def _escape_special_chars(self, text):\n", "code": "escaped = []\nis_html_markup = False\nfor token in self._sorta_html_tokenize_re.split(text):\n    if is_html_markup:\n        # Within tags/HTML-comments/auto-links, encode * and _\n        # so they don't conflict with their use in Markdown for\n        # italics and strong.  We're replacing each such\n        # character with its corresponding MD5 checksum value;\n        # this is likely overkill, but it should prevent us from\n        # colliding with the escape values by accident.\n        escaped.append(token.replace('*', g_escape_table['*'])\n                            .replace('_', g_escape_table['_']))\n    else:\n        escaped.append(self._encode_backslash_escapes(token))\n    is_html_markup = not is_html_markup\nreturn ''.join(escaped)", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Return a dictionary of emacs-style local variables.\n\nParsing is done loosely according to this spec (and according to\nsome in-practice deviations from this):\nhttp://www.gnu.org/software/emacs/manual/html_node/emacs/Specifying-File-Variables.html#Specifying-File-Variables\n\"\"\"\n", "func_signal": "def _get_emacs_vars(self, text):\n", "code": "emacs_vars = {}\nSIZE = pow(2, 13) # 8kB\n\n# Search near the start for a '-*-'-style one-liner of variables.\nhead = text[:SIZE]\nif \"-*-\" in head:\n    match = self._emacs_oneliner_vars_pat.search(head)\n    if match:\n        emacs_vars_str = match.group(1)\n        assert '\\n' not in emacs_vars_str\n        emacs_var_strs = [s.strip() for s in emacs_vars_str.split(';')\n                          if s.strip()]\n        if len(emacs_var_strs) == 1 and ':' not in emacs_var_strs[0]:\n            # While not in the spec, this form is allowed by emacs:\n            #   -*- Tcl -*-\n            # where the implied \"variable\" is \"mode\". This form\n            # is only allowed if there are no other variables.\n            emacs_vars[\"mode\"] = emacs_var_strs[0].strip()\n        else:\n            for emacs_var_str in emacs_var_strs:\n                try:\n                    variable, value = emacs_var_str.strip().split(':', 1)\n                except ValueError:\n                    log.debug(\"emacs variables error: malformed -*- \"\n                              \"line: %r\", emacs_var_str)\n                    continue\n                # Lowercase the variable name because Emacs allows \"Mode\"\n                # or \"mode\" or \"MoDe\", etc.\n                emacs_vars[variable.lower()] = value.strip()\n\ntail = text[-SIZE:]\nif \"Local Variables\" in tail:\n    match = self._emacs_local_vars_pat.search(tail)\n    if match:\n        prefix = match.group(\"prefix\")\n        suffix = match.group(\"suffix\")\n        lines = match.group(\"content\").splitlines(0)\n        #print \"prefix=%r, suffix=%r, content=%r, lines: %s\"\\\n        #      % (prefix, suffix, match.group(\"content\"), lines)\n\n        # Validate the Local Variables block: proper prefix and suffix\n        # usage.\n        for i, line in enumerate(lines):\n            if not line.startswith(prefix):\n                log.debug(\"emacs variables error: line '%s' \"\n                          \"does not use proper prefix '%s'\"\n                          % (line, prefix))\n                return {}\n            # Don't validate suffix on last line. Emacs doesn't care,\n            # neither should we.\n            if i != len(lines)-1 and not line.endswith(suffix):\n                log.debug(\"emacs variables error: line '%s' \"\n                          \"does not use proper suffix '%s'\"\n                          % (line, suffix))\n                return {}\n\n        # Parse out one emacs var per line.\n        continued_for = None\n        for line in lines[:-1]: # no var on the last line (\"PREFIX End:\")\n            if prefix: line = line[len(prefix):] # strip prefix\n            if suffix: line = line[:-len(suffix)] # strip suffix\n            line = line.strip()\n            if continued_for:\n                variable = continued_for\n                if line.endswith('\\\\'):\n                    line = line[:-1].rstrip()\n                else:\n                    continued_for = None\n                emacs_vars[variable] += ' ' + line\n            else:\n                try:\n                    variable, value = line.split(':', 1)\n                except ValueError:\n                    log.debug(\"local variables error: missing colon \"\n                              \"in local variables entry: '%s'\" % line)\n                    continue\n                # Do NOT lowercase the variable name, because Emacs only\n                # allows \"mode\" (and not \"Mode\", \"MoDe\", etc.) in this block.\n                value = value.strip()\n                if value.endswith('\\\\'):\n                    value = value[:-1].rstrip()\n                    continued_for = variable\n                else:\n                    continued_for = None\n                emacs_vars[variable] = value\n\n# Unquote values.\nfor var, val in emacs_vars.items():\n    if len(val) > 1 and (val.startswith('\"') and val.endswith('\"')\n       or val.startswith('\"') and val.endswith('\"')):\n        emacs_vars[var] = val[1:-1]\n\nreturn emacs_vars", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "# Setext-style headers:\n#     Header 1\n#     ========\n#  \n#     Header 2\n#     --------\n", "func_signal": "def _do_headers(self, text):\n", "code": "text = self._setext_h_re.sub(self._setext_h_sub, text)\n\n# atx-style headers:\n#   # Header 1\n#   ## Header 2\n#   ## Header 2 with closing hashes ##\n#   ...\n#   ###### Header 6\ntext = self._atx_h_re.sub(self._atx_h_sub, text)\n\nreturn text", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "# Smart processing for ampersands and angle brackets that need\n# to be encoded.\n", "func_signal": "def _encode_amps_and_angles(self, text):\n", "code": "text = self._ampersand_re.sub('&amp;', text)\n    \n# Encode naked <'s\ntext = self._naked_lt_re.sub('&lt;', text)\n\n# Encode naked >'s\n# Note: Other markdown implementations (e.g. Markdown.pl, PHP\n# Markdown) don't do this.\ntext = self._naked_gt_re.sub('&gt;', text)\nreturn text", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "# These are all the transformations that form block-level\n# tags like paragraphs, headers, and list items.\n\n", "func_signal": "def _run_block_gamut(self, text):\n", "code": "text = self._do_headers(text)\n\n# Do Horizontal Rules:\nhr = \"\\n<hr\"+self.empty_element_suffix+\"\\n\"\nfor hr_re in self._hr_res:\n    text = hr_re.sub(hr, text)\n\ntext = self._do_lists(text)\n\nif \"pyshell\" in self.extras:\n    text = self._prepare_pyshell_blocks(text)\n\ntext = self._do_code_blocks(text)\n\ntext = self._do_block_quotes(text)\n\n# We already ran _HashHTMLBlocks() before, in Markdown(), but that\n# was to escape raw HTML in the original Markdown source. This time,\n# we're escaping the markup we've just created, so that we don't wrap\n# <p> tags around block-level tags.\ntext = self._hash_html_blocks(text)\n\ntext = self._form_paragraphs(text)\n\nreturn text", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Convert the given text.\"\"\"\n# Main function. The order in which other subs are called here is\n# essential. Link and image substitutions need to happen before\n# _EscapeSpecialChars(), so that any *'s or _'s in the <a>\n# and <img> tags get encoded.\n\n# Clear the global hashes. If we don't clear these, you get conflicts\n# from other articles when generating a page which contains more than\n# one article (e.g. an index page that shows the N most recent\n# articles):\n", "func_signal": "def convert(self, text):\n", "code": "self.reset()\n\nif not isinstance(text, unicode):\n    #TODO: perhaps shouldn't presume UTF-8 for string input?\n    text = unicode(text, 'utf-8')\n\nif self.use_file_vars:\n    # Look for emacs-style file variable hints.\n    emacs_vars = self._get_emacs_vars(text)\n    if \"markdown-extras\" in emacs_vars:\n        splitter = re.compile(\"[ ,]+\")\n        for e in splitter.split(emacs_vars[\"markdown-extras\"]):\n            if '=' in e:\n                ename, earg = e.split('=', 1)\n                try:\n                    earg = int(earg)\n                except ValueError:\n                    pass\n            else:\n                ename, earg = e, None\n            self.extras[ename] = earg\n\n# Standardize line endings:\ntext = re.sub(\"\\r\\n|\\r\", \"\\n\", text)\n\n# Make sure $text ends with a couple of newlines:\ntext += \"\\n\\n\"\n\n# Convert all tabs to spaces.\ntext = self._detab(text)\n\n# Strip any lines consisting only of spaces and tabs.\n# This makes subsequent regexen easier to write, because we can\n# match consecutive blank lines with /\\n+/ instead of something\n# contorted like /[ \\t]*\\n+/ .\ntext = self._ws_only_line_re.sub(\"\", text)\n\nif self.safe_mode:\n    text = self._hash_html_spans(text)\n\n# Turn block-level HTML blocks into hash entries\ntext = self._hash_html_blocks(text, raw=True)\n\n# Strip link definitions, store in hashes.\nif \"footnotes\" in self.extras:\n    # Must do footnotes first because an unlucky footnote defn\n    # looks like a link defn:\n    #   [^4]: this \"looks like a link defn\"\n    text = self._strip_footnote_definitions(text)\ntext = self._strip_link_definitions(text)\n\ntext = self._run_block_gamut(text)\n\nif \"footnotes\" in self.extras:\n    text = self._add_footnotes(text)\n\ntext = self._unescape_special_chars(text)\n\nif self.safe_mode:\n    text = self._unhash_html_spans(text)\n\ntext += \"\\n\"\nreturn text", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "# Strip leading and trailing lines:\n", "func_signal": "def _form_paragraphs(self, text):\n", "code": "text = text.strip('\\n')\n\n# Wrap <p> tags.\ngrafs = re.split(r\"\\n{2,}\", text)\nfor i, graf in enumerate(grafs):\n    if graf in self.html_blocks:\n        # Unhashify HTML blocks\n        grafs[i] = self.html_blocks[graf]\n    else:\n        # Wrap <p> tags.\n        graf = self._run_span_gamut(graf)\n        grafs[i] = \"<p>\" + graf.lstrip(\" \\t\") + \"</p>\"\n\nreturn \"\\n\\n\".join(grafs)", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"'foo'    -> re.compile(re.escape('foo'))\n   '/foo/'  -> re.compile('foo')\n   '/foo/i' -> re.compile('foo', re.I)\n\"\"\"\n", "func_signal": "def _regex_from_encoded_pattern(s):\n", "code": "if s.startswith('/') and s.rfind('/') != 0:\n    # Parse it: /PATTERN/FLAGS\n    idx = s.rfind('/')\n    pattern, flags_str = s[1:idx], s[idx+1:]\n    flag_from_char = {\n        \"i\": re.IGNORECASE,\n        \"l\": re.LOCALE,\n        \"s\": re.DOTALL,\n        \"m\": re.MULTILINE,\n        \"u\": re.UNICODE,\n    }\n    flags = 0\n    for char in flags_str:\n        try:\n            flags |= flag_from_char[char]\n        except KeyError:\n            raise ValueError(\"unsupported regex flag: '%s' in '%s' \"\n                             \"(must be one of '%s')\"\n                             % (char, s, ''.join(flag_from_char.keys())))\n    return re.compile(s[1:idx], flags)\nelse: # not an encoded regex\n    return re.compile(re.escape(s))", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Process Markdown `<pre><code>` blocks.\"\"\"\n", "func_signal": "def _do_code_blocks(self, text):\n", "code": "code_block_re = re.compile(r'''\n    (?:\\n\\n|\\A)\n    (               # $1 = the code block -- one or more lines, starting with a space/tab\n      (?:\n        (?:[ ]{%d} | \\t)  # Lines must start with a tab or a tab-width of spaces\n        .*\\n+\n      )+\n    )\n    ((?=^[ ]{0,%d}\\S)|\\Z)   # Lookahead for non-space at line-start, or end of doc\n    ''' % (self.tab_width, self.tab_width),\n    re.M | re.X)\n\nreturn code_block_re.sub(self._code_block_sub, text)", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Starts the mock S3 server on the given port at the given path.\"\"\"\n", "func_signal": "def start(port, root_directory=\"/tmp/s3\", bucket_depth=0):\n", "code": "application = S3Application(root_directory, bucket_depth)\nhttp_server = httpserver.HTTPServer(application)\nhttp_server.listen(port)\nioloop.IOLoop.instance().start()", "path": "vender\\tornado\\build\\lib.linux-x86_64-2.6\\tornado\\s3server.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Turns on colored logging output for stderr if we are in a tty.\"\"\"\n", "func_signal": "def enable_pretty_logging():\n", "code": "if not curses: return\ntry:\n    if not sys.stderr.isatty(): return\n    curses.setupterm()\nexcept:\n    return\nchannel = logging.StreamHandler()\nchannel.setFormatter(_ColorLogFormatter())\nlogging.getLogger().addHandler(channel)", "path": "vender\\tornado\\tornado\\options.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Returns a comma-separated list for the given list of parts.\n\nThe format is, e.g., \"A, B and C\", \"A and B\" or just \"A\" for lists\nof size 1.\n\"\"\"\n", "func_signal": "def list(self, parts):\n", "code": "_ = self.translate\nif len(parts) == 0: return \"\"\nif len(parts) == 1: return parts[0]\ncomma = u' \\u0648 ' if self.code.startswith(\"fa\") else u\", \"\nreturn _(\"%(commas)s and %(last)s\") % {\n    \"commas\": comma.join(parts[:-1]),\n    \"last\": parts[len(parts) - 1],\n}", "path": "vender\\tornado\\tornado\\locale.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Ensure that Python interactive shell sessions are put in\ncode blocks -- even if not properly indented.\n\"\"\"\n", "func_signal": "def _prepare_pyshell_blocks(self, text):\n", "code": "if \">>>\" not in text:\n    return text\n\nless_than_tab = self.tab_width - 1\n_pyshell_block_re = re.compile(r\"\"\"\n    ^([ ]{0,%d})>>>[ ].*\\n   # first line\n    ^(\\1.*\\S+.*\\n)*         # any number of subsequent lines\n    ^\\n                     # ends with a blank line\n    \"\"\" % less_than_tab, re.M | re.X)\n\nreturn _pyshell_block_re.sub(self._pyshell_block_sub, text)", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Standalone XML processing instruction regex.\"\"\"\n", "func_signal": "def _xml_oneliner_re_from_tab_width(tab_width):\n", "code": "return re.compile(r\"\"\"\n    (?:\n        (?<=\\n\\n)       # Starting after a blank line\n        |               # or\n        \\A\\n?           # the beginning of the doc\n    )\n    (                           # save in $1\n        [ ]{0,%d}\n        (?:\n            <\\?\\w+\\b\\s+.*?\\?>   # XML processing instruction\n            |\n            <\\w+:\\w+\\b\\s+.*?/>  # namespaced single tag\n        )\n        [ \\t]*\n        (?=\\n{2,}|\\Z)       # followed by a blank line or end of document\n    )\n    \"\"\" % (tab_width - 1), re.X)", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"_dedent(text, tabsize=8, skip_first_line=False) -> dedented text\n\n    \"text\" is the text to dedent.\n    \"tabsize\" is the tab width to use for indent width calculations.\n    \"skip_first_line\" is a boolean indicating if the first line should\n        be skipped for calculating the indent width and for dedenting.\n        This is sometimes useful for docstrings and similar.\n\ntextwrap.dedent(s), but don't expand tabs to spaces\n\"\"\"\n", "func_signal": "def _dedent(text, tabsize=8, skip_first_line=False):\n", "code": "lines = text.splitlines(1)\n_dedentlines(lines, tabsize=tabsize, skip_first_line=skip_first_line)\nreturn ''.join(lines)", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "# These are all the transformations that occur *within* block-level\n# tags like paragraphs, headers, and list items.\n    \n", "func_signal": "def _run_span_gamut(self, text):\n", "code": "text = self._do_code_spans(text)\n    \ntext = self._escape_special_chars(text)\n    \n# Process anchor and image tags.\ntext = self._do_links(text)\n    \n# Make links out of things like `<http://example.com/>`\n# Must come after _do_links(), because you can use < and >\n# delimiters in inline links like [this](<url>).\ntext = self._do_auto_links(text)\n\nif \"link-patterns\" in self.extras:\n    text = self._do_link_patterns(text)\n    \ntext = self._encode_amps_and_angles(text)\n    \ntext = self._do_italics_and_bold(text)\n    \n# Do hard breaks:\ntext = re.sub(r\" {2,}\\n\", \" <br%s\\n\" % self.empty_element_suffix, text)\n    \nreturn text", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Turn Markdown link shortcuts into XHTML <a> and <img> tags.\n\nThis is a combination of Markdown.pl's _DoAnchors() and\n_DoImages(). They are done together because that simplified the\napproach. It was necessary to use a different approach than\nMarkdown.pl because of the lack of atomic matching support in\nPython's regex engine used in $g_nested_brackets.\n\"\"\"\n", "func_signal": "def _do_links(self, text):\n", "code": "MAX_LINK_TEXT_SENTINEL = 3000  # markdown2 issue 24\n\n# `anchor_allowed_pos` is used to support img links inside\n# anchors, but not anchors inside anchors. An anchor's start\n# pos must be `>= anchor_allowed_pos`.\nanchor_allowed_pos = 0\n\ncurr_pos = 0\nwhile True: # Handle the next link.\n    # The next '[' is the start of:\n    # - an inline anchor:   [text](url \"title\")\n    # - a reference anchor: [text][id]\n    # - an inline img:      ![text](url \"title\")\n    # - a reference img:    ![text][id]\n    # - a footnote ref:     [^id]\n    #   (Only if 'footnotes' extra enabled)\n    # - a footnote defn:    [^id]: ...\n    #   (Only if 'footnotes' extra enabled) These have already\n    #   been stripped in _strip_footnote_definitions() so no\n    #   need to watch for them.\n    # - a link definition:  [id]: url \"title\"\n    #   These have already been stripped in\n    #   _strip_link_definitions() so no need to watch for them.\n    # - not markup:         [...anything else...\n    try:\n        start_idx = text.index('[', curr_pos)\n    except ValueError:\n        break\n    text_length = len(text)\n\n    # Find the matching closing ']'.\n    # Markdown.pl allows *matching* brackets in link text so we\n    # will here too. Markdown.pl *doesn't* currently allow\n    # matching brackets in img alt text -- we'll differ in that\n    # regard.\n    bracket_depth = 0\n    for p in range(start_idx+1, min(start_idx+MAX_LINK_TEXT_SENTINEL, \n                                    text_length)):\n        ch = text[p]\n        if ch == ']':\n            bracket_depth -= 1\n            if bracket_depth < 0:\n                break\n        elif ch == '[':\n            bracket_depth += 1\n    else:\n        # Closing bracket not found within sentinel length.\n        # This isn't markup.\n        curr_pos = start_idx + 1\n        continue\n    link_text = text[start_idx+1:p]\n\n    # Possibly a footnote ref?\n    if \"footnotes\" in self.extras and link_text.startswith(\"^\"):\n        normed_id = re.sub(r'\\W', '-', link_text[1:])\n        if normed_id in self.footnotes:\n            self.footnote_ids.append(normed_id)\n            result = '<sup class=\"footnote-ref\" id=\"fnref-%s\">' \\\n                     '<a href=\"#fn-%s\">%s</a></sup>' \\\n                     % (normed_id, normed_id, len(self.footnote_ids))\n            text = text[:start_idx] + result + text[p+1:]\n        else:\n            # This id isn't defined, leave the markup alone.\n            curr_pos = p+1\n        continue\n\n    # Now determine what this is by the remainder.\n    p += 1\n    if p == text_length:\n        return text\n\n    # Inline anchor or img?\n    if text[p] == '(': # attempt at perf improvement\n        match = self._tail_of_inline_link_re.match(text, p)\n        if match:\n            # Handle an inline anchor or img.\n            is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n            if is_img:\n                start_idx -= 1\n\n            url, title = match.group(\"url\"), match.group(\"title\")\n            if url and url[0] == '<':\n                url = url[1:-1]  # '<url>' -> 'url'\n            # We've got to encode these to avoid conflicting\n            # with italics/bold.\n            url = url.replace('*', g_escape_table['*']) \\\n                     .replace('_', g_escape_table['_'])\n            if title:\n                title_str = ' title=\"%s\"' \\\n                    % title.replace('*', g_escape_table['*']) \\\n                           .replace('_', g_escape_table['_']) \\\n                           .replace('\"', '&quot;')\n            else:\n                title_str = ''\n            if is_img:\n                result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                    % (url, link_text.replace('\"', '&quot;'),\n                       title_str, self.empty_element_suffix)\n                curr_pos = start_idx + len(result)\n                text = text[:start_idx] + result + text[match.end():]\n            elif start_idx >= anchor_allowed_pos:\n                result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                result = '%s%s</a>' % (result_head, link_text)\n                # <img> allowed from curr_pos on, <a> from\n                # anchor_allowed_pos on.\n                curr_pos = start_idx + len(result_head)\n                anchor_allowed_pos = start_idx + len(result)\n                text = text[:start_idx] + result + text[match.end():]\n            else:\n                # Anchor not allowed here.\n                curr_pos = start_idx + 1\n            continue\n\n    # Reference anchor or img?\n    else:\n        match = self._tail_of_reference_link_re.match(text, p)\n        if match:\n            # Handle a reference-style anchor or img.\n            is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n            if is_img:\n                start_idx -= 1\n            link_id = match.group(\"id\").lower()\n            if not link_id:\n                link_id = link_text.lower()  # for links like [this][]\n            if link_id in self.urls:\n                url = self.urls[link_id]\n                # We've got to encode these to avoid conflicting\n                # with italics/bold.\n                url = url.replace('*', g_escape_table['*']) \\\n                         .replace('_', g_escape_table['_'])\n                title = self.titles.get(link_id)\n                if title:\n                    title = title.replace('*', g_escape_table['*']) \\\n                                 .replace('_', g_escape_table['_'])\n                    title_str = ' title=\"%s\"' % title\n                else:\n                    title_str = ''\n                if is_img:\n                    result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                        % (url, link_text.replace('\"', '&quot;'),\n                           title_str, self.empty_element_suffix)\n                    curr_pos = start_idx + len(result)\n                    text = text[:start_idx] + result + text[match.end():]\n                elif start_idx >= anchor_allowed_pos:\n                    result = '<a href=\"%s\"%s>%s</a>' \\\n                        % (url, title_str, link_text)\n                    result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                    result = '%s%s</a>' % (result_head, link_text)\n                    # <img> allowed from curr_pos on, <a> from\n                    # anchor_allowed_pos on.\n                    curr_pos = start_idx + len(result_head)\n                    anchor_allowed_pos = start_idx + len(result)\n                    text = text[:start_idx] + result + text[match.end():]\n                else:\n                    # Anchor not allowed here.\n                    curr_pos = start_idx + 1\n            else:\n                # This id isn't defined, leave the markup alone.\n                curr_pos = match.end()\n            continue\n\n    # Otherwise, it isn't markup.\n    curr_pos = start_idx + 1\n\nreturn text", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"Formats the given date as a day of week.\n\nExample: \"Monday, January 22\". You can remove the day of week with\ndow=False.\n\"\"\"\n", "func_signal": "def format_day(self, date, gmt_offset=0, dow=True):\n", "code": "local_date = date - datetime.timedelta(minutes=gmt_offset)\n_ = self.translate\nif dow:\n    return _(\"%(weekday)s, %(month_name)s %(day)s\") % {\n        \"month_name\": self._months[local_date.month - 1],\n        \"weekday\": self._weekdays[local_date.weekday()],\n        \"day\": str(local_date.day),\n    }\nelse:\n    return _(\"%(month_name)s %(day)s\") % {\n        \"month_name\": self._months[local_date.month - 1],\n        \"day\": str(local_date.day),\n    }", "path": "vender\\tornado\\tornado\\locale.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "# Process the contents of a single ordered or unordered list,\n# splitting it into individual list items.\n    \n# The $g_list_level global keeps track of when we're inside a list.\n# Each time we enter a list, we increment it; when we leave a list,\n# we decrement. If it's zero, we're not in a list anymore.\n#\n# We do this because when we're not inside a list, we want to treat\n# something like this:\n#\n#       I recommend upgrading to version\n#       8. Oops, now this line is treated\n#       as a sub-list.\n#\n# As a single paragraph, despite the fact that the second line starts\n# with a digit-period-space sequence.\n#\n# Whereas when we're inside a list (or sub-list), that line will be\n# treated as the start of a sub-list. What a kludge, huh? This is\n# an aspect of Markdown's syntax that's hard to parse perfectly\n# without resorting to mind-reading. Perhaps the solution is to\n# change the syntax rules such that sub-lists must start with a\n# starting cardinal number; e.g. \"1.\" or \"a.\".\n", "func_signal": "def _process_list_items(self, list_str):\n", "code": "self.list_level += 1\nself._last_li_endswith_two_eols = False\nlist_str = list_str.rstrip('\\n') + '\\n'\nlist_str = self._list_item_re.sub(self._list_item_sub, list_str)\nself.list_level -= 1\nreturn list_str", "path": "vender\\tornado\\demos\\appengine\\markdown.py", "repo_name": "netguy204/scan_server", "stars": 5, "license": "None", "language": "python", "size": 1256}
{"docstring": "\"\"\"\nCheck if the screenshot file looks ok.\n\"\"\"\n", "func_signal": "def check_screenshot(self, filename):\n", "code": "if not os.path.exists(filename):\n    raise RuntimeError(\"screenshot file %s not found\" % filename)\nif not os.path.getsize(filename):\n    raise RuntimeError(\"screenshot file %s is empty\" % filename)\nmagic, width, height, maxval = hashmatch.read_ppm_header(\n    open(filename, 'rb'))\nif magic != 'P6':\n    raise RuntimeError(\"%s isn't a PPM file with 24 bpp\" % filename)\nif width != self.width or height != self.height:\n    raise RuntimeError(\n        \"%s has %dx%d pixels, not the requested size %dx%d\" %\n        (filename, width, height, self.width, self.height))\nif maxval != 255:\n    raise RuntimeError(\n        \"%s has maxval %d, but only maxval 255 is supported\" %\n        (maxval, filename))", "path": "shotfactory04\\gui\\__init__.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nGet the offset with the most votes, but 0 only if no other option exists.\nAll entries with less than minimum votes will be ignored.\n>>> winner({0:0, 1:1, 2:2, 3:3}, 1)\n3\n>>> winner({0:100, 1:1, 2:2, 3:3}, 1)\n3\n>>> winner({0:100, 1:1, 2:2, 3:3}, 0)\n3\n>>> winner({0:100, 1:1, 2:2, 3:3}, 4)\n0\n>>> winner({}, 1)\n0\n\"\"\"\n", "func_signal": "def winner(votes, minimum):\n", "code": "maximum = minimum - 1\nresult = 0\nfor offset, count in votes.items():\n    if count > maximum and offset > 0:\n        maximum = count\n        result = offset\nreturn result", "path": "shotfactory04\\image\\hashmatch.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nSave settings for internal use later.\n\"\"\"\n", "func_signal": "def __init__(self, config, options):\n", "code": "self.request = config['request']\nself.width = config['width']\nif not self.width:\n    self.width = 1024\nself.height = config['height']\nif not self.height:\n    if self.width == 1280:\n        self.height = 1024\n    else:\n        self.height = self.width / 4 * 3\nself.bpp = config['bpp']\nif not self.bpp:\n    self.bpp = 24\nself.dpi = 90\nif hasattr(options, 'display'):\n    self.display = options.display\nif hasattr(options, 'rfbport'):\n    self.rfbport = options.rfbport\nif hasattr(options, 'verbose'):\n    self.verbose = options.verbose\nself.max_pages = options.max_pages\nself.top_skip = 0\nself.bottom_skip = 0", "path": "shotfactory04\\gui\\__init__.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"Close the active window.\"\"\"\n", "func_signal": "def close_window(self):\n", "code": "self.shell('xte \"keydown Alt_L\"')\nself.shell('xte \"key F4\"')\nself.shell('xte \"keyup Alt_L\"')", "path": "shotfactory04\\gui\\linux\\__init__.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nBuild a dict from a vertical column of detail markers.\nNon-unique markers will be removed.\n\"\"\"\n", "func_signal": "def build_hash(pixels, start, height, row_skip):\n", "code": "positions = {}\nfrequencies = {}\nfrequencies_get = frequencies.get\nprevious = pixels[start:start+STEP]\nfor y in range(1, height):\n    start += row_skip\n    this = pixels[start:start+STEP]\n    marker = previous + this\n    previous = this\n    frequencies[marker] = frequencies_get(marker, 0) + 1\n    positions[marker] = y\npositions_pop = positions.pop\nfor marker, counter in frequencies.iteritems():\n    if counter > 1:\n        positions_pop(marker)\nreturn positions", "path": "shotfactory04\\image\\hashmatch.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nRead a PPM file header and return magic, width, height, maxval.\n\"\"\"\n", "func_signal": "def read_ppm_header(infile):\n", "code": "header = []\nwhile True:\n    line = infile.readline()\n    sharp = line.find('#')\n    if sharp > -1:\n        line = line[:sharp]\n    line = line.strip()\n    if not line:\n        continue\n    header.append(line)\n    match = header_match(' '.join(header))\n    if match:\n        magic = match.group(1)\n        width = int(match.group(2))\n        height = int(match.group(3))\n        maxval = int(match.group(4))\n        return magic, width, height, maxval\n    elif len(header) >= 4:\n        raise SyntaxError(\"could not parse PPM header\")", "path": "shotfactory04\\image\\hashmatch.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\n>>> overlap_test()\n\"\"\"\n", "func_signal": "def overlap_test(max_overlap=1000):\n", "code": "for overlap in range(max_overlap):\n    assert overlap == overlap_top(overlap) + overlap_bottom(overlap)", "path": "shotfactory04\\gui\\__init__.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nFind the best vertical match between two PPM files.\nReturn the offset in pixels.\n\"\"\"\n", "func_signal": "def find_offset(filename1, filename2):\n", "code": "infile1 = open(filename1, 'rb')\ninfile2 = open(filename2, 'rb')\nheader1 = read_ppm_header(infile1)\nheader2 = read_ppm_header(infile2)\nassert header1[0] == header2[0] == 'P6'\nassert header1[3] == header2[3] == 255\nassert header1[1] == header2[1]\nwidth = header1[1]\nheight1 = header1[2]\nheight2 = header2[2]\n\npixels1 = infile1.read()\npixels2 = infile2.read()\n# print width*height1*3, len(pixels1), width*height2*3, len(pixels2)\n\nrow_skip = 3*width\nvotes = {}\nfor start in range(0, row_skip, STEP):\n    positions = build_hash(pixels1, start, height1, row_skip)\n    match_markers(pixels2, start, height2, row_skip, positions, votes)\n# debug_values(votes, minimum = 1)\nreturn winner(votes, 3*width/STEP)", "path": "shotfactory04\\image\\hashmatch.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"Scroll down to the bottom of the page.\"\"\"\n", "func_signal": "def scroll_bottom(self):\n", "code": "raise NotImplementedError(\n    \"%s.scroll_bottom() is not implemented\" % self.__class__)", "path": "shotfactory04\\gui\\__init__.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"Take a screenshot and save it to a PPM file.\"\"\"\n", "func_signal": "def screenshot(self, filename):\n", "code": "raise NotImplementedError(\n    \"%s.screenshot(filename) is not implemented\" % self.__class__)", "path": "shotfactory04\\gui\\__init__.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"Find scrollable window.\"\"\"\n", "func_signal": "def find_scrollable(self):\n", "code": "minefield = self.find_window_by_title_suffix(' Minefield')\nreturn self.get_child_window(minefield)", "path": "shotfactory04\\gui\\windows\\minefield.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nTry to get the number of processes in the system run queue,\naveraged over the last minute. If this info is unavailable,\nreturn None.\n\"\"\"\n", "func_signal": "def systemload():\n", "code": "try:\n    return max(os.getloadavg())\nexcept (AttributeError, OSError):\n    return None", "path": "shotfactory.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nProcess a screenshot request and upload the resulting PNG file.\n\"\"\"\n", "func_signal": "def browsershot(options, server, config, password):\n", "code": "browser_module = config['browser'].lower().replace('-', '_')\nif browser_module == 'internet explorer':\n    browser_module = 'msie'\nplatform_name = platform.system()\nif platform_name in ('Microsoft', 'Microsoft Windows'):\n    platform_name = 'Windows'\nif platform_name in ('FreeBSD', 'OpenBSD', 'NetBSD'):\n    platform_name = 'Linux' # use Linux browser modules on *BSD\nif platform_name in ('Linux', 'Darwin', 'Windows'):\n    module_name = 'shotfactory04.gui.%s.%s' % (\n        platform_name.lower(), browser_module)\nelse:\n    raise NotImplementedError(\"unsupported platform: \" + platform_name)\ngui_module = __import__(module_name, globals(), locals(), ['non-empty'])\ngui = gui_module.Gui(config, options)\nurl = server.get_request_url(config)\n\nif can_reuse_vnc_server(options, config, options.previous):\n    try:\n        if can_reuse_browser(options, gui, config, options.previous):\n            gui.reuse_browser(config, url, options)\n        else:\n            gui.close_all_browsers()\n            gui.reset_browser()\n            gui.start_browser(config, url, options)\n    finally:\n        options.reuse_count += 1\nelse:\n    gui.close()\n    gui.prepare_screen()\n    gui.reset_browser()\n    gui.start_browser(config, url, options)\n    options.reuse_count = 1\n\n# Make screenshots\npngfilename = '%s.png' % config['request']\nif os.path.exists(pngfilename):\n    os.remove(pngfilename)\ngui.browsershot(pngfilename)\n\nif not options.reuse_browser:\n    gui.close()\noptions.previous = config\n\n# Upload PNG file\nbytes = server.upload_png(config, pngfilename)\nif os.path.exists(pngfilename):\n    os.remove(pngfilename)\nreturn bytes", "path": "shotfactory.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nAdd a line to the log file.\n\"\"\"\n", "func_signal": "def log(message, extra=None):\n", "code": "logfile = open('shotfactory.log', 'a')\nlogfile.write(time.strftime('%Y-%m-%d %H:%M:%S'))\nlogfile.write(' ')\nlogfile.write(message)\nif extra is not None:\n    logfile.write(' ')\n    logfile.write(str(extra))\nlogfile.write('\\n')\nlogfile.close()", "path": "shotfactory.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nMatch markers and collect votes for different offset positions.\n\"\"\"\n", "func_signal": "def match_markers(pixels, start, height, row_skip, positions, votes):\n", "code": "positions_get = positions.get\nvotes_get = votes.get\nprevious = pixels[start:start+STEP]\nfor y in range(1, height):\n    start += row_skip\n    this = pixels[start:start+STEP]\n    marker = previous + this\n    previous = this\n    position = positions_get(marker, -1)\n    if position > -1:\n        offset = position - y\n        votes[offset] = votes_get(offset, 0) + 1", "path": "shotfactory04\\image\\hashmatch.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nDelete previous session and browser cache.\n\"\"\"\n", "func_signal": "def reset_browser(self):\n", "code": "appdata = shell.SHGetFolderPath(0, shellcon.CSIDL_LOCAL_APPDATA, 0, 0)\nself.delete_if_exists(os.path.join(\n    appdata, 'Mozilla', 'Firefox', 'Profiles', '*', 'Cache'))\nself.delete_if_exists(os.path.join(\n    appdata, 'Mozilla', 'Firefox', 'Profiles', '*', 'urlclassifier3.sqlite'))\nappdata = shell.SHGetFolderPath(0, shellcon.CSIDL_APPDATA, 0, 0)\nself.delete_if_exists(os.path.join(\n    appdata, 'Mozilla', 'Firefox', 'Profiles', '*', 'sessionstore.js'))\nself.delete_if_exists(os.path.join(\n    appdata, 'Mozilla', 'Firefox', 'Profiles', '*', 'history.dat'))\nself.delete_if_exists(os.path.join(\n    appdata, 'Mozilla', 'Firefox', 'Profiles', '*', 'cookies.txt'))", "path": "shotfactory04\\gui\\windows\\minefield.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"Maximize the active window.\"\"\"\n", "func_signal": "def maximize(self):\n", "code": "self.shell('xte \"keydown Alt_L\"')\nself.shell('xte \"key F10\"')\nself.shell('xte \"keyup Alt_L\"')", "path": "shotfactory04\\gui\\linux\\__init__.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nAdjust the screen resolution as requested.\n\"\"\"\n", "func_signal": "def prepare_screen(self):\n", "code": "self.bottom_skip = 4\nself.safari = None\n# Set screen resolution and color depth with Lynn Pye's cscreen\n# Freeware, available from http://www.pyehouse.com/lynn/cscreen.php\nself.shell('./cscreen -x %d -y %d -d %d -f'\n           % (self.width, self.height, self.bpp))", "path": "shotfactory04\\gui\\darwin\\__init__.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"Delete cache, history, cookies, previous sessions...\"\"\"\n", "func_signal": "def reset_browser(self):\n", "code": "raise NotImplementedError(\n    \"%s.reset_browser() is not implemented\" % self.__class__)", "path": "shotfactory04\\gui\\__init__.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nDelete crash dialog and browser cache.\n\"\"\"\n", "func_signal": "def reset_browser(self):\n", "code": "home = os.environ['HOME']\nself.delete_if_exists(os.path.join(\n    home, '.mozilla', 'firefox*', '*', 'Cache'))\nself.delete_if_exists(os.path.join(\n    home, '.mozilla', 'firefox*', '*', 'sessionstore.js'))\nself.delete_if_exists(os.path.join(\n    home, '.mozilla', 'firefox*', '*', 'history.dat'))\nself.delete_if_exists(os.path.join(\n    home, '.mozilla', 'firefox*', '*', 'cookies.txt'))\nself.delete_if_exists(os.path.join(\n    home, '.mozilla', 'firefox*', 'Crash Reports'))\nself.delete_if_exists(os.path.join(\n    home, '.mozilla', 'firefox*', '*', 'minidumps'))\nself.delete_if_exists(os.path.join(\n    home, '.iceweasel', '*', 'Cache'))\nself.delete_if_exists(os.path.join(\n    home, '.iceweasel', '*', 'sessionstore.js'))\nself.delete_if_exists(os.path.join(\n    home, '.iceweasel', '*', 'history.dat'))\nself.delete_if_exists(os.path.join(\n    home, '.iceweasel', '*', 'cookies.txt'))\nself.delete_if_exists(os.path.join(\n    home, '.iceweasel', 'Crash Reports'))\nself.delete_if_exists(os.path.join(\n    home, '.iceweasel', '*', 'minidumps'))", "path": "shotfactory04\\gui\\linux\\firefox.py", "repo_name": "hugs/shotfactory", "stars": 4, "license": "gpl-3.0", "language": "python", "size": 142}
{"docstring": "\"\"\"\nTakes xml a string and returns a list of dicts containing realtime data.\n\"\"\"\n", "func_signal": "def _parse_realtime_data(xmlstr):\n", "code": "doc = minidom.parseString(xmlstr)\nret = []\nelem_map = {\"LineID\": \"id\", \"DirectionID\": \"direction\",\n            \"DestinationStop\": \"destination\" }\n\nack = _single_element(doc, \"Acknowledge\")\nif ack == None or ack.attributes[\"Result\"].nodeValue != \"ok\":\n    return None\n\ncurtime = time.mktime(time.strptime(\n    ack.attributes[\"TimeStamp\"].nodeValue[:-10], \"%Y-%m-%dT%H:%M:%S\"))\n\nfor elem in doc.getElementsByTagName(\"DISDeviation\"):\n    entry = {\"is_realtime\": False}\n\n    for name, value in [ (e.nodeName, _get_text(e.childNodes)) \\\n                          for e in elem.childNodes \\\n                          if e.nodeType == e.ELEMENT_NODE ]:\n        if name in elem_map:\n            entry[elem_map[name]] = unicode(value)\n        elif name == \"TripStatus\":\n            entry[\"is_realtime\"] = value == \"Real\"\n\n    if entry[\"is_realtime\"]:\n        timeele = _single_element(elem, \"ExpectedDISDepartureTime\")\n    else:\n        timeele = _single_element(elem, \"ScheduledDISDepartureTime\")\n\n    parsed_time = time.strptime(\n        _get_text(timeele.childNodes)[:-10], \"%Y-%m-%dT%H:%M:%S\")\n    entry[\"time\"] = parsed_time\n    entry[\"wait_time\"] = int(time.mktime(parsed_time) - curtime)\n    ret.append(entry)\n\nreturn ret", "path": "trafikanten\\api.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Serialize as search result in res to xml. Returns xml as a string.\nFormat of xml is as follows:\n<stations>\n    <station>\n        <id>123123</id>\n        <name>asdf</name\n        <disctrict>Oslo</district>\n\n        fixme: more\n\n\"\"\"\n\n", "func_signal": "def make_xml_stations(res):\n", "code": "impl = minidom.getDOMImplementation()\ndocument = impl.createDocument(None, \"stations\", None)\nbody = document.documentElement\nfor hit in res:\n    station = document.createElement(\"station\")\n    for key, val in hit.items():\n        ele = document.createElement(key)\n        ele.appendChild(document.createTextNode(val))\n        station.appendChild(ele)\n    body.appendChild(station)\n\npretty = document.toprettyxml()\n# fixme: minidom docs says this is needed due to bugs with cyclic gc. \n# Is this still the case though?\ndocument.unlink() \n\nreturn pretty", "path": "trafikanten\\util.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Makes  text representation of a search result that is suitable for \nshowing to a user. Returns a string\"\"\"\n", "func_signal": "def make_human_readable_stations(res):\n", "code": "ret = []\nif not res:\n    ret.append( u\"\"\"No stations found\"\"\")\nelse:\n    format = u\"%%(id)%ds %%(name)s (%%(district)s)\" % \\\n                            max([ len(e[\"id\"]) for e in res ])\n    for dep in res:\n        ret.append(format % dep)\nreturn \"\\n\".join(ret)", "path": "trafikanten\\util.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Get a cached serch data for term. Of the term is not cached, \nreturn None\"\"\"\n", "func_signal": "def _get_cached_search(self, term):\n", "code": "raise NotImplementedError(\"%s must be overridden in a subclass.\" %\n                          inspect.currentframe().f_code.co_name)", "path": "trafikanten\\classes.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"The desctructor just makes sure that if we're using a temporary\ncache directory, it gets deleted on object gc\"\"\"\n", "func_signal": "def __del__(self):\n", "code": "if self.uses_temp:\n    shutil.rmtree(self.cache_location)", "path": "trafikanten\\classes.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Stupid helper for dom. give me .textContent plx\nReturns the contcatentated contents of all text nodes in nodelist\"\"\"\n", "func_signal": "def _get_text(nodelist):\n", "code": "text = u\"\"\nfor node in nodelist:\n    if node.nodeType == node.TEXT_NODE:\n        text = text + unicode(node.data)\nreturn text", "path": "trafikanten\\api.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Makes a fuzzy representation of the time to wait. Returns a tuple\ncontaining (result_type, result).\n\nIf the wait time is less than one minute returns (FUZZY_NOW, \"Now\").\nIf the wait time is less than mins_cutoff (in minutes), returns\n(FUZZY_MINS, number_of_minutes_to_wait)\nIf the wait time is equal or greater than mins_cutoff returns \n(FUZZY_TIME, time_for_departure). The time will be formated according\nto time_format, which is in the format of time.striftime.\"\"\"\n", "func_signal": "def fuzzy_departure(dep, mins_cutoff=10, time_format=\"%H.%M\"):\n", "code": "wait = int(dep[\"wait_time\"])\nif wait < 60:\n    return (FUZZY_NOW, None)\nelif wait < mins_cutoff*60:\n    return (FUZZY_MINS, int(math.floor(wait/60)))\nelse:\n    return (FUZZY_TIME, time.strftime(time_format, dep[\"time\"]))", "path": "trafikanten\\util.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Return cached realtime data for the station sid. If no cached\ndata is available, return None\"\"\"\n", "func_signal": "def _get_cached_realtime(self, sid):\n", "code": "raise NotImplementedError(\"%s must be overridden in a subclass.\" %\n                          inspect.currentframe().f_code.co_name)", "path": "trafikanten\\classes.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Converts from utm to lat_lng. Default zone is the one in which\nOslo and Akershus falls.\"\"\"\n", "func_signal": "def utm_to_lat_lng(easting, northing, zone=32, northernHemisphere=True):\n", "code": "if not northernHemisphere:\n    northing = 10000000 - northing\n\na = 6378137\ne = 0.081819191\ne1sq = 0.006739497\nk0 = 0.9996\n\narc = northing / k0\nmu = arc / (a * (1 - math.pow(e, 2) / 4.0 - 3 * math.pow(e, 4) / 64.0 - 5 * math.pow(e, 6) / 256.0))\n\nei = (1 - math.pow((1 - e * e), (1 / 2.0))) / (1 + math.pow((1 - e * e), (1 / 2.0)))\n\nca = 3 * ei / 2 - 27 * math.pow(ei, 3) / 32.0\n\ncb = 21 * math.pow(ei, 2) / 16 - 55 * math.pow(ei, 4) / 32\ncc = 151 * math.pow(ei, 3) / 96\ncd = 1097 * math.pow(ei, 4) / 512\nphi1 = mu + ca * math.sin(2 * mu) + cb * math.sin(4 * mu) + cc * math.sin(6 * mu) + cd * math.sin(8 * mu)\n\nn0 = a / math.pow((1 - math.pow((e * math.sin(phi1)), 2)), (1 / 2.0))\n\nr0 = a * (1 - e * e) / math.pow((1 - math.pow((e * math.sin(phi1)), 2)), (3 / 2.0))\nfact1 = n0 * math.tan(phi1) / r0\n\n_a1 = 500000 - easting\ndd0 = _a1 / (n0 * k0)\nfact2 = dd0 * dd0 / 2\n\nt0 = math.pow(math.tan(phi1), 2)\nQ0 = e1sq * math.pow(math.cos(phi1), 2)\nfact3 = (5 + 3 * t0 + 10 * Q0 - 4 * Q0 * Q0 - 9 * e1sq) * math.pow(dd0, 4) / 24\n\nfact4 = (61 + 90 * t0 + 298 * Q0 + 45 * t0 * t0 - 252 * e1sq - 3 * Q0 * Q0) * math.pow(dd0, 6) / 720\n\nlof1 = _a1 / (n0 * k0)\nlof2 = (1 + 2 * t0 + Q0) * math.pow(dd0, 3) / 6.0\nlof3 = (5 - 2 * Q0 + 28 * t0 - 3 * math.pow(Q0, 2) + 8 * e1sq + 24 * math.pow(t0, 2)) * math.pow(dd0, 5) / 120\n_a2 = (lof1 - lof2 + lof3) / math.cos(phi1)\n_a3 = _a2 * 180 / math.pi\n\nlatitude = 180 * (phi1 - fact1 * (fact2 + fact3 + fact4)) / math.pi\n\nif not northernHemisphere:\n    latitude = -latitude\n\nlongitude = ((zone > 0) and (6 * zone - 183.0) or 3.0) - _a3\n\nreturn (latitude, longitude)", "path": "trafikanten\\util.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Emit realtimeresult delimited by separator. If header is true, include\nfield names as the first line, using same delimiter\n\"\"\"\n", "func_signal": "def make_delimited_realtime(res, header=False, separator=\";\"):\n", "code": "if not res:\n    return \"\"\n\nclean_res = [\n    { \"id\": e[\"id\"],\n      \"destination\": e[\"destination\"],\n      \"direction\": e[\"direction\"],\n      \"is_realtime\": str(int(e[\"is_realtime\"])),\n      \"wait_time\": e[\"wait_time\"],\n      \"time\": time.asctime(e[\"time\"])\n    } for e in res\n]\n\nformat = separator.join((\"%(id)s\", \"%(destination)s\", \"%(direction)s\",\n          \"%(is_realtime)s\", \"%(wait_time)s\", \"%(time)s\"))\n\nret = []\nif header:\n    ret.append(separator.join((\"id\", \"destination\", \"direction\",\n                          \"is_realtime\", \"wait_time\", \"time\")))\nfor dep in clean_res:\n    ret.append(format % dep)\n\nreturn \"\\n\".join(ret)", "path": "trafikanten\\util.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Identical arguments and return value as trafikanten.find_station.\nWill call _get_cached_search and _cache_search to handle caching\nproperly. Subclasses probably do not need to override this method.\"\"\"\n", "func_signal": "def find_station(self, term):\n", "code": "data = self._get_cached_search(term)\nif data == None:\n    data = trafikanten.find_station(term)\n    self._cache_search(term, data)\n\nreturn data", "path": "trafikanten\\classes.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"If node contains one or more children with nodename \"name\", return the\nfirst one. If not, return None\"\"\"\n", "func_signal": "def _single_element(node, name):\n", "code": "elems = node.getElementsByTagName(name)\nif elems: return elems[0]\nelse: return None", "path": "trafikanten\\api.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Cache data for search term. Returns nothing. This call\nis assumed to always succeed. If implementing a caching backend\nthat may fail, this method should probably throw an exception\"\"\"\n", "func_signal": "def _cache_search(self, term, data):\n", "code": "raise NotImplementedError(\"%s must be overridden in a subclass.\" %\n                          inspect.currentframe().f_code.co_name)", "path": "trafikanten\\classes.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Emit station search result delimited by separator. If header is\ntrue, include field names as the first line, using same delimiter.\n\"\"\"\n", "func_signal": "def make_delimited_stations(res, header=False, separator=\";\"):\n", "code": "if not res:\n    return \"\"\n\nseparator = unicode(separator)\n\nformat = separator.join((\"%(id)s\", \"%(name)s\", \"%(district)s\",\n          \"%(xcoord)s\", \"%(ycoord)s\"))\n\nret = []\nif header:\n    ret.append(\n        separator.join((\"id\", \"name\", \"district\", \"xcoord\", \"ycoord\")))\n\nfor dep in res:\n    ret.append(format % dep)\n\nreturn \"\\n\".join(ret)", "path": "trafikanten\\util.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Cache realtime data for station sid. Returns nothing. This call\nis assumed to always succeed. If implementing a caching backend\nthat may fail, this method should probably throw an exception\"\"\"\n", "func_signal": "def _cache_realtime(self, sid, data):\n", "code": "raise NotImplementedError(\"%s must be overridden in a subclass.\" %\n                          inspect.currentframe().f_code.co_name)", "path": "trafikanten\\classes.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Get realtime data for station with id \"sid\". Returns a list of\ndictionaries. If no realtime data is found, the returned list will be\nempty. If the station doesn't exist, or any other error occurs, returns\nNone.\n\"\"\"\n", "func_signal": "def get_realtime(sid):\n", "code": "if not sid:\n    return None\n\nif sid in _subway_stations:\n    url = _subway_rt_url % sid\nelse:\n    url = _non_subway_rt_url % sid\n\ndata = urllib.urlopen(url).read()\nresult = _parse_realtime_data(data)\n\nif result:\n    result.sort(key=lambda e: int(e[\"wait_time\"]))\n\nreturn result", "path": "trafikanten\\api.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Identical arguments and return value as trafikanten.get_realtime.\nWill call _get_cached_realtime and _cache_realtime to handle caching\nproperly. Subclasses probably do not need to override this method.\"\"\"\n", "func_signal": "def get_realtime(self, sid):\n", "code": "data = self._get_cached_realtime(sid)\nif data == None:\n    data = trafikanten.get_realtime(sid)\n    self._cache_realtime(sid, data)\n\nreturn data", "path": "trafikanten\\classes.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Serialize realtime data in res to xml. Returns xml as a string.\nFormat of xml is as follows:\n<realtime>\n    <departure>\n        <id>123123</id>\n        <name>asdf</name\n        <time>result from time.asctime</time>\n        <is_realtime>yes|no</is_realtime>\n        <wait_time>234</wait_time>\n        fixme: more\n\n\"\"\"\n", "func_signal": "def make_xml_realtime(res):\n", "code": "impl = minidom.getDOMImplementation()\n\ntype_mapper = {\n    \"time\": lambda t: time.asctime(t),\n    \"wait_time\": unicode,\n    \"is_realtime\": lambda b: u\"yes\" if b else u\"no\"\n}\n\ndocument = impl.createDocument(None, \"realtime\", None)\nbody = document.documentElement\nfor hit in res:\n    departure = document.createElement(\"departure\")\n    for key, val in hit.items():\n        ele = document.createElement(key)\n        if key in type_mapper:\n            ele.appendChild(document.createTextNode(type_mapper[key](val)))\n        else:\n            ele.appendChild(document.createTextNode(val))\n        departure.appendChild(ele)\n    body.appendChild(departure)\n\npretty = document.toprettyxml()\n\n# fixme: minidom docs says this is needed due to bugs with cyclic gc. \n# Is this still the case though?\ndocument.unlink()\nreturn pretty", "path": "trafikanten\\util.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"\nTakes xml a string and returns a list of dicts containing station data.\n\"\"\"\n", "func_signal": "def _parse_station_data(xmlstr):\n", "code": "doc = minidom.parseString(xmlstr)\nret = []\nelem_map = {\"fromid\": \"id\", \"StopName\": \"name\", \"District\": \"district\",\n            \"XCoordinate\": \"xcoord\", \"YCoordinate\": \"ycoord\"}\nfor elem in doc.getElementsByTagName(\"StopMatch\"):\n    entry = {\"district\": u\"\"}\n    for name, value in [ (e.nodeName, _get_text(e.childNodes)) \\\n                          for e in elem.childNodes \\\n                          if e.nodeType == e.ELEMENT_NODE  ]:\n        if name in elem_map:\n            entry[elem_map[name]] = unicode(value)\n\n    if \"id\" in entry and \"name\" in entry:\n        add_lat_lng_to_entry(entry)\n        ret.append(entry)\n\nreturn ret", "path": "trafikanten\\api.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Search for stations matching term.\nReturns a list of dicts of the form:\nmatch = {\n    \"id\": \"03232323\",\n    \"name\": \"some station name\",\n    \"district\": \"district for station\",\n    \"xcoord\": \"0\",\n    \"ycoord\"; \"0\"\n}\nIf no matches where found, the list has length 0. If the search did not\ncomplete successfully, return None.\n\"\"\"\n", "func_signal": "def find_station(term):\n", "code": "if not term:\n    return None\n\nurl = _station_url % urllib.quote(term.encode(\"utf-8\"))\ndata = urllib.urlopen(url).read()\nreturn _parse_station_data(data)", "path": "trafikanten\\api.py", "repo_name": "zmalltalker/pytrafikanten", "stars": 4, "license": "bsd-2-clause", "language": "python", "size": 96}
{"docstring": "\"\"\"Volta a permitir que o programa durma entre os passos do algoritmo\n\nVeja freeze_sleep\"\"\"\n", "func_signal": "def thaw_sleep ():\n", "code": "global dont_sleep\ndont_sleep = dont_sleep - 1\nif dont_sleep < 0:\n\tdont_sleep = 0", "path": "geocomp\\common\\control.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"Impede a atualizacao da tela.\n\nJunto com thaw_update permite reduzir a quantidade de flicker\nquando um segmento de reta \u00e9 desenhado e apagado muitas vezes\nem seguida\"\"\"\n", "func_signal": "def freeze_update (amount = 1):\n", "code": "global dont_update\ndont_update = dont_update + amount", "path": "geocomp\\common\\control.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"Constroi o fecho de a ate b. \n\nTodos os pontos de S estao \u00e0 direita de ab\n\"\"\"\n\n", "func_signal": "def quickhull_rec (a, b, S):\n", "code": "if len (S) == 0:\n\ta.lineto (b)\n\treturn [a]\n\nj = 0\narea_j = area2 (b, a, S[j])\nfor i in range (1, len(S)):\n\tarea_i = area2 (b, a, S[i])\n\tif area_i > area_j:\n\t\tj = i\n\t\tarea_j = area_i\n\nc = S[j]\n\nS1 = []\nS2 = []\n\nlado_a = left (b, c, a)\nlado_b = left (a, c, b)\n\nid = a.lineto (c, config.COLOR_ALT5)\nfor i in range (0, len (S)):\n\tif right (a, c, S[i]):\n\t\tS1.append (S[i])\na.remove_lineto (c, id)\nid = a.lineto (c, config.COLOR_ALT4)\nfecho = quickhull_rec (a, c, S1)\n\na.remove_lineto (c, id)\nid = c.lineto (b, config.COLOR_ALT5)\nfor i in range (0, len (S)):\n\tif right (c, b, S[i]):\n\t\tS2.append (S[i])\nc.remove_lineto (b, id)\n\nid = c.lineto (b, config.COLOR_ALT4)\nfecho.extend (quickhull_rec (c, b, S2))\nc.remove_lineto (b, id)\n\nreturn fecho", "path": "geocomp\\convexhull\\quickhull.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"Algoritmo Merge Hull para o problema do Fecho Convexo\"\"\"\n", "func_signal": "def Mergehull (l):\n", "code": "if len (l) == 0: return None\n\ndef cmp (a, b):\n\tif a.x < b.x: return -1\n\tif a.x > b.x: return 1\n\tif a.y < b.y: return -1\n\treturn a.y > b.y\n\nl.sort (cmp)\n\n(min_pt, max_pt, hull) = mergehull_rec (l)\n\nhull.extra_info = 'vertices: %d'%len (hull.to_list ())\nreturn hull", "path": "geocomp\\convexhull\\mergehull.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\" fechar tudo \"\"\"\n", "func_signal": "def delete_cb (self, arg=None):\n", "code": "self.step.set (1)\nself.tk.destroy ()", "path": "tkgeocomp.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"Constroi a parte inferior do fecho convexo\"\"\"\n\n# step 1\n\n\n# step 1/2/3\n", "func_signal": "def bhatta_sen_lower_rec (a, b, S):\n", "code": "again = 1\n\nwhile again:\n\tif len (S) == 1:\n\t\treturn [ S[0] ]\n\tj = int (random.uniform (0, len (S)/2))\n\tagain = 0\n\n\tp1 = S[2*j+1]\n\tp2 = S[2*j]\n\tp1.hilight ()\n\tp2.hilight ()\n\tif inside_restricted (b, a, S[2*j+1], S[2*j]):\n\t\tS.remove (S[2*j])\n\t\tagain = 1\n\telif inside_restricted (b, a, S[2*j], S[2*j+1]):\n\t\tS.remove (S[2*j+1])\n\t\tagain = 1\n\n\tp1.unhilight ()\n\tp2.unhilight ()\n\n# step 4\nm = 0\nif left (S[2*j], S[2*j+1], a):\n\tp1 = S[2*j+1]\n\tp2 = S[2*j]\nelse:\n\tp1 = S[2*j]\n\tp2 = S[2*j+1]\nid = p1.lineto (p2, config.COLOR_ALT4)\narea_m = area2 (p1, p2, S[m])\nfor i in range (1, len(S)):\n\tarea_i = area2 (p1, p2, S[i])\n\tif area_i > area_m:\n\t\tm = i\n\t\tarea_m = area_i\npm = S[m]\ncontrol.plot_delete (id)\n\nid = pm.hilight (config.COLOR_ALT1)\ncontrol.sleep ()\n\nS1 = []\nS2 = []\n\n# step 5\ni = j\n#map (lambda p: p.hilight (config.COLOR_ALT2), S)\n#control.sleep ()\n#map (lambda p: p.unhilight (), S)\ncontrol.sleep ()\ncont = []\nfor j in range (0, len(S)/2):\n\tcont.extend ([2*j, 2*j+1])\n\tif S[2*j].x < S[2*j+1].x:\n\t\tp1 = S[2*j]\n\t\tp2 = S[2*j+1]\n\telse:\n\t\tp1 = S[2*j+1]\n\t\tp2 = S[2*j]\n\n\tp1.hilight ()\n\tp2.hilight (config.COLOR_ALT3)\n\n\tif p2.x <= pm.x:\n\t\tif right (pm, p2, p1):\n\t\t\tS1.append (p1)\n\t\t\tS1.append (p2)\n\t\telse:\n\t\t\tS1.append (p1)\n\telif pm.x <= p1.x:\n\t\tif right (pm, p1, p2):\n\t\t\tS2.append (p2)\n\t\telse:\n\t\t\tS2.append (p1)\n\t\t\tS2.append (p2)\n\telse: # p1.x < pm.x < p2.x\n\t\tcontrol.sleep ()\n\t\tS1.append (p1)\n\t\tS2.append (p2)\n\n\tp1.unhilight ()\n\tp2.unhilight ()\n\npm.unhilight (id)\nif len (S) % 2 == 1:\n\tS1.append (S[-1])\n\tS2.append (S[-1])\ncontrol.sleep ()\n\nmap (lambda p: p.hilight (), S1)\ncontrol.sleep ()\n\n# step 6\nfor p in S1[:]:\n\tif not right (a, pm, p):\n\t\tS1.remove (p)\n\t\tp.unhilight ()\n\ncontrol.sleep ()\nmap (lambda p: p.unhilight (), S1)\ncontrol.sleep ()\nmap (lambda p: p.hilight (), S2)\ncontrol.sleep ()\n\nfor p in S2[:]:\n\tif not right (pm, b, p):\n\t\tS2.remove (p)\n\t\tp.unhilight ()\ncontrol.sleep ()\nmap (lambda p: p.unhilight (), S2)\n\n# step 7\nret1 = []\nret2 = []\nif len (S1) != 0:\n\tid = a.lineto (pm, config.COLOR_ALT4)\n\tret1 = bhatta_sen_lower_rec (a, pm, S1)\n\ta.remove_lineto (pm, id)\n\tret1[-1].lineto (pm)\nret1.append (pm)\nif len (S2) != 0:\n\tid = pm.lineto (b, config.COLOR_ALT4)\n\tret2 = bhatta_sen_lower_rec (pm, b, S2) \n\tpm.remove_lineto (b, id)\n\tpm.lineto (ret2[0])\n\nret1.extend (ret2)\n\nreturn ret1", "path": "geocomp\\convexhull\\bhatta_sen.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"verifica se p esta dentro do triangulo a,b,c\n\nAdmite que left (a, b, c) == left (a, b, p) == TRUE\n\"\"\"\n", "func_signal": "def inside_restricted (a, b, c, p):\n", "code": "if not left_on (b, c, p):\n\treturn 0\nelif not left_on (c, a, p):\n\treturn 0\nreturn 1", "path": "geocomp\\convexhull\\bhatta_sen.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"reativa maior parte dos botoes\"\"\"\n#self.main_frame.grab_release ()\n#self.left['takefocus'] = 1\n", "func_signal": "def enable (self):\n", "code": "self.filelist['state'] = NORMAL\nself.filelist['takefocus'] = 1", "path": "tkgeocomp.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"desativa maior parte dos botoes\n\n(tenta) impedir que outro algoritmo seja iniciado concorrentemente\"\"\"\n#self.main_frame.grab_set ()\n#self.left['takefocus'] = 0\n", "func_signal": "def disable (self):\n", "code": "self.filelist['state'] = DISABLED\nself.filelist['takefocus'] = 0", "path": "tkgeocomp.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"apaga da tela o elemento com identificador id\"\"\"\n", "func_signal": "def plot_delete (id):\n", "code": "if skip: return 0\ngui.plot_delete (id)\nupdate ()", "path": "geocomp\\common\\control.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"Constroi a parte superior do fecho convexo\"\"\"\n\n# step 1/2/3\n", "func_signal": "def bhatta_sen_upper_rec (a, b, S):\n", "code": "again = 1\n\nwhile again:\n\tif len (S) == 1:\n\t\treturn [ S[0] ]\n\tj = int (random.uniform (0, len (S)/2))\n\tagain = 0\n\n\tp1 = S[2*j+1]\n\tp2 = S[2*j]\n\tp1.hilight ()\n\tp2.hilight ()\n\tif inside_restricted (b, a, S[2*j+1], S[2*j]):\n\t\tS.remove (S[2*j])\n\t\tagain = 1\n\telif inside_restricted (b, a, S[2*j], S[2*j+1]):\n\t\tS.remove (S[2*j+1])\n\t\tagain = 1\n\n\tp1.unhilight ()\n\tp2.unhilight ()\n\n\n# step 4\nm = 0\nif left (S[2*j], S[2*j+1], a):\n\tp1 = S[2*j+1]\n\tp2 = S[2*j]\nelse:\n\tp1 = S[2*j]\n\tp2 = S[2*j+1]\nid = p1.lineto (p2, config.COLOR_ALT4)\narea_m = area2 (p1, p2, S[m])\nfor i in range (1, len(S)):\n\tarea_i = area2 (p1, p2, S[i])\n\tif area_i > area_m:\n\t\tm = i\n\t\tarea_m = area_i\npm = S[m]\ncontrol.plot_delete (id)\n\nid = pm.hilight (config.COLOR_ALT1)\ncontrol.sleep ()\n\nS1 = []\nS2 = []\n\n# step 5\ni = j\n#map (lambda p: p.hilight (config.COLOR_ALT2), S)\n#control.sleep ()\n#map (lambda p: p.unhilight (), S)\ncontrol.sleep ()\ncont = []\nfor j in range (0, len(S)/2):\n\tcont.extend ([2*j, 2*j+1])\n\tif S[2*j].x < S[2*j+1].x:\n\t\tp1 = S[2*j]\n\t\tp2 = S[2*j+1]\n\telse:\n\t\tp1 = S[2*j+1]\n\t\tp2 = S[2*j]\n\n\tp1.hilight ()\n\tp2.hilight (config.COLOR_ALT3)\n\n\tif p2.x <= pm.x:\n\t\tif left (pm, p2, p1):\n\t\t\tS1.append (p1)\n\t\t\tS1.append (p2)\n\t\telse:\n\t\t\tS1.append (p1)\n\telif pm.x <= p1.x:\n\t\tif left (pm, p1, p2):\n\t\t\tS2.append (p2)\n\t\telse:\n\t\t\tS2.append (p1)\n\t\t\tS2.append (p2)\n\telse: # p1.x < pm.x < p2.x\n\t\tcontrol.sleep ()\n\t\tS1.append (p1)\n\t\tS2.append (p2)\n\n\tp1.unhilight ()\n\tp2.unhilight ()\n\npm.unhilight (id)\nif len (S) % 2 == 1:\n\tS1.append (S[-1])\n\tS2.append (S[-1])\ncontrol.sleep ()\n\nmap (lambda p: p.hilight (), S1)\ncontrol.sleep ()\n\n# step 6\nfor p in S1[:]:\n\tif not left (b, pm, p):\n\t\tS1.remove (p)\n\t\tp.unhilight ()\n\ncontrol.sleep ()\nmap (lambda p: p.unhilight (), S1)\ncontrol.sleep ()\nmap (lambda p: p.hilight (), S2)\ncontrol.sleep ()\n\nfor p in S2[:]:\n\tif not left (pm, a, p):\n\t\tS2.remove (p)\n\t\tp.unhilight ()\ncontrol.sleep ()\nmap (lambda p: p.unhilight (), S2)\n\n# step 7\nret1 = []\nret2 = []\nif len (S2) != 0:\n\tid = a.lineto (pm, config.COLOR_ALT4)\n\tret2 = bhatta_sen_upper_rec (a, pm, S2)\n\ta.remove_lineto (pm, id)\n\tret2[-1].lineto (pm)\nret2.append (pm)\nif len (S1) != 0:\n\tid = pm.lineto (b, config.COLOR_ALT4)\n\tret1 = bhatta_sen_upper_rec (pm, b, S1) \n\tpm.remove_lineto (b, id)\n\tpm.lineto (ret1[0])\n\nret2.extend (ret1)\n\nreturn ret2", "path": "geocomp\\convexhull\\bhatta_sen.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"cria botoes p/ algoritmos e problemas\"\"\"\n", "func_signal": "def create_buttons (self, clicked, initial = None):\n", "code": "if initial:\n\tproblem = geocomp\n\tparent = None\nelse:\n\tproblem = clicked.problem\n\tparent = clicked.parent\n\nif self.panel.has_key (problem):\n\tself.buttons.pack_forget ()\n\tself.buttons = self.panel[problem]\n\tself.buttons.pack (fill = BOTH, side = BOTTOM)\n\tself.buttons.focus.focus_set ()\n\treturn\n\nif hasattr (self, 'buttons'):\n\tself.buttons.pack_forget ()\n\nchildren = getattr (problem, 'children')\nbuttons = Frame (self.left)\nbuttons.pack (fill = BOTH, side = BOTTOM)\nrow = 0\nfirst = 1\nfor a in children:\n\tb = Button (buttons, text = a[-1])\n\tif a[1] == None:\n\t\tb['command'] = lambda self=self, b=b: \\\n\t\t\t\tself.create_buttons (b)\n\t\tb.problem = getattr (problem, a[0])\n\t\tb.parent = problem\n\t\tb.grid (row = row, column = 0, \n\t\t\tcolumnspan = 2, sticky = W+E)\n\telse:\n\t\talg = getattr (problem, a[0])\n\t\tfunc = getattr (alg, a[1])\n\t\talg_name = a[1]\n\t\tb['command'] = lambda self=self, func=func,\\\n\t\t\talg_name=alg_name, b=b: \\\n\t\t\tself.run_algorithm (func, b, alg_name)\n\t\tb.grid (row = row, column = 0, sticky = W+E)\n\t\tl = Label (buttons, text = '------')\n\t\tl.grid (row = row, column=1, sticky = W+E)\n\t\tself.labels.append (l)\n\t\tb.label = l\n\n\trow = row + 1\n\tif first: \n\t\tb.focus_set ()\n\t\tfirst = 0\n\t\tbuttons.focus = b\n\nif parent != None:\n\tb = Button (buttons, text = 'Voltar')\n\tb['command'] = lambda b=b, self=self: self.create_buttons (b)\n\tb.problem = parent\n\tb.parent = parent\n\tb.grid (row = row, column = 0, columnspan = 1, \n\t\tsticky = W+E)\nelse:\n\tb = Button (buttons, text = 'Sair')\n\tb['command'] = self.delete_cb\n\tb.grid (row = row, column = 0, columnspan = 2,\n\t\tsticky = W+E)\n\nself.panel[problem] = buttons\nself.buttons = buttons\n\nreturn", "path": "tkgeocomp.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"desenha um disco de centro (x,y), raio r e cor color na tela\"\"\"\n", "func_signal": "def plot_disc (x, y, color, r):\n", "code": "if skip: return 0\nplot_id = gui.plot_disc (x, y, color, r)\nupdate ()\nreturn plot_id", "path": "geocomp\\common\\control.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"Funcao recursiva que implementa o Merge Hull\n\nRetorna os pontos com coordenada x minima e maxima do fecho convexo\nencontrado, alem do proprio fecho.\n\"\"\"\n", "func_signal": "def mergehull_rec (l):\n", "code": "n = len (l)\nif n == 1:\n\t#l[0].prev = l[0].next = l[0]\n\tpol = Polygon ( [ l[0] ] )\n\tpol.plot ()\n\t#control.sleep ()\n\treturn (l[0], l[0], pol)\n\n# Divisao\nl1 = l[:n/2]\nl2 = l[n/2:]\n\nid = control.plot_vert_line ((l2[0].x + l1[-1].x) / 2.)\ncontrol.sleep ()\n\n# Conquista\nch1 = mergehull_rec (l1)\n\nch2 = mergehull_rec (l2)\n\nv = ch1[1]\nu = ch2[0]\n\ncontrol.plot_delete (id)\n\n# Combinar\nsup = superior_tangent (v, u)\nid_sup = sup[0].lineto (sup[1], config.COLOR_ALT1)\ninf = inferior_tangent (v, u)\nid_inf = inf[0].lineto (inf[1], config.COLOR_ALT1)\n\ncontrol.freeze_update ()\ncontrol.plot_delete (id_sup)\ncontrol.plot_delete (id_inf)\nch1[2].hide ()\nch2[2].hide ()\n\nsup[0].prev = sup[1]\nsup[1].next = sup[0]\n\ninf[0].next = inf[1]\ninf[1].prev = inf[0]\n\nch1[2].pts = inf[0]\nch1[2].plot ()\ncontrol.thaw_update ()\n\nreturn (ch1[0], ch2[1], ch1[2])", "path": "geocomp\\convexhull\\mergehull.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"Determina a tangente superior aos poligonos que contem v e u\"\"\"\n", "func_signal": "def superior_tangent (v, u):\n", "code": "v.lineto (u, config.COLOR_ALT1)\nhiv = v.hilight ()\nhiu = u.hilight ()\n\nch1 = is_sup_tan_ch1 (v, u)\nch2 = is_sup_tan_ch2 (v, u)\nwhile not (ch1 and ch2):\n\twhile not ch1:\n\t\tv.remove_lineto (u)\n\t\tv.unhilight (hiv)\n\n\t\tv = v.next\n\n\t\thiv = v.hilight ()\n\t\tv.lineto (u, config.COLOR_ALT1)\n\t\tcontrol.sleep ()\n\t\tch1 = is_sup_tan_ch1 (v, u)\n\n\tch2 = is_sup_tan_ch2 (v, u)\n\twhile not ch2:\n\t\tv.remove_lineto (u)\n\t\tu.unhilight (hiu)\n\n\t\tu = u.prev\n\n\t\thiu = u.hilight ()\n\t\tv.lineto (u, config.COLOR_ALT1)\n\t\tcontrol.sleep ()\n\t\tch2 = is_sup_tan_ch2 (v, u)\n\n\tch1 = is_sup_tan_ch1 (v, u)\n\nv.unhilight (hiv)\nu.unhilight (hiu)\nv.remove_lineto (u)\nreturn (v, u)", "path": "geocomp\\convexhull\\mergehull.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"Algoritmo Diametro para encontrar o par de pontos mais distantes\n\nEle consiste de:\n- determinar o fecho convexo dos pontos passados\n- determinar o conjunto de pares antipodas do fecho convexo\n- determinar o par antipoda cujos pontos estao a uma distancia maxima\n\"\"\"\n\n", "func_signal": "def Diameter (l):\n", "code": "if len (l) < 2: return None\nif len (l) == 2:\n\tret = Segment (l[0], l[1])\n\tret.extra_info = 'distancia: %.2f'%math.sqrt (dist2 (l[0], l[1]))\n\treturn ret\n\nch = Graham (l)\nch.hide ()\nch.plot (config.COLOR_ALT4)\n\ncontrol.sleep ()\n\npairs = antipodes (ch)\ncores = (config.COLOR_ALT1,)\n\n#print `pairs`\n\ni=0\nfor p,q in pairs:\n\tp.hilight (cores[i])\n\tq.hilight (cores[i])\n\tp.lineto (q, cores[i])\n\ti = (i+1) % len (cores)\n\ncontrol.sleep ()\n\nfarthest = dist2 (pairs[0][0], pairs[0][1])\na = pairs[0][0]\nb = pairs[0][1]\nhia = a.hilight ()\nhib = b.hilight ()\nid = a.lineto (b)\n\nfor i in range (1, len (pairs)):\n\tdist = dist2 (pairs[i][0], pairs[i][1])\n\tif dist > farthest:\n\t\tcontrol.freeze_update ()\n\t\ta.unhilight (hia)\n\t\tb.unhilight (hib)\n\t\tcontrol.plot_delete (id)\n\n\t\tfarthest = dist\n\t\ta = pairs[i][0]\n\t\tb = pairs[i][1]\n\n\t\thia = a.hilight ()\n\t\thib = b.hilight ()\n\t\tid = a.lineto (b)\n\t\tcontrol.thaw_update ()\n\nret = Segment (a, b)\nret.extra_info = 'distancia: %.2f'%math.sqrt (farthest)\nreturn ret", "path": "geocomp\\farthest\\diameter.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"Nao permite que o programa durma entre os passos do algoritmo\n\nVeja thaw_sleep\"\"\"\n", "func_signal": "def freeze_sleep ():\n", "code": "global dont_sleep\ndont_sleep = dont_sleep + 1", "path": "geocomp\\common\\control.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "# iteramos na lista, procurando 'segmento'\n", "func_signal": "def predecessor(self, segmento):\n", "code": "celula = self.cabeca\ni = self.nivel - 1\n\nwhile i >= 0: # vai ate' o nivel 0 \n    while(celula.ptrs[i] != self.nil and celula.ptrs[i].valor < segmento): \n        celula = celula.ptrs[i]\n\n    i = i - 1\n\n# celula aponta para a cabeca ou para o maior elemento que e' menor que 'segmento'\nif celula.ehCabeca() == True: return None # 'segmento' e' o primeiro element da lista\nelse: return celula.valor # predecessor proprio", "path": "geocomp\\intersection\\skiplist.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"passo completado\"\"\"\n", "func_signal": "def step_cb (self, event):\n", "code": "self.step.set (1)\nreturn 'break'", "path": "tkgeocomp.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "\"\"\"roda o algoritmo alg\"\"\"\n", "func_signal": "def run_algorithm (self, alg, widget, alg_name):\n", "code": "self.file_cont = 0\nself.current_algorithm = alg_name\n\nif len (self.input) == 0:\n\treturn\n\nself.bottom_label['text'] = '----------'\nself.disable ()\nself.canvas.focus_set ()\nself.tk.bind ('<space>', self.step_cb)\n\ncont, extra = geocomp.run_algorithm (alg, self.input)\n\nwidget.label['text'] = \"%6d\"%cont\nif extra is not None:\n\tself.bottom_label['text'] = extra\n\nself.tk.unbind ('<space>')\nself.enable ()", "path": "tkgeocomp.py", "repo_name": "luizhespanha/geometry-algorithms-simulation", "stars": 5, "license": "None", "language": "python", "size": 1448}
{"docstring": "#\u73fe\u5728\u6642\u523b\u3092\u8868\u793a\n\n", "func_signal": "def SetNowTime2StatusBar(self):\n", "code": "nowtime = strftime(\"%H:%M:%S\", localtime())\nsb = wx.GetApp().GetTopWindow().GetStatusBar()\nsb.SetStatusText(nowtime+u\"\u306b\u66f4\u65b0\u3057\u307e\u3057\u305f\")", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#print (\"start insert data\")\n", "func_signal": "def InsertData(self,data,hiddenData):\n", "code": "i = self.count\n#\u3053\u3053\u3067\u30ed\u30c3\u30af\nself.dataListLock.acquire()\nself.tmpDataList.insert(i,data)# + self.dataList\nself.tmpHiddenDataList.insert(i,hiddenData)# + self.hiddenDataList\nself.dataListLock.release()\n#\u3053\u3053\u3067\u30a2\u30f3\u30ed\u30c3\u30af\nself.count += 1\n\n#print (\"end insert data\")", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#\u3053\u3053\u306b\u901a\u4fe1\u51e6\u7406\u3092\u66f8\u304f\n", "func_signal": "def run(self):\n", "code": "self.lock.acquire()\n#try:\na = self.tw.get(\"\")\nself.func(a)\n#except:\n#\tprint \"Error:TwHttpFrame\", sys.exc_info()[0]\n#finally:\nself.lock.release()", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#\u8868\u793a\u30e1\u30cb\u30e5\u30fc\u9805\u76ee\u4f5c\u308b\n", "func_signal": "def CreateMenu(self):\n", "code": "viewMenu = wx.Menu()\nself.popupCheck = viewMenu.Append(self.ID_MNU_VIEW_POPUP,u\"\u65b0\u7740\u901a\u77e5\",u\"\u65b0\u7740\u8868\u793a\",kind = wx.ITEM_CHECK)\nself.listIconCheck = viewMenu.Append(self.ID_MNU_VIEW_LISTICON,u\"\u30ea\u30b9\u30c8\u30a2\u30a4\u30b3\u30f3\",u\"\u30ea\u30b9\u30c8\u30dc\u30c3\u30af\u30b9\u306e\u6a2a\u306e\u30a2\u30a4\u30b3\u30f3\u306e\u8868\u793a\uff0f\u975e\u8868\u793a\u3092\u5207\u308a\u66ff\u3048\u307e\u3059\",kind=wx.ITEM_CHECK)\n\nviewMenu.Check(self.ID_MNU_VIEW_POPUP, g_config['popup'])\nmenuBar = wx.MenuBar()\nmenuBar.Append(viewMenu,u\"\u8868\u793a\")\n\nwx.EVT_MENU(self, self.ID_MNU_VIEW_POPUP, self.OnMenuViewPopUp_Click)\nwx.EVT_MENU(self, self.ID_MNU_VIEW_LISTICON, self.OnMenuViewListIcon_Click)\nself.SetMenuBar(menuBar)", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "# \u9001\u4fe1\u3059\u308b\n# \u30b3\u30f3\u30dc\u30dc\u30c3\u30af\u30b9\u306e\u4e2d\u8eab\u3092\u7a7a\u306b\u3059\u308b\n", "func_signal": "def OnSendTW(self, event):\n", "code": "combo =\tself.text \nself.tw.put(combo.GetValue())\t\ncombo.SetValue(\"\")\nself.RefreshTw()", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#lock\u304b\u3051\u305f\u65b9\u304c\u3044\u3044\u306e\u304b\u3082\u3002\u3002\n", "func_signal": "def RefleshList(self,a):\n", "code": "self.ResetCount()\nuser = self.owner.tw.user['user']\nfor x in a:\n\tflag = 0\n\t\n\t# \u91cd\u8907\u767a\u8a00\u30c1\u30a7\u30c3\u30af\n\tfor d in self.dataList:\n\t\tif d[2] == x[1]:\n\t\t\tflag = 1\n\t\t\tbreak\n\tif flag == 0 : \n\t\tdataListElement = []\n\t\tdataListElement.append(\"\")\n\t\tdataListElement.append(x[0])\n\t\tdataListElement.append(x[1])\n\t\tdataListElement.append(x[2])\n\t\tdataListElement.append(x[3])\n\n\t\thiddenDataListElement = []\n\t\thiddenDataListElement.append(x[4])#\u767a\u8a00id\n\t\n\t\tself.InsertData(dataListElement,hiddenDataListElement)", "path": "chat_mini.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#\u30ed\u30b0\u30a4\u30f3\u5fc5\u8981\uff1f\n", "func_signal": "def getWithScraping(self,user,num=1):\n", "code": "opener = self.singIn(\"\")\ns = \"http://\"+self.url+\"/home?page=\"+str(num)\ndata = opener.open(s)\nurlstring = data.read()\n#print urlstring\nif num == 1:\n\treturn self.scrapeTwit(urlstring,True)\nelse:\n\treturn self.scrapeTwit(urlstring,False)", "path": "twitter3.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#\u73fe\u5728\u6642\u523b\u3092\u8868\u793a\n", "func_signal": "def SetNowTime2StatusBar(self):\n", "code": "from time import localtime, strftime\nnowtime = strftime(\"%H:%M:%S\", localtime())\nsb = wx.GetApp().GetTopWindow().GetStatusBar()\nsb.SetStatusText(nowtime+u\"\u306b\u66f4\u65b0\u3057\u307e\u3057\u305f\")", "path": "chat_mini.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#\u30e6\u30fc\u30b6\u540d\u7b49\u8a2d\u5b9a\u3059\u308b\n#\u521d\u56de\u6642\u306e\u307f\u3067\u5145\u5206\u304b\u306a\u3041\n", "func_signal": "def setAuthHandler(self):\n", "code": "auth_handler = urllib2.HTTPBasicAuthHandler()\nauth_handler.add_password(self.serviceName,self.serviceURL,self.user['user'],self.user['pass'])\nopener = urllib2.build_opener(auth_handler)\nurllib2.install_opener(opener)\nreturn opener", "path": "twitter3.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#lock\u304b\u3051\u305f\u65b9\u304c\u3044\u3044\u306e\u304b\u3082\u3002\u3002\n", "func_signal": "def RefreshList(self,a):\n", "code": "self.ResetCount()\nuser = self.owner.tw.user['user']\noutCount = 0\noutString = \"\"\nfor x in a:\n\tflag = 0\n\t\n\t# \u91cd\u8907\u767a\u8a00\u30c1\u30a7\u30c3\u30af\n\tfor d in self.dataList:\n\t\tif d[2] == x[1]:\n\t\t\tflag = 1\n\t\t\tbreak\n\tif flag == 0 : \n\t\tdataListElement = []\n\t\tdataListElement.append(\"\")\n\t\tdataListElement.append(x[0])\n\t\tdataListElement.append(x[1])\n\t\t#x2 = toDate2.toDate2(x[2]).strftime(\"%y-%m-%d %H:%M:%S\")\n\t\tx2 = \"--\"\n\t\tdataListElement.append(x2)\n\t\tdataListElement.append(x[3])\n\n\t\thiddenDataListElement = []\n\t\thiddenDataListElement.append(x[4])#\u767a\u8a00id\n\t\n\t\tself.InsertData(dataListElement,hiddenDataListElement)\t\n\n\t\tif g_growl == True:\n\t\t\toutCount += 1\n\t\t\tx[1]= self.RemoveATag(x[1])\n\t\t\toutString += x[0]+\" : \"+x[1]+\"\\n\"\n\nif g_growl == True and g_config['popup']:\n\tif(outCount > 0):\n\t\tTmpTwitPage.CallGrowl(self,\"DM \u65b0\u7740\"+str(outCount)+\"\u4ef6\",\n\t\t\toutString,self.img)", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "# \u9001\u4fe1\u3059\u308b\n# \u30b3\u30f3\u30dc\u30dc\u30c3\u30af\u30b9\u306e\u4e2d\u8eab\u3092\u7a7a\u306b\u3059\u308b\n", "func_signal": "def OnSendTW(self, event):\n", "code": "combo =\tself.text \nself.tw.put(combo.GetValue())\t\ncombo.SetValue(\"\")\nself.RefleshTw()", "path": "chat_mini.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u3044\u3066\u3001\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5909\u63db\u3059\u308b\n", "func_signal": "def loadUserData(fileName):\n", "code": "file = open(fileName,'r')\na = json.read(file.read())\nfile.close()\nreturn a", "path": "tabconfig_readtest.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u3044\u3066\u3001\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5909\u63db\u3059\u308b\n#\u30c7\u30fc\u30bf\u5f62\u5f0f\u306f(user,password)\n#try\n", "func_signal": "def loadUserData(self, fileName):\n", "code": "file = open(fileName,'r')\na = simplejson.loads(file.read())\nfile.close()\nreturn a\n#catch exit(1)", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#from pit import Pit\n#twUserdata = Pit.get('twitter.com',{'require' : {'user':'','pass':''}})\n", "func_signal": "def __init__(self, parent=None):\n", "code": "try:\n\ttwUserdata = self.loadUserData(\".chat/twdata\")\nexcept:\n\ttwUserdata = configDialog.ConfigDialog(None, -1, 'crochet config').GetAccount()\n\tif twUserdata[\"user\"] and twUserdata[\"pass\"]:\n\t\tfile = open(\".chat/twdata\",\"w\")\n\t\tfile.write(\"{\\\"user\\\":\\\"\"+twUserdata[\"user\"]+\n\t\t\t   \"\\\",\\\"pass\\\":\\\"\"+twUserdata[\"pass\"]+\"\\\"}\\n\")\n\t\tfile.close()\n\telse:\n\t\texit(1)\ntwTabConfig = self.loadUserData(\".chat/tabconfig\")\n\nwx.Frame.__init__(self,None, -1, \"crochet\")\n\nif g_growl == True:\n\tself.g = Growl.GrowlNotifier(\n\t\tapplicationName='crochet',notifications=['newTwit','newReply'])\n\tself.g.register()\n\tself.img = Growl.Image.imageFromPath('reply.png')\t\nself.CreateStatusBar()\n\nself.selectedRow = -1\ntext = self.text = wx.TextCtrl(self,-1,style=wx.TE_PROCESS_ENTER)\ntext.Bind(wx.EVT_TEXT_ENTER, self.OnSendTW)\nbutton = self.button = wx.Button(self, -1, \"Send\")\nself.button.Bind(wx.EVT_BUTTON, self.OnSendTW) \n\t\nnotebook = self.notebook = wx.aui.AuiNotebook(self,-1,style=wx.aui.AUI_NB_BOTTOM|wx.aui.AUI_NB_WINDOWLIST_BUTTON|wx.aui.AUI_NB_TAB_SPLIT)\n\nself.imageThreadLock = thread.allocate_lock()\nself.httpThreadLock = thread.allocate_lock()\nself.dataListThreadLock = thread.allocate_lock()\t\nfilter = []\nfor f in twTabConfig['tabFilter']:\n\tfilter.append(BranchFilter(f[0],f[1],f[2],f[3],f[4],f[5],f[6])\t)\t\n\nself.recentPage = RecentPage(self,self.httpThreadLock,filter)\nself.replyPage = ReplyPage(self,self.httpThreadLock)\nself.directPage = DMPage(self,self.httpThreadLock) \n\nfor p in twTabConfig['tabName']:\n\tpage = CustomPage(p,self,self.httpThreadLock)\n\tself.recentPage.AppendCustomPage(page,p)\n\ninputSizer = wx.BoxSizer(wx.HORIZONTAL)\ninputSizer.Add(self.text,2)\ninputSizer.Add(self.button,0)\n\n#messageText=self.messageText = wx.TextCtrl(self,-1,style=wx.TE_MULTILINE|wx.TE_AUTO_URL|wx.TE_READONLY,size=(-1,65))\nmessageText = self.messageText = TwitHtml(self)\nuserIcon = self.userIcon = wx.StaticBitmap(self,-1,wx.NullBitmap,(0,0),(64,64))\nuserName = self.userName = wx.StaticText(self,-1,\"test\")\ntwitTime = self.twitTime = wx.StaticText(self,-1,\"---\")\n\nmessageSizer3 = wx.BoxSizer(wx.HORIZONTAL)\nmessageSizer3.Add(userName,2,wx.EXPAND)\nmessageSizer3.Add(twitTime,3,wx.EXPAND,wx.ALIGN_RIGHT)\n\nmessageSizer2 = wx.BoxSizer(wx.VERTICAL)\nmessageSizer2.Add(messageSizer3,0,wx.EXPAND)\nmessageSizer2.Add(messageText,0,wx.EXPAND)\n\nmessageSizer1 = wx.BoxSizer(wx.HORIZONTAL)\nmessageSizer1.Add(userIcon,0,wx.EXPAND)\nmessageSizer1.Add(messageSizer2,1,wx.EXPAND)\n\t\nmessageSizer = wx.BoxSizer(wx.VERTICAL)\t\nmessageSizer.Add(messageSizer1,0,wx.EXPAND)\nmessageSizer.Add(inputSizer,0,wx.EXPAND)\n\nself.sizer = wx.BoxSizer(wx.VERTICAL)\nself.sizer.Add(notebook,2,wx.EXPAND)\nself.sizer.Add(messageSizer,0,wx.EXPAND)\n\nself.SetSizer(self.sizer)\nself.SetAutoLayout(True)\ninputSizer.Fit(self)\nself.sizer.Fit(self)\n\t\nself.tw = twitter3.Twitter(twUserdata)\nself.tw.setAuthService(\"twitter\")\nself.SetIcon(main_icon.getIcon())\nself.SetSize((600,400))\nself.timer = wx.Timer(self,self.TIMER_ID)\nwx.EVT_TIMER(self,self.TIMER_ID,self.OnUpdate)\nself.timer.Start(60000)\n\nself.timer11 = wx.Timer(self,self.TIMER_ID3+1)\nwx.EVT_TIMER(self,self.TIMER_ID3+1,self.OnUpdate2)\nself.timer11.Start(3000)\n\nself.timer2 = wx.Timer(self,self.TIMER_ID2)\nwx.EVT_TIMER(self,self.TIMER_ID2,self.OnReplyUpdate)\nself.timer2.Start(300000)\n\nself.timer3 = wx.Timer(self,self.TIMER_ID3)\nwx.EVT_TIMER(self,self.TIMER_ID3,self.OnDMUpdate)\nself.timer3.Start(300000)\n\nself.RefreshTw()\nself.replyPage.Refresh()\nself.directPage.Refresh()\n\nself.SetNowTime2StatusBar()\nself.CreateMenu()", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#lock\u304b\u3051\u305f\u65b9\u304c\u3044\u3044\u306e\u304b\u3082\u3002\u3002\n", "func_signal": "def RefreshList(self,a):\n", "code": "self.ResetCount()\noutCount = 0\noutString = \"\"\nuser = self.owner.tw.user['user']\nfor x in a:\n\tflag = 0\n\t\n\t# \u91cd\u8907\u767a\u8a00\u30c1\u30a7\u30c3\u30af\n\tfor d in self.dataList:\n\t\tif d[2] == x[1]:\n\t\t\tflag = 1\n\t\t\tbreak\n\tif flag == 0 : \n\t\tdataListElement = []\n\t\tdataListElement.append(\"\")\n\t\tdataListElement.append(x[0])\n\t\tdataListElement.append(x[1])\n\t\t#x2 = toDate2.toDate2(x[2]).strftime(\"%y-%m-%d %H:%M:%S\")\n\t\tx2 = \"--\"\n\n\t\tdataListElement.append(x2)\n\t\tdataListElement.append(x[3])\n\n\t\thiddenDataListElement = []\n\t\thiddenDataListElement.append(x[4])#\u767a\u8a00id\n\t\n\t\tself.InsertData(dataListElement,hiddenDataListElement)\t\n\n\t\tif g_growl == True:\n\t\t\toutCount += 1\n\t\t\tx[1]= self.RemoveATag(x[1])\n\t\t\toutString += x[0]+\" : \"+x[1]+\"\\n\"\n\nif g_growl == True and g_config['popup']:\n\tif(outCount > 0):\n\t\tTmpTwitPage.CallGrowl(self,\"Reply \u65b0\u7740\"+str(outCount)+\"\u4ef6\",\n\t\t\toutString,self.img)", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#self.SetStatusBar(u\"\u65b0\u7740\u53d6\u5f97\u4e2d...\")\n", "func_signal": "def OnUpdate2(self, event):\n", "code": "b1 = self.recentPage.CheckUpdate()\nfor c in self.recentPage.customPages:\n\tb2 = self.recentPage.customPages[c].CheckUpdate()\nself.replyPage.CheckUpdate()\nself.directPage.CheckUpdate()\nif b1 == True:\n\tself.SetNowTime2StatusBar()", "path": "chat_mini.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#from pit import Pit\n#twUserdata = Pit.get('twitter.com',{'require' : {'user':'','pass':''}})\n", "func_signal": "def __init__(self, parent=None):\n", "code": "twUserdata = self.loadUserData(\".chat/twdata\")\ntwTabConfig = self.loadUserData(\".chat/tabconfig\")\n\nwx.Frame.__init__(self,None, -1, \"crochet\")\n\nself.CreateStatusBar()\n\nself.selectedRow = -1\ntext = self.text = wx.TextCtrl(self,-1,style=wx.TE_PROCESS_ENTER)\ntext.Bind(wx.EVT_TEXT_ENTER, self.OnSendTW)\nbutton = self.button = wx.Button(self, -1, \"Send\")\nself.button.Bind(wx.EVT_BUTTON, self.OnSendTW) \n\t\nnotebook = self.notebook = wx.Notebook(self,-1,style=wx.NB_BOTTOM|wx.NB_MULTILINE)\n\nself.imageThreadLock = thread.allocate_lock()\nself.httpThreadLock = thread.allocate_lock()\nself.dataListThreadLock = thread.allocate_lock()\t\nfilter = []\nfor f in twTabConfig['tabFilter']:\n\tfilter.append(BranchFilter(f[0],f[1],f[2],f[3],f[4],f[5],f[6])\t)\t\n\nself.recentPage = RecentPage(self,self.httpThreadLock,filter)\nself.replyPage = ReplyPage(self,self.httpThreadLock)\nself.directPage = DMPage(self,self.httpThreadLock) \n\nfor p in twTabConfig['tabName']:\n\tpage = CustomPage(p,self,self.httpThreadLock)\n\tself.recentPage.AppendCustomPage(page,p)\t\n\ninputSizer = wx.BoxSizer(wx.HORIZONTAL)\ninputSizer.Add(self.text,2)\ninputSizer.Add(self.button,0)\n\n\nmessageText=self.messageText = wx.TextCtrl(self,-1,style=wx.TE_MULTILINE|wx.TE_AUTO_URL|wx.TE_READONLY,size=(-1,65))\nuserIcon = self.userIcon = wx.StaticBitmap(self,-1,wx.NullBitmap,(0,0),(64,64))\nuserName = self.userName = wx.StaticText(self,-1,\"test\")\ntwitTime = self.twitTime = wx.StaticText(self,-1,\"---\")\n\nmessageSizer3 = wx.BoxSizer(wx.HORIZONTAL)\nmessageSizer3.Add(userName,1,wx.EXPAND)\nmessageSizer3.Add(twitTime,1,wx.EXPAND)\n\nmessageSizer2 = wx.BoxSizer(wx.VERTICAL)\nmessageSizer2.Add(messageSizer3,0,wx.EXPAND)\nmessageSizer2.Add(messageText,0,wx.EXPAND)\n\nmessageSizer1 = wx.BoxSizer(wx.HORIZONTAL)\nmessageSizer1.Add(userIcon,0,wx.EXPAND)\nmessageSizer1.Add(messageSizer2,1,wx.EXPAND)\n\t\nmessageSizer = wx.BoxSizer(wx.VERTICAL)\t\nmessageSizer.Add(messageSizer1,0,wx.EXPAND)\nmessageSizer.Add(inputSizer,0,wx.EXPAND)\n\nself.sizer = wx.BoxSizer(wx.VERTICAL)\nself.sizer.Add(notebook,2,wx.EXPAND)\nself.sizer.Add(messageSizer,0,wx.EXPAND)\n\n\nself.SetSizer(self.sizer)\nself.SetAutoLayout(True)\ninputSizer.Fit(self)\nself.sizer.Fit(self)\n\t\nself.tw = twitter3.Twitter(twUserdata)\nself.tw.setAuthService(\"twitter\")\nself.SetIcon(main_icon.getIcon())\nself.SetSize((300,400))\nself.timer = wx.Timer(self,self.TIMER_ID)\nwx.EVT_TIMER(self,self.TIMER_ID,self.OnUpdate)\nself.timer.Start(60000)\n\nself.timer11 = wx.Timer(self,self.TIMER_ID3+1)\nwx.EVT_TIMER(self,self.TIMER_ID3+1,self.OnUpdate2)\nself.timer11.Start(3000)\n\nself.timer2 = wx.Timer(self,self.TIMER_ID2)\nwx.EVT_TIMER(self,self.TIMER_ID2,self.OnReplyUpdate)\nself.timer2.Start(300000)\n\nself.timer3 = wx.Timer(self,self.TIMER_ID3)\nwx.EVT_TIMER(self,self.TIMER_ID3,self.OnDMUpdate)\nself.timer3.Start(300000)\nself.RefleshTw()\n\nself.replyPage.Reflesh()\nself.directPage.Reflesh()\n\nself.SetNowTime2StatusBar()", "path": "chat_mini.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#self.SetStatusBar(u\"\u65b0\u7740\u53d6\u5f97\u4e2d...\")\n", "func_signal": "def OnUpdate2(self, event):\n", "code": "b1 = self.recentPage.CheckUpdate()\nfor c in self.recentPage.customPages:\n\tb2 = self.recentPage.customPages[c].CheckUpdate()\nself.replyPage.CheckUpdate()\nself.directPage.CheckUpdate()\nif b1 == True:\n\tself.SetNowTime2StatusBar()", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#print (\"setstatusbar\"+str,)\t\n", "func_signal": "def SetStatusBar(self,str):\n", "code": "sb = wx.GetApp().GetTopWindow().GetStatusBar()\nsb.SetStatusText(str)", "path": "crochet.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "#\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u3044\u3066\u3001\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5909\u63db\u3059\u308b\n#\u30c7\u30fc\u30bf\u5f62\u5f0f\u306f(user,password)\n#try\n", "func_signal": "def loadUserData(self, fileName):\n", "code": "file = open(fileName,'r')\na = simplejson.read(file.read())\nfile.close()\nreturn a\n#catch exit(1)", "path": "chat_mini.py", "repo_name": "showyou/crochet", "stars": 5, "license": "None", "language": "python", "size": 116}
{"docstring": "\"\"\"Tests that the options stub has the required properties by Django.\"\"\"\n# Django requires object_name and app_label for serialization output.\n", "func_signal": "def testDjangoModelOptionsStub(self):\n", "code": "self.assertEqual(\"RegistrationTestModel\",\n                 RegistrationTestModel._meta.object_name)\nself.assertEqual(\"appengine_django\", RegistrationTestModel._meta.app_label)\n# The pk.name member is required during serialization for dealing with\n# related fields.\nself.assertEqual(\"key_name\", RegistrationTestModel._meta.pk.name)\n# The many_to_many method is called by Django in the serialization code to\n# find m2m relationships. m2m is not supported by the datastore.\nself.assertEqual([], RegistrationTestModel._meta.many_to_many)", "path": "appengine_django\\tests\\model_test.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Removes incompatible commands and installs replacements where possible.\"\"\"\n", "func_signal": "def ModifyAvailableCommands():\n", "code": "if have_appserver:\n  # Commands are not used when running from an appserver.\n  return\nfrom django.core import management\nproject_directory = os.path.join(__path__[0], \"../\")\nif have_django_zip:\n  FindCommandsInZipfile.orig = management.find_commands\n  management.find_commands = FindCommandsInZipfile\nmanagement.get_commands()\n# Replace startapp command which is set by previous call to get_commands().\nfrom appengine_django.management.commands.startapp import ProjectCommand\nmanagement._commands['startapp'] = ProjectCommand(project_directory) \nRemoveCommands(management._commands)\nlogging.debug(\"Removed incompatible Django manage.py commands\")", "path": "appengine_django\\__init__.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Disables Django's model validation routines.\n\nThe model validation is primarily concerned with validating foreign key\nreferences. There is no equivalent checking code for datastore References at\nthis time.\n\nValidation needs to be disabled or serialization/deserialization will fail.\n\"\"\"\n", "func_signal": "def DisableModelValidation():\n", "code": "from django.core.management import validation\nvalidation.get_validation_errors = lambda x, y=0: 0\nlogging.debug(\"Django SQL model validation disabled\")", "path": "appengine_django\\__init__.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"\nGiven a path to a management directory, returns a list of all the command\nnames that are available.\n\nThis implementation also works when Django is loaded from a zip.\n\nReturns an empty list if no commands are defined.\n\"\"\"\n", "func_signal": "def FindCommandsInZipfile(management_dir):\n", "code": "zip_marker = \".zip%s\" % os.sep\nif zip_marker not in management_dir:\n  return FindCommandsInZipfile.orig(management_dir)\n\n# Django is sourced from a zipfile, ask zip module for a list of files.\nfilename, path = management_dir.split(zip_marker)\nzipinfo = zipfile.ZipFile(\"%s.zip\" % filename)\n\n# Add commands directory to management path.\npath = os.path.join(path, \"commands\")\n\n# The zipfile module returns paths in the format of the operating system\n# that created the zipfile! This may not match the path to the zipfile\n# itself. Convert operating system specific characters to a standard\n# character (#) to compare paths to work around this.\npath_normalise = re.compile(r\"[/\\\\]\")\npath = path_normalise.sub(\"#\", path)\ndef _IsCmd(t):\n  \"\"\"Returns true if t matches the criteria for a command module.\"\"\"\n  filename = os.path.basename(t)\n  t = path_normalise.sub(\"#\", t)\n  if not t.startswith(path):\n    return False\n  if filename.startswith(\"_\") or not t.endswith(\".py\"):\n    return False\n  return True\n\nreturn [os.path.basename(f)[:-3] for f in zipinfo.namelist() if _IsCmd(f)]", "path": "appengine_django\\__init__.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"\nReturns site-specific profile for this user. Raises\nSiteProfileNotAvailable if this site does not allow profiles.\n\nWhen using the App Engine authentication framework, users are created\nautomatically.\n\"\"\"\n", "func_signal": "def get_profile(self):\n", "code": "from django.contrib.auth.models import SiteProfileNotAvailable\nif not hasattr(self, '_profile_cache'):\n  from django.conf import settings\n  if not hasattr(settings, \"AUTH_PROFILE_MODULE\"):\n    raise SiteProfileNotAvailable\n  try:\n    app_label, model_name = settings.AUTH_PROFILE_MODULE.split('.')\n    model = models.get_model(app_label, model_name)\n    self._profile_cache = model.all().filter(\"user =\", self).get()\n    if not self._profile_cache:\n      raise model.DoesNotExist\n  except (ImportError, ImproperlyConfigured):\n    raise SiteProfileNotAvailable\nreturn self._profile_cache", "path": "appengine_django\\auth\\models.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Gets and deletes messages for this user\"\"\"\n", "func_signal": "def get_and_delete_messages(self):\n", "code": "msgs = []\nfor msg in self.message_set:\n  msgs.append(msg)\n  msg.delete()\nreturn msgs", "path": "appengine_django\\auth\\models.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Temporary adjust django.__path__ to load app templates from the\nhelpers directory.\n\"\"\"\n", "func_signal": "def handle_label(self, *args, **kwds):\n", "code": "old_path = django.__path__\ndjango.__path__ = appengine_django.__path__\nstartapp.Command.handle_label(self, *args, **kwds)\ndjango.__path__ = old_path", "path": "appengine_django\\management\\commands\\startapp.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Initialises the list of Django properties.\n\nThis method takes care of wrapping the properties created by the superclass\nso that they look like Django properties and installing them into the\n._meta object of the class so that Django can find them at the appropriate\ntime.\n\"\"\"\n", "func_signal": "def __init__(cls, name, bases, attrs):\n", "code": "super(PropertiedClassWithDjango, cls).__init__(name, bases, attrs)\nif name == 'BaseModel':\n  # This metaclass only acts on subclasses of BaseModel.\n  return\n\nfields = [PropertyWrapper(p) for p in cls._properties.values()]\ncls._meta.local_fields = fields", "path": "appengine_django\\models.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Tests that each model instance has a 'primary key' generated.\"\"\"\n", "func_signal": "def testDjangoModelPK(self):\n", "code": "obj = RegistrationTestModel(key_name=\"test\")\nobj.put()\npk = obj._get_pk_val()\nself.assert_(pk)\nnew_obj = RegistrationTestModel.get(pk)\nself.assertEqual(obj.key(), new_obj.key())", "path": "appengine_django\\tests\\model_test.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Installs the Google memcache into Django.\n\nBy default django tries to import standard memcache module.\nBecause appengine memcache is API compatible with Python memcache module,\nwe can trick Django to think it is installed and to use it.\n\nNow you can use CACHE_BACKEND = 'memcached://' in settings.py. IP address\nand port number are not required.\n\"\"\"\n", "func_signal": "def InstallGoogleMemcache():\n", "code": "from google.appengine.api import memcache\nsys.modules['memcache'] = memcache\nlogging.debug(\"Installed App Engine memcache backend\")", "path": "appengine_django\\__init__.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Deserialize simple Python objects back into Model instances.\n\nIt's expected that you pass the Python objects themselves (instead of a\nstream or a string) to the constructor\n\"\"\"\n", "func_signal": "def Deserializer(object_list, **options):\n", "code": "models.get_apps()\nfor d in object_list:\n  # Look up the model and starting build a dict of data for it.\n  Model = python._get_model(d[\"model\"])\n  data = {}\n  key = resolve_key(Model._meta.module_name, d[\"pk\"])\n  if key.name():\n    data[\"key_name\"] = key.name()\n  parent = None\n  if key.parent():\n    parent = FakeParent(key.parent())\n  m2m_data = {}\n\n  # Handle each field\n  for (field_name, field_value) in d[\"fields\"].iteritems():\n    if isinstance(field_value, str):\n      field_value = smart_unicode(\n          field_value, options.get(\"encoding\",\n                                   settings.DEFAULT_CHARSET),\n          strings_only=True)\n    field = Model.properties()[field_name]\n\n    if isinstance(field, db.Reference):\n      # Resolve foreign key references.\n      data[field.name] = resolve_key(Model._meta.module_name, field_value)\n      if not data[field.name].name():\n        raise base.DeserializationError(u\"Cannot load Reference with \"\n                                        \"unnamed key: '%s'\" % field_value)\n    else:\n      data[field.name] = field.validate(field_value)\n  # Create the new model instance with all it's data, but no parent.\n  object = Model(**data)\n  # Now add the parent into the hidden attribute, bypassing the type checks\n  # in the Model's __init__ routine.\n  object._parent = parent\n  # When the deserialized object is saved our replacement DeserializedObject\n  # class will set object._parent to force the real parent model to be loaded\n  # the first time it is referenced.\n  yield base.DeserializedObject(object, m2m_data)", "path": "appengine_django\\serializer\\python.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Destroys the test datastore files.\"\"\"\n", "func_signal": "def destroy_test_db(self, *args, **kw):\n", "code": "from appengine_django.db.base import destroy_datastore\nfrom appengine_django.db.base import get_test_datastore_paths\ndestroy_datastore(*get_test_datastore_paths())\nlogging.debug(\"Destroyed test datastore\")", "path": "appengine_django\\db\\creation.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Wrapper for db.Property to make it look like a Django model Property\"\"\"\n", "func_signal": "def PropertyWrapper(prop):\n", "code": "if isinstance(prop, db.Reference):\n  prop.rel = Relation(prop.reference_class)\nelse:\n  prop.rel = None\nprop.serialize = True\nreturn prop", "path": "appengine_django\\models.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Installs the appengine database backend into Django.\n\nThe appengine database lives in the db/ subdirectory of this package, but is\nknown as \"appengine\" to Django. This function installs the module where\nDjango expects to find its database backends.\n\"\"\"\n", "func_signal": "def InstallAppengineDatabaseBackend():\n", "code": "from appengine_django import db\nsys.modules['django.db.backends.appengine'] = db\nlogging.debug(\"Installed appengine database backend\")", "path": "appengine_django\\__init__.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Install a replacement for the imp module removed by the appserver.\n\nThis is only to find mangement modules provided by applications.\n\"\"\"\n", "func_signal": "def InstallReplacementImpModule():\n", "code": "if not have_appserver:\n  return\nmodname = 'appengine_django.replacement_imp'\nimp_mod = __import__(modname, {}, [], [''])\nsys.modules['imp'] = imp_mod\nlogging.debug(\"Installed replacement imp module\")", "path": "appengine_django\\__init__.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Tests that a combined model class has (faked) Django fields.\"\"\"\n", "func_signal": "def testDjangoModelFields(self):\n", "code": "fields = TestModelWithProperties._meta.local_fields\nself.assertEqual(3, len(fields))\n# Check each fake field has the minimal properties that Django needs.\nfor field in fields:\n  # The Django serialization code looks for rel to determine if the field\n  # is a relationship/reference to another model.\n  self.assert_(hasattr(field, \"rel\"))\n  # serialize is required to tell Django to serialize the field.\n  self.assertEqual(True, field.serialize)\n  if field.name == \"property3\":\n    # Extra checks for the Reference field.\n    # rel.field_name is used during serialization to find the field in the\n    # other model that this field is related to. This should always be\n    # 'key_name' for appengine models.\n    self.assertEqual(\"key_name\", field.rel.field_name)", "path": "appengine_django\\tests\\model_test.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Monkey patches the Django serialization modules.\n\nThe standard Django serialization modules to not correctly handle the\ndatastore models provided by this package. This method installs replacements\nfor selected modules and methods to give Django the capability to correctly\nserialize and deserialize datastore models.\n\"\"\"\n# These can't be imported until InstallAppengineDatabaseBackend has run.\n", "func_signal": "def PatchDjangoSerializationModules():\n", "code": "from django.core.serializers import python\nfrom appengine_django.serializer.python import Deserializer\nif not hasattr(settings, \"SERIALIZATION_MODULES\"):\n  settings.SERIALIZATION_MODULES = {}\nbase_module = \"appengine_django\"\nsettings.SERIALIZATION_MODULES[\"xml\"] = \"%s.serializer.xml\" % base_module\npython.Deserializer = Deserializer\nPatchDeserializedObjectClass()\nDisableModelValidation()\nlogging.debug(\"Installed appengine json and python serialization modules\")", "path": "appengine_django\\__init__.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Creates a combined appengine and Django model.\n\nThe resulting model will be known to both the appengine libraries and\nDjango.\n\"\"\"\n", "func_signal": "def __new__(cls, name, bases, attrs):\n", "code": "if name == 'BaseModel':\n  # This metaclass only acts on subclasses of BaseModel.\n  return super(PropertiedClassWithDjango, cls).__new__(cls, name,\n                                                       bases, attrs)\n\nnew_class = super(PropertiedClassWithDjango, cls).__new__(cls, name,\n                                                          bases, attrs)\n\nnew_class._meta = ModelOptions(new_class)\nnew_class.objects = ModelManager(new_class)\nnew_class._default_manager = new_class.objects\nnew_class.DoesNotExist = types.ClassType('DoesNotExist',\n                                         (ObjectDoesNotExist,), {})\n\nm = get_model(new_class._meta.app_label, name, False)\nif m:\n  return m\n\nregister_models(new_class._meta.app_label, new_class)\nreturn get_model(new_class._meta.app_label, name, False)", "path": "appengine_django\\models.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Create a string that can be used to construct an equivalent object.\n\ne.g. eval(repr(obj)) == obj\n\"\"\"\n# First, creates a dictionary of property names and values. Note that\n# property values, not property objects, has to be passed in to constructor.\n", "func_signal": "def __repr__(self):\n", "code": "def _MakeReprTuple(prop_name):\n  prop = getattr(self.__class__, prop_name)\n  return (prop_name, prop.get_value_for_datastore(self))\n\nd = dict([_MakeReprTuple(prop_name) for prop_name in self.properties()])\nreturn \"%s(**%s)\" % (self.__class__.__name__, repr(d))", "path": "appengine_django\\models.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "\"\"\"Sends an email to this user.\n\nAccording to the App Engine email API the from_email must be the\nemail address of a registered administrator for the application.\n\"\"\"\n", "func_signal": "def email_user(self, subject, message, from_email):\n", "code": "mail.send_mail(subject,\n               message,\n               from_email,\n               [self.email])", "path": "appengine_django\\auth\\models.py", "repo_name": "sigma/foundation-toolkit", "stars": 4, "license": "None", "language": "python", "size": 219}
{"docstring": "# look in the subject first.\n", "func_signal": "def _getNamedMethod(self, name, numArgs, fixture, subject):\n", "code": "if fixture != subject:\n    result = self._searchForNamedMethod(name, numArgs, subject)\n    if result is not None:\n        return result\n# now look in the fixture\nresult = self._searchForNamedMethod(name, numArgs, fixture)\nif result is None:\n    return [None, None, None]\nreturn result", "path": "fit\\fitLib\\MethodTarget.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# need a way to back out changes to the PythonPath\n", "func_signal": "def establishTestLevelOptions(self, opts, pythonPath):\n", "code": "if pythonPath: #pragma: no cover # fitnesse only.\n    sys.path[1:1] = pythonPath\nFitGlobal.Options = opts\nself._extractDiagnosticOptions(opts, FitGlobal.diagnosticOptions)\nself._extractRunLevelSymbols(opts, FitGlobal.topLevelSymbols)\nresult = self._loadTestLevelAppConfigurationModule(opts)\nreturn result", "path": "fit\\fit\\RunnerCommon.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# the character compare takes place in MethodTarget, so\n# it doesn't ever get here...\n", "func_signal": "def cellEquals(self, cell, right):\n", "code": "body = cell.body.replace(\"\\n\", \"\")\nleft = self.parse(body)\ntOrF = self.equals(left, right)\nif tOrF:\n    return tOrF\nright = self.toString(right)\ntOrF = body == right\nreturn tOrF", "path": "fit\\eg\\MusicPlayer.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# in integers representing hundredths of a second\n", "func_signal": "def __str__(self):\n", "code": "self.elapsed = int((self.currentTime - self.start) * 100)\nhours = self.d(360000)\nminutes = self.d(6000)\nseconds = self.d(100)\nhundredths = self.elapsed\nif hours > 0:\n    return \"%s:%02d:%02d\" % (hours, minutes, seconds)\nelse:\n    return \"%s:%02d.%02d\" % (minutes, seconds, hundredths)", "path": "fit\\fit\\Fixture.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# added for python auto-parsing variant: let key be a number\n", "func_signal": "def key(self,key):\n", "code": "self.flash = False\nif (type(key) in (IntType, LongType, FloatType, ComplexType) or\n    self.numeric(key)):self.push(float(key))\nelif key==\"enter\":   self.push()\nelif key==\"+\":       self.push(self.pop()+self.pop())\nelif key==\"-\":       t=self.pop(); self.push(self.pop()-t)\nelif key==\"*\":       self.push(self.pop()*self.pop())\nelif key==\"/\":\n    try:\n        t=self.pop()\n        self.push(self.pop()/t)\n    except ZeroDivisionError:\n        self.flash = True\n        raise\nelif key==\"x^y\":     self.push(math.exp(math.log(self.pop())*\\\n                                        self.pop()))\nelif key==\"clx\":     self.r[0]=0\nelif key==\"clr\":     self.r[0]=self.r[1]=self.r[2]=self.r[3]=0\nelif key==\"chs\":     self.r[0]=-self.r[0]\nelif key==\"x<>y\":    t=self.r[0]; self.r[0]=self.r[1]; self.r[1]=t\nelif key==\"r!\":      self.r[3]=self.pop()\nelif key==\"sto\":     self.s=self.r[0]\nelif key==\"rcl\":     self.push(self.s)\nelif key==\"sqrt\":    self.push(math.sqrt(self.pop()))\nelif key==\"ln\":      self.push(math.log(self.pop()))", "path": "fit\\eg\\Calculator.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# XXX should throw exception or something if there was something\n#     already in the cell.\n", "func_signal": "def displayExecutor(self, cell, accessor):\n", "code": "self.executeIfFirstResult(cell)\nobj = accessor.get()\ntheString = accessor.toString(obj)\ncell.body = self.gray(theString)\nreturn None", "path": "fit\\fit\\ColumnFixture.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# most of these are updated by dependency injection either\n# in one of the runners or in the recursive invocation in doTables.\n", "func_signal": "def __init__(self):\n", "code": "self.counts = Counts()\nself.summary = {}\nself.args = []\nself.argCells = []", "path": "fit\\fit\\Fixture.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# NB, Parse makes the leader a \"\\n\" by default.\n", "func_signal": "def fixTrailers(front, back):\n", "code": "frontLast = front.last()\nfrontTrailer = frontLast.trailer\nextra = frontTrailer\n# this is clearly a special case, but why?\n# should it attempt to migrate the </body></html> to the end?\nindex = frontTrailer.find(END_BODY)\nif index >= 0:\n    extra = frontTrailer[0:index]\nif extra != \"\":\n    if back.leader in (\"\\n<br>\", \"\\n\", \"<br>\"):\n        back.leader = extra + back.leader\n    else:\n        back.leader = extra + \"<br>\" + back.leader\nfrontLast.trailer = \"\"", "path": "fit\\fitLib\\ParseUtility.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# XXX the symbol versions don't play well with the cell handlers.\n", "func_signal": "def checkSavedExecutor(self, cell, accessor):\n", "code": "self.executeIfFirstResult(cell)\nsymbolName = cell.text()\nsymbolObj = self.getSymbol(symbolName)\nsymbolString = str(symbolObj)\ncell.addToBody(self.gray(\" = \" + symbolString))\nsavedBody = cell.body\ncell.body = symbolString\nself.check(cell, accessor)\nif cell.body == symbolString:\n    cell.body = savedBody\n    return None\ncell.body = \"%s<hr>%s\" % (savedBody, cell.body)\nreturn None", "path": "fit\\fit\\ColumnFixture.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# FIXME This is scheduled to be rewritten in 0.9 (hope, hope!)\n", "func_signal": "def _createTempDotFile(self, aDir, aFile):\n", "code": "__pychecker__ = 'no-local'\nif FitGlobal.outFileName != \"\":\n    dirName = os.path.dirname(FitGlobal.outFileName)", "path": "fit\\fitLib\\DotGraphic.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# !!! note that this isn't perfect.\n#     it does not handle escape characters\n#     nor path separators inside quotes\n#     nor path separators when the target system has a different\n#      os than the FitNesse server.\n", "func_signal": "def extractPythonPath(self, pathString, opts):\n", "code": "pathString = pathString.replace(\"'\", \"\")\npathString = pathString.replace('\"', \"\")\npathList = pathString.split(os.pathsep)\npythonPath = []\nrenameFileName = \"\"\nfor path in pathList:\n    lowerpath = path.lower()\n    if (lowerpath.endswith(\".jar\") or lowerpath.endswith(\".class\")\n        or path.endswith(\".*\") or lowerpath == \"classes\"):\n        continue\n    if lowerpath.endswith(\".txt\"):\n        renameFileName = path\n        continue\n    if lowerpath.endswith(\".sym\"):\n        opts.runLevelSymbols.append(path[:-4])\n        continue\n    if (lowerpath.endswith(\".true\") or lowerpath.endswith(\".false\")\n        or lowerpath.endswith(\".z\")):\n        opts.diagnosticOptions.append(path)\n        continue\n    if lowerpath.endswith(\".std\"):\n        opts.standardsLevel = path[:-4]\n    if lowerpath.endswith(\".py\"):\n        opts.appConfigurationModule = path\n        continue\n    if lowerpath.endswith(\".parm\"):\n        opts.appConfigurationParms.append(path[:-5])\n        continue\n    pythonPath.append(path)\nreturn pythonPath, renameFileName", "path": "fit\\fitnesse\\FitServerImplementation.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# !!! the iMin etc isn't because I like hungarian notation. It's\n#     because min and max are builtin functions, and it's not\n#     good practice to name variables the same as builtins.\n", "func_signal": "def chopMonday(self, key, array):\n", "code": "iMin = 0\niMax = len(array) - 1\nwhile iMin <= iMax:\n    probe = (iMin + iMax) // 2 # !!! Requires 2.2 to force integer divide\n    if key == array[probe]:\n        return probe\n    elif key > array[probe]:\n        iMin = probe + 1\n    else:\n        iMax = probe - 1\nreturn -1", "path": "fit\\eg\\BinaryChop.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# this section handles the first table. Fixture has already\n# loaded the fixture named in the first line; rather obviously\n# or we wouldn't be here.\n# Now we need to handle the rest of the table, if there is a\n# rest of the table.\n", "func_signal": "def interpretTables(self, tables):\n", "code": "if tables.parts.more is not None:\n    # Interpret any actions in the rest of the table:\n    restOfTable = Parse(tag=\"table\", body=\"\", parts=tables.parts)\n    self.doTable(restOfTable)\n    self.listener.tableFinished(tables)\n\n# This handles all remaining tables.\ntables = tables.more\nwhile tables is not None and not (\n        self.stopOnError and self.problem(self.counts)):\n    self.interpretTable(tables)\n    self.listener.tableFinished(tables)\n    tables = tables.more\nself.listener.tablesFinished(self.counts)", "path": "fit\\fitLib\\DoFixture.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# FIXME there's no reason to _fake_ a runtime exception\n", "func_signal": "def _fakeRuntimeException(self, cell):\n", "code": "cell.error(\"Fake stack Trace\")\nself.counts.exceptions += 1", "path": "fit\\fitLib\\specify\\FixtureUnderTest.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# called either when both sizes are one, or we've run out of columns\n# to compare. The first set of tests stops the recursion; neither one\n# should trigger on the initial entry from match(...)\n", "func_signal": "def check (self, eList, cList):\n", "code": "if not eList:\n    self.surplus.extend(cList)\n    return\nif not cList:\n    self.missing.extend(eList)\n    return\n# There is at least one in each list. Process the first one,\n# and then recurse on the shorter lists\n# ??? Recursion is neat, but this might blow the stack for very\n#     large numbers of duplicates. Should I recast this routine\n#     as an iterative loop?\nrow = eList.pop(0)\ncell = row.parts\nobj = cList.pop(0)\nfor a, name, isSymRef in self.columnBindings:\n    hasKey, value = self._getObj(name, obj)\n    if not hasKey:\n        self.ignore(cell)\n    elif a is None:\n        self.checkString(cell, value)\n    else:\n        a.target = obj\n        a.set(value)\n        if isSymRef:\n            self._checkRef(a, cell, value)\n        else:\n            super(RowFixture, self).check(cell, a)\n    cell = cell.more\nself.check(eList, cList)", "path": "fit\\fit\\RowFixture.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# ??? Do we want to provide an option to avoid checking?\n", "func_signal": "def _isFieldInAnyObject(self, fieldName, actuals):\n", "code": "if len(actuals) == 0:\n    return True\nfor objectOrDict in actuals:\n    hasKey, obj = self._getObj(fieldName, objectOrDict)\n    if hasKey:\n        return True\nreturn False", "path": "fit\\fit\\RowFixture.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "#this.table = new Parse (\"table\", null, copy(rows), null);\n#// evaluate the rest of the table like a runner\n#(new Fixture()).doTables(this.table);\n", "func_signal": "def doRows(self, rows):\n", "code": "table = Parse(tag=\"table\", body=\"\", parts=self.copy(rows), more=None)\nself.setSymbol(\"Table\", table)\nFixture().doTables(table)", "path": "fit\\fat\\Table.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# ??? should cell be here?\n", "func_signal": "def handleBlankCell(self, unused='cell'):\n", "code": "result = None\ntry:\n    result = self.get()\n    checkResult = CheckResult_Info(self.toString(result))\nexcept Exception:\n    checkResult = CheckResult_Info(\"error\")\ncheckResult.value = result\nreturn checkResult", "path": "fit\\fit\\TypeAdapter.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# ??? should we avoid adding elements that are already there?\n", "func_signal": "def establishCommonTopLevelOptions(self, opts, pythonPath):\n", "code": "if pythonPath:\n    sys.path[1:1] = pythonPath\nFG.RunOptions = opts\nif opts.standardsLevel in (\"1.1\", \"2.0\"):\n    FG.SpecificationLevel = opts.standardsLevel\nself._extractDiagnosticOptions(opts, FG.RunDiagnosticOptions)\nself._extractRunLevelSymbols(opts, FG.RunLevelSymbols)\nself._loadAppConfigurationModule(opts)\nVariations.returnVariation()\nreturn", "path": "fit\\fitnesse\\FitServerImplementation.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "#this.table = new Parse (\"table\", null, copy(rows), null);\n#// evaluate the rest of the table like a runner\n#(new Fixture()).doTables(this.table);\n", "func_signal": "def doRows(self, rows):\n", "code": "table = Parse(tag=\"table\", body=\"\", parts=self.copy(rows), more=None)\nself.setSymbol(\"Table\", table)\nFixture().doTables(table)", "path": "fit\\fat\\Fit1_1Tests\\Table.py", "repo_name": "mrc/pyfit", "stars": 5, "license": "None", "language": "python", "size": 832}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = json.loads(JSON)\nout = json.dumps(res)\nself.assertEquals(res, json.loads(out))", "path": "simplejson\\tests\\test_pass3.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "# Every mask is 7 days longer to handle cross-year weekly periods.\n", "func_signal": "def rebuild(self, year, month):\n", "code": "rr = self.rrule\nif year != self.lastyear:\n    self.yearlen = 365+calendar.isleap(year)\n    self.nextyearlen = 365+calendar.isleap(year+1)\n    firstyday = datetime.date(year, 1, 1)\n    self.yearordinal = firstyday.toordinal()\n    self.yearweekday = firstyday.weekday()\n\n    wday = datetime.date(year, 1, 1).weekday()\n    if self.yearlen == 365:\n        self.mmask = M365MASK\n        self.mdaymask = MDAY365MASK\n        self.nmdaymask = NMDAY365MASK\n        self.wdaymask = WDAYMASK[wday:]\n        self.mrange = M365RANGE\n    else:\n        self.mmask = M366MASK\n        self.mdaymask = MDAY366MASK\n        self.nmdaymask = NMDAY366MASK\n        self.wdaymask = WDAYMASK[wday:]\n        self.mrange = M366RANGE\n\n    if not rr._byweekno:\n        self.wnomask = None\n    else:\n        self.wnomask = [0]*(self.yearlen+7)\n        #no1wkst = firstwkst = self.wdaymask.index(rr._wkst)\n        no1wkst = firstwkst = (7-self.yearweekday+rr._wkst)%7\n        if no1wkst >= 4:\n            no1wkst = 0\n            # Number of days in the year, plus the days we got\n            # from last year.\n            wyearlen = self.yearlen+(self.yearweekday-rr._wkst)%7\n        else:\n            # Number of days in the year, minus the days we\n            # left in last year.\n            wyearlen = self.yearlen-no1wkst\n        div, mod = divmod(wyearlen, 7)\n        numweeks = div+mod//4\n        for n in rr._byweekno:\n            if n < 0:\n                n += numweeks+1\n            if not (0 < n <= numweeks):\n                continue\n            if n > 1:\n                i = no1wkst+(n-1)*7\n                if no1wkst != firstwkst:\n                    i -= 7-firstwkst\n            else:\n                i = no1wkst\n            for j in range(7):\n                self.wnomask[i] = 1\n                i += 1\n                if self.wdaymask[i] == rr._wkst:\n                    break\n        if 1 in rr._byweekno:\n            # Check week number 1 of next year as well\n            # TODO: Check -numweeks for next year.\n            i = no1wkst+numweeks*7\n            if no1wkst != firstwkst:\n                i -= 7-firstwkst\n            if i < self.yearlen:\n                # If week starts in next year, we\n                # don't care about it.\n                for j in range(7):\n                    self.wnomask[i] = 1\n                    i += 1\n                    if self.wdaymask[i] == rr._wkst:\n                        break\n        if no1wkst:\n            # Check last week number of last year as\n            # well. If no1wkst is 0, either the year\n            # started on week start, or week number 1\n            # got days from last year, so there are no\n            # days from last year's last week number in\n            # this year.\n            if -1 not in rr._byweekno:\n                lyearweekday = datetime.date(year-1,1,1).weekday()\n                lno1wkst = (7-lyearweekday+rr._wkst)%7\n                lyearlen = 365+calendar.isleap(year-1)\n                if lno1wkst >= 4:\n                    lno1wkst = 0\n                    lnumweeks = 52+(lyearlen+\n                                   (lyearweekday-rr._wkst)%7)%7//4\n                else:\n                    lnumweeks = 52+(self.yearlen-no1wkst)%7//4\n            else:\n                lnumweeks = -1\n            if lnumweeks in rr._byweekno:\n                for i in range(no1wkst):\n                    self.wnomask[i] = 1\n\nif (rr._bynweekday and\n    (month != self.lastmonth or year != self.lastyear)):\n    ranges = []\n    if rr._freq == YEARLY:\n        if rr._bymonth:\n            for month in rr._bymonth:\n                ranges.append(self.mrange[month-1:month+1])\n        else:\n            ranges = [(0, self.yearlen)]\n    elif rr._freq == MONTHLY:\n        ranges = [self.mrange[month-1:month+1]]\n    if ranges:\n        # Weekly frequency won't get here, so we may not\n        # care about cross-year weekly periods.\n        self.nwdaymask = [0]*self.yearlen\n        for first, last in ranges:\n            last -= 1\n            for wday, n in rr._bynweekday:\n                if n < 0:\n                    i = last+(n+1)*7\n                    i -= (self.wdaymask[i]-wday)%7\n                else:\n                    i = first+(n-1)*7\n                    i += (7-self.wdaymask[i]+wday)%7\n                if first <= i <= last:\n                    self.nwdaymask[i] = 1\n\nif rr._byeaster:\n    self.eastermask = [0]*(self.yearlen+7)\n    eyday = easter.easter(year).toordinal()-self.yearordinal\n    for offset in rr._byeaster:\n        self.eastermask[eyday+offset] = 1\n\nself.lastyear = year\nself.lastmonth = month", "path": "dateutil\\rrule.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "'''Create only one instance of each distinct datetime'''\n", "func_signal": "def memorized_datetime(seconds):\n", "code": "try:\n    return _datetime_cache[seconds]\nexcept KeyError:\n    # NB. We can't just do datetime.utcfromtimestamp(seconds) as this\n    # fails with negative values under Windows (Bug #90096)\n    dt = _epoch + timedelta(seconds=seconds)\n    _datetime_cache[seconds] = dt\n    return dt", "path": "pytz\\tzinfo.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "# move to info\n", "func_signal": "def validate(self, res):\n", "code": "if res.year is not None:\n    res.year = self.convertyear(res.year)\nif res.tzoffset == 0 and not res.tzname or res.tzname == 'Z':\n    res.tzname = \"UTC\"\n    res.tzoffset = 0\nelif res.tzoffset != 0 and res.tzname and self.utczone(res.tzname):\n    res.tzoffset = 0\nreturn True", "path": "dateutil\\parser.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "\"\"\"Decode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning\nwith a JSON document) and return a 2-tuple of the Python\nrepresentation and the index in ``s`` where the document ended.\n\nThis can be used to decode a JSON document from a string that may\nhave extraneous data at the end.\n\n\"\"\"\n", "func_signal": "def raw_decode(self, s, idx=0):\n", "code": "try:\n    obj, end = self.scan_once(s, idx)\nexcept StopIteration:\n    raise ValueError(\"No JSON object could be decoded\")\nreturn obj, end", "path": "simplejson\\decoder.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "'''Create only one instance of each distinct timedelta'''\n", "func_signal": "def memorized_timedelta(seconds):\n", "code": "try:\n    return _timedelta_cache[seconds]\nexcept KeyError:\n    delta = timedelta(seconds=seconds)\n    _timedelta_cache[seconds] = delta\n    return delta", "path": "pytz\\tzinfo.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "# We can't use mktime here. It is unstable when deciding if\n# the hour near to a change is DST or not.\n# \n# timestamp = time.mktime((dt.year, dt.month, dt.day, dt.hour,\n#                         dt.minute, dt.second, dt.weekday(), 0, -1))\n# return time.localtime(timestamp).tm_isdst\n#\n# The code above yields the following result:\n#\n#>>> import tz, datetime\n#>>> t = tz.tzlocal()\n#>>> datetime.datetime(2003,2,15,23,tzinfo=t).tzname()\n#'BRDT'\n#>>> datetime.datetime(2003,2,16,0,tzinfo=t).tzname()\n#'BRST'\n#>>> datetime.datetime(2003,2,15,23,tzinfo=t).tzname()\n#'BRST'\n#>>> datetime.datetime(2003,2,15,22,tzinfo=t).tzname()\n#'BRDT'\n#>>> datetime.datetime(2003,2,15,23,tzinfo=t).tzname()\n#'BRDT'\n#\n# Here is a more stable implementation:\n#\n", "func_signal": "def _isdst(self, dt):\n", "code": "timestamp = ((dt.toordinal() - EPOCHORDINAL) * 86400\n             + dt.hour * 3600\n             + dt.minute * 60\n             + dt.second)\nreturn time.localtime(timestamp+time.timezone).tm_isdst", "path": "dateutil\\tz.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "\"\"\"Return true if the given resource exists\"\"\"\n", "func_signal": "def resource_exists(name):\n", "code": "try:\n    open_resource(name)\n    return True\nexcept IOError:\n    return False", "path": "pytz\\__init__.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "\"\"\"Scan the string s for a JSON string. End is the index of the\ncharacter in s after the quote that started the JSON string.\nUnescapes all valid JSON string escape sequences and raises ValueError\non attempt to decode an invalid string. If strict is False then literal\ncontrol characters are allowed in the string.\n\nReturns a tuple of the decoded string and the index of the character in s\nafter the end quote.\"\"\"\n", "func_signal": "def py_scanstring(s, end, encoding=None, strict=True, _b=BACKSLASH, _m=STRINGCHUNK.match):\n", "code": "if encoding is None:\n    encoding = DEFAULT_ENCODING\nchunks = []\n_append = chunks.append\nbegin = end - 1\nwhile 1:\n    chunk = _m(s, end)\n    if chunk is None:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    end = chunk.end()\n    content, terminator = chunk.groups()\n    # Content is contains zero or more unescaped string characters\n    if content:\n        if not isinstance(content, unicode):\n            content = unicode(content, encoding)\n        _append(content)\n    # Terminator is the end of string, a literal control character,\n    # or a backslash denoting that an escape sequence follows\n    if terminator == '\"':\n        break\n    elif terminator != '\\\\':\n        if strict:\n            msg = \"Invalid control character %r at\" % (terminator,)\n            #msg = \"Invalid control character {0!r} at\".format(terminator)\n            raise ValueError(errmsg(msg, s, end))\n        else:\n            _append(terminator)\n            continue\n    try:\n        esc = s[end]\n    except IndexError:\n        raise ValueError(\n            errmsg(\"Unterminated string starting at\", s, begin))\n    # If not a unicode escape sequence, must be in the lookup table\n    if esc != 'u':\n        try:\n            char = _b[esc]\n        except KeyError:\n            msg = \"Invalid \\\\escape: \" + repr(esc)\n            raise ValueError(errmsg(msg, s, end))\n        end += 1\n    else:\n        # Unicode escape sequence\n        esc = s[end + 1:end + 5]\n        next_end = end + 5\n        if len(esc) != 4:\n            msg = \"Invalid \\\\uXXXX escape\"\n            raise ValueError(errmsg(msg, s, end))\n        uni = int(esc, 16)\n        # Check for surrogate pair on UCS-4 systems\n        if 0xd800 <= uni <= 0xdbff and sys.maxunicode > 65535:\n            msg = \"Invalid \\\\uXXXX\\\\uXXXX surrogate pair\"\n            if not s[end + 5:end + 7] == '\\\\u':\n                raise ValueError(errmsg(msg, s, end))\n            esc2 = s[end + 7:end + 11]\n            if len(esc2) != 4:\n                raise ValueError(errmsg(msg, s, end))\n            uni2 = int(esc2, 16)\n            uni = 0x10000 + (((uni - 0xd800) << 10) | (uni2 - 0xdc00))\n            next_end += 6\n        char = unichr(uni)\n        end = next_end\n    # Append the unescaped character\n    _append(char)\nreturn u''.join(chunks), end", "path": "simplejson\\decoder.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "\"\"\"Return the Python representation of ``s`` (a ``str`` or ``unicode``\ninstance containing a JSON document)\n\n\"\"\"\n", "func_signal": "def decode(self, s, _w=WHITESPACE.match):\n", "code": "obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nend = _w(s, end).end()\nif end != len(s):\n    raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\nreturn obj", "path": "simplejson\\decoder.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = json.loads(JSON)\nout = json.dumps(res)\nself.assertEquals(res, json.loads(out))\ntry:\n    json.dumps(res, allow_nan=False)\nexcept ValueError:\n    pass\nelse:\n    self.fail(\"23456789012E666 should be out of range\")", "path": "simplejson\\tests\\test_pass1.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "'''Create only one instance of each distinct tuple'''\n", "func_signal": "def memorized_ttinfo(*args):\n", "code": "try:\n    return _ttinfo_cache[args]\nexcept KeyError:\n    ttinfo = (\n            memorized_timedelta(args[0]),\n            memorized_timedelta(args[1]),\n            args[2]\n            )\n    _ttinfo_cache[args] = ttinfo\n    return ttinfo", "path": "pytz\\tzinfo.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "\"\"\"Open a resource from the zoneinfo subdir for reading.\n\nUses the pkg_resources module if available and no standard file\nfound at the calculated location.\n\"\"\"\n", "func_signal": "def open_resource(name):\n", "code": "name_parts = name.lstrip('/').split('/')\nfor part in name_parts:\n    if part == os.path.pardir or os.path.sep in part:\n        raise ValueError('Bad path segment: %r' % part)\nfilename = os.path.join(os.path.dirname(__file__),\n                        'zoneinfo', *name_parts)\nif not os.path.exists(filename) and resource_stream is not None:\n    # http://bugs.launchpad.net/bugs/383171 - we avoid using this\n    # unless absolutely necessary to help when a broken version of\n    # pkg_resources is installed.\n    return resource_stream(__name__, 'zoneinfo/' + name)\nreturn open(filename, 'rb')", "path": "pytz\\__init__.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "# We need to handle cross-year weeks here.\n", "func_signal": "def wdayset(self, year, month, day):\n", "code": "set = [None]*(self.yearlen+7)\ni = datetime.date(year, month, day).toordinal()-self.yearordinal\nstart = i\nfor j in range(7):\n    set[i] = i\n    i += 1\n    #if (not (0 <= i < self.yearlen) or\n    #    self.wdaymask[i] == self.rrule._wkst):\n    # This will cross the year boundary, if necessary.\n    if self.wdaymask[i] == self.rrule._wkst:\n        break\nreturn set, start, i", "path": "dateutil\\rrule.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "# Note that this function is called from _speedups\n", "func_signal": "def errmsg(msg, doc, pos, end=None):\n", "code": "lineno, colno = linecol(doc, pos)\nif end is None:\n    #fmt = '{0}: line {1} column {2} (char {3})'\n    #return fmt.format(msg, lineno, colno, pos)\n    fmt = '%s: line %d column %d (char %d)'\n    return fmt % (msg, lineno, colno, pos)\nendlineno, endcolno = linecol(doc, end)\n#fmt = '{0}: line {1} column {2} - line {3} column {4} (char {5} - {6})'\n#return fmt.format(msg, lineno, colno, endlineno, endcolno, pos, end)\nfmt = '%s: line %d column %d - line %d column %d (char %d - %d)'\nreturn fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)", "path": "simplejson\\decoder.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "# Special pickle to zone remains a singleton and to cope with\n# database changes.\n", "func_signal": "def __reduce__(self):\n", "code": "return pytz._p, (\n        self.zone,\n        _to_seconds(self._utcoffset),\n        _to_seconds(self._dst),\n        self._tzname\n        )", "path": "pytz\\tzinfo.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "\"\"\"Parse a I[.F] seconds value into (seconds, microseconds).\"\"\"\n", "func_signal": "def _parsems(value):\n", "code": "if \".\" not in value:\n    return int(value), 0\nelse:\n    i, f = value.split(\".\")\n    return int(i), int(f.ljust(6, \"0\")[:6])", "path": "dateutil\\parser.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "# Several optimizations were made that skip over calls to\n# the whitespace regex, so this test is designed to try and\n# exercise the uncommon cases. The array cases are already covered.\n", "func_signal": "def test_decoder_optimizations(self):\n", "code": "rval = json.loads('{   \"key\"    :    \"value\"    ,  \"k\":\"v\"    }')\nself.assertEquals(rval, {\"key\":\"value\", \"k\":\"v\"})", "path": "simplejson\\tests\\test_decode.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "\"\"\"return a fixed-offset timezone based off a number of minutes.\n\n    >>> one = FixedOffset(-330)\n    >>> one\n    pytz.FixedOffset(-330)\n    >>> one.utcoffset(datetime.datetime.now())\n    datetime.timedelta(-1, 66600)\n\n    >>> two = FixedOffset(1380)\n    >>> two\n    pytz.FixedOffset(1380)\n    >>> two.utcoffset(datetime.datetime.now())\n    datetime.timedelta(0, 82800)\n\nThe datetime.timedelta must be between the range of -1 and 1 day,\nnon-inclusive.\n\n    >>> FixedOffset(1440)\n    Traceback (most recent call last):\n    ...\n    ValueError: ('absolute offset is too large', 1440)\n\n    >>> FixedOffset(-1440)\n    Traceback (most recent call last):\n    ...\n    ValueError: ('absolute offset is too large', -1440)\n\nAn offset of 0 is special-cased to return UTC.\n\n    >>> FixedOffset(0) is UTC\n    True\n\nThere should always be only one instance of a FixedOffset per timedelta.\nThis should be true for multiple creation calls.\n\n    >>> FixedOffset(-330) is one\n    True\n    >>> FixedOffset(1380) is two\n    True\n\nIt should also be true for pickling.\n\n    >>> import pickle\n    >>> pickle.loads(pickle.dumps(one)) is one\n    True\n    >>> pickle.loads(pickle.dumps(two)) is two\n    True\n\"\"\"\n", "func_signal": "def FixedOffset(offset, _tzinfos = {}):\n", "code": "if offset == 0:\n    return UTC\n\ninfo = _tzinfos.get(offset)\nif info is None:\n    # We haven't seen this one before. we need to save it.\n\n    # Use setdefault to avoid a race condition and make sure we have\n    # only one\n    info = _tzinfos.setdefault(offset, _FixedOffset(offset))\n\nreturn info", "path": "pytz\\__init__.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "\"\"\"Factory function for unpickling pytz tzinfo instances.\n\nThis is shared for both StaticTzInfo and DstTzInfo instances, because\ndatabase changes could cause a zones implementation to switch between\nthese two base classes and we can't break pickles on a pytz version\nupgrade.\n\"\"\"\n# Raises a KeyError if zone no longer exists, which should never happen\n# and would be a bug.\n", "func_signal": "def unpickler(zone, utcoffset=None, dstoffset=None, tzname=None):\n", "code": "tz = pytz.timezone(zone)\n\n# A StaticTzInfo - just return it\nif utcoffset is None:\n    return tz\n\n# This pickle was created from a DstTzInfo. We need to\n# determine which of the list of tzinfo instances for this zone\n# to use in order to restore the state of any datetime instances using\n# it correctly.\nutcoffset = memorized_timedelta(utcoffset)\ndstoffset = memorized_timedelta(dstoffset)\ntry:\n    return tz._tzinfos[(utcoffset, dstoffset, tzname)]\nexcept KeyError:\n    # The particular state requested in this timezone no longer exists.\n    # This indicates a corrupt pickle, or the timezone database has been\n    # corrected violently enough to make this particular\n    # (utcoffset,dstoffset) no longer exist in the zone, or the\n    # abbreviation has been changed.\n    pass\n\n# See if we can find an entry differing only by tzname. Abbreviations\n# get changed from the initial guess by the database maintainers to\n# match reality when this information is discovered.\nfor localized_tz in tz._tzinfos.values():\n    if (localized_tz._utcoffset == utcoffset\n            and localized_tz._dst == dstoffset):\n        return localized_tz\n\n# This (utcoffset, dstoffset) information has been removed from the\n# zone. Add it back. This might occur when the database maintainers have\n# corrected incorrect information. datetime instances using this\n# incorrect information will continue to do so, exactly as they were\n# before being pickled. This is purely an overly paranoid safety net - I\n# doubt this will ever been needed in real life.\ninf = (utcoffset, dstoffset, tzname)\ntz._tzinfos[inf] = tz.__class__(inf, tz._tzinfos)\nreturn tz._tzinfos[inf]", "path": "pytz\\tzinfo.py", "repo_name": "themattharris/json-dsttime", "stars": 7, "license": "mit", "language": "python", "size": 563}
{"docstring": "\"\"\"\nTest that the registration view rejects invalid submissions,\nand creates a new user and redirects after a valid submission.\n\n\"\"\"\n# Invalid data fails.\n", "func_signal": "def test_registration_view(self):\n", "code": "response = self.client.post(reverse('registration_register'),\n                            data={ 'username': 'alice', # Will fail on username uniqueness.\n                                   'email': 'foo@example.com',\n                                   'password1': 'foo',\n                                   'password2': 'foo' })\nself.assertEqual(response.status_code, 200)\nself.failUnless(response.context['form'])\nself.failUnless(response.context['form'].errors)\n\nresponse = self.client.post(reverse('registration_register'),\n                            data={ 'username': 'foo',\n                                   'email': 'foo@example.com',\n                                   'password1': 'foo',\n                                   'password2': 'foo' })\nself.assertEqual(response.status_code, 302)\nself.assertEqual(response['Location'], 'http://testserver%s' % reverse('registration_complete'))\nself.assertEqual(RegistrationProfile.objects.count(), 3)", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nCheck the supplied email address against a list of known free\nwebmail domains.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email_domain = self.cleaned_data['email'].split('@')[1]\nif email_domain in self.bad_domains:\n    raise forms.ValidationError(_(u'Registration using free email addresses is prohibited. Please supply a different email address.'))\nreturn self.cleaned_data['email']", "path": "registration\\forms.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that ``RegistrationProfile.activation_key_expired()``\nreturns ``True`` for expired users and for active users, and\n``False`` otherwise.\n\n\"\"\"\n# Unexpired user returns False.\n", "func_signal": "def test_account_expiration_condition(self):\n", "code": "self.failIf(RegistrationProfile.objects.get(user=self.sample_user).activation_key_expired())\n\n# Expired user returns True.\nself.failUnless(RegistrationProfile.objects.get(user=self.expired_user).activation_key_expired())\n\n# Activated user returns True.\nRegistrationProfile.objects.activate_user(RegistrationProfile.objects.get(user=self.sample_user).activation_key)\nself.failUnless(RegistrationProfile.objects.get(user=self.sample_user).activation_key_expired())", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that user activation actually activates the user and\nproperly resets the activation key, and fails for an\nalready-active or expired user, or an invalid key.\n\n\"\"\"\n# Activating a valid user returns the user.\n", "func_signal": "def test_activation(self):\n", "code": "self.failUnlessEqual(RegistrationProfile.objects.activate_user(RegistrationProfile.objects.get(user=self.sample_user).activation_key).pk,\n                     self.sample_user.pk)\n\n# The activated user must now be active.\nself.failUnless(user_model.objects.get(pk=self.sample_user.pk).is_active)\n\n# The activation key must now be reset to the \"already activated\" constant.\nself.failUnlessEqual(RegistrationProfile.objects.get(user=self.sample_user).activation_key,\n                     RegistrationProfile.ACTIVATED)\n\n# Activating an expired user returns False.\nself.failIf(RegistrationProfile.objects.activate_user(RegistrationProfile.objects.get(user=self.expired_user).activation_key))\n\n# Activating from a key that isn't a SHA1 hash returns False.\nself.failIf(RegistrationProfile.objects.activate_user('foo'))\n\n# Activating from a key that doesn't exist returns False.\nself.failIf(RegistrationProfile.objects.activate_user(sha.new('foo').hexdigest()))", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nValidate an activation key and activate the corresponding\n``user_model`` if valid.\n\nIf the key is valid and has not expired, return the ``user_model``\nafter activating.\n\nIf the key is not valid or has expired, return ``False``.\n\nIf the key is valid but the ``user_model`` is already active,\nreturn ``False``.\n\nTo prevent reactivation of an account which has been\ndeactivated by site administrators, the activation key is\nreset to the string constant ``RegistrationProfile.ACTIVATED``\nafter successful activation.\n\nTo execute customized logic when a ``user_model`` is activated,\nconnect a function to the signal\n``registration.signals.user_activated``; this signal will be\nsent (with the ``user_model`` as the value of the keyword argument\n``user``) after a successful activation.\n\n\"\"\"\n", "func_signal": "def activate_user(self, activation_key):\n", "code": "from registration.signals import user_activated\n\n# Make sure the key we're trying conforms to the pattern of a\n# SHA1 hash; if it doesn't, no point trying to look it up in\n# the database.\nif SHA1_RE.search(activation_key):\n    try:\n        profile = self.get(activation_key=activation_key)\n    except self.model.DoesNotExist:\n        return False\n    if not profile.activation_key_expired():\n        user = profile.user\n        user.is_active = True\n        user.save()\n        profile.activation_key = self.model.ACTIVATED\n        profile.save()\n        user_activated.send(sender=self.model, user=user)\n        return user\nreturn False", "path": "registration\\models.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that ``RegistrationFormTermsOfService`` requires\nagreement to the terms of service.\n\n\"\"\"\n", "func_signal": "def test_registration_form_tos(self):\n", "code": "form = forms.RegistrationFormTermsOfService(data={ 'username': 'foo',\n                                                   'email': 'foo@example.com',\n                                                   'password1': 'foo',\n                                                   'password2': 'foo' })\nself.failIf(form.is_valid())\nself.assertEqual(form.errors['tos'], [u\"You must agree to the terms to register\"])\n\nform = forms.RegistrationFormTermsOfService(data={ 'username': 'foo',\n                                                   'email': 'foo@example.com',\n                                                   'password1': 'foo',\n                                                   'password2': 'foo',\n                                                   'tos': 'on' })\nself.failUnless(form.is_valid())", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nDetermine whether this ``RegistrationProfile``'s activation\nkey has expired, returning a boolean -- ``True`` if the key\nhas expired.\n\nKey expiration is determined by a two-step process:\n\n1. If the user has already activated, the key will have been\n   reset to the string constant ``ACTIVATED``. Re-activating\n   is not permitted, and so this method returns ``True`` in\n   this case.\n\n2. Otherwise, the date the user signed up is incremented by\n   the number of days specified in the setting\n   ``ACCOUNT_ACTIVATION_DAYS`` (which should be the number of\n   days after signup during which a user is allowed to\n   activate their account); if the result is less than or\n   equal to the current date, the key has expired and this\n   method returns ``True``.\n\n\"\"\"\n", "func_signal": "def activation_key_expired(self):\n", "code": "expiration_date = datetime.timedelta(days=settings.ACCOUNT_ACTIVATION_DAYS)\nreturn self.activation_key == self.ACTIVATED or \\\n       (self.user.date_joined + expiration_date <= datetime.datetime.now())", "path": "registration\\models.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that\n``RegistrationProfile.objects.delete_expired_users()`` deletes\nonly inactive users whose activation window has expired.\n\n\"\"\"\n", "func_signal": "def test_expired_user_deletion(self):\n", "code": "RegistrationProfile.objects.delete_expired_users()\nself.assertEqual(RegistrationProfile.objects.count(), 1)", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that ``RegistrationFormUniqueEmail`` validates uniqueness\nof email addresses.\n\n\"\"\"\n", "func_signal": "def test_registration_form_unique_email(self):\n", "code": "form = forms.RegistrationFormUniqueEmail(data={ 'username': 'foo',\n                                                'email': 'alice@example.com',\n                                                'password1': 'foo',\n                                                'password2': 'foo' })\nself.failIf(form.is_valid())\nself.assertEqual(form.errors['email'], [u\"This email address is already in use. Please supply a different email address.\"])\n\nform = forms.RegistrationFormUniqueEmail(data={ 'username': 'foo',\n                                                'email': 'foo@example.com',\n                                                'password1': 'foo',\n                                                'password2': 'foo' })\nself.failUnless(form.is_valid())", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nVerifiy that the values entered into the two password fields\nmatch. Note that an error here will end up in\n``non_field_errors()`` because it doesn't apply to a single\nfield.\n\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "if 'password1' in self.cleaned_data and 'password2' in self.cleaned_data:\n    if self.cleaned_data['password1'] != self.cleaned_data['password2']:\n        raise forms.ValidationError(_(u'You must type the same password each time'))\nreturn self.cleaned_data", "path": "registration\\forms.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that the ``user_registered`` and ``user_activated``\nsignals are sent, and that they send the ``user_model`` as an\nargument.\n\n\"\"\"\n", "func_signal": "def test_signals(self):\n", "code": "def receiver(sender, **kwargs):\n    self.assert_('user' in kwargs)\n    self.assertEqual(kwargs['user'].username, u'signal_test')\n    received_signals.append(kwargs.get('signal'))\n\nreceived_signals = []\nexpected_signals = [signals.user_registered, signals.user_activated]\nfor signal in expected_signals:\n    signal.connect(receiver)\n\nRegistrationProfile.objects.create_inactive_user(username='signal_test',\n                                                 password='foo',\n                                                 email='nobody@example.com',\n                                                 send_email=False)\nRegistrationProfile.objects.activate_user(RegistrationProfile.objects.get(user__username='signal_test').activation_key)\n\nself.assertEqual(received_signals, expected_signals)", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nValidate that the supplied email address is unique for the\nsite.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "if user_model.objects.filter(email__iexact=self.cleaned_data['email']):\n    raise forms.ValidationError(_(u'This email address is already in use. Please supply a different email address.'))\nreturn self.cleaned_data['email']", "path": "registration\\forms.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nRemove expired instances of ``RegistrationProfile`` and their\nassociated ``user_model``s.\n\nAccounts to be deleted are identified by searching for\ninstances of ``RegistrationProfile`` with expired activation\nkeys, and then checking to see if their associated ``user_model``\ninstances have the field ``is_active`` set to ``False``; any\n``user_model`` who is both inactive and has an expired activation\nkey will be deleted.\n\nIt is recommended that this method be executed regularly as\npart of your routine site maintenance; this application\nprovides a custom management command which will call this\nmethod, accessible as ``manage.py cleanupregistration``.\n\nRegularly clearing out accounts which have never been\nactivated serves two useful purposes:\n\n1. It alleviates the ocasional need to reset a\n   ``RegistrationProfile`` and/or re-send an activation email\n   when a user does not receive or does not act upon the\n   initial activation email; since the account will be\n   deleted, the user will be able to simply re-register and\n   receive a new activation key.\n\n2. It prevents the possibility of a malicious user registering\n   one or more accounts and never activating them (thus\n   denying the use of those usernames to anyone else); since\n   those accounts will be deleted, the usernames will become\n   available for use again.\n\nIf you have a troublesome ``user_model`` and wish to disable their\naccount while keeping it in the database, simply delete the\nassociated ``RegistrationProfile``; an inactive ``user_model`` which\ndoes not have an associated ``RegistrationProfile`` will not\nbe deleted.\n\n\"\"\"\n", "func_signal": "def delete_expired_users(self):\n", "code": "for profile in self.all():\n    if profile.activation_key_expired():\n        user = profile.user\n        if not user.is_active:\n            user.delete()", "path": "registration\\models.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nCreate a ``RegistrationProfile`` for a given\n``user_model``, and return the ``RegistrationProfile``.\n\nThe activation key for the ``RegistrationProfile`` will be a\nSHA1 hash, generated from a combination of the ``user_model``'s\nusername and a random salt.\n\n\"\"\"\n", "func_signal": "def create_profile(self, user):\n", "code": "salt = sha_constructor(str(random.random())).hexdigest()[:5]\nactivation_key = sha_constructor(salt+user.username).hexdigest()\nreturn self.create(user=user,\n                   activation_key=activation_key)", "path": "registration\\models.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that activation email can be disabled.\n\n\"\"\"\n", "func_signal": "def test_activation_email_disable(self):\n", "code": "RegistrationProfile.objects.create_inactive_user(username='noemail',\n                                                 password='foo',\n                                                 email='nobody@example.com',\n                                                 send_email=False)\nself.assertEqual(len(mail.outbox), 2)", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that ``RegistrationFormNoFreeEmail`` disallows\nregistration with free email addresses.\n\n\"\"\"\n", "func_signal": "def test_registration_form_no_free_email(self):\n", "code": "base_data = { 'username': 'foo',\n              'password1': 'foo',\n              'password2': 'foo' }\nfor domain in ('aim.com', 'aol.com', 'email.com', 'gmail.com',\n               'googlemail.com', 'hotmail.com', 'hushmail.com',\n               'msn.com', 'mail.ru', 'mailinator.com', 'live.com'):\n    invalid_data = base_data.copy()\n    invalid_data['email'] = u\"foo@%s\" % domain\n    form = forms.RegistrationFormNoFreeEmail(data=invalid_data)\n    self.failIf(form.is_valid())\n    self.assertEqual(form.errors['email'], [u\"Registration using free email addresses is prohibited. Please supply a different email address.\"])\n\nbase_data['email'] = 'foo@example.com'\nform = forms.RegistrationFormNoFreeEmail(data=base_data)\nself.failUnless(form.is_valid())", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nValidate that the username is alphanumeric and is not already\nin use.\n\n\"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "try:\n    user = user_model.objects.get(username__iexact=self.cleaned_data['username'])\nexcept user_model.DoesNotExist:\n    return self.cleaned_data['username']\nraise forms.ValidationError(_(u'This username is already taken. Please choose another.'))", "path": "registration\\forms.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that ``RegistrationForm`` enforces username constraints\nand matching passwords.\n\n\"\"\"\n", "func_signal": "def test_registration_form(self):\n", "code": "invalid_data_dicts = [\n    # Non-alphanumeric username.\n    {\n    'data':\n    { 'username': 'foo/bar',\n      'email': 'foo@example.com',\n      'password1': 'foo',\n      'password2': 'foo' },\n    'error':\n    ('username', [u\"Enter a valid value.\"])\n    },\n    # Already-existing username.\n    {\n    'data':\n    { 'username': 'alice',\n      'email': 'alice@example.com',\n      'password1': 'secret',\n      'password2': 'secret' },\n    'error':\n    ('username', [u\"This username is already taken. Please choose another.\"])\n    },\n    # Mismatched passwords.\n    {\n    'data':\n    { 'username': 'foo',\n      'email': 'foo@example.com',\n      'password1': 'foo',\n      'password2': 'bar' },\n    'error':\n    ('__all__', [u\"You must type the same password each time\"])\n    },\n    ]\n\nfor invalid_dict in invalid_data_dicts:\n    form = forms.RegistrationForm(data=invalid_dict['data'])\n    self.failIf(form.is_valid())\n    self.assertEqual(form.errors[invalid_dict['error'][0]], invalid_dict['error'][1])\n\nform = forms.RegistrationForm(data={ 'username': 'foo',\n                                     'email': 'foo@example.com',\n                                     'password1': 'foo',\n                                     'password2': 'foo' })\nself.failUnless(form.is_valid())", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that the activation view activates the user from a valid\nkey and fails if the key is invalid or has expired.\n       \n\"\"\"\n# Valid user puts the user account into the context.\n", "func_signal": "def test_activation_view(self):\n", "code": "response = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': RegistrationProfile.objects.get(user=self.sample_user).activation_key }))\nself.assertEqual(response.status_code, 200)\nself.assertEqual(response.context['account'].pk, self.sample_user.pk)\n\n# Expired user sets the account to False.\nresponse = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': RegistrationProfile.objects.get(user=self.expired_user).activation_key }))\nself.failIf(response.context['account'])\n\n# Invalid key gets to the view, but sets account to False.\nresponse = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': 'foo' }))\nself.failIf(response.context['account'])\n\n# Nonexistent key sets the account to False.\nresponse = self.client.get(reverse('registration_activate',\n                                   kwargs={ 'activation_key': sha.new('foo').hexdigest() }))\nself.failIf(response.context['account'])", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"\nTest that ``manage.py cleanupregistration`` functions\ncorrectly.\n\n\"\"\"\n", "func_signal": "def test_management_command(self):\n", "code": "management.call_command('cleanupregistration')\nself.assertEqual(RegistrationProfile.objects.count(), 1)", "path": "registration\\tests.py", "repo_name": "incuna/django-registration", "stars": 4, "license": "bsd-3-clause", "language": "python", "size": 517}
{"docstring": "\"\"\"Return count of all links, or specific links if\n`other_extent_name` and `other_field_name` are supplied.\"\"\"\n", "func_signal": "def count(self, other_extent_name=None, other_field_name=None):\n", "code": "i = self._i\nreturn i._db._entity_links(i._extent.name, i._oid, other_extent_name,\n                           other_field_name, return_count=True)", "path": "schevo\\entityns.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return dictionary of (extent_name, field_name): entity_list\npairs, or list of linking entities if `other_extent_name` and\n`other_field_name` are supplied.\"\"\"\n", "func_signal": "def links(self, other_extent_name=None, other_field_name=None):\n", "code": "i = self._i\nreturn i._db._entity_links(i._extent.name, i._oid,\n                           other_extent_name, other_field_name)", "path": "schevo\\entityns.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "# Database starts out closed.\n", "func_signal": "def perform(self):\n", "code": "url = self.url\ndb = schevo.database.open(url)\ntry:\n    try:\n        items = self._extents_extraneous.items()\n        for (extent_name, extraneous_field_ids) in items:\n            extent_map = db._extent_map(extent_name)\n            # For all formats, remove extraneouse field IDs from\n            # entity_field_ids.\n            entity_field_ids = set(extent_map['entity_field_ids'])\n            entity_field_ids -= extraneous_field_ids\n            extent_map['entity_field_ids'] = tuple(entity_field_ids)\n            # For format 2, also iterate over each entity in the\n            # extent and remove extraneous related_entities sets.\n            if db.format == 2:\n                for entity_map in extent_map['entities'].itervalues():\n                    related_entities = entity_map['related_entities']\n                    for field_id in extraneous_field_ids:\n                        if field_id in related_entities:\n                            del related_entities[field_id]\n        db._commit()\n    except:\n        db._rollback()\n        raise\nfinally:\n    db.close()", "path": "schevo\\repair.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"The field names of immediate Match subqueries.\"\"\"\n", "func_signal": "def match_names(self):\n", "code": "field_names = []\nfor query in self.queries:\n    if isinstance(query, Match):\n        field_names.append(query.field_name)\nreturn field_names", "path": "schevo\\query.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return a sequence of valid operators based on the\nFieldClass.\"\"\"\n", "func_signal": "def valid_operators(self):\n", "code": "FieldClass = self.FieldClass\nvalid = []\nif issubclass(FieldClass, field.Field):\n    valid.append(o_any)\n    valid.append(o_assigned)\n    valid.append(o_unassigned)\n    valid.append(o_eq)\n    valid.append(o_ne)\nif issubclass(FieldClass, (field.String, field.Unicode)):\n    valid.append(o_contains)\n    valid.append(o_startswith)\nif not issubclass(FieldClass, field.Entity):\n    valid.append(o_le)\n    valid.append(o_lt)\n    valid.append(o_ge)\n    valid.append(o_gt)\nreturn tuple(valid)", "path": "schevo\\query.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return a callable that returns the current list of linking\nentities whenever called.\"\"\"\n", "func_signal": "def links_filter(self, other_extent_name, other_field_name):\n", "code": "db = self._i._db\ntry:\n    extent = db.extent(other_extent_name)\nexcept KeyError:\n    raise ExtentDoesNotExist(other_extent_name)\nif other_field_name not in extent.field_spec:\n    raise FieldDoesNotExist(other_extent_name, other_field_name)\ndef _filter():\n    return self.links(other_extent_name, other_field_name)\nreturn _filter", "path": "schevo\\entityns.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "# Database starts out closed.\n", "func_signal": "def perform(self):\n", "code": "url = self.url\ndb = schevo.database.open(url)\ntry:\n    try:\n        extent_id_name = db._extent_id_name\n        extent_maps_by_id = db._extent_maps_by_id\n        for extent_id, extent_map in extent_maps_by_id.iteritems():\n            extent_name = extent_map['name']\n            for oid, entity_map in extent_map['entities'].iteritems():\n                links = entity_map['links']\n                for key in links.keys():\n                    other_extent_id, other_field_id = key\n                    if other_extent_id not in extent_id_name:\n                        link_count = len(links[key])\n                        del links[key]\n                        entity_map['link_count'] -= link_count\n                entity = db.extent(extent_name)[oid]\n                len_links = sum(\n                    len(v) for v in entity.s.links().itervalues())\n                assert len_links == entity.s.count()\n        db._commit()\n    except:\n        db._rollback()\n        raise\nfinally:\n    db.close()", "path": "schevo\\repair.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"If pkg_or_path is a module, return its path; otherwise,\nreturn pkg_or_path.\"\"\"\n", "func_signal": "def package_path(pkg_or_path):\n", "code": "from_list = pkg_or_path.split('.')[:1]\ntry:\n    pkg = __import__(pkg_or_path, {}, {}, from_list)\nexcept ImportError:\n    return os.path.abspath(pkg_or_path)\nif '__init__.py' in pkg.__file__:\n    # Package was specified; return the dir it's in.\n    return os.path.abspath(os.path.dirname(pkg.__file__))\nelse:\n    # Module was specified; return its filename.\n    return os.path.abspath(pkg.__file__)", "path": "schevo\\script\\path.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return field_map for the entity, filtered by optional\ncallable objects specified in `filters`.\"\"\"\n", "func_signal": "def field_map(self, *filters):\n", "code": "i = self._i\ndb = i._db\nstored_values = i._db._entity_fields(i._extent.name, i._oid)\nentity_field_map = i._field_spec.field_map(i, stored_values)\n# Remove fields that should not be included.\nnew_fields = entity_field_map.itervalues()\nfor filt in filters:\n    new_fields = [field for field in new_fields if filt(field)]\nentity_field_map = FieldMap(\n    (field.name, field) for field in new_fields)\nfor field in entity_field_map.itervalues():\n    if field.fget is not None:\n        # Update fields that have fget callables.\n        value = field.fget[0](i)\n    else:\n        # Allow fields to restore themselves from a stored\n        # value.\n        field._restore(db)\nreturn entity_field_map", "path": "schevo\\entityns.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\" Return a decorator for optimizing global references.\n\nReplaces global references with their currently defined values.\nIf not defined, the dynamic (runtime) global lookup is left undisturbed.\nIf builtin_only is True, then only builtins are optimized.\nVariable names in the stoplist are also left undisturbed.\nAlso, folds constant attr lookups and tuples of constants.\nIf verbose is True, prints each substitution as is occurs\n\n\"\"\"\n", "func_signal": "def make_constants(builtin_only=False, stoplist=[], verbose=False):\n", "code": "if type(builtin_only) == type(make_constants):\n    raise ValueError(\"The bind_constants decorator must have arguments.\")\nreturn lambda f: _make_constants(f, builtin_only, stoplist, verbose)", "path": "schevo\\lib\\optimize.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Update transaction subclasses that set _require_changes to\nFalse succeed if no changes have been made to any fields.\"\"\"\n", "func_signal": "def test_do_not_require_changes(self):\n", "code": "a = db.execute(db.Alpha.t.create(foo=1))\na = db.execute(a.t.update())\nassert a.foo == 1", "path": "schevo\\test\\test_transaction_requires_changes.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return True if the entity exists; False if it was deleted.\"\"\"\n", "func_signal": "def exists(self):\n", "code": "instance = self._i\noid = instance._oid\nextent = instance._extent\nreturn oid in extent", "path": "schevo\\entityns.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return tuple of entity values in a form suitable for\ninitial or sample data in a schema.\"\"\"\n", "func_signal": "def as_data(self):\n", "code": "def resolve(entity, fieldname):\n    field = entity.f[fieldname]\n    value = getattr(entity, fieldname)\n    if isinstance(value, base.Entity):\n        entity = value\n        values = []\n        for fieldname in entity.s.extent.default_key:\n            value = resolve(entity, fieldname)\n            values.append(value)\n        if len(field.allow) > 1:\n            values = (entity.s.extent.name, tuple(values))\n        return tuple(values)\n    else:\n        return value\nvalues = []\ninstance = self._i\ncreate = instance.t_create()\nfor f_name in instance.f:\n    if (f_name not in create.f\n        or create.f[f_name].hidden or create.f[f_name].readonly):\n        # Don't include a field that doesn't exist in the\n        # create transaction, or is hidden or readonly.\n        continue\n    f = instance.f[f_name]\n    if f.fget is not None or f.hidden:\n        pass\n    else:\n        value = resolve(instance, f_name)\n        values.append(value)\nreturn tuple(values)", "path": "schevo\\entityns.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return a canonical many name.\"\"\"\n# Strip all but alphanumeric and spaces.\n", "func_signal": "def _many_name(name):\n", "code": "name = ''.join(c for c in name if c in _ALLOWED)\n# Convert to lowercase 8-bit string.\nname = str(name).lower()\n# Replace spaces with underscores.\nname = name.replace(' ', '_')\nreturn name", "path": "schevo\\entityns.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return a decorated object based on ``obj`` that mixes in the\n`schevo.base.Results` type.\"\"\"\n", "func_signal": "def results(obj):\n", "code": "if isinstance(obj, frozenset):\n    return ResultsFrozenset(obj)\nelif isinstance(obj, list):\n    return ResultsList(obj)\nelif isinstance(obj, set):\n    return ResultsSet(obj)\nelif isinstance(obj, tuple):\n    return ResultsTuple(obj)\nelse:\n    return ResultsIterator(obj)", "path": "schevo\\query.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Remove the the first immediate Match subquery with the\ngiven field name.\"\"\"\n", "func_signal": "def remove_match(self, field_name):\n", "code": "for query in self.queries:\n    if isinstance(query, Match) and query.field_name == field_name:\n        self.queries.remove(query)\n        return\nraise schevo.error.FieldDoesNotExist(self, field_name)", "path": "schevo\\query.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return a list of repair types that should be performed in order to\nrepair the given database.\"\"\"\n", "func_signal": "def repairs_needed(db, url):\n", "code": "needed = []\nfor repair_type in [\n    EntityFieldIdsRepair,\n    OrphanLinkStructuresRepair,\n    ]:\n    repair = repair_type(db, url)\n    if repair.is_needed:\n        needed.append(repair)\nreturn needed", "path": "schevo\\repair.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Return a many function.\"\"\"\n", "func_signal": "def _many_func(db, extent_name, oid, other_extent_name, other_field_name):\n", "code": "links = db._entity_links\ndef many(other_field_name=other_field_name):\n    return links(extent_name, oid, other_extent_name, other_field_name)\nreturn many", "path": "schevo\\entityns.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Suspend this account.\"\"\"\n", "func_signal": "def t_suspend(self):\n", "code": "tx = T.Suspend()\ntx.account = self\ntx.f.account.readonly = True\nreturn tx", "path": "schevo\\test\\test_bank.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Decorates a function as __do_not_optimize__.\"\"\"\n", "func_signal": "def do_not_optimize(fn):\n", "code": "fn.__do_not_optimize__ = True\nreturn fn", "path": "schevo\\lib\\optimize.py", "repo_name": "Schevo/schevo", "stars": 5, "license": "mit", "language": "python", "size": 4359}
{"docstring": "\"\"\"Convert working copy path or repos URL to a repos URL.\"\"\"\n", "func_signal": "def target_to_url(target):\n", "code": "if is_wc(target):\n    info = get_svninfo(target)\n    return info[\"URL\"]\nreturn target", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Initialize for merges.\"\"\"\n# Check that directory is ready for being modified\n", "func_signal": "def action_init(target_dir, target_props):\n", "code": "check_dir_clean(target_dir)\n\ntarget_pathid = PathIdentifier.from_target(target_dir)\nsource_pathid = opts['source-pathid']\nif source_pathid == target_pathid:\n    error(\"cannot init integration source path '%s'\\nIts path identifier does not \"\n          \"differ from the path identifier of the current directory, '%s'.\"\n          % (source_pathid, target_pathid))\n\nsource_url = opts['source-url']\n\n# If the user hasn't specified the revisions to use, see if the\n# \"source\" is a copy from the current tree and if so, we can use\n# the version data obtained from it.\nrevision_range = opts[\"revision\"]\nif not revision_range:\n    # If source was originally copied from target, and we are merging\n    # changes from source to target (the copy target is the merge source,\n    # and the copy source is the merge target), then we want to mark as\n    # integrated up to the rev in which the copy was committed which\n    # created the merge source:\n    cf_source, cf_rev, copy_committed_in_rev = get_copyfrom(source_url)\n\n    cf_pathid = None\n    if cf_source:\n        cf_url = get_repo_root(source_url) + cf_source\n        if is_url(cf_url) and check_url(cf_url):\n            cf_pathid = PathIdentifier.from_target(cf_url)\n\n    if target_pathid == cf_pathid:\n        report('the source \"%s\" was copied from \"%s\" in rev %s and committed in rev %s' %\n               (source_url, target_dir, cf_rev, copy_committed_in_rev))\n        revision_range = \"1-\" + str(copy_committed_in_rev)\n\nif not revision_range:\n    # If the reverse is true: copy source is the merge source, and\n    # the copy target is the merge target, then we want to mark as\n    # integrated up to the specific rev of the merge target from\n    # which the merge source was copied.  (Longer discussion at:\n    # http://subversion.tigris.org/issues/show_bug.cgi?id=2810  )\n    cf_source, cf_rev, copy_committed_in_rev = get_copyfrom(target_dir)\n\n    cf_pathid = None\n    if cf_source:\n        cf_url = get_repo_root(target_dir) + cf_source\n        if is_url(cf_url) and check_url(cf_url):\n            cf_pathid = PathIdentifier.from_target(cf_url)\n\n    source_pathid = PathIdentifier.from_target(source_url)\n    if source_pathid == cf_pathid:\n        report('the target \"%s\" was copied the source \"%s\" in rev %s and committed in rev %s' %\n               (target_dir, source_url, cf_rev, copy_committed_in_rev))\n        revision_range = \"1-\" + cf_rev\n\n# When neither the merge source nor target is a copy of the other, and\n# the user did not specify a revision range, then choose a default which is\n# the current revision; saying, in effect, \"everything has been merged, so\n# mark as integrated up to the latest rev on source url).\nif not revision_range:\n    revision_range = \"1-\" + get_latest_rev(source_url)\n\nrevs = RevisionSet(revision_range)\n\nreport('marking \"%s\" as already containing revisions \"%s\" of \"%s\"' %\n       (target_dir, revs, source_url))\n\nrevs = str(revs)\n# If the local svnmerge-integrated property already has an entry\n# for the source-pathid, simply error out.\nif not opts[\"force\"] and target_props.has_key(source_pathid):\n    error('Repository-relative path %s has already been initialized at %s\\n'\n          'Use --force to re-initialize' % (source_pathid, target_dir))\n# set the pathid's external_form based on the user's options\nsource_pathid.external_form = source_pathid.format(opts['location-type'])\n\nrevs = str(revs)\ntarget_props[source_pathid] = revs\n\n# Set property\nset_merge_props(target_dir, target_props)\n\n# Write out commit message if desired\nif opts[\"commit-file\"]:\n    f = open(opts[\"commit-file\"], \"w\")\n    print >>f, 'Initialized merge tracking via \"%s\" with revisions \"%s\" from ' \\\n        % (NAME, revs)\n    print >>f, '%s' % source_url\n    f.close()\n    report('wrote commit message to \"%s\"' % opts[\"commit-file\"])", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Set the property 'prop' of directory 'dir' to value 'value'. We go\nthrough a temporary file to not run into command line length limits.\"\"\"\n", "func_signal": "def _run_propset(dir, prop, value):\n", "code": "try:\n    fd, fname = tempfile.mkstemp()\n    f = os.fdopen(fd, \"wb\")\nexcept AttributeError:\n    # Fallback for Python <= 2.3 which does not have mkstemp (mktemp\n    # suffers from race conditions. Not that we care...)\n    fname = tempfile.mktemp()\n    f = open(fname, \"wb\")\n\ntry:\n    f.write(value)\n    f.close()\n    report(\"property data written to temp file: %s\" % value)\n    svn_command('propset \"%s\" -F \"%s\" \"%s\"' % (prop, fname, dir))\nfinally:\n    os.remove(fname)", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Like str.rsplit, which is Python 2.4+ only.\"\"\"\n", "func_signal": "def rsplit(s, sep, maxsplits=0):\n", "code": "L = s.split(sep)\nif not 0 < maxsplits <= len(L):\n    return L\nreturn [sep.join(L[0:-maxsplits])] + L[-maxsplits:]", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Get the latest revision of the repository of which URL is part.\"\"\"\n", "func_signal": "def get_latest_rev(url):\n", "code": "try:\n    info = get_svninfo(url)\n    if not info.has_key(\"Revision\"):\n        error(\"Not a valid URL: %s\" % url)\n    return info[\"Revision\"]\nexcept LaunchError:\n    # Alternative method for latest revision checking (for svn < 1.2)\n    report('checking latest revision of \"%s\"' % url)\n    L = launchsvn('proplist --revprop -r HEAD \"%s\"' % opts[\"source-url\"])[0]\n    rev = re.search(\"revision (\\d+)\", L).group(1)\n    report('latest revision of \"%s\" is %s' % (url, rev))\n    return rev", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"\nTry to extract the command name from the argument list. This is\nnon-trivial because we want to allow command-specific options even\nbefore the command itself.\n\"\"\"\n", "func_signal": "def _extract_command(self, args):\n", "code": "opts = self.gopts[:]\nfor cmd in self.ctable.values():\n    opts.extend(cmd.opts)\nsfl, lfl, _ = self._compute_flags(opts, check_conflicts=False)\n\nlopts,largs = getopt.getopt(args, sfl, lfl)\nif not largs:\n    return None\nreturn self._command(largs[0])", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Block revisions.\"\"\"\n# Check branch directory is ready for being modified\n", "func_signal": "def action_block(branch_dir, branch_props):\n", "code": "check_dir_clean(branch_dir)\n\nsource_revs, phantom_revs, reflected_revs, initialized_revs = \\\n           analyze_source_revs(branch_dir, opts[\"source-url\"])\nrevs_to_block = source_revs - opts[\"merged-revs\"]\n\n# Limit to revisions specified by -r (if any)\nif opts[\"revision\"]:\n    revs_to_block = RevisionSet(opts[\"revision\"]) & revs_to_block\n\nif not revs_to_block:\n    error('no available revisions to block')\n\n# Change blocked information\nblocked_revs = get_blocked_revs(branch_dir, opts[\"source-pathid\"])\nblocked_revs = blocked_revs | revs_to_block\nset_blocked_revs(branch_dir, opts[\"source-pathid\"], blocked_revs)\n\n# Write out commit message if desired\nif opts[\"commit-file\"]:\n    f = open(opts[\"commit-file\"], \"w\")\n    print >>f, 'Blocked revisions %s via %s' % (revs_to_block, NAME)\n    if opts[\"commit-verbose\"]:\n        print >>f\n        print >>f, construct_merged_log_message(opts[\"source-url\"],\n                                                revs_to_block),\n\n    f.close()\n    report('wrote commit message to \"%s\"' % opts[\"commit-file\"])", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Compute the root repos URL given a working-copy path, or a URL.\"\"\"\n# Try using \"svn info WCDIR\". This works only on SVN clients >= 1.3\n", "func_signal": "def get_repo_root(target):\n", "code": "if not is_url(target):\n    try:\n        info = get_svninfo(target)\n        root = info[\"Repository Root\"]\n        _cache_reporoot[root] = None\n        return root\n    except KeyError:\n        pass\n    url = target_to_url(target)\n    assert url[-1] != '/'\nelse:\n    url = target\n\n# Go through the cache of the repository roots. This avoids extra\n# server round-trips if we are asking the root of different URLs\n# in the same repository (the cache in get_svninfo() cannot detect\n# that of course and would issue a remote command).\nassert is_url(url)\nfor r in _cache_reporoot:\n    if url.startswith(r):\n        return r\n\n# Try using \"svn info URL\". This works only on SVN clients >= 1.2\ntry:\n    info = get_svninfo(url)\n    # info may be {}, in which case we'll see KeyError here\n    root = info[\"Repository Root\"]\n    _cache_reporoot[root] = None\n    return root\nexcept (KeyError, LaunchError):\n    pass\n\n# Constrained to older svn clients, we are stuck with this ugly\n# trial-and-error implementation. It could be made faster with a\n# binary search.\nwhile url:\n    temp = os.path.dirname(url)\n    try:\n        launchsvn('proplist \"%s\"' % temp)\n    except LaunchError:\n        _cache_reporoot[url] = None\n        return rstrip(url, \"/\")\n    url = temp\n\nerror(\"svn repos root of %s not found\" % target)", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Test whether str is a substring of any representation of this\nPathIdentifier.\"\"\"\n", "func_signal": "def match_substring(self, str):\n", "code": "if self.repo_relative_path.find(str) >= 0:\n    return True\n\nif self.uuid:\n    if (\"uuid://%s%s\" % (self.uuid, self.repo_relative_path)).find(str) >= 0:\n        return True\n\nif self.url:\n    if (self.url + self.repo_relative_path).find(str) >= 0:\n        return True\n\nreturn False", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Show commits available for merges.\"\"\"\n", "func_signal": "def action_avail(branch_dir, branch_props):\n", "code": "source_revs, phantom_revs, reflected_revs, initialized_revs = \\\n           analyze_source_revs(branch_dir, opts[\"source-url\"],\n                               find_reflected=\n                                   should_find_reflected(branch_dir))\nreport('skipping phantom revisions: %s' % phantom_revs)\nif reflected_revs:\n    report('skipping reflected revisions: %s' % reflected_revs)\n    report('skipping initialized revisions: %s' % initialized_revs)\n\nblocked_revs = get_blocked_revs(branch_dir, opts[\"source-pathid\"])\navail_revs = source_revs - opts[\"merged-revs\"] - blocked_revs - \\\n             reflected_revs - initialized_revs\n\n# Compose the set of revisions to show\nrevs = RevisionSet(\"\")\nreport_msg = \"revisions available to be merged are:\"\nif \"avail\" in opts[\"avail-showwhat\"]:\n    revs |= avail_revs\nif \"blocked\" in opts[\"avail-showwhat\"]:\n    revs |= blocked_revs\n    report_msg = \"revisions blocked are:\"\n\n# Limit to revisions specified by -r (if any)\nif opts[\"revision\"]:\n    revs = revs & RevisionSet(opts[\"revision\"])\n\ndisplay_revisions(revs, opts[\"avail-display\"],\n                  report_msg,\n                  opts[\"source-url\"])", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Get the width of the console screen (if any).\"\"\"\n", "func_signal": "def console_width():\n", "code": "try:\n    return int(os.environ[\"COLUMNS\"])\nexcept (KeyError, ValueError):\n    pass\n\ntry:\n    # Call the Windows API (requires ctypes library)\n    from ctypes import windll, create_string_buffer\n    h = windll.kernel32.GetStdHandle(-11)\n    csbi = create_string_buffer(22)\n    res = windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)\n    if res:\n        import struct\n        (bufx, bufy,\n         curx, cury, wattr,\n         left, top, right, bottom,\n         maxx, maxy) = struct.unpack(\"hhhhHhhhhhh\", csbi.raw)\n        return right - left + 1\nexcept ImportError:\n    pass\n\n# Parse the output of stty -a\nif os.isatty(1):\n    out = os.popen(\"stty -a\").read()\n    m = re.search(r\"columns (\\d+);\", out)\n    if m:\n        return int(m.group(1))\n\n# sensible default\nreturn 80", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Rollback previously integrated revisions.\"\"\"\n\n# Make sure the revision arguments are present\n", "func_signal": "def action_rollback(branch_dir, branch_props):\n", "code": "if not opts[\"revision\"]:\n    error(\"The '-r' option is mandatory for rollback\")\n\n# Check branch directory is ready for being modified\ncheck_dir_clean(branch_dir)\n\n# Extract the integration info for the branch_dir\nbranch_props = get_merge_props(branch_dir)\n# Get the list of all revisions already merged into this source-pathid.\nmerged_revs = merge_props_to_revision_set(branch_props,\n                                          opts[\"source-pathid\"])\n\n# At which revision was the src created?\noldest_src_rev = get_created_rev(opts[\"source-url\"])\nsrc_pre_exist_range = RevisionSet(\"1-%d\" % oldest_src_rev)\n\n# Limit to revisions specified by -r (if any)\nrevs = merged_revs & RevisionSet(opts[\"revision\"])\n\n# make sure there's some revision to rollback\nif not revs:\n    report(\"Nothing to rollback in revision range r%s\" % opts[\"revision\"])\n    return\n\n# If even one specified revision lies outside the lifetime of the\n# merge source, error out.\nif revs & src_pre_exist_range:\n    err_str  = \"Specified revision range falls out of the rollback range.\\n\"\n    err_str += \"%s was created at r%d\" % (opts[\"source-pathid\"],\n                                          oldest_src_rev)\n    error(err_str)\n\nrecord_only = opts[\"record-only\"]\n\nif record_only:\n    report('recording rollback of revision(s) %s from \"%s\"' %\n           (revs, opts[\"source-url\"]))\nelse:\n    report('rollback of revision(s) %s from \"%s\"' %\n           (revs, opts[\"source-url\"]))\n\n# Do the reverse merge(s). Note: the starting revision number\n# to 'svn merge' is NOT inclusive so we have to subtract one from start.\n# We try to keep the number of merge operations as low as possible,\n# because it is faster and reduces the number of conflicts.\nrollback_intervals = minimal_merge_intervals(revs, [])\n# rollback in the reverse order of merge\nrollback_intervals.reverse()\nfor start, end in rollback_intervals:\n    if not record_only:\n        # Do the merge\n        svn_command(\"merge --force -r %d:%d %s %s\" % \\\n                    (end, start - 1, opts[\"source-url\"], branch_dir))\n\n# Write out commit message if desired\n# calculate the phantom revs first\nif opts[\"commit-file\"]:\n    f = open(opts[\"commit-file\"], \"w\")\n    if record_only:\n        print >>f, 'Recorded rollback of revisions %s via %s from ' % \\\n              (revs , NAME)\n    else:\n        print >>f, 'Rolled back revisions %s via %s from ' % \\\n              (revs , NAME)\n    print >>f, '%s' % opts[\"source-url\"]\n\n    f.close()\n    report('wrote commit message to \"%s\"' % opts[\"commit-file\"])\n\n# Update the set of merged revisions.\nmerged_revs = merged_revs - revs\nbranch_props[opts[\"source-pathid\"]] = str(merged_revs)\nset_merge_props(branch_dir, branch_props)", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Unblock revisions.\"\"\"\n# Check branch directory is ready for being modified\n", "func_signal": "def action_unblock(branch_dir, branch_props):\n", "code": "check_dir_clean(branch_dir)\n\nblocked_revs = get_blocked_revs(branch_dir, opts[\"source-pathid\"])\nrevs_to_unblock = blocked_revs\n\n# Limit to revisions specified by -r (if any)\nif opts[\"revision\"]:\n    revs_to_unblock = revs_to_unblock & RevisionSet(opts[\"revision\"])\n\nif not revs_to_unblock:\n    error('no available revisions to unblock')\n\n# Change blocked information\nblocked_revs = blocked_revs - revs_to_unblock\nset_blocked_revs(branch_dir, opts[\"source-pathid\"], blocked_revs)\n\n# Write out commit message if desired\nif opts[\"commit-file\"]:\n    f = open(opts[\"commit-file\"], \"w\")\n    print >>f, 'Unblocked revisions %s via %s' % (revs_to_unblock, NAME)\n    if opts[\"commit-verbose\"]:\n        print >>f\n        print >>f, construct_merged_log_message(opts[\"source-url\"],\n                                                revs_to_unblock),\n    f.close()\n    report('wrote commit message to \"%s\"' % opts[\"commit-file\"])", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Return the default source for branch_target (given its branch_props).\nError out if there is ambiguity.\"\"\"\n", "func_signal": "def get_default_source(branch_target, branch_props):\n", "code": "if not branch_props:\n    error(\"no integration info available\")\n\nprops = branch_props.copy()\npathid = PathIdentifier.from_target(branch_target)\n\n# To make bidirectional merges easier, find the target's\n# repository local path so it can be removed from the list of\n# possible integration sources.\nif props.has_key(pathid):\n    del props[pathid]\n\nif len(props) > 1:\n    err_msg = \"multiple sources found. \"\n    err_msg += \"Explicit source argument (-S/--source) required.\\n\"\n    err_msg += \"The merge sources available are:\"\n    for prop in props:\n      err_msg += \"\\n  \" + str(prop)\n    error(err_msg)\n\nreturn props.keys()[0]", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Produce the smallest number of intervals suitable for merging. revs\nis the RevisionSet which we want to merge, and phantom_revs are phantom\nrevisions which can be used to concatenate intervals, thus minimizing the\nnumber of operations.\"\"\"\n", "func_signal": "def minimal_merge_intervals(revs, phantom_revs):\n", "code": "revnums = revs.normalized()\nret = []\n\ncur = revnums.pop()\nwhile revnums:\n    next = revnums.pop()\n    assert next[1] < cur[0]      # otherwise it is not ordered\n    assert cur[0] - next[1] > 1  # otherwise it is not normalized\n    for i in range(next[1]+1, cur[0]):\n        if i not in phantom_revs:\n            ret.append(cur)\n            cur = next\n            break\n    else:\n        cur = (next[0], cur[1])\n\nret.append(cur)\nret.reverse()\nreturn ret", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Extract info from a svn keyword string.\"\"\"\n", "func_signal": "def kwextract(s):\n", "code": "try:\n    return strip(s, \"$\").strip().split(\": \")[1]\nexcept IndexError:\n    return \"<unknown>\"", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"\nLoad the history of property changes from the specified\nRevisionLog object.\n\"\"\"\n\n# Get the property value before the range of the log\n", "func_signal": "def load(self, log):\n", "code": "if log.begin > 1:\n    self.revs.append(log.begin-1)\n    try:\n        self._initial_value = self.raw_get(log.begin-1)\n    except LaunchError:\n        # The specified URL might not exist before the\n        # range of the log. If so, we can safely assume\n        # that the property was empty at that time.\n        self._initial_value = { }\n    self.values.append(self._initial_value)\nelse:\n    self._initial_value = { }\n    self.values[0] = self._initial_value\n\n# Cache the property values in the log range\nold_value = self._initial_value\nfor rev in log.propchange_revs:\n    new_value = self.raw_get(rev)\n    if new_value != old_value:\n        self._changed_revs.append(rev)\n        self._changed_values.append(new_value)\n        self.revs.append(rev)\n        self.values.append(new_value)\n        old_value = new_value\n\n# Indicate that we know nothing about the value of the property\n# after the range of the log.\nif log.revs:\n    self.revs.append(log.end+1)\n    self.values.append(None)", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Formats the hash PROPS as a string suitable for use as a\nSubversion property value.\"\"\"\n", "func_signal": "def format_merge_props(props, sep=\" \"):\n", "code": "assert sep in [\"\\t\", \"\\n\", \" \"]   # must be a whitespace\nprops = props.items()\nprops.sort()\nL = []\nfor h, r in props:\n    L.append(\"%s:%s\" % (h, r))\nreturn sep.join(L)", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"convert pathid_str to a PathIdentifier\"\"\"\n", "func_signal": "def from_pathid(pathid_str):\n", "code": "if not PathIdentifier.locobjs.has_key(pathid_str):\n    if is_url(pathid_str):\n        # we can determine every form; PathIdentifier.hint knows how to do that\n        PathIdentifier.hint(pathid_str)\n    elif pathid_str[:7] == 'uuid://':\n        mo = re.match('uuid://([^/]*)(.*)', pathid_str)\n        if not mo:\n            error(\"Invalid path identifier '%s'\" % pathid_str)\n        uuid, repo_relative_path = mo.groups()\n        pathid = PathIdentifier(repo_relative_path, uuid=uuid)\n        # we can cache this by uuid:// pathid and by repo-relative path\n        PathIdentifier.locobjs[pathid_str] = PathIdentifier.locobjs[repo_relative_path] = pathid\n    elif pathid_str and pathid_str[0] == '/':\n        # strip any trailing slashes\n        pathid_str = pathid_str.rstrip('/')\n        pathid = PathIdentifier(repo_relative_path=pathid_str)\n        # we can only cache this by repo-relative path\n        PathIdentifier.locobjs[pathid_str] = pathid\n    else:\n        error(\"Invalid path identifier '%s'\" % pathid_str)\nreturn PathIdentifier.locobjs[pathid_str]", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"For the given branch and source, extract the real and phantom\nsource revisions.\"\"\"\n", "func_signal": "def analyze_source_revs(branch_target, source_url, **kwargs):\n", "code": "branch_url = target_to_url(branch_target)\nbranch_pathid = PathIdentifier.from_target(branch_target)\n\n# Extract the latest repository revision from the URL of the branch\n# directory (which is already cached at this point).\nend_rev = get_latest_rev(source_url)\n\n# Calculate the base of analysis. If there is a \"1-XX\" interval in the\n# merged_revs, we do not need to check those.\nbase = 1\nr = opts[\"merged-revs\"].normalized()\nif r and r[0][0] == 1:\n    base = r[0][1] + 1\n\n# See if the user filtered the revision set. If so, we are not\n# interested in something outside that range.\nif opts[\"revision\"]:\n    revs = RevisionSet(opts[\"revision\"]).sorted()\n    if base < revs[0]:\n        base = revs[0]\n    if end_rev > revs[-1]:\n        end_rev = revs[-1]\n\nreturn analyze_revs(branch_pathid, source_url, base, end_rev, **kwargs)", "path": "extra\\subversion\\svnmerge.py", "repo_name": "jensp/Arch-Linux-on-i586", "stars": 6, "license": "None", "language": "python", "size": 19810}
{"docstring": "\"\"\"Return this robot's capabilities as an XML string.\"\"\"\n", "func_signal": "def GetCapabilitiesXml(self):\n", "code": "lines = ['<w:version>%s</w:version>' % self.version]\n\nlines.append('<w:capabilities>')\nfor capability in self._handlers:\n  lines.append('  <w:capability name=\"%s\"/>' % capability)\nlines.append('</w:capabilities>')\n\nif self.cron_jobs:\n  lines.append('<w:crons>')\n  for job in self.cron_jobs:\n    lines.append('  <w:cron path=\"%s\" timerinseconds=\"%s\"/>' % job)\n  lines.append('</w:crons>')\n\nrobot_attrs = ' name=\"%s\"' % self.name\nif self.image_url:\n  robot_attrs += ' imageurl=\"%s\"' % self.image_url\nif self.profile_url:\n  robot_attrs += ' profileurl=\"%s\"' % self.profile_url\nlines.append('<w:profile%s/>' % robot_attrs)\nreturn ('<?xml version=\"1.0\"?>\\n'\n        '<w:robot xmlns:w=\"http://wave.google.com/extensions/robots/1.0\">\\n'\n        '%s\\n</w:robot>\\n') % ('\\n'.join(lines))", "path": "waveapi\\robot_abstract.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Create a new wave with the initial participants on it.\"\"\"\n# we shouldn't need a wave/wavelet id here, but we do\n", "func_signal": "def NewWave(context, participants=None):\n", "code": "wavelet = context.GetRootWavelet()\nreturn context.builder.WaveletCreate(wavelet.GetWaveId(), wavelet.GetId(), participants)", "path": "waveapi\\robot_abstract.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Returns JSON body for any profile handler.\n\nReturns:\n  String of JSON to be sent as a response.\n\"\"\"\n", "func_signal": "def GetProfileJson(self):\n", "code": "data = {}\ndata['name'] = self.name\ndata['imageUrl'] = self.image_url\ndata['profileUrl'] = self.profile_url\n# TODO(davidbyttow): Remove this java nonsense.\ndata['javaClass'] = 'com.google.wave.api.ParticipantProfile'\nreturn simplejson.dumps(data)", "path": "waveapi\\robot_abstract.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"consonant2moras(string) -> list\n\nCreate list of mora starting with consonant.\n\n>>> consonant2moras('z')\n['zo', 'ze', 'za', 'zu', 'zzyo', 'zi', 'zzu', 'zzo', 'zzi', 'zza', \\\n'zze', 'zyo', 'zzya', 'zya', 'zyu', 'zzyu']\n\n\"\"\"\n", "func_signal": "def consonant2moras(consonant):\n", "code": "results = []\nfor roma in romkans.keys():\n    if re.match(consonant, roma):\n        results.append(roma)\nreturn results", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"katahira(string) -> string\n\nConverts katakana string into hiragana.\n\n\"\"\"\n", "func_signal": "def katahira(word):\n", "code": "s = u''\nuniword = unistr(word)\nfor char in uniword:\n    if ord(char) > 0x30A0 and ord(char) < 0x30F7:\n        s += unichr(ord(char) - 0x60)\n    else:\n        s += char\nreturn s.encode('utf-8')", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"hirakata(string) -> string\n\nConverts hiragana string into katakana.\n\n\"\"\"\n", "func_signal": "def hirakata(word):\n", "code": "s = u''\nuniword = unistr(word)\nfor char in uniword:\n    if ord(char) > 0x3040 and ord(char) < 0x3097:\n        s += unichr(ord(char) + 0x60)\n    else:\n        s += char\nreturn s.encode('utf-8')", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Parse a JSON string and return a context and an event list.\"\"\"\n", "func_signal": "def ParseJSONBody(json_body):\n", "code": "json = simplejson.loads(json_body)\n# TODO(davidbyttow): Remove this once no longer needed.\ndata = util.CollapseJavaCollections(json)\ncontext = ops.CreateContext(data)\nevent_list = [model.Event(event_data) for event_data in data['events']]\nreturn context, event_list", "path": "waveapi\\robot_abstract.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"kanrom(string) -> string\n\nConverts hiragana string into Hepburn romaji string.\n\n\"\"\"\n", "func_signal": "def kanrom(word):\n", "code": "word = normalize_double_n(word)\nword = ck_re.sub(lambda m: m.groups()[0] + kanroms[m.groups()[1]], word)\nword = n_re.sub(\"n\", word)\n# small katakana letters don't get the 'x' taken out when they are\n# 'improperly' used to lengthen vowel sounds (ex. \u30ab-\u30d3\u30a3 -> ka-bii)\nword = word.replace('x', '')\nreturn word", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "# Read JSON POST input to jsonfilter.json if matching mime type\n", "func_signal": "def __call__(self, environ, start_response):\n", "code": "response = {'status': '200 OK', 'headers': []}\ndef json_start_response(status, headers):\n    response['status'] = status\n    response['headers'].extend(headers)\nenviron['jsonfilter.mime_type'] = self.mime_type\nif environ.get('REQUEST_METHOD', '') == 'POST':\n    if environ.get('CONTENT_TYPE', '') == self.mime_type:\n        args = [_ for _ in [environ.get('CONTENT_LENGTH')] if _]\n        data = environ['wsgi.input'].read(*map(int, args))\n        environ['jsonfilter.json'] = simplejson.loads(data)\nres = simplejson.dumps(self.app(environ, json_start_response))\njsonp = cgi.parse_qs(environ.get('QUERY_STRING', '')).get('jsonp')\nif jsonp:\n    content_type = 'text/javascript'\n    res = ''.join(jsonp + ['(', res, ')'])\nelif 'Opera' in environ.get('HTTP_USER_AGENT', ''):\n    # Opera has bunk XMLHttpRequest support for most mime types\n    content_type = 'text/plain'\nelse:\n    content_type = self.mime_type\nheaders = [\n    ('Content-type', content_type),\n    ('Content-length', len(res)),\n]\nheaders.extend(response['headers'])\nstart_response(response['status'], headers)\nreturn [res]", "path": "waveapi\\simplejson\\jsonfilter.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Return the Python representation of ``s`` (a ``str`` or ``unicode``\ninstance containing a JSON document)\n\n\"\"\"\n", "func_signal": "def decode(self, s, _w=WHITESPACE.match):\n", "code": "obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nend = _w(s, end).end()\nif end != len(s):\n    raise JSONDecodeError(\"Extra data\", s, end, len(s))\nreturn obj", "path": "waveapi\\simplejson\\decoder.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Registers all event handlers exported by the given object.\n\nArgs:\n  listener: an object with methods corresponding to wave events.\n    Methods should be named either in camel case, e.g. 'OnBlipSubmitted',\n    or in lowercase, e.g. 'on_blip_submitted', with names corresponding\n    to the event names in the events module.\n\"\"\"\n", "func_signal": "def RegisterListener(self, listener):\n", "code": "for event in dir(events):\n  if event.startswith('_'):\n    continue\n  lowercase_method_name = 'on_' + event.lower()\n  camelcase_method_name = 'On' + util.ToUpperCamelCase(event)\n  if hasattr(listener, lowercase_method_name):\n    handler = getattr(listener, lowercase_method_name)\n  elif hasattr(listener, camelcase_method_name):\n    handler = getattr(listener, camelcase_method_name)\n  else:\n    continue\n  if callable(handler):\n    self.RegisterHandler(event, handler)", "path": "waveapi\\robot_abstract.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"isvowel(string) -> bool\n\nReturns true if string is a vowel.\n\n\"\"\"\n", "func_signal": "def isvowel(char):\n", "code": "if vowre.match(char):\n    return True\nelse:\n    return False", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"isconsonant(string) -> bool\n\nReturns true if string is a consonant.\n\n\"\"\"\n", "func_signal": "def isconsonant(char):\n", "code": "if conre.match(char):\n    return True\nelse:\n    return False", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"romkan(string) -> string\n\nConverts romaji sequence into hiragana. Can handle both Hepburn and\nKunrei formats.\n\n\"\"\"\n", "func_signal": "def romkan(word):\n", "code": "word = normalize_double_n(word)\nword = cr_re.sub(lambda m: m.groups()[0] + romkans[m.groups()[1]], word)\nreturn word", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Initializes self with robot information.\"\"\"\n", "func_signal": "def __init__(self, name, version, image_url='', profile_url=''):\n", "code": "self._handlers = {}\nself.name = name\nself.version = version\nself.image_url = image_url\nself.profile_url = profile_url\nself.cron_jobs = []", "path": "waveapi\\robot_abstract.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"init_pattern(list) -> string\n\nCreates pattern from list sorted by length in descending order.\n\n\"\"\"\n", "func_signal": "def init_pattern(elements):\n", "code": "items = sorted(elements, key=lambda a: len(a), reverse=True)\npattern = '|'.join(items)\nreturn pattern", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Return a JSON string representing the given context.\"\"\"\n", "func_signal": "def SerializeContext(context, version):\n", "code": "context_dict = util.Serialize(context)\ncontext_dict['version'] = str(version)\nreturn simplejson.dumps(context_dict)", "path": "waveapi\\robot_abstract.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"defullw(string) -> string\n\nConverts Fullwidth unicode characters to ascii equivalents.\n\"\"\"\n", "func_signal": "def defullw(word):\n", "code": "s = u''\nuniword = unistr(word)\nfor char in uniword:\n    if ord(char) >= 0xFF00 and ord(char) <= 0xff5f:\n        s += unichr(ord(char) - 0xfee0)\n    else:\n        s += char\nreturn s.encode('utf-8')", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"Calls all of the handlers associated with an event.\"\"\"\n", "func_signal": "def HandleEvent(self, event, context):\n", "code": "for handler in self._handlers.get(event.type, []):\n  # TODO(jacobly): pass the event in to the handlers directly\n  # instead of passing the properties dictionary.\n  handler(event.properties, context)", "path": "waveapi\\robot_abstract.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"romrom(string) -> string\n\nNormalizes romaji string into hepburn.\n\n>>> romrom('kannzi')\n'kanji'\n>>> romrom('hurigana')\n'furigana'\n>>> romrom('utukusii')\n'utsukushii'\n>>> romrom('tiezo')\n'chiezo'\n\n\"\"\"\n", "func_signal": "def romrom(word):\n", "code": "word = normalize_double_n(word)\nword = hk_re.sub(lambda m: m.groups()[0] + romroms[m.groups()[1]], word)\nreturn word", "path": "uromkan.py", "repo_name": "gimite/kanjy-wave", "stars": 6, "license": "None", "language": "python", "size": 128}
{"docstring": "\"\"\"\nCreate the new ``User`` and ``RegistrationProfile``, and\nreturns the ``User``.\n\nThis is essentially a light wrapper around\n``RegistrationProfile.objects.create_inactive_user()``,\nfeeding it the form data and a profile callback (see the\ndocumentation on ``create_inactive_user()`` for details) if\nsupplied.\n\n\"\"\"\n", "func_signal": "def save(self, domain_override=\"\"):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(\n    username=self.cleaned_data['username'],\n    password=self.cleaned_data['password1'],\n    email=self.cleaned_data['email'],\n    domain_override=domain_override)\nself.instance = new_user\nreturn super(UserRegistrationForm, self).save()", "path": "myapp\\forms.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nCreate the new ``User`` and ``RegistrationProfile``, and\nreturns the ``User`` (by calling\n``RegistrationProfile.objects.create_inactive_user()``).\n\n\"\"\"\n", "func_signal": "def save(self, domain_override=\"\"):\n", "code": "new_user = RegistrationProfile.objects.create_inactive_user(username=self.cleaned_data['username'],\n                                                            password=self.cleaned_data['password1'],\n                                                            email=self.cleaned_data['email'],\n                                                            domain_override=domain_override,\n                                                            )\nreturn new_user", "path": "registration\\forms.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"Helper method for get_xxx_or_404.\"\"\"\n", "func_signal": "def get_filtered(data, *filters):\n", "code": "for filter in get_filters(*filters):\n    data.filter(*filter)\nreturn data", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nCheck the supplied email address against a list of known free\nwebmail domains.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email_domain = self.cleaned_data['email'].split('@')[1]\nif email_domain in self.bad_domains:\n    raise forms.ValidationError(_(u'Registration using free email addresses is prohibited. Please supply a different email address.'))\nreturn self.cleaned_data['email']", "path": "registration\\forms.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "# test in/out equivalence and parsing\n", "func_signal": "def test_parse(self):\n", "code": "res = json.loads(JSON)\nout = json.dumps(res)\nself.assertEquals(res, json.loads(out))\ntry:\n    json.dumps(res, allow_nan=False)\nexcept ValueError:\n    pass\nelse:\n    self.fail(\"23456789012E666 should be out of range\")", "path": "results\\simplejson\\tests\\test_pass1.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nDereferences the given (Key)ReferenceProperty fields of a list of objects\nin as few get() calls as possible.\n\"\"\"\n", "func_signal": "def prefetch_references(object_list, references, cache=None):\n", "code": "if object_list and references:\n    if not isinstance(references, (list, tuple)):\n        references = (references,)\n    model = object_list[0].__class__\n    targets = {}\n    # Collect models and keys of all reference properties.\n    # Storage format of targets: models -> keys -> instance, property\n    for name in set(references):\n        property = getattr(model, name)\n        is_key_reference = isinstance(property, KeyReferenceProperty)\n        if is_key_reference:\n            target_model = property.target_model\n        else:\n            target_model = property.reference_class\n        prefetch = targets.setdefault(target_model.kind(),\n                                      (target_model, {}))[1]\n        for item in object_list:\n            if is_key_reference:\n                # Check if we already dereferenced the property\n                if hasattr(item, '_ref_cache_for_' + property.target_name):\n                    continue\n                key = getattr(item, property.target_name)\n                if property.use_key_name and key:\n                    key = db.Key.from_path(target_model.kind(), key)\n            else:\n                if ReferenceProperty.is_resolved(property, item):\n                    continue\n                key = property.get_value_for_datastore(item)\n            if key:\n                # Check if we already have a matching item in cache\n                if cache:\n                    found_cached = None\n                    for cached_item in cache:\n                        if cached_item.key() == key:\n                            found_cached = cached_item\n                    if found_cached:\n                        setattr(item, name, found_cached)\n                        continue\n                # No item found in cache. Retrieve it.\n                key = str(key)\n                prefetch[key] = prefetch.get(key, ()) + ((item, name),)\n    for target_model, prefetch in targets.values():\n        prefetched_items = target_model.get(prefetch.keys())\n        for prefetched, group in zip(prefetched_items, prefetch.values()):\n            for item, reference in group:\n                # If prefetched is None we only update the cache\n                if not prefetched:\n                    property = getattr(model, reference)\n                    if isinstance(property, KeyReferenceProperty):\n                        setattr(item,\n                            '_ref_cache_for_' + property.target_name, None)\n                    else:\n                        continue\n                setattr(item, reference, prefetched)\nreturn object_list", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"Encode the given object and yield each string\nrepresentation as available.\n\nFor example::\n\n    for chunk in JSONEncoder().iterencode(bigobject):\n        mysocket.write(chunk)\n\n\"\"\"\n", "func_signal": "def iterencode(self, o, _one_shot=False):\n", "code": "if self.check_circular:\n    markers = {}\nelse:\n    markers = None\nif self.ensure_ascii:\n    _encoder = encode_basestring_ascii\nelse:\n    _encoder = encode_basestring\nif self.encoding != 'utf-8':\n    def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):\n        if isinstance(o, str):\n            o = o.decode(_encoding)\n        return _orig_encoder(o)\n\ndef floatstr(o, allow_nan=self.allow_nan, _repr=FLOAT_REPR, _inf=INFINITY, _neginf=-INFINITY):\n    # Check for specials.  Note that this type of test is processor- and/or\n    # platform-specific, so do tests which don't depend on the internals.\n\n    if o != o:\n        text = 'NaN'\n    elif o == _inf:\n        text = 'Infinity'\n    elif o == _neginf:\n        text = '-Infinity'\n    else:\n        return _repr(o)\n\n    if not allow_nan:\n        raise ValueError(\n            \"Out of range float values are not JSON compliant: \" +\n            repr(o))\n\n    return text\n\n\nif _one_shot and c_make_encoder is not None and not self.indent and not self.sort_keys:\n    _iterencode = c_make_encoder(\n        markers, self.default, _encoder, self.indent,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, self.allow_nan)\nelse:\n    _iterencode = _make_iterencode(\n        markers, self.default, _encoder, self.indent, floatstr,\n        self.key_separator, self.item_separator, self.sort_keys,\n        self.skipkeys, _one_shot)\nreturn _iterencode(o, 0)", "path": "results\\simplejson\\encoder.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nValidate that the username is alphanumeric and is not already\nin use.\n\n\"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "user = User.get_by_key_name(\"key_\"+self.cleaned_data['username'].lower())\nif user and user.is_active:\n    raise forms.ValidationError(__(u'This username is already taken. Please choose another.'))\nreturn self.cleaned_data['username']", "path": "myapp\\forms.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nValidate that the username is alphanumeric and is not already\nin use.\n\n\"\"\"\n", "func_signal": "def clean_username(self):\n", "code": "user = User.get_by_key_name(\"key_\"+self.cleaned_data['username'].lower())\nif user:\n    raise forms.ValidationError(_(u'This username is already taken. Please choose another.'))\nreturn self.cleaned_data['username']", "path": "registration\\forms.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nEscapes a string such that it can be used safely as a key_name.\n\nYou can pass multiple values in order to build a path.\n\"\"\"\n", "func_signal": "def generate_key_name(*values):\n", "code": "return KEY_NAME_PREFIX + '/'.join(\n    [value.replace('%', '%1').replace('/', '%2') for value in values])", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"Return an ASCII-only JSON representation of a Python string\n\n\"\"\"\n", "func_signal": "def py_encode_basestring_ascii(s):\n", "code": "if isinstance(s, str) and HAS_UTF8.search(s) is not None:\n    s = s.decode('utf-8')\ndef replace(match):\n    s = match.group(0)\n    try:\n        return ESCAPE_DCT[s]\n    except KeyError:\n        n = ord(s)\n        if n < 0x10000:\n            #return '\\\\u{0:04x}'.format(n)\n            return '\\\\u%04x' % (n,)\n        else:\n            # surrogate pair\n            n -= 0x10000\n            s1 = 0xd800 | ((n >> 10) & 0x3ff)\n            s2 = 0xdc00 | (n & 0x3ff)\n            #return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\n            return '\\\\u%04x\\\\u%04x' % (s1, s2)\nreturn '\"' + str(ESCAPE_ASCII.sub(replace, s)) + '\"'", "path": "results\\simplejson\\encoder.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nConverts a models into dicts for use with JSONResponse.\n\nYou can either pass a single model instance and get a single dict\nor a list of models and get a list of dicts.\n\nFor security reasons only the properties in the property_list will get\nadded. If the value of the property has a json_data function its result\nwill be added, instead.\n\"\"\"\n", "func_signal": "def to_json_data(model_instance, property_list):\n", "code": "if hasattr(model_instance, '__iter__'):\n    return [to_json_data(item, property_list) for item in model_instance]\njson_data = {}\nfor property in property_list:\n    property_instance = None\n    try:\n        property_instance = getattr(model_instance.__class__,\n            property.split('.', 1)[0])\n    except:\n        pass\n    key_access = property[len(property.split('.', 1)[0]):]\n    if isinstance(property_instance, db.ReferenceProperty) and \\\n            key_access in ('.key', '.key.name'):\n        key = property_instance.get_value_for_datastore(model_instance)\n        if key_access == '.key':\n            json_data[property] = str(key)\n        else:\n            json_data[property] = key.name()\n        continue\n    value = getattr_by_path(model_instance, property, None)\n    value = getattr_by_path(value, 'json_data', value)\n    json_data[property] = value\nreturn json_data", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\" extending the clean method to work with GAE keys \"\"\"\n", "func_signal": "def clean(self, value):\n", "code": "value = value.strip()\nif (value): keys = value.strip().split(\"\\n\")\nelse: keys = []\nreturn [self.Model.get(key.strip()) for key in keys]", "path": "norex\\views.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nVerifiy that the values entered into the two password fields\nmatch. Note that an error here will end up in\n``non_field_errors()`` because it doesn't apply to a single\nfield.\n\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "if 'password1' in self.cleaned_data and 'password2' in self.cleaned_data:\n    if self.cleaned_data['password1'] != self.cleaned_data['password2']:\n        raise forms.ValidationError(__(u'You must type the same password each time'))\nreturn self.cleaned_data", "path": "myapp\\forms.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nThis function creates an object transactionally if it does not exist in\nthe datastore. Otherwise it returns None.\n\"\"\"\n", "func_signal": "def db_add(model, key_name, parent=None, **kwargs):\n", "code": "existing = model.get_by_key_name(key_name, parent=parent)\nif not existing:\n    new_entity = model(parent=parent, key_name=key_name, **kwargs)\n    new_entity.put()\n    return new_entity\nreturn None", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nVerifiy that the values entered into the two password fields\nmatch. Note that an error here will end up in\n``non_field_errors()`` because it doesn't apply to a single\nfield.\n\n\"\"\"\n", "func_signal": "def clean(self):\n", "code": "if 'password1' in self.cleaned_data and 'password2' in self.cleaned_data:\n    if self.cleaned_data['password1'] != self.cleaned_data['password2']:\n        raise forms.ValidationError(_(u'You must type the same password each time'))\nreturn self.cleaned_data", "path": "registration\\forms.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"Return a JSON string representation of a Python data structure.\n\n>>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n'{\"foo\": [\"bar\", \"baz\"]}'\n\n\"\"\"\n# This is for extremely simple cases and benchmarks.\n", "func_signal": "def encode(self, o):\n", "code": "if isinstance(o, basestring):\n    if isinstance(o, str):\n        _encoding = self.encoding\n        if (_encoding is not None\n                and not (_encoding == 'utf-8')):\n            o = o.decode(_encoding)\n    if self.ensure_ascii:\n        return encode_basestring_ascii(o)\n    else:\n        return encode_basestring(o)\n# This doesn't pass the iterator directly to ''.join() because the\n# exceptions aren't as detailed.  The list call should be roughly\n# equivalent to the PySequence_Fast that ''.join() would do.\nchunks = self.iterencode(o, _one_shot=True)\nif not isinstance(chunks, (list, tuple)):\n    chunks = list(chunks)\nreturn ''.join(chunks)", "path": "results\\simplejson\\encoder.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "# Models can define a CLEANUP_REFERENCES attribute if they have\n# reference properties that must get geleted with the model.\n", "func_signal": "def _get_included_cleanup_entities(entities, rels_seen, to_delete, to_put):\n", "code": "include_references = getattr(entities[0], 'CLEANUP_REFERENCES', None)\nif include_references:\n    if not isinstance(include_references, (list, tuple)):\n        include_references = (include_references,)\n    prefetch_references(entities, include_references)\n    for entity in entities:\n        for name in include_references:\n            subentity = getattr(entity, name)\n            to_delete.append(subentity)\n            get_cleanup_entities(subentity, rels_seen=rels_seen,\n                    to_delete=to_delete, to_put=to_put)", "path": "common\\appenginepatch\\ragendja\\dbutils.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"\nValidate that the supplied email address is unique for the\nsite.\n\n\"\"\"\n", "func_signal": "def clean_email(self):\n", "code": "email = self.cleaned_data['email'].lower()\nif User.all().filter('email =', email).filter(\n        'is_active =', True).count(1):\n    raise forms.ValidationError(__(u'This email address is already in use. Please supply a different email address.'))\nreturn email", "path": "myapp\\forms.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"Maintain history\n@summary\n  Disable rather than delete old items\n\"\"\"\n", "func_signal": "def delete(self):\n", "code": "self._active = False\nself._timestamp = dt.datetime.now()\nsuper(_SaveHistory,self).put()", "path": "norex\\db.py", "repo_name": "thurloat/results", "stars": 5, "license": "None", "language": "python", "size": 16508}
{"docstring": "\"\"\"shutdown_modules()\n\nTerminates all running modules and cleans up.\n\n\"\"\"\n\n# Shutdown submodules in correct order\n", "func_signal": "def shutdown_all_submodules(self):\n", "code": "self.controller_logger.info(\"Shutting down submodules\")\nfor module_identifier in reversed(self.module_start_order):\n    self.shutdown_submodule(module_identifier)", "path": "fw_modules\\module_controller.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"unlock_next_thread()\n\nUnlocks semaphore to signalise that next thread can be started.\nIs called by a recently started thread to tell controller that\nthread is up and running.\n\n\"\"\"\n\n", "func_signal": "def unlock_next_thread(self):\n", "code": "try:\n    self.thread_lock_semaphore.release()\nexcept ValueError:\n    self.controller_logger.warning(\"Thread lock semaphore is already released\")", "path": "fw_modules\\module_controller.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"Constructor\n\nConstructor description.\n\n\"\"\"\n\n", "func_signal": "def __init__(self, controller_reference, parameter_dictionary, module_logger):\n", "code": "fw_modules.module_template.ModuleClass.__init__(self, controller=controller_reference, param_dict=parameter_dictionary, logger=module_logger)\n# Helper values.\nself.event_id_counter = 0", "path": "fw_modules\\module_detection_engine.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"Function name\n\nOpens a pcap session and captures frames from a pcap dumpfile\n\tsupported from a Cisco WLC.\n\tNot yet implemented.\n\n\"\"\"\n  \n", "func_signal": "def capture_frames_from_cisco(self, filename):\n", "code": "\n\"\"\"capture_frames_from_file()\n\nOpens a pcap session and captures frames from a pcap dumpfile.\n\n\"\"\"\n\nself.module_logger.info(\"Opening file capture session for file \" + filename)\n# Capture offline from pcap file.\ncapture_descriptor = pcapy.open_offline(filename)\n# Next is needed for thread control because loop can't be aborted via condition and function has to return.\n(header, data) = capture_descriptor.next()\nwhile not self.stop_thread and header:\n    self.decode_WLAN_frame(header, data)\n    try:\n        (header, data) = capture_descriptor.next()          # throws exception at file end!?\n    except pcapy.PcapError:         # disable exception temporarily\n        return True\nself.module_logger.info(\"No more frames to parse from file \" + filename)\nreturn True", "path": "fw_modules\\module_capture_pcapy.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"after_run()\n\nWaits for an active capture thread to terminate.\n\n\"\"\"\n\n", "func_signal": "def after_run(self):\n", "code": "if self.capture_thread:\n    self.capture_thread.join(self.join_timeout)\nif self.capture_thread.isAlive():\n    self.module_logger.error(\"Couldn't terminate capture thread\")\n    return False\nreturn True", "path": "fw_modules\\module_capture_pcapy.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"input()\n\nInput interface.\nDecodes received data.\n\n\"\"\"\n\n", "func_signal": "def process(self, input):\n", "code": "self.module_logger.debug(\"Raw input: \" + str(input))\n# Decode and check received data.\ntry:\n   msg_dict = dict(item.split('_', 1) for item in input.split('|'))\nexcept ValueError as err:\n        self.module_logger.warning(\"Message information not valid; details: \" + err.__str__())\nelse:\n    # Relay message data to output templates.\n    for template_identifier, template_info in self.template_status_dict.items():\n        self.module_logger.info(\"Sending message data to template \" + template_identifier)\n        try:\n            template_info['template_reference'].template_input(msg_dict)\n        except AttributeError as err:\n            self.module_logger.error(\"Template reference for template \" + template_identifier + \" invalid or error in process method; details: \" + err.__str__())", "path": "fw_modules\\module_prevention_engine.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"template_shutdown()\n\nClose dumpfile.\n\n\"\"\"\n\n", "func_signal": "def template_shutdown(self):\n", "code": "self.template_logger.info(\"Shutting down\")\nclose(self.DUMPFILE)", "path": "fw_modules\\fw_output_templates\\template_dumper.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"before_run()\n\nCheck file presence if offline capture is requested.\n\tStart capture in a separate thread, either from file or interface.\n\n\"\"\"\n\n", "func_signal": "def before_run(self):\n", "code": "if self.capture_mode == 1:\n    if not os.path.isfile(os.path.abspath(self.capture_source)):\n        self.module_logger.error(\"Couldn't find capture file \" + self.capture_source)\n        return False \nif self.capture_mode == 0:\n    self.capture_thread = threading.Thread(target=self.capture_frames_from_live, args=(self.capture_source,))\n    self.capture_thread.start()\nelif self.capture_mode == 1:\n    self.capture_thread = threading.Thread(target=self.capture_frames_from_file, args=(self.capture_source,))\n    self.capture_thread.start()\nreturn True", "path": "fw_modules\\module_capture_pcapy.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"input()\n\nInput interface.\nDecodes received frame data.\n\n\"\"\"\n\n", "func_signal": "def template_input(self, input):\n", "code": "msg_stringified = []\n\nself.template_logger.debug(\"Raw input: \" + str(input))\ntry:\n    for msg_tag, msg_value in input.items():\n        msg_stringified.append(msg_tag + ':' + msg_value)\nexcept (AttributeError, TypeError, ValueError) as err:\n    self.template_logger.warning(\"Message data invalid; details: \" + err.__str__())\nelse:\n    try:\n        self.DUMPFILE.write(','.join(msg_stringified) + \"\\n\")\n        self.DUMPFILE.flush()\n    except IOError as err:\n        self.template_logger.warning(\"Couldn't dump to file; details: \" + err.__str__())", "path": "fw_modules\\fw_output_templates\\template_dumper.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"shutdown_submodule()\n\nTerminates a single submodule.\n\n\"\"\"\n\n# Check if module status is 'running' in status_dict prior\n", "func_signal": "def shutdown_submodule(self, module_identifier):\n", "code": "self.controller_logger.info(\"Shutting down module \" + module_identifier)\nif self.status_dict.has_key(module_identifier):\n    if self.status_dict[module_identifier]['module_status'] == 'running':\n        self.controller_logger.info(\"Stopping module\" + module_identifier)\n        module_reference = self.status_dict[module_identifier]['module_reference']\n        if module_reference:\n            # Call stop function of module instance and send a 'STOP' directive to its input buffer.\n            module_reference.stop()\n            module_reference.input('STOP')\n            module_reference.join(self.join_timeout)\n            for active_module_id in self.status_dict:\n                try:\n                    self.status_dict[active_module_id]['module_reference'].remove_target(module_identifier)         # remove inactive module from receiver lists of remaining modules\n                except FwNoStatusEntryError as err:\n                    self.controller.logger.error(\"Couldn't remove module \" + err.missingentry + \", not in status dict\")", "path": "fw_modules\\module_controller.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"add_submodule_receivers()\n\nAdds receivers to context information of a submodule\nin the controller's status dictionary.\n\n\"\"\"\n\n", "func_signal": "def add_submodule_receivers(self, module_identifier, module_receivers):\n", "code": "try:\n    self.status_dict[module_identifier]['module_receivers'] = module_receivers\nexcept KeyError:\n    self.controller_logger.error(\"Couldn't add receivers to module \" + module_identifier + \", not in status dict\")\nself.controller_logger.debug(\"Status dictionary: \" + str(self.status_dict))", "path": "fw_modules\\module_controller.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"capture_frames_from_live()\n\nOpen a pcap live session and capture frames from an interface.\n\n\"\"\"\n\n", "func_signal": "def capture_frames_from_live(self, interface):\n", "code": "self.module_logger.info(\"Opening live capture session on interface \" + interface)\n# Set max length for frames to not truncate them and set no read timeout.\ncapture_descriptor = pcapy.open_live(interface, 65535, 0, 0)\n# Next is needed for thread control because loop can't be aborted via condition, and function has to return.\nwhile not self.stop_thread:\n    (header, data) = capture_descriptor.next()\n    if header and data:\n        self.decode_WLAN_frame(header, data)\n    else:\n        break", "path": "fw_modules\\module_capture_pcapy.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"template_setup()\n\n\"\"\"\n\n", "func_signal": "def template_setup(self):\n", "code": "self.template_logger.info(\"Setting up template...\")\ntry:\n    self.DUMPFILE = open(self.dumpfile_path, \"w\")\nexcept IOError:\n    self.template_logger.error(\"Couldn't open file \" + self.dumpfile_path)\n    return False\nreturn True", "path": "fw_modules\\fw_output_templates\\template_dumper.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"set_module_status()\n\nSets status of module in status dictionary.\nIs called by submodules to update their status in the controller.\n\n\"\"\"\n\n", "func_signal": "def set_module_status(self, module_identifier, new_status):\n", "code": "self.controller_logger.info(\"Set status of module \" + module_identifier + \" to \" + new_status)\nself.status_dict[module_identifier]['module_status'] = new_status\nself.controller_logger.debug(\"Status dictionary: \" + str(self.status_dict))", "path": "fw_modules\\module_controller.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"\nRestart the daemon\n\"\"\"\n\n", "func_signal": "def restart(self):\n", "code": "self.stop()\nself.start()", "path": "fw_modules\\module_daemon.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"\nStart the daemon\n\"\"\"\n\n# Check for a pidfile to see if the daemon already runs\n", "func_signal": "def start(self):\n", "code": "try:\n    pf = file(self.pidfile,'r')\n    pid = int(pf.read().strip())\n    pf.close()\nexcept IOError:\n    pid = None\n    \nif pid:\n    message = \"pidfile %s already exist. Daemon already running?\\n\"\n    sys.stderr.write(message % self.pidfile)\n    sys.exit(1)\n\n# Start the daemon\nself.daemonize()\nself.run()", "path": "fw_modules\\module_daemon.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"\nbefore_run()\n\nLoads all requested detection templates.\n\n\"\"\"\n\n# Load detection templates.\n", "func_signal": "def before_run(self):\n", "code": "try:\n    self.load_templates()\nexcept FwTemplateSetupError as err:\n    self.module_logger.error(err.errmsg)\n    return False\nelse:\n    return True", "path": "fw_modules\\module_detection_engine.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"signal_handler()\n\nHandler for signal receipt.\n\n\"\"\"\n\n", "func_signal": "def signal_handler(self, signum, frame):\n", "code": "if signum == 1:             # SIGHUP\n    self.controller_logger.info(\"Received SIGHUP, restarting\")\n    self.restart()\nelif signum == 10:          # SIGUSR1\n    self.controller_logger.info(\"Received SIGUSR1, shutting down running modules\")\n    self.shutdown_all_submodules()\nelif signum == 15:          # SIGTERM\n    self.controller_logger.info(\"Received SIGTERM, shutting down\")\n    self.shutdown_controller()", "path": "fw_modules\\module_controller.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"Constructor\n\nSets default receiver groups.\n\n\"\"\"\n\n", "func_signal": "def __init__(self, controller_reference, parameter_dictionary, module_logger):\n", "code": "fw_modules.module_template.ModuleClass.__init__(self, controller=controller_reference, param_dict=parameter_dictionary, logger=module_logger)\n# Default values.\ntry:\n    self.command_receiver_group = self.param_dict['command_receiver_group']\nexcept KeyError:\n    self.command_receiver_group = 'command_receiver'\ntry:\n    self.prevention_receiver_group = self.param_dict['prevention_receiver_group']\nexcept KeyError:\n    self.prevention_receiver_group = 'prevention_receiver'\n    # Helper values.\nself.command_id_counter = 0", "path": "fw_modules\\module_prevention_engine.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\"add_submodule()\n\nAdds context information of a submodule to the controller's status dictionary.\n\n\"\"\"\n\n", "func_signal": "def add_submodule(self, module_identifier, module_reference, module_status):\n", "code": "self.controller_logger.info(\"Add module \" + module_identifier + \" with status \" + module_status + \" to status dictionary\")\nself.status_dict[module_identifier] = {'module_reference':module_reference, 'module_receivers':None,'module_status':module_status}\nself.controller_logger.debug(\"Status dictionary: \" + str(self.status_dict))", "path": "fw_modules\\module_controller.py", "repo_name": "pkrebs/WIDPS", "stars": 4, "license": "gpl-2.0", "language": "python", "size": 144}
{"docstring": "\"\"\" set the gtk.TreeModel that contains the current class tree.\nparser must be an instance of a subclass of ClassParserInterface. \"\"\"\n", "func_signal": "def set_model(self, treemodel, parser=None):\n", "code": "self.browser.set_model(treemodel)\nif parser:\n    self.column.set_cell_data_func(self.crt, parser.cellrenderer)\n    self.column.set_cell_data_func(self.cellrendererpixbuf, parser.pixbufrenderer)\nself.parser = parser\nself.browser.queue_draw()", "path": "plugins\\classbrowser\\browserwidget.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\" Parse a string containing a function or class definition and return\n    a tuple containing information about the function, or None if the\n    parsing failed.\n\n    Example: \n        \"#def foo(bar):\" would return :\n        {'comment':True,'type':\"def\",'name':\"foo\",'params':\"bar\" } \"\"\"\n\n", "func_signal": "def functionTokenFromString(string):\n", "code": "try:\n    e = r\"([# ]*?)([a-zA-Z0-9_]+)( +)([a-zA-Z0-9_]+)(.*)\"\n    r = re.match(e,string).groups()\n    token = Token()\n    token.comment = '#' in r[0]\n    token.type = r[1]\n    token.name = r[3]\n    token.params = r[4]\n    token.original = string\n    return token\nexcept: return None # return None to skip if unable to parse", "path": "plugins\\classbrowser\\parser_python.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\" Add new parsers here. \"\"\"\n", "func_signal": "def register_parsers(self, window):\n", "code": "self.tabwatch.defaultparser = CTagsParser()\nself.tabwatch.register_parser(\"Python\",PythonParser(window))", "path": "plugins\\classbrowser\\__init__.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nChecks whether some file type pertains to this plugin (i.e., can possibly be tidied).\n\nKeyword arguments:\n\nconfig_dict -- The configuration dictionary describing the user preferences (see config_dict.py)\nf_name -- File's name\nmime_type -- gedit's MIME type for the content\t\n\"\"\"\n", "func_signal": "def can_tidy(config_dict, f_name, mime_type):\n", "code": "log_utils.debug('checking if can tidy %s with mime type %s' % (f_name, mime_type))\n\nv = config_dict[consts.type_config_category]\n\nif v == consts.mime_type_config:\n\treturn _can_tidy_mime_type(mime_type)\t\t\nif v == consts.ext_type_config:\n\treturn _can_tidy_ext(config_dict, f_name)\nif v == consts.all_type_config:\n\treturn _can_tidy_all_type()\t\t\n\nlog_utils.warn('can\\'t figure out type_config_category %s in config dict' % v)\n\nassert False\n\nreturn True", "path": "plugins\\html-tidy\\file_types_filter.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nWrites the configuration dictionary to a predefined file (defined in consts.py).\n\"\"\"\n", "func_signal": "def write_config_dict(d):\n", "code": "log_utils.debug('writing config dict')\n\t\t\t\t\ncustom_dict = tidy_opt_utils.names_dicts_to_dict(d[consts.custom_opts_names_dicts_category])\ntidy_opt_utils.write_dict(custom_dict, consts.custom_opt_file_name)\n\ntmp_d = {}\n\nfor k in [k for k in d.keys() if k != consts.custom_opts_names_dicts_category]:\n\ttmp_d[k] = d[k]\n\nf_name = os.path.join(gen_utils.data_dir(), consts.config_f_name)\n\nf = open(f_name, 'w')\nopt_stream_utils.dict_to_opt_stream(tmp_d, f)\nf.close()\n\t\nlog_utils.debug('wrote config dict')", "path": "plugins\\html-tidy\\config_dict.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nReturns a configuration dictionary with the default choices. Used when can't open\n\ta file describing these choices.\n\"\"\"\n", "func_signal": "def _default_config_dict():\n", "code": "d = {}\n\nd[consts.tidy_opts_config_category] = consts.default_tidy_opts_config\n\nd[consts.type_config_category] = consts.mime_type_config\n\nd[consts.type_ext_category] = consts.html_xhtml_and_xml_exts\n\nd[consts.opt_file_name_category] = consts.opt_file_name\n\nd[consts.custom_opts_names_dicts_category] = tidy_opt_utils.default_names_dicts()\n\nreturn d", "path": "plugins\\html-tidy\\config_dict.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nKeyword arguments:\ndict -- An options dictionary within some HTML-Tidy options category.\n\t    sensitive -- Wether the options are sensitive (i.e., can be modified).\n\"\"\"\n", "func_signal": "def __init__(self, opt_dict, sensitive):\n", "code": "super(tab, self).__init__(0, 0)\n\nself._dict = opt_dict.copy()\n\nself.set_border_width(10)\n\nnames = self._dict.keys()\nnames.sort()\n\ni = 0\nnum_vert = 16\n\nhbox = gtk.HBox(True,  4)\n\t\nwhile i <= len(names):\n\tvbox = gtk.VBox(True,  2)\n\n\tfor name in names[i : min(len(names), i + num_vert)]:\n\t\twidget = self._make_widget(name, self._dict[name], sensitive)\n\t\tvbox.pack_start(widget, True, False)\n\t\n\tvbox.show()\n\t\n\thbox.pack_start(vbox, False, True)\n\n\ti = i + num_vert\n\t\nself.add(hbox)\n\nself.show()", "path": "plugins\\html-tidy\\opts_tab.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nKeyword arguments:\n\t    parent -- gtk.Window parent.\n\t    tabs -- A list of pairs of (logical category name, options dictionary within the category).\n\t    sensitive -- Wether the options are sensitive (i.e., can be modified).\n\"\"\"\n", "func_signal": "def __init__(self, parent, tabs, sensitive):\n", "code": "title = 'HTML-Tidy Options'\nflags = gtk.DIALOG_MODAL | gtk.DIALOG_DESTROY_WITH_PARENT\t\nif sensitive:\n\tbuttons = (gtk.STOCK_OK, gtk.RESPONSE_OK, gtk.STOCK_CANCEL, gtk.RESPONSE_CANCEL)\nelse:\n\tbuttons = (gtk.STOCK_OK, gtk.RESPONSE_OK)\t\t      \n\nsuper(dlg, self).__init__(title, parent, flags, buttons)\n\t\t      \nlog_utils.debug('setting up opts dialog')\n\nself._n = opts_notebook.notebook(tabs, sensitive)\n\nself.vbox.pack_start(self._n, True, True)\n\nlog_utils.debug('set up opts dialog')\n\nself.show_all()", "path": "plugins\\html-tidy\\opts_dlg.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\" Append a class attribute to the class a given token belongs to. \"\"\"\n\n# get next parent class\n", "func_signal": "def __appendClassAttribute(self, token, attrName, linenumber):\n", "code": "while token.type != \"class\":\n    token = token.parent\n    if not token: return   \n    \n# make sure attribute is not set yet\nfor i in token.attributes:\n    if i.name == attrName: return\n             \n# append a new attribute\nattr = Token()\nattr.type = \"attribute\"\nattr.name = attrName\nattr.start = linenumber\nattr.end = linenumber\nattr.pythonfile = self\ntoken.attributes.append(attr)", "path": "plugins\\classbrowser\\parser_python.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\" Get the line number where this token's declaration, including all\n    its children, finishes. Use it for copy operations.\"\"\"\n", "func_signal": "def get_endline(self):\n", "code": "if len(self.children) > 0:\n    return self.children[-1].get_endline()\nreturn self.end\n\ndef test_nested():\n    pass", "path": "plugins\\classbrowser\\parser_python.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\" get the token at the specified line number \"\"\"\n", "func_signal": "def getTokenAtLine(self, line):\n", "code": "for token in self.tokens:\n    if token.start <= line and token.end > line:\n        return token\nreturn None", "path": "plugins\\classbrowser\\parser_python.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nGiven a configuration dictionary, returns the effective HTML-Tidy options dictionary (default, from file, or custom).\n\"\"\"\n", "func_signal": "def effective_opts_dict(d):\n", "code": "k = d[consts.tidy_opts_config_category]\n\nif k == consts.default_tidy_opts_config:\n\treturn tidy_opt_utils.names_dicts_to_dict( tidy_opt_utils.default_names_dicts() )\nelif  k == consts.from_file_tidy_opts_config:\n\treturn tidy_opt_utils.read_dict( d[consts.opt_file_name_category] )\nelif  k == consts.custom_tidy_opts_config:\n\treturn tidy_opt_utils.names_dicts_to_dict( d[consts.custom_opts_names_dicts_category] )\nelse:\n\tassert False", "path": "plugins\\html-tidy\\config_dict.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nReturns a list of pairs of (logical category name, options dictionary within the category).\n\"\"\"\n", "func_signal": "def names_dicts(self):\n", "code": "children = [super(notebook, self).get_nth_page(i) for i in range(super(notebook, self).get_n_pages())]\n\nreturn [(super(notebook, self).get_tab_label_text(child), child.opts_dict()) for child in children]", "path": "plugins\\html-tidy\\opts_notebook.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nKeyword arguments:\n\t    tabs -- A list of pairs of (logical category name, options dictionary within the category).\n\t    sensitive -- Wether the options are sensitive (i.e., can be modified).\n\"\"\"\n", "func_signal": "def __init__(self, tabs, sensitive):\n", "code": "log_utils.debug('setting up opts notebook')\n\nsuper(notebook, self).__init__()\n\nfor (tab_name, opts_dict) in tabs:\n\to = opts_tab.tab(opts_dict, sensitive)\t\t\t\n\tself.append_page(o, gtk.Label(tab_name))\n\t\t\nlog_utils.debug('set up opts notebook')", "path": "plugins\\html-tidy\\opts_notebook.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nKeyword arguments:\n\t    on_activated -- Callback for when a row is activated.\n\"\"\"\n", "func_signal": "def __init__(self, on_activated):\n", "code": "super(output_pane, self).__init__()\n\t\t\nself.set_policy(gtk.POLICY_NEVER, gtk.POLICY_AUTOMATIC);\nself.set_shadow_type(gtk.SHADOW_IN)\n\nself._box = _output_box(on_activated)\n\t\t\nself.add_with_viewport(self._box)\nself._box.show()\n\nself.target_uri = None", "path": "plugins\\html-tidy\\output_pane.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nClear all rows.\n\"\"\"\n", "func_signal": "def clear(self):\n", "code": "self._box.clear()\n\nself.target_uri = None", "path": "plugins\\html-tidy\\output_pane.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\" geditwindow -- an instance of gedit.Window \"\"\"\n\n", "func_signal": "def __init__(self, geditwindow):\n", "code": "imagelibrary.initialise()\n\ngtk.VBox.__init__(self)\nself.geditwindow = geditwindow\n\ntry: self.encoding = gedit.encoding_get_current()\nexcept: self.encoding = gedit.gedit_encoding_get_current()\n\nself.active_timeout = False\n\nself.parser = None\nself.document_history = [] # contains tuple (doc,line,col)\nself.history_pos = 0\nself.previousline = 0\n\nself.back = gtk.ToolButton(gtk.STOCK_GO_BACK)\nself.back.connect(\"clicked\",self.history_back)\nself.back.set_sensitive(False)\nself.forward = gtk.ToolButton(gtk.STOCK_GO_FORWARD)\nself.forward.connect(\"clicked\",self.history_forward)\nself.forward.set_sensitive(False)\n\ntb = gtk.Toolbar()\ntb.add(self.back)\ntb.add(self.forward)\n#self.pack_start(tb,False,False)\n\n# add a treeview\nsw = gtk.ScrolledWindow()\nsw.set_policy(gtk.POLICY_AUTOMATIC, gtk.POLICY_AUTOMATIC)\nsw.set_shadow_type(gtk.SHADOW_IN)\nself.browser = gtk.TreeView()\nself.browser.set_headers_visible(False)\nsw.add(self.browser)\nself.browser.connect(\"button_press_event\",self.__onClick)\nself.pack_start(sw)\n\n# add a text column to the treeview\nself.column = gtk.TreeViewColumn()\nself.browser.append_column(self.column)\n\nself.cellrendererpixbuf = gtk.CellRendererPixbuf()\nself.column.pack_start(self.cellrendererpixbuf,False)\n\nself.crt = gtk.CellRendererText()\nself.column.pack_start(self.crt,False)\n\n# connect stuff\nself.browser.connect(\"row-activated\",self.on_row_activated)\nself.show_all()", "path": "plugins\\classbrowser\\browserwidget.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"\nI need to catch changes in the cursor position to highlight the current tag\nin the class browser. Unfortunately, there is no signal that gets emitted\n*after* the cursor has been changed, so I have to use a timeout.\n\"\"\"\n", "func_signal": "def on_cursor_changed(self, *args):\n", "code": "if not self.active_timeout:\n    gobject.timeout_add(100,self.update_cursor)\n    self.active_timeout = True", "path": "plugins\\classbrowser\\browserwidget.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\" \nCreate a gtk.TreeModel with the class elements of the document\n\nThe parser uses the ctags command from the shell to create a ctags file,\nthen parses the file, and finally populates a treemodel.\n\"\"\"\n    \n", "func_signal": "def parse(self, doc):\n", "code": "self.pythonfile = PythonFile(doc)\nself.pythonfile.parse(options.singleton().verbose)\nself.__browsermodel = gtk.TreeStore(gobject.TYPE_PYOBJECT)\nfor child in self.pythonfile.children:\n    self.appendTokenToBrowser(child,None)\nreturn self.__browsermodel", "path": "plugins\\classbrowser\\parser_python.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\" Try to get the class a token is in. \"\"\"\n    \n", "func_signal": "def get_toplevel_class(self):\n", "code": "if self.type == \"class\":\n    return self    \n\nif self.parent is not None:\n    tc = self.parent.get_toplevel_class()\n    if tc is None or tc.type == \"file\": return self #hack\n    else: return tc\n        \nreturn None", "path": "plugins\\classbrowser\\parser_python.py", "repo_name": "shiloa/power-gedit", "stars": 4, "license": "None", "language": "python", "size": 404}
{"docstring": "\"\"\"Template tag registered as 'auth_login_url' and 'auth_logout_url'\nwhen the module is imported.\n\nBoth tags take an optional argument that specifies the redirect URL and\ndefaults to '/'.\n\"\"\"\n", "func_signal": "def auth_login_urls(parser, token):\n", "code": "bits = list(token.split_contents())\nif len(bits) == 2:\n  redirect = bits[1]\nelse:\n  redirect = \"/\"\nlogin = bits[0] == \"auth_login_url\"\nreturn AuthLoginUrlsNode(login, redirect)", "path": "appengine_django\\auth\\templatetags.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Tests that a simple set/get operation through the cache works.\"\"\"\n", "func_signal": "def testSimpleSetGet(self):\n", "code": "self._cache.set(\"test_key\", \"test_value\")\nself.assertEqual(self._cache.get(\"test_key\"), \"test_value\")", "path": "appengine_django\\tests\\memcache_test.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Tests the parts of a model required by Django are correctly stubbed.\"\"\"\n# Django requires model options to be found at ._meta.\n", "func_signal": "def testDjangoModelClass(self):\n", "code": "self.assert_(isinstance(RegistrationTestModel._meta, ModelOptions))\n# Django requires a manager at .objects\nself.assert_(isinstance(RegistrationTestModel.objects, ModelManager))\n# Django requires ._default_manager.\nself.assert_(hasattr(RegistrationTestModel, \"_default_manager\"))", "path": "appengine_django\\tests\\model_test.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Tests that delete removes values from the cache.\"\"\"\n", "func_signal": "def testDelete(self):\n", "code": "self._cache.set(\"test_key\", \"test_value\")\nself.assertEqual(self._cache.has_key(\"test_key\"), True)\nself._cache.delete(\"test_key\")\nself.assertEqual(self._cache.has_key(\"test_key\"), False)", "path": "appengine_django\\tests\\memcache_test.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Tests that each model instance has a 'primary key' generated.\"\"\"\n", "func_signal": "def testDjangoModelPK(self):\n", "code": "obj = RegistrationTestModel(key_name=\"test\")\nobj.put()\npk = obj._get_pk_val()\nself.assert_(pk)\nnew_obj = RegistrationTestModel.get(pk)\nself.assertEqual(obj.key(), new_obj.key())", "path": "appengine_django\\tests\\model_test.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Replace Django ModelForm with the AppEngine ModelForm.\"\"\"\n# This MUST happen as early as possible, but after any auth model patching.\n", "func_signal": "def InstallModelForm():\n", "code": "from google.appengine.ext.db import djangoforms as aeforms\ntry:\n  # pre 1.0\n  from django import newforms as forms\nexcept ImportError:\n  from django import forms\n\nforms.ModelForm = aeforms.ModelForm\n\n# Extend ModelForm with support for EmailProperty\n# TODO: This should be submitted to the main App Engine SDK.\nfrom google.appengine.ext.db import EmailProperty\ndef get_form_field(self, **kwargs):\n  \"\"\"Return a Django form field appropriate for an email property.\"\"\"\n  defaults = {'form_class': forms.EmailField}\n  defaults.update(kwargs)\n  return super(EmailProperty, self).get_form_field(**defaults)\nEmailProperty.get_form_field = get_form_field", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Disables Django's model validation routines.\n\nThe model validation is primarily concerned with validating foreign key\nreferences. There is no equivalent checking code for datastore References at\nthis time.\n\nValidation needs to be disabled or serialization/deserialization will fail.\n\"\"\"\n", "func_signal": "def DisableModelValidation():\n", "code": "from django.core.management import validation\nvalidation.get_validation_errors = lambda x, y=0: 0\nlogging.debug(\"Django SQL model validation disabled\")", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "# Ensure the Django zipfile is in the path if required.\n", "func_signal": "def main():\n", "code": "if have_django_zip and django_zip_path not in sys.path:\n  sys.path.insert(1, django_zip_path)\n\n# Create a Django application for WSGI.\napplication = django.core.handlers.wsgi.WSGIHandler()\n\n# Run the WSGI CGI handler with that application.\nutil.run_wsgi_app(application)", "path": "main.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Install a replacement for the imp module removed by the appserver.\n\nThis is only to find mangement modules provided by applications.\n\"\"\"\n", "func_signal": "def InstallReplacementImpModule():\n", "code": "if not have_appserver:\n  return\nmodname = 'appengine_django.replacement_imp'\nimp_mod = __import__(modname, {}, [], [''])\nsys.modules['imp'] = imp_mod\nlogging.debug(\"Installed replacement imp module\")", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Tests that the options stub has the required properties by Django.\"\"\"\n# Django requires object_name and app_label for serialization output.\n", "func_signal": "def testDjangoModelOptionsStub(self):\n", "code": "self.assertEqual(\"RegistrationTestModel\",\n                 RegistrationTestModel._meta.object_name)\nself.assertEqual(\"appengine_django\", RegistrationTestModel._meta.app_label)\n# The pk.name member is required during serialization for dealing with\n# related fields.\nself.assertEqual(\"key_name\", RegistrationTestModel._meta.pk.name)\n# The many_to_many method is called by Django in the serialization code to\n# find m2m relationships. m2m is not supported by the datastore.\nself.assertEqual([], RegistrationTestModel._meta.many_to_many)", "path": "appengine_django\\tests\\model_test.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"\nGiven a path to a management directory, returns a list of all the command\nnames that are available.\n\nThis implementation also works when Django is loaded from a zip.\n\nReturns an empty list if no commands are defined.\n\"\"\"\n", "func_signal": "def FindCommandsInZipfile(management_dir):\n", "code": "zip_marker = \".zip%s\" % os.sep\nif zip_marker not in management_dir:\n  return FindCommandsInZipfile.orig(management_dir)\n\n# Django is sourced from a zipfile, ask zip module for a list of files.\nfilename, path = management_dir.split(zip_marker)\nzipinfo = zipfile.ZipFile(\"%s.zip\" % filename)\n\n# Add commands directory to management path.\npath = os.path.join(path, \"commands\")\n\n# The zipfile module returns paths in the format of the operating system\n# that created the zipfile! This may not match the path to the zipfile\n# itself. Convert operating system specific characters to a standard\n# character (#) to compare paths to work around this.\npath_normalise = re.compile(r\"[/\\\\]\")\npath = path_normalise.sub(\"#\", path)\ndef _IsCmd(t):\n  \"\"\"Returns true if t matches the criteria for a command module.\"\"\"\n  filename = os.path.basename(t)\n  t = path_normalise.sub(\"#\", t)\n  if not t.startswith(path):\n    return False\n  if filename.startswith(\"_\") or not t.endswith(\".py\"):\n    return False\n  return True\n\nreturn [os.path.basename(f)[:-3] for f in zipinfo.namelist() if _IsCmd(f)]", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Installs the appengine database backend into Django.\n\nThe appengine database lives in the db/ subdirectory of this package, but is\nknown as \"appengine\" to Django. This function installs the module where\nDjango expects to find its database backends.\n\"\"\"\n", "func_signal": "def InstallAppengineDatabaseBackend():\n", "code": "from appengine_django import db\nsys.modules['django.db.backends.appengine'] = db\nlogging.debug(\"Installed appengine database backend\")", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Patches the DeserializedObject class.\n\nThe default implementation calls save directly on the django Model base\nclass to avoid pre-save handlers. The model class provided by this package\nis not derived from the Django Model class and therefore must be called\ndirectly.\n\nAdditionally we need to clear the internal _parent attribute as it may\ncontain a FakeParent class that is used to deserialize instances without\nneeding to load the parent instance itself. See the PythonDeserializer for\nmore details.\n\"\"\"\n# This can't be imported until InstallAppengineDatabaseBackend has run.\n", "func_signal": "def PatchDeserializedObjectClass():\n", "code": "from django.core.serializers import base\nclass NewDeserializedObject(base.DeserializedObject):\n  def save(self, save_m2m=True):\n    self.object.save()\n    self.object._parent = None\nbase.DeserializedObject = NewDeserializedObject\nlogging.debug(\"Replacement DeserializedObject class installed\")", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Installs and Patches all of the classes/methods required for integration.\n\nIf the variable DEBUG_APPENGINE_DJANGO is set in the environment verbose\nlogging of the actions taken will be enabled.\n\"\"\"\n", "func_signal": "def InstallAppengineHelperForDjango():\n", "code": "if VERSION < (1, 0, None):\n  logging.error(\"Django 1.0 or greater is required!\")\n  sys.exit(1)\n\nif os.getenv(\"DEBUG_APPENGINE_DJANGO\"):\n  logging.getLogger().setLevel(logging.DEBUG)\nelse:\n  logging.getLogger().setLevel(logging.INFO)\nlogging.debug(\"Loading the Google App Engine Helper for Django...\")\n\n# Force Django to reload its settings.\nsettings._target = None\n\nLoadAppengineEnvironment()\nInstallReplacementImpModule()\nInstallAppengineDatabaseBackend()\nInstallModelForm()\nInstallGoogleMemcache()\nInstallDjangoModuleReplacements()\nPatchDjangoSerializationModules()\nCleanupDjangoSettings()\nModifyAvailableCommands()\nInstallGoogleSMTPConnection()\nInstallAuthentication()\n\nlogging.debug(\"Successfully loaded the Google App Engine Helper for Django.\")", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Tests that a combined model class has (faked) Django fields.\"\"\"\n", "func_signal": "def testDjangoModelFields(self):\n", "code": "fields = TestModelWithProperties._meta.local_fields\nself.assertEqual(3, len(fields))\n# Check each fake field has the minimal properties that Django needs.\nfor field in fields:\n  # The Django serialization code looks for rel to determine if the field\n  # is a relationship/reference to another model.\n  self.assert_(hasattr(field, \"rel\"))\n  # serialize is required to tell Django to serialize the field.\n  self.assertEqual(True, field.serialize)\n  if field.name == \"property3\":\n    # Extra checks for the Reference field.\n    # rel.field_name is used during serialization to find the field in the\n    # other model that this field is related to. This should always be\n    # 'key_name' for appengine models.\n    self.assertEqual(\"key_name\", field.rel.field_name)", "path": "appengine_django\\tests\\model_test.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Removes incompatible entries from the django settings module.\"\"\"\n\n# Ensure this module is installed as an application.\n", "func_signal": "def CleanupDjangoSettings():\n", "code": "apps = getattr(settings, \"INSTALLED_APPS\", ())\nfound = False\nfor app in apps:\n  if app.endswith(\"appengine_django\"):\n    found = True\n    break\nif not found:\n  logging.warn(\"appengine_django module is not listed as an application!\")\n  apps += (\"appengine_django\",)\n  setattr(settings, \"INSTALLED_APPS\", apps)\n  logging.info(\"Added 'appengine_django' as an application\")\n\n# Ensure the database backend is appropriately configured.\ndbe = getattr(settings, \"DATABASE_ENGINE\", \"\")\nif dbe != \"appengine\":\n  settings.DATABASE_ENGINE = \"appengine\"\n  logging.warn(\"DATABASE_ENGINE is not configured as 'appengine'. \"\n               \"Value overriden!\")\nfor var in [\"NAME\", \"USER\", \"PASSWORD\", \"HOST\", \"PORT\"]:\n  val = getattr(settings, \"DATABASE_%s\" % var, \"\")\n  if val:\n    setattr(settings, \"DATABASE_%s\" % var, \"\")\n    logging.warn(\"DATABASE_%s should be blank. Value overriden!\")\n\n# Remove incompatible middleware modules.\nmw_mods = list(getattr(settings, \"MIDDLEWARE_CLASSES\", ()))\ndisallowed_middleware_mods = (\n  'django.middleware.doc.XViewMiddleware',)\nfor modname in mw_mods[:]:\n  if modname in disallowed_middleware_mods:\n    # Currently only the CommonMiddleware has been ported.  As other base\n    # modules are converted, remove from the disallowed_middleware_mods\n    # tuple.\n    mw_mods.remove(modname)\n    logging.warn(\"Middleware module '%s' is not compatible. Removed!\" %\n                 modname)\nsetattr(settings, \"MIDDLEWARE_CLASSES\", tuple(mw_mods))\n\n# Remove incompatible application modules\napp_mods = list(getattr(settings, \"INSTALLED_APPS\", ()))\ndisallowed_apps = (\n  'django.contrib.contenttypes',\n  'django.contrib.sites',)\nfor app in app_mods[:]:\n  if app in disallowed_apps:\n    app_mods.remove(app)\n    logging.warn(\"Application module '%s' is not compatible. Removed!\" % app)\nsetattr(settings, \"INSTALLED_APPS\", tuple(app_mods))\n\n# Remove incompatible session backends.\nsession_backend = getattr(settings, \"SESSION_ENGINE\", \"\")\nif session_backend.endswith(\"file\"):\n  logging.warn(\"File session backend is not compatible. Overriden \"\n               \"to use db backend!\")\n  setattr(settings, \"SESSION_ENGINE\", \"django.contrib.sessions.backends.db\")", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Installs the Google memcache into Django.\n\nBy default django tries to import standard memcache module.\nBecause appengine memcache is API compatible with Python memcache module,\nwe can trick Django to think it is installed and to use it.\n\nNow you can use CACHE_BACKEND = 'memcached://' in settings.py. IP address\nand port number are not required.\n\"\"\"\n", "func_signal": "def InstallGoogleMemcache():\n", "code": "from google.appengine.api import memcache\nsys.modules['memcache'] = memcache\nlogging.debug(\"Installed App Engine memcache backend\")", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Monkey patches the Django serialization modules.\n\nThe standard Django serialization modules to not correctly handle the\ndatastore models provided by this package. This method installs replacements\nfor selected modules and methods to give Django the capability to correctly\nserialize and deserialize datastore models.\n\"\"\"\n# These can't be imported until InstallAppengineDatabaseBackend has run.\n", "func_signal": "def PatchDjangoSerializationModules():\n", "code": "from django.core.serializers import python\nfrom appengine_django.serializer.python import Deserializer\nif not hasattr(settings, \"SERIALIZATION_MODULES\"):\n  settings.SERIALIZATION_MODULES = {}\nbase_module = \"appengine_django\"\nsettings.SERIALIZATION_MODULES[\"xml\"] = \"%s.serializer.xml\" % base_module\npython.Deserializer = Deserializer\nPatchDeserializedObjectClass()\nDisableModelValidation()\nlogging.debug(\"Installed appengine json and python serialization modules\")", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Removes incompatible commands from the specified command dictionary.\"\"\"\n", "func_signal": "def RemoveCommands(command_dict):\n", "code": "for cmd in command_dict.keys():\n  if cmd.startswith(\"sql\"):\n    del command_dict[cmd]\n  elif cmd in INCOMPATIBLE_COMMANDS:\n    del command_dict[cmd]", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Removes incompatible commands and installs replacements where possible.\"\"\"\n", "func_signal": "def ModifyAvailableCommands():\n", "code": "if have_appserver:\n  # Commands are not used when running from an appserver.\n  return\nfrom django.core import management\nproject_directory = os.path.join(__path__[0], \"../\")\nif have_django_zip:\n  FindCommandsInZipfile.orig = management.find_commands\n  management.find_commands = FindCommandsInZipfile\nmanagement.get_commands()\n# Replace startapp command which is set by previous call to get_commands().\nfrom appengine_django.management.commands.startapp import ProjectCommand\nmanagement._commands['startapp'] = ProjectCommand(project_directory) \nRemoveCommands(management._commands)\nlogging.debug(\"Removed incompatible Django manage.py commands\")", "path": "appengine_django\\__init__.py", "repo_name": "mattwilliamson/Browser-Based-Map-Reduce", "stars": 5, "license": "None", "language": "python", "size": 3776}
{"docstring": "\"\"\"Send a LINKS command.\"\"\"\n", "func_signal": "def links(self, remote_server=\"\", server_mask=\"\"):\n", "code": "command = \"LINKS\"\nif remote_server:\n    command = command + \" \" + remote_server\nif server_mask:\n    command = command + \" \" + server_mask\nself.send_raw(command)", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _handle_event(self, connection, event):\n", "code": "h = self.handlers\nfor handler in h.get(\"all_events\", []) + h.get(event.eventtype(), []):\n    if handler[1](connection, event) == \"NO MORE\":\n        return", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Clear mode on the channel.\n\nArguments:\n\n    mode -- The mode (a single-character string).\n\n    value -- Value\n\"\"\"\n", "func_signal": "def clear_mode(self, mode, value=None):\n", "code": "try:\n    if mode == \"o\":\n        del self.operdict[value]\n    elif mode == \"v\":\n        del self.voiceddict[value]\n    else:\n        del self.modes[mode]\nexcept KeyError:\n    pass", "path": "wheelman\\libs\\ircbot.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Constructor of Event objects.\n\nArguments:\n\n    eventtype -- A string describing the event.\n\n    source -- The originator of the event (a nick mask or a server).\n\n    target -- The target of the event (a nick or a channel).\n\n    arguments -- Any event specific arguments.\n\"\"\"\n", "func_signal": "def __init__(self, eventtype, source, target, arguments=None):\n", "code": "self._eventtype = eventtype\nself._source = source\nself._target = target\nif arguments:\n    self._arguments = arguments\nelse:\n    self._arguments = []", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Get the (real) server name.\n\nThis method returns the (real) server name, or, more\nspecifically, what the server calls itself.\n\"\"\"\n\n", "func_signal": "def get_server_name(self):\n", "code": "if self.real_server_name:\n    return self.real_server_name\nelse:\n    return \"\"", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Process data from connections once.\n\nArguments:\n\n    timeout -- How long the select() call should wait if no\n               data is available.\n\nThis method should be called periodically to check and process\nincoming data, if there are any.  If that seems boring, look\nat the process_forever method.\n\"\"\"\n", "func_signal": "def process_once(self, timeout=0):\n", "code": "sockets = map(lambda x: x._get_socket(), self.connections)\nsockets = filter(lambda x: x != None, sockets)\nif sockets:\n    (i, o, e) = select.select(sockets, [], [], timeout)\n    self.process_data(i)\nelse:\n    time.sleep(timeout)\nself.process_timeout()", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Default handler for ctcp events.\n\nReplies to VERSION and PING requests and relays DCC requests\nto the on_dccchat method.\n\"\"\"\n", "func_signal": "def on_ctcp(self, c, e):\n", "code": "if e.arguments()[0] == \"VERSION\":\n    c.ctcp_reply(nm_to_n(e.source()),\n                 \"VERSION \" + self.get_version())\nelif e.arguments()[0] == \"PING\":\n    if len(e.arguments()) > 1:\n        c.ctcp_reply(nm_to_n(e.source()),\n                     \"PING \" + e.arguments()[1])\nelif e.arguments()[0] == \"DCC\" and e.arguments()[1].split(\" \", 1)[0] == \"CHAT\":\n    self.on_dccchat(c, e)", "path": "wheelman\\libs\\ircbot.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Listen for connections from a DCC peer.\n\nReturns a DCCConnection instance.\n\"\"\"\n", "func_signal": "def dcc_listen(self, dcctype=\"chat\"):\n", "code": "dcc = self.ircobj.dcc(dcctype)\nself.dcc_connections.append(dcc)\ndcc.listen()\nreturn dcc", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _connected_checker(self):\n", "code": "if not self.connection.is_connected():\n    self.connection.execute_delayed(self.reconnection_interval,\n                                    self._connected_checker)\n    self.jump_server()", "path": "wheelman\\libs\\ircbot.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Disconnects all connections.\"\"\"\n", "func_signal": "def disconnect_all(self, message=\"\"):\n", "code": "for c in self.connections:\n    c.disconnect(message)", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Send a TOPIC command.\"\"\"\n", "func_signal": "def topic(self, channel, new_topic=None):\n", "code": "if new_topic is None:\n    self.send_raw(\"TOPIC \" + channel)\nelse:\n    self.send_raw(\"TOPIC %s :%s\" % (channel, new_topic))", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Constructor for SingleServerIRCBot objects.\n\nArguments:\n\n    server_list -- A list of tuples (server, port) that\n                   defines which servers the bot should try to\n                   connect to.\n\n    nickname -- The bot's nickname.\n\n    realname -- The bot's realname.\n\n    reconnection_interval -- How long the bot should wait\n                             before trying to reconnect.\n\n    dcc_connections -- A list of initiated/accepted DCC\n    connections.\n\"\"\"\n\n", "func_signal": "def __init__(self, server_list, nickname, realname, reconnection_interval=60):\n", "code": "SimpleIRCClient.__init__(self)\nself.channels = IRCDict()\nself.server_list = server_list\nif not reconnection_interval or reconnection_interval < 0:\n    reconnection_interval = 2**31\nself.reconnection_interval = reconnection_interval\n\nself._nickname = nickname\nself._realname = realname\nfor i in [\"disconnect\", \"join\", \"kick\", \"mode\",\n          \"namreply\", \"nick\", \"part\", \"quit\"]:\n    self.connection.add_global_handler(i,\n                                       getattr(self, \"_on_\" + i),\n                                       -10)", "path": "wheelman\\libs\\ircbot.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _on_part(self, c, e):\n", "code": "nick = nm_to_n(e.source())\nchannel = e.target()\n\nif nick == c.get_nickname():\n    del self.channels[channel]\nelse:\n    self.channels[channel].remove_user(nick)", "path": "wheelman\\libs\\ircbot.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Get the user part of a nickmask.\n\n(The source of an Event is a nickmask.)\n\"\"\"\n", "func_signal": "def nm_to_u(s):\n", "code": "s = s.split(\"!\")[1]\nreturn s.split(\"@\")[0]", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Execute a function after a specified time.\n\nArguments:\n\n    delay -- How many seconds to wait.\n\n    function -- Function to call.\n\n    arguments -- Arguments to give the function.\n\"\"\"\n", "func_signal": "def execute_delayed(self, delay, function, arguments=()):\n", "code": "bisect.insort(self.delayed_commands, (delay+time.time(), function, arguments))\nif self.fn_to_add_timeout:\n    self.fn_to_add_timeout(delay)", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Check if a nick matches a mask.\n\nReturns true if the nick matches, otherwise false.\n\"\"\"\n", "func_signal": "def mask_matches(nick, mask):\n", "code": "nick = irc_lower(nick)\nmask = irc_lower(mask)\nmask = mask.replace(\"\\\\\", \"\\\\\\\\\")\nfor ch in \".$|[](){}+\":\n    mask = mask.replace(ch, \"\\\\\" + ch)\nmask = mask.replace(\"?\", \".\")\nmask = mask.replace(\"*\", \".*\")\nr = re.compile(mask, re.IGNORECASE)\nreturn r.match(nick)", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Send a WHOWAS command.\"\"\"\n", "func_signal": "def whowas(self, nick, max=\"\", server=\"\"):\n", "code": "self.send_raw(\"WHOWAS %s%s%s\" % (nick,\n                                 max and (\" \" + max),\n                                 server and (\" \" + server)))", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"[Internal]\"\"\"\n", "func_signal": "def _on_quit(self, c, e):\n", "code": "nick = nm_to_n(e.source())\nfor ch in self.channels.values():\n    if ch.has_user(nick):\n        ch.remove_user(nick)", "path": "wheelman\\libs\\ircbot.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Removes a global handler function.\n\nArguments:\n\n    event -- Event type (a string).\n\n    handler -- Callback function.\n\nReturns 1 on success, otherwise 0.\n\"\"\"\n", "func_signal": "def remove_global_handler(self, event, handler):\n", "code": "if not event in self.handlers:\n    return 0\nfor h in self.handlers[event]:\n    if handler == h[1]:\n        self.handlers[event].remove(h)\nreturn 1", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\"Called when a timeout notification is due.\n\nSee documentation for IRC.__init__.\n\"\"\"\n", "func_signal": "def process_timeout(self):\n", "code": "t = time.time()\nwhile self.delayed_commands:\n    if t >= self.delayed_commands[0][0]:\n        self.delayed_commands[0][1](*self.delayed_commands[0][2])\n        del self.delayed_commands[0]\n    else:\n        break", "path": "wheelman\\libs\\irclib.py", "repo_name": "sshirokov/WheelMan", "stars": 4, "license": "None", "language": "python", "size": 136}
{"docstring": "\"\"\" Return a relative path from self to dest.\n\nIf there is no relative path from self to dest, for example if\nthey reside on different drives in Windows, then this returns\ndest.abspath().\n\"\"\"\n", "func_signal": "def relpathto(self, dest):\n", "code": "origin = self.abspath()\ndest = self.__class__(dest).abspath()\n\norig_list = origin.normcase().splitall()\n# Don't normcase dest!  We want to preserve the case.\ndest_list = dest.splitall()\n\nif orig_list[0] != os.path.normcase(dest_list[0]):\n    # Can't get here from there.\n    return dest\n\n# Find the location where the two paths start to differ.\ni = 0\nfor start_seg, dest_seg in zip(orig_list, dest_list):\n    if start_seg != os.path.normcase(dest_seg):\n        break\n    i += 1\n\n# Now i is the point where the two paths diverge.\n# Need a certain number of \"os.pardir\"s to work up\n# from the origin to the point of divergence.\nsegments = [os.pardir] * (len(orig_list) - i)\n# Need to add the diverging part of dest_list.\nsegments += dest_list[i:]\nif len(segments) == 0:\n    # If they happen to be identical, use os.curdir.\n    relpath = os.curdir\nelse:\n    relpath = os.path.join(*segments)\nreturn self.__class__(relpath)", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" D.walk() -> iterator over files and subdirs, recursively.\n\nThe iterator yields path objects naming each child item of\nthis directory and its descendants.  This requires that\nD.isdir().\n\nThis performs a depth-first traversal of the directory tree.\nEach directory is returned just before all its children.\n\nThe errors= keyword argument controls behavior when an\nerror occurs.  The default is 'strict', which causes an\nexception.  The other allowed values are 'warn', which\nreports the error via warnings.warn(), and 'ignore'.\n\"\"\"\n", "func_signal": "def walk(self, pattern=None, errors='strict'):\n", "code": "if errors not in ('strict', 'warn', 'ignore'):\n    raise ValueError(\"invalid errors parameter\")\n\ntry:\n    childList = self.listdir()\nexcept Exception:\n    if errors == 'ignore':\n        return\n    elif errors == 'warn':\n        warnings.warn(\n            \"Unable to list directory '%s': %s\"\n            % (self, sys.exc_info()[1]),\n            TreeWalkWarning)\n        return\n    else:\n        raise\n\nfor child in childList:\n    if pattern is None or child.fnmatch(pattern):\n        yield child\n    try:\n        isdir = child.isdir()\n    except Exception:\n        if errors == 'ignore':\n            isdir = False\n        elif errors == 'warn':\n            warnings.warn(\n                \"Unable to access '%s': %s\"\n                % (child, sys.exc_info()[1]),\n                TreeWalkWarning)\n            isdir = False\n        else:\n            raise\n\n    if isdir:\n        for item in child.walk(pattern, errors):\n            yield item", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" Open this file, read all bytes, return them as a string. \"\"\"\n", "func_signal": "def bytes(self):\n", "code": "f = self.open('rb')\ntry:\n    return f.read()\nfinally:\n    f.close()", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" p.splitpath() -> Return (p.parent, p.name). \"\"\"\n", "func_signal": "def splitpath(self):\n", "code": "parent, child = os.path.split(self)\nreturn self.__class__(parent), child", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "# 1st time, to avoid syncing with others, wait random time\n", "func_signal": "def update(self):\n", "code": "time = random.random() * self.agent.update_time\nyield hold, self, time\n\nwhile 1:\n    state = ResourceState(self.agent, self.agent.resource.free)\n    msg = Update(state)\n    msg.send_msg(self.agent.node, self.agent.broker.node)\n    yield hold, self, self.agent.update_time", "path": "mds\\processes.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "# sending confirmation message\n", "func_signal": "def confirm_and_start_job(self, job, other, value):\n", "code": "confirm = Confirm(self, other, value)\nconfirm.send_msg(self.node, other.node)\n\n# start procsses to listen for cancellations\nconfirm_process = ConfirmProcess(self, value)\nactivate(confirm_process, confirm_process.confirm())\nself.confirm_processes[job.id] = confirm_process\n\n# start the job\nself.resource.start(job, confirm_process)", "path": "agents.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" Return a list of path objects that match the pattern.\n\npattern - a path relative to this directory, with wildcards.\n\nFor example, path('/users').glob('*/bin/*') returns a list\nof all the files users have in their bin directories.\n\"\"\"\n", "func_signal": "def glob(self, pattern):\n", "code": "cls = self.__class__\nreturn [cls(s) for s in glob.glob(_base(self / pattern))]", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "# 1st time, to avoid syncing with others, wait random time\n", "func_signal": "def update(self):\n", "code": "time = random.random() * self.broker.sync_time\nyield hold, self, time\n\nwhile 1:\n    self.broker.send_sync()\n    yield hold, self, self.broker.sync_time", "path": "mds\\processes.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" p.splitdrive() -> Return (p.drive, <the rest of p>).\n\nSplit the drive specifier from this path.  If there is\nno drive specifier, p.drive is empty, so the return value\nis simply (path(''), p).  This is always the case on Unix.\n\"\"\"\n", "func_signal": "def splitdrive(self):\n", "code": "drive, rel = os.path.splitdrive(self)\nreturn self.__class__(drive), rel", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" D.walkdirs() -> iterator over subdirs, recursively.\n\nWith the optional 'pattern' argument, this yields only\ndirectories whose names match the given pattern.  For\nexample, mydir.walkdirs('*test') yields only directories\nwith names ending in 'test'.\n\nThe errors= keyword argument controls behavior when an\nerror occurs.  The default is 'strict', which causes an\nexception.  The other allowed values are 'warn', which\nreports the error via warnings.warn(), and 'ignore'.\n\"\"\"\n", "func_signal": "def walkdirs(self, pattern=None, errors='strict'):\n", "code": "if errors not in ('strict', 'warn', 'ignore'):\n    raise ValueError(\"invalid errors parameter\")\n\ntry:\n    dirs = self.dirs()\nexcept Exception:\n    if errors == 'ignore':\n        return\n    elif errors == 'warn':\n        warnings.warn(\n            \"Unable to list directory '%s': %s\"\n            % (self, sys.exc_info()[1]),\n            TreeWalkWarning)\n        return\n    else:\n        raise\n\nfor child in dirs:\n    if pattern is None or child.fnmatch(pattern):\n        yield child\n    for subsubdir in child.walkdirs(pattern, errors):\n        yield subsubdir", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" D.walkfiles() -> iterator over files in D, recursively.\n\nThe optional argument, pattern, limits the results to files\nwith names that match the pattern.  For example,\nmydir.walkfiles('*.tmp') yields only files with the .tmp\nextension.\n\"\"\"\n", "func_signal": "def walkfiles(self, pattern=None, errors='strict'):\n", "code": "if errors not in ('strict', 'warn', 'ignore'):\n    raise ValueError(\"invalid errors parameter\")\n\ntry:\n    childList = self.listdir()\nexcept Exception:\n    if errors == 'ignore':\n        return\n    elif errors == 'warn':\n        warnings.warn(\n            \"Unable to list directory '%s': %s\"\n            % (self, sys.exc_info()[1]),\n            TreeWalkWarning)\n        return\n    else:\n        raise\n\nfor child in childList:\n    try:\n        isfile = child.isfile()\n        isdir = not isfile and child.isdir()\n    except:\n        if errors == 'ignore':\n            continue\n        elif errors == 'warn':\n            warnings.warn(\n                \"Unable to access '%s': %s\"\n                % (self, sys.exc_info()[1]),\n                TreeWalkWarning)\n            continue\n        else:\n            raise\n\n    if isfile:\n        if pattern is None or child.fnmatch(pattern):\n            yield child\n    elif isdir:\n        for f in child.walkfiles(pattern, errors):\n            yield f", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "# only first time out triggers the bidding\n", "func_signal": "def quote_timedout(self):\n", "code": "if not self.has_timedout:\n    self.trace and self.trace(\"buyer timed out\")\n    record.buyer_timeouts.observe(now() - self.start_time)\n    self.has_timedout = True\n    self.accept_best_quote()", "path": "sealedbid\\buyer.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "# initial advert and quote\n# TODO: add a initial buyer quoted price?\n", "func_signal": "def advertise(self):\n", "code": "quote = Bid(self, None, self.job, None)\nadvert = Advert(quote)\n\n# send to self\nself.trace and self.trace(\"buyer sending to self\")\nadvert.send_msg(self.node, self.node, ttl=0)\n\n# send to others\nself.trace and self.trace(\"buyer shouting to %d nodes, ttl %s\" \n        % (self.node.degree, self.ttl))\nself.node.shout_msg(advert, ttl=self.ttl)\nreturn quote", "path": "sealedbid\\buyer.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\"Updates the price using the Widrow-Hoff learning rule\"\"\"\n", "func_signal": "def update(self, target):\n", "code": "change = (self.coeff * (target - self.price) +\n          self.momentum * self.last_change)\nself.last_change = change\n\nlimiter = self.buyer and min or max\nself.price = int(limiter(self.price + change, self.limit))", "path": "rationales.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" D.listdir() -> List of items in this directory.\n\nUse D.files() or D.dirs() instead if you want a listing\nof just files or just subdirectories.\n\nThe elements of the list are path objects.\n\nWith the optional 'pattern' argument, this only lists\nitems whose names match the given pattern.\n\"\"\"\n", "func_signal": "def listdir(self, pattern=None):\n", "code": "names = os.listdir(self)\nif pattern is not None:\n    names = fnmatch.filter(names, pattern)\nreturn [self / child for child in names]", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" p.splitext() -> Return (p.stripext(), p.ext).\n\nSplit the filename extension from this path and return\nthe two parts.  Either part may be empty.\n\nThe extension is everything from '.' to the end of the\nlast path segment.  This has the property that if\n(a, b) == p.splitext(), then a + b == p.\n\"\"\"\n", "func_signal": "def splitext(self):\n", "code": "filename, ext = os.path.splitext(self)\nreturn self.__class__(filename), ext", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" Return the path to which this symbolic link points.\n\nThe result is always an absolute path.\n\"\"\"\n", "func_signal": "def readlinkabs(self):\n", "code": "p = self.readlink()\nif p.isabs():\n    return p\nelse:\n    return (self.parent / p).abspath()", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" Calculate the md5 hash for this file.\n\nThis reads through the entire file.\n\"\"\"\n", "func_signal": "def read_md5(self):\n", "code": "f = self.open('rb')\ntry:\n    m = md5.new()\n    while True:\n        d = f.read(8192)\n        if not d:\n            break\n        m.update(d)\nfinally:\n    f.close()\nreturn m.digest()", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" Return this path as a relative path,\nbased from the current working directory.\n\"\"\"\n", "func_signal": "def relpath(self):\n", "code": "cwd = self.__class__(os.getcwd())\nreturn cwd.relpathto(self)", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
{"docstring": "\"\"\" Open this file and write the given bytes to it.\n\nDefault behavior is to overwrite any existing file.\nCall p.write_bytes(bytes, append=True) to append instead.\n\"\"\"\n", "func_signal": "def write_bytes(self, bytes, append=False):\n", "code": "if append:\n    mode = 'ab'\nelse:\n    mode = 'wb'\nf = self.open(mode)\ntry:\n    f.write(bytes)\nfinally:\n    f.close()", "path": "scripts\\path.py", "repo_name": "bloodearnest/deal", "stars": 6, "license": "None", "language": "python", "size": 236}
